<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta property="og:description" content="Network 분야에 관심이 많은 개발자로 Computer Engineering 관련 Posting을 주로 다룹니다."/><meta property="og:type" content="blog"/><meta property="og:site_name" content="JustLog"/><meta property="og:image" content="https://euidong.github.io/logo192.png"/><title>[ML] 3. Logistic Regression</title><meta property="og:title" content="[ML] 3. Logistic Regression"/><meta name="description" content="이전까지 우리는 input data가 들어왔을 때, continuos한 output을 얻는 것을 목표로 했다. 하지만 현실에서는 대게 정확한 수치보다는 특정 분류로 나누는 것이 효과적인 경우가 많다. 예를 들어, spam 필터링, object detection, 등 등. 따라서, 해당 포스팅에서는 classification을 위해서 사용할 수 있는 logistic regression에 대해서 살펴볼 것이다."/><meta property="og:description" content="이전까지 우리는 input data가 들어왔을 때, continuos한 output을 얻는 것을 목표로 했다. 하지만 현실에서는 대게 정확한 수치보다는 특정 분류로 나누는 것이 효과적인 경우가 많다. 예를 들어, spam 필터링, object detection, 등 등. 따라서, 해당 포스팅에서는 classification을 위해서 사용할 수 있는 logistic regression에 대해서 살펴볼 것이다."/><meta property="og:url" content="https://euidong.github.io/posts/ml-logistic-regression"/><link rel="canonical" href="https://euidong.github.io/posts/ml-logistic-regression"/><meta property="og:image" content="https://euidong.github.io/images/ml-thumbnail.jpg"/><meta name="next-head-count" content="13"/><link rel="icon" href="https://euidong.github.io/favicon.png"/><link rel="apple-touch-icon" href="https://euidong.github.io/logo192.png"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7452732177557701" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/css/d4ec5c8b3df09443.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d4ec5c8b3df09443.css" data-n-g=""/><link rel="preload" href="/_next/static/css/4e506edd7924fba2.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4e506edd7924fba2.css" data-n-p=""/><link rel="preload" href="/_next/static/css/936c1a0848bb34d6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/936c1a0848bb34d6.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-81da43a8dcd978d9.js" defer=""></script><script src="/_next/static/chunks/main-7b6c38cbad60dfcf.js" defer=""></script><script src="/_next/static/chunks/pages/_app-16ba73d9a19266be.js" defer=""></script><script src="/_next/static/chunks/78e521c3-cbc72355a4ceeb71.js" defer=""></script><script src="/_next/static/chunks/175675d1-f160d4a5df49f5d3.js" defer=""></script><script src="/_next/static/chunks/675-ae8e8a351ce30ae2.js" defer=""></script><script src="/_next/static/chunks/602-aae13ef79a8283ee.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-3a18390189e5c715.js" defer=""></script><script src="/_next/static/eGkZYbzPXM35ApK3V1pre/_buildManifest.js" defer=""></script><script src="/_next/static/eGkZYbzPXM35ApK3V1pre/_ssgManifest.js" defer=""></script><script src="/_next/static/eGkZYbzPXM35ApK3V1pre/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_wrapper__dKJSz root"><header class="Layout_header__XosLl" style="position:sticky"><div><button tabindex="1" class="SideBarToggler_search_bar_toggler__CEuUg"><svg stroke="currentColor" fill="none" stroke-width="0" viewBox="0 0 24 24" height="35px" width="35px" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></button><nav class="SideBar_side_bar__wrapper--close__8Nwnr"><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/">Home</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/tags">Tags</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Algorithm">Algorithm<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Computer%20Architecture">Computer Architecture<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->7<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Tech">Tech<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->17<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Web">Web<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Network">Network<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/AI">AI<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->8<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Paper">Paper<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->1<!-- -->)<!-- --></span></a></nav></div><a class="Logo_logo___yD0t" tabindex="1" href="/"></a><div><button class="SearchBarToggler_search_bar_toggler__3dHbA" tabindex="2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg></button></div></header><section><div class="Post_post__wrapper__Qq8vV"><h1 class="Post_post__title__CYNLY">[ML] 3. Logistic Regression</h1><p class="Post_post__date__Sx37s">2022년 10월 18일 09시 58분</p><ul class="Post_post__tags__SU5Ql"><li class="Post_post__tags__element__SYmey"># ML</li><li class="Post_post__tags__element__SYmey"># LogisticRegression</li><li class="Post_post__tags__element__SYmey"># Classification</li><li class="Post_post__tags__element__SYmey"># SigmoidFunction</li><li class="Post_post__tags__element__SYmey"># SoftmaxFunction</li><li class="Post_post__tags__element__SYmey"># NewtonMethod</li></ul><article class="markdown-body MarkDown_markdown-body__ABwUt"><h2>Intro<!-- --></h2>
<!-- --><p>이전까지 우리는 input data가 들어왔을 때, continuos한 output을 얻는 것을 목표로 했다. 하지만 현실에서는 대게 정확한 수치보다는 특정 분류로 나누는 것이 효과적인 경우가 많다. 예를 들어, spam 필터링, object detection, 등 등. 따라서, 해당 포스팅에서는 classification을 위해서 사용할 수 있는 logistic regression에 대해서 살펴볼 것이다.<!-- --></p>
<!-- --><h2>Classification<!-- --></h2>
<!-- --><p><strong>Classification<!-- --></strong>이란 결국 특정 input이 들어왔을 때, 이를 하나의 Class라는 output을 내보내는 것이다. 즉, output은 연속적이지 않고, descret하다. 대게 Classification에서는 Class의 갯수를 K라고 표기하고, <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C<!-- --></mi><mi>k<!-- --></mi></msub></mrow><annotation encoding="application/x-tex">C_k<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>는 k 번째 Class라는 의미로 사용되어진다.<!-- --></p>
<!-- --><p>그렇다면, 어떻게 Class를 나눌 수 있는 것일까? 매우 단순하게도 이는 <!-- --><strong>Decision Boundary<!-- --></strong>라는 선을 그어서 해결 할 수 있다.<!-- --></p>
<!-- --><p><img src="/images/decision-boundary.jpg" alt="decision-boundary"/></p>
<!-- --><p>위의 예시처럼 우리는 선을 하나 그어서 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="#df0030"><mtext>x<!-- --></mtext></mstyle></mrow><annotation encoding="application/x-tex">\red{\text{x}}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord text" style="color:#df0030"><span class="mord" style="color:#df0030">x<!-- --></span></span></span></span></span></span>와 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="#6495ed"><mtext>o<!-- --></mtext></mstyle></mrow><annotation encoding="application/x-tex">\blue{\text{o}}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord text" style="color:#6495ed"><span class="mord" style="color:#6495ed">o<!-- --></span></span></span></span></span></span>를 구분할 수 있다. 이를 통해서 우리는 Class 1에 해당할 것이라고 예측하는 구간 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R<!-- --></mi><mn>1<!-- --></mn></msub></mrow><annotation encoding="application/x-tex">R_1<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>이 만들어지고, Class 2라고 예측하는 구간 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R<!-- --></mi><mn>2<!-- --></mn></msub></mrow><annotation encoding="application/x-tex">R_2<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>를 구성할 수 있다.<!-- --></p>
<!-- --><p>즉, classification을 수행하기 위해서 해야할 일은 기존의 Regression 과정과 마찬가지로 선을 찾는 것이다.<!-- --></p>
<!-- --><p>결국 찾고자 하는 것이 선이라면, 이것을 Linear Regression으로 해결할 수 있을 것이다. 따라서, 우리는 다음과 같은 식으로 간단히 Linear Regression을 바꿔서 생각할 수 있다.<!-- --></p>
<!-- --><ul>
<!-- --><li>예측값(<!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span></span></span></span></span>, <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow><annotation encoding="application/x-tex">h(\bold{x})<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">h<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span></span></span></span></span>)<!-- --><br/>
<!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo>=<!-- --></mo><mtext>sign<!-- --></mtext><mo stretchy="false">(<!-- --></mo><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo>=<!-- --></mo><mrow><mo fence="true">{<!-- --></mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>+<!-- --></mo><mn>1<!-- --></mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><mi mathvariant="bold">x<!-- --></mi><mo>≥<!-- --></mo><mn>0<!-- --></mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−<!-- --></mo><mn>1<!-- --></mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise<!-- --></mtext></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">h(\bold{x}) = \text{sign}(\bold{w}^{\top}\bold{x}) = \begin{cases} +1 &amp; \bold{w}^{\top}\bold{x} \geq 0 \\ -1 &amp; \text{otherwise}\end{cases}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">h<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">sign<!-- --></span></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size4">{<!-- --></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em"><span style="top:-3.69em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord">+<!-- --></span><span class="mord">1<!-- --></span></span></span><span style="top:-2.25em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord">−<!-- --></span><span class="mord">1<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.19em"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em"><span style="top:-3.69em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathbf">x<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≥<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord">0<!-- --></span></span></span><span style="top:-2.25em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord text"><span class="mord">otherwise<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.19em"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></li>
<!-- --><li>Least Squared Error(LS, MLE)<!-- --><br/>
실제로 parameter를 구할 때에는 sign을 취하지 않는데, sign을 취하게 되면 모두 LS는 결국 오답의 갯수 정도로 취급된다. 즉, 얼마나 예측이 잘못되었는지를 반영할 수 없다는 것이다. 따라서, 이는 기존 Linear Regression의 LS를 구하는 방법과 동일하게 수행한다.<!-- --><br/>
<!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">arg min<!-- --></mi><mo>⁡<!-- --></mo></mrow><mi>w<!-- --></mi></msub><mfrac><mn>1<!-- --></mn><mn>2<!-- --></mn></mfrac><msubsup><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></msubsup><mrow><mo stretchy="false">(<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><mo>−<!-- --></mo><mo stretchy="false">(<!-- --></mo><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo><msup><mo stretchy="false">)<!-- --></mo><mn>2<!-- --></mn></msup></mrow></mrow><annotation encoding="application/x-tex">\argmin_{w} {1\over2}\sum_{n=1}^{N}{(y_n - (\bold{w}^{\top}\bold{x}))^2}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3262em;vertical-align:-0.345em"></span><span class="mop"><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">min<!-- --></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.0573em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2<!-- --></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span><span class="mclose"><span class="mclose">)<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2<!-- --></span></span></span></span></span></span></span></span></span></span></span></span></span></li>
<!-- --></ul>
<!-- --><p>이렇게 Linear Regression을 적용하면 문제가 없을 거 같다. 하지만, 실제로는 문제가 있다. 바로, 데이터가 불균형할 때이다. 만약 데이터가 decision boundary를 기준으로 대칭(symmetric)인 형태로 존재한다면, 문제가 없다. 하지만, 비대칭(asymmetric)인 경우 제대로 동작하지 않는다. 왜냐하면, linear regression은 최적에서 데이터의 평균을 반영하는데 불균형한 경우 데이터의 평균이 Decision Boundary가 되는 것은 문제가 있다.<!-- --></p>
<!-- --><p><img src="/images/linear-in-classification.jpg" alt="linear-in-classification"/></p>
<!-- --><h2>Logistic Regression<!-- --></h2>
<!-- --><p>위에서 제시한 문제를 해결하기 위해서 Classification에서는 Linear Regression이 아닌 Logistic Regression을 활용한다. 이를 이해하기 위해서 기반이 될 요소들을 먼저 살펴보자.<!-- --></p>
<!-- --><blockquote>
<!-- --><p><strong>Discriminant Function<!-- --></strong></p>
<!-- --></blockquote>
<!-- --><p>판별함수(Discriminant Function, Score Function) 등으로 불리는 해당 함수는 특정 data가 특정 class에 속할 가능성(likelihood, probability, score)을 나타내는 함수이다. 즉, input으로 data를 받고, output으로 class에 속할 확률을 내보낸다.<!-- --></p>
<!-- --><p>이를 통해서 우리는 다음과 같은 과정을 할 수 있다.<!-- --></p>
<!-- --><p>만약, <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f<!-- --></mi><mi>k<!-- --></mi></msub><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo>&gt;<!-- --></mo><msub><mi>f<!-- --></mi><mi>j<!-- --></mi></msub><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow><annotation encoding="application/x-tex">f_k(\bold{x}) \gt f_j(\bold{x})<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span></span></span></span></span>이라면, <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x<!-- --></mi></mrow><annotation encoding="application/x-tex">\bold{x}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em"></span><span class="mord mathbf">x<!-- --></span></span></span></span></span>의 class는 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C<!-- --></mi><mi>k<!-- --></mi></msub></mrow><annotation encoding="application/x-tex">C_k<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>이다.<!-- --></p>
<!-- --><p>따라서, 우리는 다음과 같은 식으로 여러 개의 Class가 있는 공간에서 data를 분류할 수 있다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>h<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo>=<!-- --></mo><munder><mrow><mi mathvariant="normal">arg max<!-- --></mi><mo>⁡<!-- --></mo></mrow><mi>k<!-- --></mi></munder><msub><mi>f<!-- --></mi><mi>k<!-- --></mi></msub><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow><annotation encoding="application/x-tex">h(\bold{x}) = \argmax_{k}f_{k}(\bold{x})<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">h<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.6965em;vertical-align:-0.9465em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.1535em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">max<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.9465em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span></span></span></span></span></div>
<!-- --><p>그렇다면, Discriminant Function으로 어떤 값을 쓰면 좋을까? 이에 대한 해결책을 Bayes Decision Rule에서 제시한다.<!-- --></p>
<!-- --><blockquote>
<!-- --><p><strong>Bayes Decision Rule<!-- --></strong></p>
<!-- --></blockquote>
<!-- --><p>만약 우리가 특정 data가 특정 Class에 속할 확률을 구한다고 하자. 우리는 먼저 Likelihood를 생각할 수 있다. <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi>x<!-- --></mi><mi mathvariant="normal">∣<!-- --></mi><mi>C<!-- --></mi><mo>=<!-- --></mo><mi>k<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo separator="true">,<!-- --></mo><mi>P<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi>x<!-- --></mi><mi mathvariant="normal">∣<!-- --></mi><mi>C<!-- --></mi><mo>=<!-- --></mo><mi>j<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow><annotation encoding="application/x-tex">P(x|C = k), P(x|C = j)<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathnormal">x<!-- --></span><span class="mord">∣<!-- --></span><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k<!-- --></span><span class="mclose">)<!-- --></span><span class="mpunct">,<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathnormal">x<!-- --></span><span class="mord">∣<!-- --></span><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j<!-- --></span><span class="mclose">)<!-- --></span></span></span></span></span>를 구하여 각 Class에 속할 확률을 비교할 수 있을까?<!-- --><br/>
물론 비교는 가능하다 하지만, 반쪽짜리 비교라고 할 수 있다. 만약, class k에 속하는 데이터보다 class j에 속하는 데이터가 훨씬 많다고 하자. 그러면, 일반적으로 class j가 발생할 확률 자체가 높다. 하지만, likelihood는 이러한 경향을 반영하지 않는다. 간단한 예시를 들어보자.<!-- --></p>
<!-- --><pre><div class="MarkDown_codeblock__wrapper__S4FFz"><div class="MarkDown_codeblock__header__h3PfO"><span class="MarkDown_codeblock__header__circle__B4MWO"></span><span class="MarkDown_codeblock__header__circle__B4MWO"></span><span class="MarkDown_codeblock__header__circle__B4MWO"></span><span class="MarkDown_codeblock__header__button__aBRNB"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M7 6V3a1 1 0 0 1 1-1h12a1 1 0 0 1 1 1v14a1 1 0 0 1-1 1h-3v3c0 .552-.45 1-1.007 1H4.007A1.001 1.001 0 0 1 3 21l.003-14c0-.552.45-1 1.007-1H7zM5.003 8L5 20h10V8H5.003zM9 6h8v10h2V4H9v2z"></path></g></svg></span></div><div style="color:#f8f8f2;background:#282a36;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code class="language-plaintext" style="color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">1<!-- --></span><span> 🤔 어떤 동물의 털에 존재하는 색의 갯수가 주어졌을 때, 고양이일 확률과 호랑이일 확률이라고 하자.
<!-- --></span><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">2<!-- --></span>
<!-- --><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">3<!-- --></span>  그리고, input data는 털에 존재하는 색의 수라고 하자. (호랑이는 대게 3가지 색, 백호 = 2가지 색, 고양이는 매우 다양)
<!-- --><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">4<!-- --></span>  그렇다면, P(털의 색 = 3|C = 호랑이), P(털의 색 = 3|C = 고양이)를 비교했을 때, 우리는 당연히 전자가 크다고 생각할 것이다.
<!-- --><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">5<!-- --></span>  하지만, 여기서 우리가 고려하지 않은 것이 있다. 바로 전체 고양이와 호랑이의 비율이다. 
<!-- --><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">6<!-- --></span>  상대적으로 고양이가 호랑이보다 압도적으로 많다는 것을 고려했을 때, 고양이의 확률이 더 높을 수도 있다. 
<!-- --><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">7<!-- --></span>
<!-- --><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">8<!-- --></span>  즉, 어떤 동물의 털에 존재하는 색의 갯수가 주어졌을 때, 고양이일 확률은 
<!-- --><span class="linenumber react-syntax-highlighter-line-number" style="display:inline-block;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;color:#6272a4">9<!-- --></span>  P(C=고양이|털의 색=3) =  P(털의 색 = 3|C = 고양이)P(C=고양이)이다. (분모는 생략함.)<!-- --></code></div></div></pre>
<!-- --><p>즉, Bayes Rule에 기반하여 우리가 원하는 output은 Posterior라는 것을 명확히 알 수 있다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><msub><mi>C<!-- --></mi><mi>k<!-- --></mi></msub><mi mathvariant="normal">∣<!-- --></mi><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=<!-- --></mo><mfrac><mrow><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mi mathvariant="normal">∣<!-- --></mi><msub><mi>C<!-- --></mi><mi>k<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><msub><mi>C<!-- --></mi><mi>k<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow><mrow><munderover><mo>∑<!-- --></mo><mrow><mi>j<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>K<!-- --></mi></munderover><mrow><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mi mathvariant="normal">∣<!-- --></mi><msub><mi>C<!-- --></mi><mi>j<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><msub><mi>C<!-- --></mi><mi>j<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>∝<!-- --></mo><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mi mathvariant="normal">∣<!-- --></mi><msub><mi>C<!-- --></mi><mi>k<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><msub><mi>C<!-- --></mi><mi>k<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
p(C_{k}|\bold{x}) &amp;= {{p(\bold{x}| C_{k}) p(C_{k})}\over{\sum_{j=1}^{K}{p(\bold{x}|C_{j})p(C_{j})}}} \\
&amp;\propto p(\bold{x}| C_{k}) p(C_{k})
\end{align*}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.534em;vertical-align:-2.017em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.517em"><span style="top:-4.517em"><span class="pstrut" style="height:3.427em"></span><span class="mord"><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span></span></span><span style="top:-2.07em"><span class="pstrut" style="height:3.427em"></span><span class="mord"></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:2.017em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.517em"><span style="top:-4.517em"><span class="pstrut" style="height:3.427em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.1288em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mord">∣<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mord">∣<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.307em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span style="top:-2.07em"><span class="pstrut" style="height:3.427em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∝<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mord">∣<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:2.017em"><span></span></span></span></span></span></span></span></span></span></span></span></div>
<!-- --><p>위의 경우 Class간의 상대 비교에 사용하는 지표로 이를 사용하기 때문에, 분모(Normalization Factor, 확률의 총합이 1이 되도록 하는 역할)를 제외하여도 상관없기에 대게 복잡한 분모 계산을 제외하고 표현하는 것이 일반적이다.<!-- --></p>
<!-- --><p>또한, 앞선 예시에서 얻을 수 있는 insight는 편향된 데이터일수록 MLE를 사용할 수 없다는 것이다. 위에서 Linear Regression이 Classification에 부적함한 경우도 데이터의 편향이 있을 경우이다. 이 역시 Linear Regression이 결국은 MLE에 기반하기 때문인 것이다.<!-- --></p>
<!-- --><p>우리는 각 Class 자체의 확률(Prior)과 Likelihood를 이용할 수 있는 Discriminant Function을 구해야 한다는 것이다.<!-- --></p>
<!-- --><blockquote>
<!-- --><p><strong>Logistic Regression<!-- --></strong></p>
<!-- --></blockquote>
<!-- --><p>자 이제 드디어 Logistric Regression을 시작해보자. 우리는 Discriminant Function을 먼저 지정해야 한다. 여러 가지 방법이 있지만, 가장 대표적으로 사용되는 방법은 <!-- --><strong>Softmax<!-- --></strong>를 활용하는 것이다. <!-- --><strong>Softmax<!-- --></strong>를 활용하여 식을 나타내면 아래와 같다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><mo>=<!-- --></mo><mi>k<!-- --></mi><mi mathvariant="normal">∣<!-- --></mi><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo separator="true">,<!-- --></mo><mi mathvariant="bold">w<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo>=<!-- --></mo><mfrac><mrow><mi>exp<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><msubsup><mi mathvariant="bold">w<!-- --></mi><mi>k<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msubsup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow><mrow><munderover><mo>∑<!-- --></mo><mrow><mi>j<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>K<!-- --></mi></munderover><mrow><mi>exp<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><msubsup><mi mathvariant="bold">w<!-- --></mi><mi>j<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msubsup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(y_n = k | \bold{x}_n, \bold{w}) = {{\exp(\bold{w}_{k}^{\top}\bold{x}_n)}\over{\sum_{j=1}^{K}{\exp(\bold{w}_{j}^{\top}\bold{x}_n)}}} <!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k<!-- --></span><span class="mord">∣<!-- --></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.8332em;vertical-align:-1.307em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.1288em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mop">exp<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8309em"><span style="top:-2.4231em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j<!-- --></span></span></span></span><span style="top:-3.0448em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.413em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">exp<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4169em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.307em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></div>
<!-- --><p>만약, class가 2개인 Binary Classification인 경우에 <!-- --><strong>Softmax<!-- --></strong>는 다음과 같아진다. 특히 이를 <!-- --><strong>Sigmoid<!-- --></strong>(<!-- --><strong>Logit<!-- --></strong>)라고 정의한다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><mo>=<!-- --></mo><mi>k<!-- --></mi><mi mathvariant="normal">∣<!-- --></mi><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo separator="true">,<!-- --></mo><mi mathvariant="bold">w<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo>=<!-- --></mo><mfrac><mn>1<!-- --></mn><mrow><mn>1<!-- --></mn><mo>+<!-- --></mo><mi>exp<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mo>−<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(y_n = k | \bold{x}_n, \bold{w}) = {1\over{1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})}}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k<!-- --></span><span class="mord">∣<!-- --></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.2574em;vertical-align:-0.936em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop">exp<!-- --></span><span class="mopen">(<!-- --></span><span class="mord">−<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7751em"><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></div>
<!-- --><p>이를 유도하는 과정은 생략하지만, 여타 다른 블로그를 더 참고하면 좋다.<!-- --></p>
<!-- --><p>이를 Linear Regression과 비교해서 살펴보자.<!-- --></p>
<!-- --><p><img src="/images/logistic-vs-linear.jpg" alt="logistic-vs-linear"/></p>
<!-- --><p>Linear Regression은 특정값을 향해 나아가고 있다. 해당 방식을 보면 x가 대상의 특성을 강하게 가지고 있다면, 명확하게 구분할 수 있는데, 이는 <!-- --><strong>sigmoid<!-- --></strong>(<!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ<!-- --></mi></mrow><annotation encoding="application/x-tex">\sigma<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ<!-- --></span></span></span></span></span>) 함수가 [0, 1] 범위 내에서 정의되기 때문에 Regression 과정에서 극단 데이터(outlier)가 가지는 영향력이 Linear Regression보다 극단적으로 적다는 것을 알 수 있다.<!-- --></p>
<!-- --><p>자 이것이 가지는 의미를 이전에 살펴본 <!-- --><strong>Bayes Decision Rule<!-- --></strong>에 기반해서 생각해보자. <!-- --><strong>sigmoid<!-- --></strong>(<!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ<!-- --></mi></mrow><annotation encoding="application/x-tex">\sigma<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ<!-- --></span></span></span></span></span>)는 결국 극단적인 데이터이든, 애매한 데이터이든 거의 비슷한 값으로 변환한다. 그렇다는 것은 기존에는 평균을 구하는데에 input(x)의 값이 큰 영향을 미쳤다면, <!-- --><strong>sigmoid<!-- --></strong>(<!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ<!-- --></mi></mrow><annotation encoding="application/x-tex">\sigma<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ<!-- --></span></span></span></span></span>)에서는 특정 class에 속하는 x의 갯수가 많은 영향을 주는 것을 알 수 있다. 이를 통해서 <!-- --><strong>sigmoid<!-- --></strong>(<!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ<!-- --></mi></mrow><annotation encoding="application/x-tex">\sigma<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ<!-- --></span></span></span></span></span>)가 완벽하지는 않지만, <!-- --><strong>Bayes Decision Rule<!-- --></strong>을 반영했다는 것을 알 수 있다.<!-- --></p>
<!-- --><p>마지막으로, MLE를 통해서 Logistic Regression의 parameter를 추정해보자. (MAP는 기존에 살펴본 Linear Regression과 동일하게 regularizer를 더해주는 방식이기 때문에 생략한다.)<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><munder><mrow><mi mathvariant="normal">arg max<!-- --></mi><mo>⁡<!-- --></mo></mrow><mi>w<!-- --></mi></munder><mi>log<!-- --></mi><mo>⁡<!-- --></mo><mrow><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="script">D<!-- --></mi><mi mathvariant="normal">∣<!-- --></mi><mi mathvariant="bold">w<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=<!-- --></mo><munder><mrow><mi mathvariant="normal">arg max<!-- --></mi><mo>⁡<!-- --></mo></mrow><mi>w<!-- --></mi></munder><munderover><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></munderover><mrow><mi>log<!-- --></mi><mo>⁡<!-- --></mo><mi>p<!-- --></mi><mo stretchy="false">(<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><mi mathvariant="normal">∣<!-- --></mi><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo separator="true">,<!-- --></mo><mi mathvariant="bold">w<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=<!-- --></mo><munder><mrow><mi mathvariant="normal">arg max<!-- --></mi><mo>⁡<!-- --></mo></mrow><mi>w<!-- --></mi></munder><munderover><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></munderover><mrow><mi>log<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mfrac><mn>1<!-- --></mn><mrow><mn>1<!-- --></mn><mo>+<!-- --></mo><mi>exp<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mo>−<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow></mfrac><mo stretchy="false">)<!-- --></mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=<!-- --></mo><munder><mrow><mi mathvariant="normal">arg max<!-- --></mi><mo>⁡<!-- --></mo></mrow><mi>w<!-- --></mi></munder><munderover><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></munderover><mrow><mo>−<!-- --></mo><mi>log<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mn>1<!-- --></mn><mo>+<!-- --></mo><mi>exp<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mo>−<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><mo stretchy="false">)<!-- --></mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=<!-- --></mo><munder><mrow><mi mathvariant="normal">arg min<!-- --></mi><mo>⁡<!-- --></mo></mrow><mi>w<!-- --></mi></munder><munderover><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></munderover><mrow><mi>log<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mn>1<!-- --></mn><mo>+<!-- --></mo><mi>exp<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mo>−<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><mo stretchy="false">)<!-- --></mo></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\argmax_{w}\log{p(\mathcal{D}|\bold{w})} &amp;= \argmax_{w}\sum_{n=1}^{N}{\log p(y_{n}|\bold{x}_{n}, \bold{w})} \\
&amp;= \argmax_{w}\sum_{n=1}^{N}{\log ({1\over{1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})}}) } \\
&amp;= \argmax_{w}\sum_{n=1}^{N}{-\log (1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})) } \\
&amp;= \argmin_{w}\sum_{n=1}^{N}{\log (1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})) } \\
\end{align*}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:13.5818em;vertical-align:-6.5409em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:7.0409em"><span style="top:-9.0409em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.2056em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w<!-- --></span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">max<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.8944em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<!-- --><span style="margin-right:0.01389em">g<!-- --></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathcal" style="margin-right:0.02778em">D<!-- --></span><span class="mord">∣<!-- --></span><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="mclose">)<!-- --></span></span></span></span><span style="top:-5.6454em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"></span></span><span style="top:-2.25em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"></span></span><span style="top:1.1454em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:6.5409em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:7.0409em"><span style="top:-9.0409em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.2056em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w<!-- --></span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">max<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.8944em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑<!-- --></span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mop">lo<!-- --><span style="margin-right:0.01389em">g<!-- --></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">p<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣<!-- --></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="mclose">)<!-- --></span></span></span></span><span style="top:-5.6454em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.2056em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w<!-- --></span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">max<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.8944em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑<!-- --></span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mop">lo<!-- --><span style="margin-right:0.01389em">g<!-- --></span></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop">exp<!-- --></span><span class="mopen">(<!-- --></span><span class="mord">−<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7751em"><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mclose">)<!-- --></span></span></span></span><span style="top:-2.25em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.2056em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w<!-- --></span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">max<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.8944em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑<!-- --></span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord">−<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<!-- --><span style="margin-right:0.01389em">g<!-- --></span></span><span class="mopen">(<!-- --></span><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop">exp<!-- --></span><span class="mopen">(<!-- --></span><span class="mord">−<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">))<!-- --></span></span></span></span><span style="top:1.1454em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6679em"><span style="top:-2.2056em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w<!-- --></span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">min<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.8944em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑<!-- --></span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mop">lo<!-- --><span style="margin-right:0.01389em">g<!-- --></span></span><span class="mopen">(<!-- --></span><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop">exp<!-- --></span><span class="mopen">(<!-- --></span><span class="mord">−<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">))<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:6.5409em"><span></span></span></span></span></span></span></span></span></span></span></span></div>
<!-- --><h2>Gradient Descent/Ascent<!-- --></h2>
<!-- --><p>위의 복잡한 식을 봤으면 알겠지만, 안타깝게도 일반식으로 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">w<!-- --></mi><mrow><mi>M<!-- --></mi><mi>L<!-- --></mi><mi>E<!-- --></mi></mrow></msub><mo separator="true">,<!-- --></mo><msub><mi mathvariant="bold">w<!-- --></mi><mrow><mi>M<!-- --></mi><mi>A<!-- --></mi><mi>P<!-- --></mi></mrow></msub></mrow><annotation encoding="application/x-tex">\bold{w}_{MLE}, \bold{w}_{MAP}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M<!-- --></span><span class="mord mathnormal mtight">L<!-- --></span><span class="mord mathnormal mtight" style="margin-right:0.05764em">E<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M<!-- --></span><span class="mord mathnormal mtight">A<!-- --></span><span class="mord mathnormal mtight" style="margin-right:0.13889em">P<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 등을 구할 수는 없다. 따라서, 우리가 믿을 것은 Gradient를 이용한 방식이다.<!-- --></p>
<!-- --><blockquote>
<!-- --><p><strong>Gradient Descent<!-- --></strong></p>
<!-- --></blockquote>
<!-- --><p>먼저, 위에서 봤겠지만, Loss는 다음과 같다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L<!-- --></mi><mo>=<!-- --></mo><munderover><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></munderover><mrow><mi>log<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mn>1<!-- --></mn><mo>+<!-- --></mo><mi>exp<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mo>−<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><mo stretchy="false">)<!-- --></mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L} = \sum_{n=1}^{N}{\log(1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n}))}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal">L<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑<!-- --></span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mop">lo<!-- --><span style="margin-right:0.01389em">g<!-- --></span></span><span class="mopen">(<!-- --></span><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop">exp<!-- --></span><span class="mopen">(<!-- --></span><span class="mord">−<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">))<!-- --></span></span></span></span></span></span></div>
<!-- --><p>이제 이를 미분해서 Gradient를 구하면 다음과 같다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="normal">∇<!-- --></mi><mi mathvariant="bold">w<!-- --></mi></msub><mi mathvariant="script">L<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">w<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo>=<!-- --></mo><munderover><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></munderover><mrow><mfrac><mrow><mo>−<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><mi>exp<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mo>−<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow><mrow><mn>1<!-- --></mn><mo>+<!-- --></mo><mi>exp<!-- --></mi><mo>⁡<!-- --></mo><mo stretchy="false">(<!-- --></mo><mo>−<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow></mfrac><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\bold{w}}\mathcal{L}(\bold{w}) = \sum_{n=1}^{N}{{{-y_{n}\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})}\over{1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})}}\bold{x}_{n}}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord">∇<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1611em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight" style="margin-right:0.01597em">w<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathcal">L<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑<!-- --></span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop">exp<!-- --></span><span class="mopen">(<!-- --></span><span class="mord">−<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7751em"><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">−<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">exp<!-- --></span><span class="mopen">(<!-- --></span><span class="mord">−<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></span></div>
<!-- --><p>따라서, Gradient Descent 방식은 다음과 같이 진행된다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">w<!-- --></mi><mrow><mi>t<!-- --></mi><mo>+<!-- --></mo><mn>1<!-- --></mn></mrow></msub><mo>=<!-- --></mo><msub><mi mathvariant="bold">w<!-- --></mi><mi>t<!-- --></mi></msub><mo>−<!-- --></mo><mi>α<!-- --></mi><msub><mi mathvariant="normal">∇<!-- --></mi><mi mathvariant="bold">w<!-- --></mi></msub><mi mathvariant="script">L<!-- --></mi><mo stretchy="false">(<!-- --></mo><msub><mi mathvariant="bold">w<!-- --></mi><mi>t<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow><annotation encoding="application/x-tex">\bold{w}_{t+1} = \bold{w}_{t} - \alpha\nabla_{\bold{w}}\mathcal{L}(\bold{w}_{t})<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6528em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t<!-- --></span><span class="mbin mtight">+<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α<!-- --></span><span class="mord"><span class="mord">∇<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1611em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight" style="margin-right:0.01597em">w<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathcal">L<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span></span></span></div>
<!-- --><blockquote>
<!-- --><p><strong>Gradient Ascent<!-- --></strong></p>
<!-- --></blockquote>
<!-- --><p>위의 방식이 가장 일반적이지만, 우리가 sigmoid의 class값으로 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y<!-- --></mi><mo>∈<!-- --></mo><mo stretchy="false">{<!-- --></mo><mo>−<!-- --></mo><mn>1<!-- --></mn><mo separator="true">,<!-- --></mo><mn>1<!-- --></mn><mo stretchy="false">}<!-- --></mo></mrow><annotation encoding="application/x-tex">y \in \{-1, 1\}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{<!-- --></span><span class="mord">−<!-- --></span><span class="mord">1<!-- --></span><span class="mpunct">,<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1<!-- --></span><span class="mclose">}<!-- --></span></span></span></span></span> 대신 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y<!-- --></mi><mo>∈<!-- --></mo><mo stretchy="false">{<!-- --></mo><mn>0<!-- --></mn><mo separator="true">,<!-- --></mo><mn>1<!-- --></mn><mo stretchy="false">}<!-- --></mo></mrow><annotation encoding="application/x-tex">y \in \{0, 1\}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{<!-- --></span><span class="mord">0<!-- --></span><span class="mpunct">,<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1<!-- --></span><span class="mclose">}<!-- --></span></span></span></span></span>을 사용했을 경우 다른 식으로도 접근이 가능하다.<!-- --></p>
<!-- --><p>이 경우에는 Loss라기 보기 어렵지만, 다른 형태의 optimization 형태가 만들어진다. (여기서 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ<!-- --></mi></mrow><annotation encoding="application/x-tex">\sigma<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ<!-- --></span></span></span></span></span>는 sigmoid 함수를 의미한다.)<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munder><mrow><mi mathvariant="normal">arg max<!-- --></mi><mo>⁡<!-- --></mo></mrow><mi mathvariant="bold">w<!-- --></mi></munder><munderover><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></munderover><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><mi>log<!-- --></mi><mo>⁡<!-- --></mo><mrow><mi>σ<!-- --></mi><mo stretchy="false">(<!-- --></mo><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><mo>+<!-- --></mo><mo stretchy="false">(<!-- --></mo><mn>1<!-- --></mn><mo>−<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><mi>log<!-- --></mi><mo>⁡<!-- --></mo><mrow><mo stretchy="false">(<!-- --></mo><mn>1<!-- --></mn><mo>−<!-- --></mo><mi>σ<!-- --></mi><mo stretchy="false">(<!-- --></mo><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><mo stretchy="false">)<!-- --></mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\argmax_{\bold{w}} \sum_{n=1}^{N}y_{n}\log{\sigma(\bold{w}^{\top}\bold{x}_{n}) + (1-y_{n})\log{(1-\sigma(\bold{w}^{\top}\bold{x}_{n}))} }<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.2056em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight" style="margin-right:0.01597em">w<!-- --></span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">max<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.8944em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑<!-- --></span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<!-- --><span style="margin-right:0.01389em">g<!-- --></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mopen">(<!-- --></span><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<!-- --><span style="margin-right:0.01389em">g<!-- --></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mopen">(<!-- --></span><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">))<!-- --></span></span></span></span></span></span></span></div>
<!-- --><p>이를 똑같이 미분하여 사용하지만, 반대로 이 경우에는 maximization 이기 때문에 Gradient Ascent를 수행해야 한다.<!-- --></p>
<!-- --><p>우선 미분 결과 얻는 Gradient는 다음과 같다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="normal">∇<!-- --></mi><mi mathvariant="bold">w<!-- --></mi></msub><mi mathvariant="script">L<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">w<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo>=<!-- --></mo><munderover><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></munderover><mrow><mo stretchy="false">[<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><mo>−<!-- --></mo><mi>σ<!-- --></mi><mo stretchy="false">(<!-- --></mo><msup><mi mathvariant="bold">w<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><mo stretchy="false">]<!-- --></mo><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\bold{w}}\mathcal{L}(\bold{w}) = \sum_{n=1}^{N}{[y_{n} - \sigma(\bold{w}^{\top}\bold{x}_{n})]\bold{x}_{n}}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord">∇<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1611em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight" style="margin-right:0.01597em">w<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathcal">L<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑<!-- --></span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mopen">[<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)]<!-- --></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></span></div>
<!-- --><p>굉장히 간단하게 정리가 되어지는 것을 볼 수 있다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">w<!-- --></mi><mrow><mi>t<!-- --></mi><mo>+<!-- --></mo><mn>1<!-- --></mn></mrow></msub><mo>=<!-- --></mo><msub><mi mathvariant="bold">w<!-- --></mi><mi>t<!-- --></mi></msub><mo>−<!-- --></mo><mi>α<!-- --></mi><msub><mi mathvariant="normal">∇<!-- --></mi><mi mathvariant="bold">w<!-- --></mi></msub><mi mathvariant="script">L<!-- --></mi><mo stretchy="false">(<!-- --></mo><msub><mi mathvariant="bold">w<!-- --></mi><mi>t<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow><annotation encoding="application/x-tex">\bold{w}_{t+1} = \bold{w}_{t} - \alpha\nabla_{\bold{w}}\mathcal{L}(\bold{w}_{t})<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6528em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t<!-- --></span><span class="mbin mtight">+<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α<!-- --></span><span class="mord"><span class="mord">∇<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1611em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight" style="margin-right:0.01597em">w<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathcal">L<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span></span></span></div>
<!-- --><p>따라서, 아래와 같이 Gradient Ascent를 활용하여 계산하는 것도 충분히 가능하다.<!-- --></p>
<!-- --><blockquote>
<!-- --><p><strong>Newton Method<!-- --></strong></p>
<!-- --></blockquote>
<!-- --><p>이러한 형태로 넘어오게 되면, 굉장히 많은 연산이 각 update마다 필요하다는 것을 알 수 있다. 따라서, 우리는 이 과정을 축약할 방법을 찾게 된다. 그 아이디어는 바로 gradient를 업데이트 할 때, linear 하게 update하는 것이 아니라 Quadratic하게 update하는 것이다. 이를 위한 방법론이 <!-- --><strong>Newton Method<!-- --></strong>이다. 이 방식을 Logistic Regression에 적용하였을 때, 이를 IRLS(Iterative Re-weighted Least Squared) Algorithm 이라고 부른다.<!-- --></p>
<!-- --><p><img src="/images/newton-method.jpg" alt="newton-method"/></p>
<!-- --><p>위 그래프에서 f(x)가 Loss 라고 할 때, 우리는 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x<!-- --></mi><mi>k<!-- --></mi></msub></mrow><annotation encoding="application/x-tex">x_k<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>에서 직선형의 gradient를 사용하는 것보다 quadratic 형태를 사용하는 것이 더 빠르게 수렴값을 찾을 수 있다는 것을 알 수 있다.<!-- --></p>
<!-- --><p>이를 사용하기 위해서는 다음 2가지에 대한 사전 이해가 필요하다.<!-- --></p>
<!-- --><ul>
<!-- --><li>Taylor Series<!-- --><br/>
smooth한 형태를 가진 x에 대한 함수를 x에 대한 급수의 형태로 변환한 것이다. 따라서 이를 식으로 나타내면 다음과 같다.<!-- --><br/>
<!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T<!-- --></mi><mi mathvariant="normal">∞<!-- --></mi></msub><mo stretchy="false">(<!-- --></mo><mi>x<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo>=<!-- --></mo><msubsup><mo>∑<!-- --></mo><mrow><mi>k<!-- --></mi><mo>=<!-- --></mo><mn>0<!-- --></mn></mrow><mi mathvariant="normal">∞<!-- --></mi></msubsup><mfrac><mrow><msup><mi>f<!-- --></mi><mrow><mo stretchy="false">(<!-- --></mo><mi>k<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow></msup><mo stretchy="false">(<!-- --></mo><msub><mi>x<!-- --></mi><mn>0<!-- --></mn></msub><mo stretchy="false">)<!-- --></mo></mrow><mrow><mi>k<!-- --></mi><mtext> ⁣<!-- --></mtext></mrow></mfrac><mo stretchy="false">(<!-- --></mo><mi>x<!-- --></mi><mo>−<!-- --></mo><msub><mi>x<!-- --></mi><mn>0<!-- --></mn></msub><msup><mo stretchy="false">)<!-- --></mo><mi>k<!-- --></mi></msup></mrow><annotation encoding="application/x-tex">T_{\infin}(x) = \sum_{k=0}^{\infin}{f^{(k)}(x_{0})\over{k\!}}(x-x_{0})^{k} <!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∞<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(<!-- --></span><span class="mord mathnormal">x<!-- --></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.5067em;vertical-align:-0.345em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">0<!-- --></span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∞<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1617em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span><span class="mspace mtight" style="margin-right:-0.1952em"></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.485em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em">f<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9667em"><span style="top:-2.9667em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(<!-- --></span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span><span class="mclose mtight">)<!-- --></span></span></span></span></span></span></span></span></span><span class="mopen mtight">(<!-- --></span><span class="mord mtight"><span class="mord mathnormal mtight">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span><span class="mclose mtight">)<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mopen">(<!-- --></span><span class="mord mathnormal">x<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span></span></span></span></span></span></span></span></span><br/>
즉, sine 함수와 같은 형태의 그래프도 x의 급수 형태로 변환이 가능하다는 것이다. Newton Method에서는 무한대까지는 사용하지 않고, 대게 K=2까지를 쓴다.<!-- --></li>
<!-- --><li>Hessian Matrix<!-- --><br/>
특정 함수 <!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow><annotation encoding="application/x-tex">f(\bold{x})<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span></span></span></span></span>를 각 feature에 대해서 이중 편미분한 결과를 저장한 행렬이다. 식은 다음과 같다.<!-- --><br/>
<!-- --><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H<!-- --></mi><mo>=<!-- --></mo><msup><mi mathvariant="normal">∇<!-- --></mi><mn>2<!-- --></mn></msup><mi>f<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi>x<!-- --></mi><mo stretchy="false">)<!-- --></mo><mo>=<!-- --></mo><mrow><mo fence="true">[<!-- --></mo><mtable rowspacing="0.16em" columnalign="center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msup><mi mathvariant="normal">∂<!-- --></mi><mn>2<!-- --></mn></msup><mi>f<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow><mrow><mi mathvariant="normal">∂<!-- --></mi><msubsup><mi>x<!-- --></mi><mn>1<!-- --></mn><mn>2<!-- --></mn></msubsup></mrow></mfrac></mstyle></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯<!-- --></mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msup><mi mathvariant="normal">∂<!-- --></mi><mn>2<!-- --></mn></msup><mi>f<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow><mrow><mi mathvariant="normal">∂<!-- --></mi><msub><mi>x<!-- --></mi><mn>1<!-- --></mn></msub><mi mathvariant="normal">∂<!-- --></mi><msub><mi>x<!-- --></mi><mi>D<!-- --></mi></msub></mrow></mfrac></mstyle></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮<!-- --></mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋱<!-- --></mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮<!-- --></mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msup><mi mathvariant="normal">∂<!-- --></mi><mn>2<!-- --></mn></msup><mi>f<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow><mrow><mi mathvariant="normal">∂<!-- --></mi><msub><mi>x<!-- --></mi><mi>D<!-- --></mi></msub><mi mathvariant="normal">∂<!-- --></mi><msub><mi>x<!-- --></mi><mn>1<!-- --></mn></msub></mrow></mfrac></mstyle></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯<!-- --></mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msup><mi mathvariant="normal">∂<!-- --></mi><mn>2<!-- --></mn></msup><mi>f<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi mathvariant="bold">x<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow><mrow><mi mathvariant="normal">∂<!-- --></mi><msubsup><mi>x<!-- --></mi><mi>n<!-- --></mi><mn>2<!-- --></mn></msubsup></mrow></mfrac></mstyle></mstyle></mtd></mtr></mtable><mo fence="true">]<!-- --></mo></mrow></mrow><annotation encoding="application/x-tex">H = \nabla^{2}f(x) =
\left[
  \begin{array}{ccc}
    \dfrac{\partial^{2} f(\mathbf{x})}{\partial x_{1}^{2}} &amp; \cdots &amp; \dfrac{\partial^{2} f(\mathbf{x})}{\partial x_{1} \partial x_{D}} \\ 
    \vdots &amp; \ddots &amp; \vdots \\
    \dfrac{\partial^{2} f(\mathbf{x})}{\partial x_{D} \partial x_{1}} &amp; \cdots &amp; \dfrac{\partial^{2} f(\mathbf{x})}{\partial x_{n}^{2}}
  \end{array}
\right]<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord"><span class="mord">∇<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10764em">f<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathnormal">x<!-- --></span><span class="mclose">)<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:6.7275em;vertical-align:-3.1138em"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.55em"><span style="top:-2.611em"><span class="pstrut" style="height:5.016em"></span><span class="delimsizinginner delim-size4"><span>⎣<!-- --></span></span></span><span style="top:-3.758em"><span class="pstrut" style="height:5.016em"></span><span style="height:3.016em;width:0.6667em"><svg xmlns="http://www.w3.org/2000/svg" width="0.6667em" height="3.016em" style="width:0.6667em" viewBox="0 0 666.67 3016" preserveAspectRatio="xMinYMin"><path d="M319 0 H403 V3016 H319z M319 0 H403 V3016 H319z"></path></svg></span></span><span style="top:-7.4111em"><span class="pstrut" style="height:5.016em"></span><span class="delimsizinginner delim-size4"><span>⎡<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:3.05em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.6138em"><span style="top:-5.8102em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4911em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:0.05556em">∂<!-- --></span><span class="mord"><span class="mord mathnormal">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959em"><span style="top:-2.4337em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.0448em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2663em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord" style="margin-right:0.05556em">∂<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10764em">f<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.9523em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.3578em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mord">⋮<!-- --></span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em"></span></span></span></span><span style="top:-1.5067em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4911em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:0.05556em">∂<!-- --></span><span class="mord"><span class="mord mathnormal">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord" style="margin-right:0.05556em">∂<!-- --></span><span class="mord"><span class="mord mathnormal">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord" style="margin-right:0.05556em">∂<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10764em">f<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.836em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:3.1138em"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em"></span><span class="arraycolsep" style="width:0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.6138em"><span style="top:-5.6227em"><span class="pstrut" style="height:3.5em"></span><span class="mord"><span class="minner">⋯<!-- --></span></span></span><span style="top:-3.1703em"><span class="pstrut" style="height:3.5em"></span><span class="mord"><span class="minner">⋱<!-- --></span></span></span><span style="top:-1.3192em"><span class="pstrut" style="height:3.5em"></span><span class="mord"><span class="minner">⋯<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:3.1138em"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em"></span><span class="arraycolsep" style="width:0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.6138em"><span style="top:-5.8102em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4911em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:0.05556em">∂<!-- --></span><span class="mord"><span class="mord mathnormal">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord" style="margin-right:0.05556em">∂<!-- --></span><span class="mord"><span class="mord mathnormal">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord" style="margin-right:0.05556em">∂<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10764em">f<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.836em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.3578em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mord">⋮<!-- --></span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em"></span></span></span></span><span style="top:-1.5067em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4911em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:0.05556em">∂<!-- --></span><span class="mord"><span class="mord mathnormal">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7401em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord" style="margin-right:0.05556em">∂<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10764em">f<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathbf">x<!-- --></span><span class="mclose">)<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.933em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:3.1138em"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em"></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.55em"><span style="top:-2.611em"><span class="pstrut" style="height:5.016em"></span><span class="delimsizinginner delim-size4"><span>⎦<!-- --></span></span></span><span style="top:-3.758em"><span class="pstrut" style="height:5.016em"></span><span style="height:3.016em;width:0.6667em"><svg xmlns="http://www.w3.org/2000/svg" width="0.6667em" height="3.016em" style="width:0.6667em" viewBox="0 0 666.67 3016" preserveAspectRatio="xMinYMin"><path d="M263 0 H347 V3016 H263z M263 0 H347 V3016 H263z"></path></svg></span></span><span style="top:-7.4111em"><span class="pstrut" style="height:5.016em"></span><span class="delimsizinginner delim-size4"><span>⎤<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:3.05em"><span></span></span></span></span></span></span></span></span></span></span></span></li>
<!-- --></ul>
<!-- --><p>이를 이용해서, Newton Method의 결과값을 정리하면 결과는 다음과 같다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi mathvariant="bold">w<!-- --></mi><mrow><mo stretchy="false">(<!-- --></mo><mi>k<!-- --></mi><mo>+<!-- --></mo><mn>1<!-- --></mn><mo stretchy="false">)<!-- --></mo></mrow></msup><mo>=<!-- --></mo><msup><mi mathvariant="bold">w<!-- --></mi><mrow><mo stretchy="false">(<!-- --></mo><mi>k<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow></msup><mo>−<!-- --></mo><mo stretchy="false">[<!-- --></mo><msup><mi mathvariant="normal">∇<!-- --></mi><mn>2<!-- --></mn></msup><mi mathvariant="script">J<!-- --></mi><mo stretchy="false">(<!-- --></mo><msup><mi mathvariant="bold">w<!-- --></mi><mrow><mo stretchy="false">(<!-- --></mo><mi>k<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow></msup><mo stretchy="false">)<!-- --></mo><msup><mo stretchy="false">]<!-- --></mo><mrow><mo>−<!-- --></mo><mn>1<!-- --></mn></mrow></msup><mi mathvariant="normal">∇<!-- --></mi><mi mathvariant="script">J<!-- --></mi><mo stretchy="false">(<!-- --></mo><msup><mi mathvariant="bold">w<!-- --></mi><mrow><mo stretchy="false">(<!-- --></mo><mi>k<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow></msup><mo stretchy="false">)<!-- --></mo></mrow><annotation encoding="application/x-tex">\bold{w}^{(k+1)} = \bold{w}^{(k)} - [\nabla^{2}\mathcal{J}(\bold{w}^{(k)})]^{-1}\nabla\mathcal{J}(\bold{w}^{(k)})<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.938em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(<!-- --></span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span><span class="mbin mtight">+<!-- --></span><span class="mord mtight">1<!-- --></span><span class="mclose mtight">)<!-- --></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0213em;vertical-align:-0.0833em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(<!-- --></span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span><span class="mclose mtight">)<!-- --></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em"></span><span class="mopen">[<!-- --></span><span class="mord"><span class="mord">∇<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathcal" style="margin-right:0.18472em">J<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(<!-- --></span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span><span class="mclose mtight">)<!-- --></span></span></span></span></span></span></span></span></span><span class="mclose">)<!-- --></span><span class="mclose"><span class="mclose">]<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span></span></span></span></span></span><span class="mord">∇<!-- --></span><span class="mord mathcal" style="margin-right:0.18472em">J<!-- --></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(<!-- --></span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span><span class="mclose mtight">)<!-- --></span></span></span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span></span></span></div>
<!-- --><p>자 이제 이것을 실제로 Logistic Regression 식에 대입해보자.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="normal">∇<!-- --></mi><mi mathvariant="script">J<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi>w<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=<!-- --></mo><mo>−<!-- --></mo><munderover><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></munderover><mo stretchy="false">(<!-- --></mo><msub><mi>y<!-- --></mi><mi>n<!-- --></mi></msub><mo>−<!-- --></mo><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><msub><mi>x<!-- --></mi><mi>n<!-- --></mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi mathvariant="normal">∇<!-- --></mi><mn>2<!-- --></mn></msup><mi mathvariant="script">J<!-- --></mi><mo stretchy="false">(<!-- --></mo><mi>w<!-- --></mi><mo stretchy="false">)<!-- --></mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=<!-- --></mo><munderover><mo>∑<!-- --></mo><mrow><mi>n<!-- --></mi><mo>=<!-- --></mo><mn>1<!-- --></mn></mrow><mi>N<!-- --></mi></munderover><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mi>n<!-- --></mi></msub><mo stretchy="false">(<!-- --></mo><mn>1<!-- --></mn><mo>−<!-- --></mo><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mi>n<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo><msub><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi></msub><msubsup><mi mathvariant="bold">x<!-- --></mi><mi>n<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msubsup></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
  \nabla\mathcal{J}(w) &amp;= - \sum_{n=1}^{N}(y_{n}-\hat{y}_{n})x_{n} \\
  \nabla^{2}\mathcal{J}(w) &amp;= \sum_{n=1}^{N}\hat{y}_{n}(1-\hat{y}_{n})\bold{x}_{n}\bold{x}_{n}^{\top}
\end{align*}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6.7909em;vertical-align:-3.1454em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.6454em"><span style="top:-5.6454em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mord">∇<!-- --></span><span class="mord mathcal" style="margin-right:0.18472em">J<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathnormal" style="margin-right:0.02691em">w<!-- --></span><span class="mclose">)<!-- --></span></span></span><span style="top:-2.25em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mord"><span class="mord">∇<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathcal" style="margin-right:0.18472em">J<!-- --></span><span class="mopen">(<!-- --></span><span class="mord mathnormal" style="margin-right:0.02691em">w<!-- --></span><span class="mclose">)<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:3.1454em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.6454em"><span style="top:-5.6454em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord">−<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑<!-- --></span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mopen">(<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span><span class="mord"><span class="mord mathnormal">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.25em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span><span class="mrel mtight">=<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑<!-- --></span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(<!-- --></span><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n<!-- --></span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:3.1454em"><span></span></span></span></span></span></span></span></span></span></span></span></div>
<!-- --><p>여기서, 아래와 같이 변수를 정의하면,<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>S<!-- --></mi><mo>=<!-- --></mo><mrow><mo fence="true">[<!-- --></mo><mtable rowspacing="0.16em" columnalign="center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mn>1<!-- --></mn></msub><mo stretchy="false">(<!-- --></mo><mn>1<!-- --></mn><mo>−<!-- --></mo><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mn>1<!-- --></mn></msub><mo stretchy="false">)<!-- --></mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯<!-- --></mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0<!-- --></mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮<!-- --></mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋱<!-- --></mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮<!-- --></mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0<!-- --></mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯<!-- --></mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mi>N<!-- --></mi></msub><mo stretchy="false">(<!-- --></mo><mn>1<!-- --></mn><mo>−<!-- --></mo><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mi>N<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]<!-- --></mo></mrow><mo separator="true">,<!-- --></mo><mi mathvariant="bold">b<!-- --></mi><mo>=<!-- --></mo><mrow><mo fence="true">[<!-- --></mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><msub><mi>y<!-- --></mi><mn>1<!-- --></mn></msub><mo>−<!-- --></mo><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mn>1<!-- --></mn></msub></mrow><mrow><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mn>1<!-- --></mn></msub><mo stretchy="false">(<!-- --></mo><mn>1<!-- --></mn><mo>−<!-- --></mo><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mn>1<!-- --></mn></msub><mo stretchy="false">)<!-- --></mo></mrow></mfrac></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮<!-- --></mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><msub><mi>y<!-- --></mi><mi>N<!-- --></mi></msub><mo>−<!-- --></mo><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mi>N<!-- --></mi></msub></mrow><mrow><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mi>N<!-- --></mi></msub><mo stretchy="false">(<!-- --></mo><mn>1<!-- --></mn><mo>−<!-- --></mo><msub><mover accent="true"><mi>y<!-- --></mi><mo>^<!-- --></mo></mover><mi>N<!-- --></mi></msub><mo stretchy="false">)<!-- --></mo></mrow></mfrac></mstyle></mtd></mtr></mtable><mo fence="true">]<!-- --></mo></mrow></mrow><annotation encoding="application/x-tex">S = 
  \begin{bmatrix}
    \hat{y}_{1}(1-\hat{y}_1)  &amp; \cdots  &amp; 0                         \\
    \vdots                    &amp; \ddots  &amp; \vdots                     \\
    0                         &amp; \cdots  &amp; \hat{y}_{N}(1-\hat{y}_N)  \\
  \end{bmatrix},

\bold{b} = 
  \begin{bmatrix}
    {{y_{1} - \hat{y}_{1}}\over{\hat{y}_{1}(1-\hat{y}_{1})}} \\
    \vdots \\
    {{y_{N} - \hat{y}_{N}}\over{\hat{y}_{N}(1-\hat{y}_{N})}}
  \end{bmatrix}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:4.26em;vertical-align:-1.88em"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35em"><span style="top:-1.95em"><span class="pstrut" style="height:3.155em"></span><span class="delimsizinginner delim-size4"><span>⎣<!-- --></span></span></span><span style="top:-3.097em"><span class="pstrut" style="height:3.155em"></span><span style="height:0.616em;width:0.6667em"><svg xmlns="http://www.w3.org/2000/svg" width="0.6667em" height="0.616em" style="width:0.6667em" viewBox="0 0 666.67 616" preserveAspectRatio="xMinYMin"><path d="M319 0 H403 V616 H319z M319 0 H403 V616 H319z"></path></svg></span></span><span style="top:-4.35em"><span class="pstrut" style="height:3.155em"></span><span class="delimsizinginner delim-size4"><span>⎡<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.85em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.38em"><span style="top:-5.2275em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(<!-- --></span><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span><span style="top:-3.3675em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mord">⋮<!-- --></span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em"></span></span></span></span><span style="top:-2.1675em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord">0<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.88em"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em"></span><span class="arraycolsep" style="width:0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.38em"><span style="top:-5.04em"><span class="pstrut" style="height:3.5em"></span><span class="mord"><span class="minner">⋯<!-- --></span></span></span><span style="top:-3.18em"><span class="pstrut" style="height:3.5em"></span><span class="mord"><span class="minner">⋱<!-- --></span></span></span><span style="top:-1.98em"><span class="pstrut" style="height:3.5em"></span><span class="mord"><span class="minner">⋯<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.88em"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em"></span><span class="arraycolsep" style="width:0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.38em"><span style="top:-5.2275em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord">0<!-- --></span></span></span><span style="top:-3.3675em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mord">⋮<!-- --></span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em"></span></span></span></span><span style="top:-2.1675em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(<!-- --></span><span class="mord">1<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.88em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35em"><span style="top:-1.95em"><span class="pstrut" style="height:3.155em"></span><span class="delimsizinginner delim-size4"><span>⎦<!-- --></span></span></span><span style="top:-3.097em"><span class="pstrut" style="height:3.155em"></span><span style="height:0.616em;width:0.6667em"><svg xmlns="http://www.w3.org/2000/svg" width="0.6667em" height="0.616em" style="width:0.6667em" viewBox="0 0 666.67 616" preserveAspectRatio="xMinYMin"><path d="M263 0 H347 V616 H263z M263 0 H347 V616 H263z"></path></svg></span></span><span style="top:-4.35em"><span class="pstrut" style="height:3.155em"></span><span class="delimsizinginner delim-size4"><span>⎤<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:1.85em"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,<!-- --></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathbf">b<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:4.8001em;vertical-align:-2.15em"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.65em"><span style="top:-1.711em"><span class="pstrut" style="height:3.216em"></span><span class="delimsizinginner delim-size4"><span>⎣<!-- --></span></span></span><span style="top:-2.858em"><span class="pstrut" style="height:3.216em"></span><span style="height:1.216em;width:0.6667em"><svg xmlns="http://www.w3.org/2000/svg" width="0.6667em" height="1.216em" style="width:0.6667em" viewBox="0 0 666.67 1216" preserveAspectRatio="xMinYMin"><path d="M319 0 H403 V1216 H319z M319 0 H403 V1216 H319z"></path></svg></span></span><span style="top:-4.7111em"><span class="pstrut" style="height:3.216em"></span><span class="delimsizinginner delim-size4"><span>⎡<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:2.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6322em"><span style="top:-5.3875em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord mtight">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span><span class="mopen mtight">(<!-- --></span><span class="mord mtight">1<!-- --></span><span class="mbin mtight">−<!-- --></span><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord mtight">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span><span class="mclose mtight">)<!-- --></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.4461em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span><span class="mbin mtight">−<!-- --></span><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord mtight">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.52em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span style="top:-3.3675em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mord">⋮<!-- --></span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em"></span></span></span></span><span style="top:-2.0753em"><span class="pstrut" style="height:3.6875em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord mtight">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span><span class="mopen mtight">(<!-- --></span><span class="mord mtight">1<!-- --></span><span class="mbin mtight">−<!-- --></span><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord mtight">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span><span class="mclose mtight">)<!-- --></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.4461em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span><span class="mbin mtight">−<!-- --></span><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y<!-- --></span></span><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord mtight">^<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.52em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:2.1322em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.65em"><span style="top:-1.711em"><span class="pstrut" style="height:3.216em"></span><span class="delimsizinginner delim-size4"><span>⎦<!-- --></span></span></span><span style="top:-2.858em"><span class="pstrut" style="height:3.216em"></span><span style="height:1.216em;width:0.6667em"><svg xmlns="http://www.w3.org/2000/svg" width="0.6667em" height="1.216em" style="width:0.6667em" viewBox="0 0 666.67 1216" preserveAspectRatio="xMinYMin"><path d="M263 0 H347 V1216 H263z M263 0 H347 V1216 H263z"></path></svg></span></span><span style="top:-4.7111em"><span class="pstrut" style="height:3.216em"></span><span class="delimsizinginner delim-size4"><span>⎤<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:2.15em"><span></span></span></span></span></span></span></span></span></span></span></span></div>
<!-- --><p>결과적으로 다음과 같은 형태를 얻을 수 있다.<!-- --></p>
<!-- --><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi mathvariant="bold">w<!-- --></mi><mrow><mi>k<!-- --></mi><mo>+<!-- --></mo><mn>1<!-- --></mn></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=<!-- --></mo><msub><mi mathvariant="bold">w<!-- --></mi><mi>k<!-- --></mi></msub><mo>+<!-- --></mo><mo stretchy="false">(<!-- --></mo><mi>X<!-- --></mi><msub><mi>S<!-- --></mi><mi>k<!-- --></mi></msub><msup><mi>X<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msup><mo stretchy="false">)<!-- --></mo><mrow><mo>−<!-- --></mo><mn>1<!-- --></mn></mrow></msup><mi>X<!-- --></mi><msub><mi>S<!-- --></mi><mi>k<!-- --></mi></msub><msub><mi mathvariant="bold">b<!-- --></mi><mi>k<!-- --></mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=<!-- --></mo><mo stretchy="false">(<!-- --></mo><mi>X<!-- --></mi><msub><mi>S<!-- --></mi><mi>k<!-- --></mi></msub><msup><mi>X<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msup><mo stretchy="false">)<!-- --></mo><mrow><mo>−<!-- --></mo><mn>1<!-- --></mn></mrow></msup><mo stretchy="false">[<!-- --></mo><mo stretchy="false">(<!-- --></mo><mi>X<!-- --></mi><msub><mi>S<!-- --></mi><mi>k<!-- --></mi></msub><msup><mi>X<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><mo stretchy="false">)<!-- --></mo><msub><mi mathvariant="bold">w<!-- --></mi><mi>k<!-- --></mi></msub><mo>+<!-- --></mo><mi>X<!-- --></mi><msub><mi>S<!-- --></mi><mi>k<!-- --></mi></msub><msub><mi mathvariant="bold">b<!-- --></mi><mi>k<!-- --></mi></msub><mo stretchy="false">]<!-- --></mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=<!-- --></mo><mo stretchy="false">(<!-- --></mo><mi>X<!-- --></mi><msub><mi>S<!-- --></mi><mi>k<!-- --></mi></msub><msup><mi>X<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msup><mo stretchy="false">)<!-- --></mo><mrow><mo>−<!-- --></mo><mn>1<!-- --></mn></mrow></msup><mi>X<!-- --></mi><msub><mi>S<!-- --></mi><mi>k<!-- --></mi></msub><mo stretchy="false">[<!-- --></mo><msup><mi>X<!-- --></mi><mi mathvariant="normal">⊤<!-- --></mi></msup><msub><mi mathvariant="bold">w<!-- --></mi><mi>k<!-- --></mi></msub><mo>+<!-- --></mo><msub><mi mathvariant="bold">b<!-- --></mi><mi>k<!-- --></mi></msub><mo stretchy="false">]<!-- --></mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\bold{w}_{k+1} &amp;= \bold{w}_{k} + (XS_{k}X^{\top})^{-1}XS_{k}\bold{b}_{k} \\
&amp;= (XS_{k}X^{\top})^{-1}[(XS_{k}X^{\top})\bold{w}_{k} + XS_{k}\bold{b}_{k}] \\
&amp;= (XS_{k}X^{\top})^{-1}XS_{k}[X^{\top}\bold{w}_{k} + \bold{b}_{k}]
\end{align*}<!-- --></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.6773em;vertical-align:-2.0887em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5887em"><span style="top:-4.6896em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span><span class="mbin mtight">+<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span><span style="top:-3.1304em"><span class="pstrut" style="height:3em"></span><span class="mord"></span></span><span style="top:-1.5713em"><span class="pstrut" style="height:3em"></span><span class="mord"></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:2.0887em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5887em"><span style="top:-4.6896em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mopen">(<!-- --></span><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">b<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.1304em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mopen">(<!-- --></span><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span></span></span></span></span></span><span class="mopen">[(<!-- --></span><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mclose">)<!-- --></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">b<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">]<!-- --></span></span></span><span style="top:-1.5713em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=<!-- --></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mopen">(<!-- --></span><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−<!-- --></span><span class="mord mtight">1<!-- --></span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">[<!-- --></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X<!-- --></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤<!-- --></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">w<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+<!-- --></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathbf">b<!-- --></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k<!-- --></span></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">]<!-- --></span></span></span></span><span class="vlist-s">​<!-- --></span></span><span class="vlist-r"><span class="vlist" style="height:2.0887em"><span></span></span></span></span></span></span></span></span></span></span></span></div>
<!-- --><p>이는 결코 계산 과정이 단순하다고는 할 수 없지만, 빠르게 수렴할 수 있기 때문에 가치있는 방법이다.<!-- --></p>
<!-- --><h2>Reference<!-- --></h2>
<!-- --><ul>
<!-- --><li>Tumbnail : Photo by <!-- --><a href="https://unsplash.com/@markuswinkler?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Markus Winkler<!-- --></a> on <!-- --><a href="https://unsplash.com/@markuswinkler?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash<!-- --></a></li>
<!-- --></ul></article><div class="Comment_comment__wrapper___sKTf"><h2 class="Comment_comment__title__hLmXO">Comments</h2><section></section></div><div class="ColumnCard_column_card__list__background__kZObh"><h2 class="ColumnCard_column_card__list__title__pawoL">Related Posts</h2><div class="ColumnCard_column_card__list__wrapper__lsbEP"><div class="ColumnCard_column_card__wrapper__iVPbY"><a class="ColumnCard_column_card__thumbnail__wrapper__M3Kfm" href="/posts/ml-base-knowledge"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27320%27%20height=%27320%27/%3e"/></span><img alt="[ML] 0. Base Knowledge" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="ColumnCard_column_card__thumbnail__bx9FL" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 0. Base Knowledge" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=384 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="ColumnCard_column_card__thumbnail__bx9FL" loading="lazy"/></noscript></span></a><div class="ColumnCard_column_card__tray__v9oLc"><a class="ColumnCard_column_card__tray__title__fEApg" tabindex="-1" href="/posts/ml-base-knowledge">[ML] 0. Base Knowledge</a><ul class="ColumnCard_column_card__tray__tag__UUiD6"><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/ML"># <!-- -->ML<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/Probability"># <!-- -->Probability<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/Calculus"># <!-- -->Calculus<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/InformationTheory"># <!-- -->InformationTheory<!-- --></a></ul></div></div><div class="ColumnCard_column_card__wrapper__iVPbY"><a class="ColumnCard_column_card__thumbnail__wrapper__M3Kfm" href="/posts/ml-parametric-estimation"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27320%27%20height=%27320%27/%3e"/></span><img alt="[ML] 1. Parametric Estimation" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="ColumnCard_column_card__thumbnail__bx9FL" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 1. Parametric Estimation" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=384 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="ColumnCard_column_card__thumbnail__bx9FL" loading="lazy"/></noscript></span></a><div class="ColumnCard_column_card__tray__v9oLc"><a class="ColumnCard_column_card__tray__title__fEApg" tabindex="-1" href="/posts/ml-parametric-estimation">[ML] 1. Parametric Estimation</a><ul class="ColumnCard_column_card__tray__tag__UUiD6"><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/ML"># <!-- -->ML<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/MLE"># <!-- -->MLE<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/MAP"># <!-- -->MAP<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/Bayesian"># <!-- -->Bayesian<!-- --></a></ul></div></div><div class="ColumnCard_column_card__wrapper__iVPbY"><a class="ColumnCard_column_card__thumbnail__wrapper__M3Kfm" href="/posts/ml-linear-regression"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27320%27%20height=%27320%27/%3e"/></span><img alt="[ML] 2. Linear Regression" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="ColumnCard_column_card__thumbnail__bx9FL" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 2. Linear Regression" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=384 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="ColumnCard_column_card__thumbnail__bx9FL" loading="lazy"/></noscript></span></a><div class="ColumnCard_column_card__tray__v9oLc"><a class="ColumnCard_column_card__tray__title__fEApg" tabindex="-1" href="/posts/ml-linear-regression">[ML] 2. Linear Regression</a><ul class="ColumnCard_column_card__tray__tag__UUiD6"><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/ML"># <!-- -->ML<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/LinearRegression"># <!-- -->LinearRegression<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/BasisFunction"># <!-- -->BasisFunction<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/Regularization"># <!-- -->Regularization<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/GradientDescent"># <!-- -->GradientDescent<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/Momentum"># <!-- -->Momentum<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/StochasticGradientDescent"># <!-- -->StochasticGradientDescent<!-- --></a></ul></div></div><div class="ColumnCard_column_card__wrapper__iVPbY"><a class="ColumnCard_column_card__thumbnail__wrapper__M3Kfm" href="/posts/ml-svm"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27320%27%20height=%27320%27/%3e"/></span><img alt="[ML] 4. SVM" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="ColumnCard_column_card__thumbnail__bx9FL" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 4. SVM" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=384 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="ColumnCard_column_card__thumbnail__bx9FL" loading="lazy"/></noscript></span></a><div class="ColumnCard_column_card__tray__v9oLc"><a class="ColumnCard_column_card__tray__title__fEApg" tabindex="-1" href="/posts/ml-svm">[ML] 4. SVM</a><ul class="ColumnCard_column_card__tray__tag__UUiD6"><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/ML"># <!-- -->ML<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/SVM"># <!-- -->SVM<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/GeneralClassifier"># <!-- -->GeneralClassifier<!-- --></a></ul></div></div><div class="ColumnCard_column_card__wrapper__iVPbY"><a class="ColumnCard_column_card__thumbnail__wrapper__M3Kfm" href="/posts/ml-multiclass-classification-in-svm"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27320%27%20height=%27320%27/%3e"/></span><img alt="[ML] 5. Multiclass Classification in SVM" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="ColumnCard_column_card__thumbnail__bx9FL" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 5. Multiclass Classification in SVM" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=384 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="ColumnCard_column_card__thumbnail__bx9FL" loading="lazy"/></noscript></span></a><div class="ColumnCard_column_card__tray__v9oLc"><a class="ColumnCard_column_card__tray__title__fEApg" tabindex="-1" href="/posts/ml-multiclass-classification-in-svm">[ML] 5. Multiclass Classification in SVM</a><ul class="ColumnCard_column_card__tray__tag__UUiD6"><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/ML"># <!-- -->ML<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/SVM"># <!-- -->SVM<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/KernelMethod"># <!-- -->KernelMethod<!-- --></a></ul></div></div><div class="ColumnCard_column_card__wrapper__iVPbY"><a class="ColumnCard_column_card__thumbnail__wrapper__M3Kfm" href="/posts/ml-nn"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27320%27%20height=%27320%27/%3e"/></span><img alt="[ML] 6. Neural Network" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="ColumnCard_column_card__thumbnail__bx9FL" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 6. Neural Network" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=384 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="ColumnCard_column_card__thumbnail__bx9FL" loading="lazy"/></noscript></span></a><div class="ColumnCard_column_card__tray__v9oLc"><a class="ColumnCard_column_card__tray__title__fEApg" tabindex="-1" href="/posts/ml-nn">[ML] 6. Neural Network</a><ul class="ColumnCard_column_card__tray__tag__UUiD6"><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/ML"># <!-- -->ML<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/NeuralNetwork"># <!-- -->NeuralNetwork<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/Perceptron"># <!-- -->Perceptron<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/Backpropagation"># <!-- -->Backpropagation<!-- --></a><a tabindex="-1" class="ColumnCard_column_card__tray__tag__li__YRXLc" href="/tags/CrossEntropyLoss"># <!-- -->CrossEntropyLoss<!-- --></a></ul></div></div></div></div></div></section><footer class="Layout_footer__EL5v8"><div class="Layout_footer__copyright__r5baC"><span>Copyright © euidong</span><br/><span>모든 컨텐츠에 대한 저작권은 작성자에게 존재합니다. <!-- --><br/>불법 복제를 통한 상업적 사용을 절대적으로 금지합니다. <!-- --><br/>단, 비상업적 이용의 경우 출처 및 링크를 적용한다면 자유롭게 사용가능 합니다.<!-- --></span><span>Also I use photos by<!-- --> <!-- --><a href="https://unsplash.com/@lorenzoherrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Lorenzo Herrera</a> <!-- -->on<!-- --> <!-- --><a href="https://unsplash.com/s/photos/tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Unsplash</a></span></div><div class="Layout_footer__contents__YZWSm"><a class="Layout_footer__contents__link__K_TKH" href="https://github.com/euidong" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>github</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://euidong.github.io/portfolio" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M6 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm-5 6s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H1zM11 3.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 0 1h-4a.5.5 0 0 1-.5-.5zm.5 2.5a.5.5 0 0 0 0 1h4a.5.5 0 0 0 0-1h-4zm2 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2zm0 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2z"></path></svg><span>portfolio</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://chrome.google.com/webstore/detail/bonfire/nkooidijgbppkojdgkoafcoppnohdfka?hl=ko" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 16c-1.066-2.219-0.498-3.49 0.321-4.688 0.897-1.312 1.129-2.61 1.129-2.61s0.706 0.917 0.423 2.352c1.246-1.387 1.482-3.598 1.293-4.445 2.817 1.969 4.021 6.232 2.399 9.392 8.631-4.883 2.147-12.19 1.018-13.013 0.376 0.823 0.448 2.216-0.313 2.893-1.287-4.879-4.468-5.879-4.468-5.879 0.376 2.516-1.364 5.268-3.042 7.324-0.059-1.003-0.122-1.696-0.649-2.656-0.118 1.823-1.511 3.309-1.889 5.135-0.511 2.473 0.383 4.284 3.777 6.197z"></path></svg><span>chat</span></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"content":"\n## Intro\n\n이전까지 우리는 input data가 들어왔을 때, continuos한 output을 얻는 것을 목표로 했다. 하지만 현실에서는 대게 정확한 수치보다는 특정 분류로 나누는 것이 효과적인 경우가 많다. 예를 들어, spam 필터링, object detection, 등 등. 따라서, 해당 포스팅에서는 classification을 위해서 사용할 수 있는 logistic regression에 대해서 살펴볼 것이다.\n\n## Classification\n\n**Classification**이란 결국 특정 input이 들어왔을 때, 이를 하나의 Class라는 output을 내보내는 것이다. 즉, output은 연속적이지 않고, descret하다. 대게 Classification에서는 Class의 갯수를 K라고 표기하고, $C_k$는 k 번째 Class라는 의미로 사용되어진다.\n\n그렇다면, 어떻게 Class를 나눌 수 있는 것일까? 매우 단순하게도 이는 **Decision Boundary**라는 선을 그어서 해결 할 수 있다.\n\n![decision-boundary](/images/decision-boundary.jpg)\n\n위의 예시처럼 우리는 선을 하나 그어서 $\\red{\\text{x}}$와 $\\blue{\\text{o}}$를 구분할 수 있다. 이를 통해서 우리는 Class 1에 해당할 것이라고 예측하는 구간 $R_1$이 만들어지고, Class 2라고 예측하는 구간 $R_2$를 구성할 수 있다.\n\n즉, classification을 수행하기 위해서 해야할 일은 기존의 Regression 과정과 마찬가지로 선을 찾는 것이다.\n\n결국 찾고자 하는 것이 선이라면, 이것을 Linear Regression으로 해결할 수 있을 것이다. 따라서, 우리는 다음과 같은 식으로 간단히 Linear Regression을 바꿔서 생각할 수 있다.\n\n- 예측값($\\hat{y}$, $h(\\bold{x})$)  \n  $h(\\bold{x}) = \\text{sign}(\\bold{w}^{\\top}\\bold{x}) = \\begin{cases} +1 \u0026 \\bold{w}^{\\top}\\bold{x} \\geq 0 \\\\ -1 \u0026 \\text{otherwise}\\end{cases}$\n- Least Squared Error(LS, MLE)  \n  실제로 parameter를 구할 때에는 sign을 취하지 않는데, sign을 취하게 되면 모두 LS는 결국 오답의 갯수 정도로 취급된다. 즉, 얼마나 예측이 잘못되었는지를 반영할 수 없다는 것이다. 따라서, 이는 기존 Linear Regression의 LS를 구하는 방법과 동일하게 수행한다.  \n  $\\argmin_{w} {1\\over2}\\sum_{n=1}^{N}{(y_n - (\\bold{w}^{\\top}\\bold{x}))^2}$\n\n이렇게 Linear Regression을 적용하면 문제가 없을 거 같다. 하지만, 실제로는 문제가 있다. 바로, 데이터가 불균형할 때이다. 만약 데이터가 decision boundary를 기준으로 대칭(symmetric)인 형태로 존재한다면, 문제가 없다. 하지만, 비대칭(asymmetric)인 경우 제대로 동작하지 않는다. 왜냐하면, linear regression은 최적에서 데이터의 평균을 반영하는데 불균형한 경우 데이터의 평균이 Decision Boundary가 되는 것은 문제가 있다.\n\n![linear-in-classification](/images/linear-in-classification.jpg)\n\n## Logistic Regression\n\n위에서 제시한 문제를 해결하기 위해서 Classification에서는 Linear Regression이 아닌 Logistic Regression을 활용한다. 이를 이해하기 위해서 기반이 될 요소들을 먼저 살펴보자.\n\n\u003e **Discriminant Function**\n\n판별함수(Discriminant Function, Score Function) 등으로 불리는 해당 함수는 특정 data가 특정 class에 속할 가능성(likelihood, probability, score)을 나타내는 함수이다. 즉, input으로 data를 받고, output으로 class에 속할 확률을 내보낸다.\n\n이를 통해서 우리는 다음과 같은 과정을 할 수 있다.\n\n만약, $f_k(\\bold{x}) \\gt f_j(\\bold{x})$이라면, $\\bold{x}$의 class는 $C_k$이다.\n\n따라서, 우리는 다음과 같은 식으로 여러 개의 Class가 있는 공간에서 data를 분류할 수 있다.\n\n$$\nh(\\bold{x}) = \\argmax_{k}f_{k}(\\bold{x})\n$$\n\n그렇다면, Discriminant Function으로 어떤 값을 쓰면 좋을까? 이에 대한 해결책을 Bayes Decision Rule에서 제시한다.\n\n\u003e **Bayes Decision Rule**\n\n만약 우리가 특정 data가 특정 Class에 속할 확률을 구한다고 하자. 우리는 먼저 Likelihood를 생각할 수 있다. $P(x|C = k), P(x|C = j)$를 구하여 각 Class에 속할 확률을 비교할 수 있을까?  \n물론 비교는 가능하다 하지만, 반쪽짜리 비교라고 할 수 있다. 만약, class k에 속하는 데이터보다 class j에 속하는 데이터가 훨씬 많다고 하자. 그러면, 일반적으로 class j가 발생할 확률 자체가 높다. 하지만, likelihood는 이러한 경향을 반영하지 않는다. 간단한 예시를 들어보자.\n\n```plaintext\n 🤔 어떤 동물의 털에 존재하는 색의 갯수가 주어졌을 때, 고양이일 확률과 호랑이일 확률이라고 하자.\n\n  그리고, input data는 털에 존재하는 색의 수라고 하자. (호랑이는 대게 3가지 색, 백호 = 2가지 색, 고양이는 매우 다양)\n  그렇다면, P(털의 색 = 3|C = 호랑이), P(털의 색 = 3|C = 고양이)를 비교했을 때, 우리는 당연히 전자가 크다고 생각할 것이다.\n  하지만, 여기서 우리가 고려하지 않은 것이 있다. 바로 전체 고양이와 호랑이의 비율이다. \n  상대적으로 고양이가 호랑이보다 압도적으로 많다는 것을 고려했을 때, 고양이의 확률이 더 높을 수도 있다. \n\n  즉, 어떤 동물의 털에 존재하는 색의 갯수가 주어졌을 때, 고양이일 확률은 \n  P(C=고양이|털의 색=3) =  P(털의 색 = 3|C = 고양이)P(C=고양이)이다. (분모는 생략함.)\n```\n\n즉, Bayes Rule에 기반하여 우리가 원하는 output은 Posterior라는 것을 명확히 알 수 있다.\n\n$$\n\\begin{align*}\np(C_{k}|\\bold{x}) \u0026= {{p(\\bold{x}| C_{k}) p(C_{k})}\\over{\\sum_{j=1}^{K}{p(\\bold{x}|C_{j})p(C_{j})}}} \\\\\n\u0026\\propto p(\\bold{x}| C_{k}) p(C_{k})\n\\end{align*}\n$$\n\n위의 경우 Class간의 상대 비교에 사용하는 지표로 이를 사용하기 때문에, 분모(Normalization Factor, 확률의 총합이 1이 되도록 하는 역할)를 제외하여도 상관없기에 대게 복잡한 분모 계산을 제외하고 표현하는 것이 일반적이다.\n\n또한, 앞선 예시에서 얻을 수 있는 insight는 편향된 데이터일수록 MLE를 사용할 수 없다는 것이다. 위에서 Linear Regression이 Classification에 부적함한 경우도 데이터의 편향이 있을 경우이다. 이 역시 Linear Regression이 결국은 MLE에 기반하기 때문인 것이다.\n\n우리는 각 Class 자체의 확률(Prior)과 Likelihood를 이용할 수 있는 Discriminant Function을 구해야 한다는 것이다.\n\n\u003e **Logistic Regression**\n\n자 이제 드디어 Logistric Regression을 시작해보자. 우리는 Discriminant Function을 먼저 지정해야 한다. 여러 가지 방법이 있지만, 가장 대표적으로 사용되는 방법은 **Softmax**를 활용하는 것이다. **Softmax**를 활용하여 식을 나타내면 아래와 같다.\n\n$$\np(y_n = k | \\bold{x}_n, \\bold{w}) = {{\\exp(\\bold{w}_{k}^{\\top}\\bold{x}_n)}\\over{\\sum_{j=1}^{K}{\\exp(\\bold{w}_{j}^{\\top}\\bold{x}_n)}}} \n$$\n\n만약, class가 2개인 Binary Classification인 경우에 **Softmax**는 다음과 같아진다. 특히 이를 **Sigmoid**(**Logit**)라고 정의한다.\n\n$$\np(y_n = k | \\bold{x}_n, \\bold{w}) = {1\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}\n$$\n\n이를 유도하는 과정은 생략하지만, 여타 다른 블로그를 더 참고하면 좋다.\n\n이를 Linear Regression과 비교해서 살펴보자.\n\n![logistic-vs-linear](/images/logistic-vs-linear.jpg)\n\nLinear Regression은 특정값을 향해 나아가고 있다. 해당 방식을 보면 x가 대상의 특성을 강하게 가지고 있다면, 명확하게 구분할 수 있는데, 이는 **sigmoid**($\\sigma$) 함수가 [0, 1] 범위 내에서 정의되기 때문에 Regression 과정에서 극단 데이터(outlier)가 가지는 영향력이 Linear Regression보다 극단적으로 적다는 것을 알 수 있다.\n\n자 이것이 가지는 의미를 이전에 살펴본 **Bayes Decision Rule**에 기반해서 생각해보자. **sigmoid**($\\sigma$)는 결국 극단적인 데이터이든, 애매한 데이터이든 거의 비슷한 값으로 변환한다. 그렇다는 것은 기존에는 평균을 구하는데에 input(x)의 값이 큰 영향을 미쳤다면, **sigmoid**($\\sigma$)에서는 특정 class에 속하는 x의 갯수가 많은 영향을 주는 것을 알 수 있다. 이를 통해서 **sigmoid**($\\sigma$)가 완벽하지는 않지만, **Bayes Decision Rule**을 반영했다는 것을 알 수 있다.\n\n마지막으로, MLE를 통해서 Logistic Regression의 parameter를 추정해보자. (MAP는 기존에 살펴본 Linear Regression과 동일하게 regularizer를 더해주는 방식이기 때문에 생략한다.)\n\n$$\n\\begin{align*}\n\\argmax_{w}\\log{p(\\mathcal{D}|\\bold{w})} \u0026= \\argmax_{w}\\sum_{n=1}^{N}{\\log p(y_{n}|\\bold{x}_{n}, \\bold{w})} \\\\\n\u0026= \\argmax_{w}\\sum_{n=1}^{N}{\\log ({1\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}) } \\\\\n\u0026= \\argmax_{w}\\sum_{n=1}^{N}{-\\log (1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})) } \\\\\n\u0026= \\argmin_{w}\\sum_{n=1}^{N}{\\log (1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})) } \\\\\n\\end{align*}\n$$\n\n## Gradient Descent/Ascent\n\n위의 복잡한 식을 봤으면 알겠지만, 안타깝게도 일반식으로 $\\bold{w}_{MLE}, \\bold{w}_{MAP}$ 등을 구할 수는 없다. 따라서, 우리가 믿을 것은 Gradient를 이용한 방식이다.\n\n\u003e **Gradient Descent**\n\n먼저, 위에서 봤겠지만, Loss는 다음과 같다.\n\n$$\n\\mathcal{L} = \\sum_{n=1}^{N}{\\log(1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n}))}\n$$\n\n이제 이를 미분해서 Gradient를 구하면 다음과 같다.\n\n$$\n\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}) = \\sum_{n=1}^{N}{{{-y_{n}\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}\\bold{x}_{n}}\n$$\n\n따라서, Gradient Descent 방식은 다음과 같이 진행된다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\alpha\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}_{t})\n$$\n\n\u003e **Gradient Ascent**\n\n위의 방식이 가장 일반적이지만, 우리가 sigmoid의 class값으로 $y \\in \\{-1, 1\\}$ 대신 $y \\in \\{0, 1\\}$을 사용했을 경우 다른 식으로도 접근이 가능하다.\n\n이 경우에는 Loss라기 보기 어렵지만, 다른 형태의 optimization 형태가 만들어진다. (여기서 $\\sigma$는 sigmoid 함수를 의미한다.)\n\n$$\n\\argmax_{\\bold{w}} \\sum_{n=1}^{N}y_{n}\\log{\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}) + (1-y_{n})\\log{(1-\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}))} }\n$$\n\n이를 똑같이 미분하여 사용하지만, 반대로 이 경우에는 maximization 이기 때문에 Gradient Ascent를 수행해야 한다. \n\n우선 미분 결과 얻는 Gradient는 다음과 같다.\n\n$$\n\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}) = \\sum_{n=1}^{N}{[y_{n} - \\sigma(\\bold{w}^{\\top}\\bold{x}_{n})]\\bold{x}_{n}}\n$$\n\n굉장히 간단하게 정리가 되어지는 것을 볼 수 있다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\alpha\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}_{t})\n$$\n\n따라서, 아래와 같이 Gradient Ascent를 활용하여 계산하는 것도 충분히 가능하다.\n\n\u003e **Newton Method**\n\n이러한 형태로 넘어오게 되면, 굉장히 많은 연산이 각 update마다 필요하다는 것을 알 수 있다. 따라서, 우리는 이 과정을 축약할 방법을 찾게 된다. 그 아이디어는 바로 gradient를 업데이트 할 때, linear 하게 update하는 것이 아니라 Quadratic하게 update하는 것이다. 이를 위한 방법론이 **Newton Method**이다. 이 방식을 Logistic Regression에 적용하였을 때, 이를 IRLS(Iterative Re-weighted Least Squared) Algorithm 이라고 부른다.\n\n![newton-method](/images/newton-method.jpg)\n\n위 그래프에서 f(x)가 Loss 라고 할 때, 우리는 $x_k$에서 직선형의 gradient를 사용하는 것보다 quadratic 형태를 사용하는 것이 더 빠르게 수렴값을 찾을 수 있다는 것을 알 수 있다.\n\n이를 사용하기 위해서는 다음 2가지에 대한 사전 이해가 필요하다.\n\n- Taylor Series  \n  smooth한 형태를 가진 x에 대한 함수를 x에 대한 급수의 형태로 변환한 것이다. 따라서 이를 식으로 나타내면 다음과 같다.  \n  $T_{\\infin}(x) = \\sum_{k=0}^{\\infin}{f^{(k)}(x_{0})\\over{k\\!}}(x-x_{0})^{k} $  \n  즉, sine 함수와 같은 형태의 그래프도 x의 급수 형태로 변환이 가능하다는 것이다. Newton Method에서는 무한대까지는 사용하지 않고, 대게 K=2까지를 쓴다.\n- Hessian Matrix  \n  특정 함수 $f(\\bold{x})$를 각 feature에 대해서 이중 편미분한 결과를 저장한 행렬이다. 식은 다음과 같다.  \n  $\n  H = \\nabla^{2}f(x) =\n  \\left[\n    \\begin{array}{ccc}\n      \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{1}^{2}} \u0026 \\cdots \u0026 \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{1} \\partial x_{D}} \\\\ \n      \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\n      \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{D} \\partial x_{1}} \u0026 \\cdots \u0026 \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{n}^{2}}\n    \\end{array}\n  \\right]\n  $\n\n이를 이용해서, Newton Method의 결과값을 정리하면 결과는 다음과 같다.\n\n$$\n\\bold{w}^{(k+1)} = \\bold{w}^{(k)} - [\\nabla^{2}\\mathcal{J}(\\bold{w}^{(k)})]^{-1}\\nabla\\mathcal{J}(\\bold{w}^{(k)})\n$$\n\n자 이제 이것을 실제로 Logistic Regression 식에 대입해보자.\n\n$$\n\\begin{align*}\n  \\nabla\\mathcal{J}(w) \u0026= - \\sum_{n=1}^{N}(y_{n}-\\hat{y}_{n})x_{n} \\\\\n  \\nabla^{2}\\mathcal{J}(w) \u0026= \\sum_{n=1}^{N}\\hat{y}_{n}(1-\\hat{y}_{n})\\bold{x}_{n}\\bold{x}_{n}^{\\top}\n\\end{align*}\n$$\n\n여기서, 아래와 같이 변수를 정의하면,\n\n$$\nS = \n  \\begin{bmatrix}\n    \\hat{y}_{1}(1-\\hat{y}_1)  \u0026 \\cdots  \u0026 0                         \\\\\n    \\vdots                    \u0026 \\ddots  \u0026 \\vdots                     \\\\\n    0                         \u0026 \\cdots  \u0026 \\hat{y}_{N}(1-\\hat{y}_N)  \\\\\n  \\end{bmatrix},\n\n\\bold{b} = \n  \\begin{bmatrix}\n    {{y_{1} - \\hat{y}_{1}}\\over{\\hat{y}_{1}(1-\\hat{y}_{1})}} \\\\\n    \\vdots \\\\\n    {{y_{N} - \\hat{y}_{N}}\\over{\\hat{y}_{N}(1-\\hat{y}_{N})}}\n  \\end{bmatrix}\n$$\n\n결과적으로 다음과 같은 형태를 얻을 수 있다.\n\n$$\n\\begin{align*}\n\\bold{w}_{k+1} \u0026= \\bold{w}_{k} + (XS_{k}X^{\\top})^{-1}XS_{k}\\bold{b}_{k} \\\\\n\u0026= (XS_{k}X^{\\top})^{-1}[(XS_{k}X^{\\top})\\bold{w}_{k} + XS_{k}\\bold{b}_{k}] \\\\\n\u0026= (XS_{k}X^{\\top})^{-1}XS_{k}[X^{\\top}\\bold{w}_{k} + \\bold{b}_{k}]\n\\end{align*}\n$$\n\n이는 결코 계산 과정이 단순하다고는 할 수 없지만, 빠르게 수렴할 수 있기 때문에 가치있는 방법이다.\n\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)","slug":"ml-logistic-regression","date":"2022-10-18 09:58","title":"[ML] 3. Logistic Regression","category":"AI","tags":["ML","LogisticRegression","Classification","SigmoidFunction","SoftmaxFunction","NewtonMethod"],"desc":"이전까지 우리는 input data가 들어왔을 때, continuos한 output을 얻는 것을 목표로 했다. 하지만 현실에서는 대게 정확한 수치보다는 특정 분류로 나누는 것이 효과적인 경우가 많다. 예를 들어, spam 필터링, object detection, 등 등. 따라서, 해당 포스팅에서는 classification을 위해서 사용할 수 있는 logistic regression에 대해서 살펴볼 것이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},"relatedPosts":[{"content":"\n## Intro\n\nMachine Learning은 data들로 부터 특정 pattern을 나타내는 function을 만드는 것이라고 할 수 있다. 즉, pattern은 data에 대한 간단한 요약본이라고 볼 수 있다.\n확률/통계 이론 및 선형대수/미적분 관련 기본을 해당 포스팅에 정리한다. 여기서 다루는 내용은 대게 많이 추상적인 내용이며, 키워드 중심의 내용이다. 만약, 추가적인 설명이 필요하다면 키워드를 기반으로 더 검색을 하는 것이 좋을 것이다.\n\n## Probability/Statisics\n\n### Probability Space\n\n확률 공간을 정의하는 것은 확률을 이해하는 토대가 된다. 확률을 적용하기 위한 공간을 먼저 살펴보자.\n\n- Sample Space($\\Omega$)  \n  가능한 모든 결과값의 집합이다.  \n  ex. 동전을 두 번을 던져서 나올 수 있는 모든 결과값은 $\\Omega = $ $\\{ hh, ht, th, tt \\}$\n- Event($E$)  \n  Sample Space의 Subset이다. Sample Space에서 발생할 수 있는 event라는 의미로 볼 수 있다.  \n  ex. 동전을 두 번을 던져서 모두 같은 면이 나오는 Event는 $E = $ $\\{ hh, tt \\}$\n- Field($\\mathcal{F}$)  \n  Sample Space에서 발생 가능한 모든 Event들의 집합이다.  \n  ex 동전을 두 번 던져서 나오는 결과값의 Field는 $\\mathcal{F} = $ $\\{$ $\\emptyset$, $\\{hh\\}$, $\\{ht\\}$, $\\{th\\}$, $\\{tt\\}$, $\\{hh, ht\\}$, $\\{hh, th\\}$, $\\{hh, tt\\}$, $\\{ht, th\\}$, $\\{ht, tt\\}$, $\\{th, tt\\}$, $\\{hh, ht, th\\}$, $\\{hh, ht, tt\\}$, $\\{hh, th, tt\\}$, $\\{ht, th, tt\\}$, $\\{hh, ht, th, tt\\}$ $\\}$\n- $\\sigma$-field  \n  자신 내부의 원소를 포함하는 합집합을 모두 포함하는 셀 수 있는 field를 sigma field라고 한다.  \n  이 $\\sigma$-field는 일반적인 확률과 특정 domain에서의 확률을 정의하는데 필요하다.  \n  우리가 sample space($\\Omega$)와 $\\sigma$-field $\\mathcal{F} \\subset 2^{\\Omega}$가 주어질 때, 확률P가 다음과 같이 mapping한다고 하자. $P: \\mathcal{F} \\mapsto [0, 1]$ 이때 P는 다음과 같은 특징을 가진다.\n  - $A \\in \\mathcal{F}$인 모든 A에 대해서 $P(A) \\leq 0$ 이다.  \n    $P(\\emptyset) = 0, P(\\Omega) = 1$\n  - $\\{A_i\\}_{i \\in I}$이고, 서로 다른 모든 i, j에 대해 $ A_{i}\\cup A_{j} = \\emptyset$이라면, 아래 식을 만족한다.  \n    $$P(\\cup_{i \\in I}A_i) = \\sum_{i \\in I}P(A_i)$$\n\n### Important properties of Probability\n\n- **Joint Probability**  \n  두 Event의 Joint Probability는 두 Event의 합집합의 확률을 의미한다.\n  $P(A, B) = P(A \\cap B)$\n- **Marginal Probability**  \n  대게 두 개 이상의 Event가 있을 때, 각 각의 Event의 확률을 특정할 때 사용한다.\n  $P(A), P(B)$\n- **Independence**  \n  두 Event가 독립이라는 의미는 서로의 Event가 서로 영향을 받지 않는다는 의미이다. 주의할 것은 이것이 의미하는 것이 두 Event의 교집합이 없다는 의미가 아니다.  \n  만약, 우리가 위에서 예시로 사용한 두 개의 동전을 던진 결과를 보자. 두 개의 동전이 모두 앞면이 나오는 경우와 모두 뒷면이 나오는 경우는 서로 독립일까? 이는 독립이 아니다. 왜냐하면, 동전이 모두 앞면이 나오는 사건은 필연적으로 모두 뒷면이 나오는 사건은 반드시 일어나지 않을 것이라는 증거가 되기 때문이다. 반대로, 모두 앞면이 나오는 사건과 한 번만 앞면이 나오는 사건을 생각해보자. 하나의 사건이 일어났다고, 반드시 그 사건이 일어났거나 안일어났다는 관계를 밝혀낼 수 없다. 따라서, 이러한 경우 두 사건이 독립적이라고 한다.  \n  이를 확률적으로 표현하면, 다음과 같이 표현할 수 있다.  \n  $P(A, B)=P(A)P(B)$\n- **Conditional Probability**  \n  두 Event가 있을 때, 하나의 Event가 발생했을 때 다른 하나의 Event가 발생할 확률을 의미한다. 따라서, 이는 다음과 같이 수식으로 표현할 수 있다.  \n  $P(A|B) = {{P(A, B)}\\over{P(B)}}, (P(B) \\neq 0)$  \n  여기서 independence 특성을 더 명확하게 확인할 수 있는데, 만약 A와 B가 독립이라면,  \n  $P(A|B) = P(A)$  \n  즉, B가 발생했는지 여부는 A의 결과에 영향을 안준다는 것이다.\n- **Partition**  \n  Sample Space($\\Omega$)를 겹치지 않고, 모두 포함하는 Event의 집합을 의미한다. 따라서, 이를 식으로 다음과 같이 표현할 수 있다.  \n  $\\cup_{i=1}^{n}{P_i} = \\Omega$ 이고, $\\cap_{i=1}^{n}{P_i} = \\emptyset$\n- **Marginalization**  \n  전체 Sample space($\\Omega$)에 대하여 **B**가 이에 대한 partition일 때, 아래 공식이 성립한다.  \n  $P(A) = \\sum_{i=1}^{n}{P(A,B_i)} = \\sum_{i=1}^{n}{P(A|B_i)P(B_i)}$\n- **Bayes' Theorem**  \n  만약 $P(B) \\neq 0$라면, 아래 공식이 성립한다. 간단히 conditional probability를 풀어주면 아래 식을 얻을 수 있다.  \n  $P(A|B) = {P(B|A)P(A)\\over{P(B)}}$  \n  해당 식은 단순히 Joint Probability로 변환하고, 다시 반대 확률로 변경했을 뿐이다. 이 공식이 중요하다기 보다는 이 공식이 가지는 의미를 이해하는 것이 중요하다. 경험에 기반하는 Frequentist Approach에서는 관측을 통해서 특정 데이터가 발생할 확률을 얻는다. 만약 우리가 원하는 확률이 관측을 통해서는 얻을 수 없는 데이터라고 하자. 이 경우에 우리는 확률의 역연산이 필요하다. 위의 공식을 보면 특이한 것이 보이는데, 바로 $P(A|B)$와 $P(A)$이다. 이는 전체 확률을 통해서 Conditional Probability를 찾는 것이다. 그렇기에 우리는 이를 역연산이라고 부르며, 우리가 가지고 있는 기존 사전 확률(Priority, 이전까지 맞을 거라고 생각한 확률)을 통해서 데이터가 주어졌을 때의 사건의 확률을 다시 계산해보는 것이다. 이 과정을 Bayesian Update라고 하는데 이 과정을 통해서 얻은 P(A|B)를 다시 다음 데이터에 대해서는 P(A)로써 활용하는 것이다. 이렇게 해서 우리는 점진적으로 P(A)를 찾아나갈 수 있다.\n\n### Random Variable\n\nRandom Variable이라는 것은 특정 사건을 수학적으로 표현하기 위해서 변형하는 과정을 의미한다. 우리는 이전 예시에서 두 개의 동전을 동시에 던져서 나온 결과를 Sample Space로 두었고, 이를 $\\Omega = $ $\\{ hh, ht, th, tt \\}$라고 표현했다. 하지만, 이와 같은 표기 방식은 수학적인 연산을 적용하기 어렵다. 따라서, 우리는 앞면이 나온 경우를 $X=1$, 뒷면이 나온 경우를 $X=-1$ 라고 하는 형태로 치환하는 것이다. 여기서 만들어진 X를 우리는 Random Variable이라고 부른다. 이런 치환을 통해서 우리는 확률을 Random Variable에 대한 함수로 표현할 수 있다.\n\n또, Random Variable을 정의하여 다음과 같은 값을 연속적으로 정의할 수 있다.\n\n- **Mean**  \n  Random Variable의 평균 또는 기댓값이라고 부른다.  \n  $\\mu_{X} = E[X] = \\sum_{x}{xP(X=x)}$\n- **Variance**  \n  평균에서 데이터가 떨어진 정도를 표현하는 값으로 분산이라고 부른다.  \n  $\\sigma_{X}^{2} = E[(X-\\mu_X)^2] = E[X^2] -\\mu_{X}^{2}$\n- **Covariance**  \n  Random Variable X와 Y의 상관관계(Correlation)을 확인하는 척도로 사용한다.  \n  $\\sigma_{X}^{2} = E[(X-\\mu_X)^2] = E[X^2] -\\mu_{X}^{2}$  \n  만약, 두 X와 Y가 서로 전혀 상관이 없다(Independent)면, $cov(X, Y) = 0$이다.\n- **Correlation Coefficient**  \n  Covariance보다 더 엄격한 상관관계를 확인하는 척도로 사용되는데, 단순히 Covariance를 각 표준편차($\\sigma$)로 나눈 것이다. 이로 인해 결과 값은 [-1, 1] 사이 값이 된다.  \n  $corr(X, Y) = {cov(X,Y)\\over{\\sigma_{X}\\sigma_{Y}}}$  \n  주의할 점은 Correlation Coefficient가 1이라고 X가 Y의 원인이 되는 것은 아니라는 것을 유의해야 한다. 단순히 X가 일어났을 때, Y가 일어날 확률이 높다는 것이다.  \n\n### Law of Large Numbers\n\n경험적 확률과 수학적 확률 사이의 관계를 나타내는 법칙으로, 전체 경우의 수와 이에 따른 확률(모집단)에서 표본(관측한 경우의 수와 이에 따른 확률)의 크기가 커질 수록 표본 평균이 모평균에 가까워짐을 의미한다.\n\n### 자주 사용되는 Probability Distribution Function\n\n- **Bernoulli distribution**  \n  하나의 사건이 일어날 확률을 의미한다. 발생하는 경우를 X=1, 그렇지 않은 경우를 X=0으로 random variable로 치환하여 나타낸 확률 분포(probability distribution)이다.  \n  따라서, 사건이 일어날 확률을 p라고 할 때, 다음과 같이 random variable에 대한 확률을 정의할 수 있다.  \n  $P(X=x) = p^{x}(1-p)^{1-x}$ \n  복잡해보이지만, 실상은 X가 0 또는 1이므로, $P(X=0)=1-p$이고, $P(X=1)=p$이다.\n  - 평균      \n    $E[X] = p$\n  - 분산  \n    $Var[X] = E[X^2] - \\mu_{X}^2 = p - p^2 = p(1-p)$\n- **Binomial Distribution**  \n  확률이 p인 사건을 n번 수행했을 때, x번 발생할 확률을 의미한다. 따라서, random variable X의 범위는 {0, 1, …, n}이 된다.  \n  이에 따라 random variable에 대한 확률을 정의하면 다음과 같다.  \n  $P(X=x) = {n \\choose x}p^x(1-p)^{n-x}$  \n  이 또한 복잡해 보이지만, 사실은 독립적인 Bernoulli의 연속 수행으로 볼 수 있다.  \n  - 평균  \n    $E[X] = np$\n  - 분산  \n    $Var[X] = Var[\\sum_{i}X_i]=\\sum_iVar[X_i]=np(1-p)$\n- **Beta Distribution**  \n  $\\alpha, \\beta \u003e 0$를 만족하는 두 parameter를 이용한 probability distribution이다.  \n  이는 [0, 1]에서 continuous한 random variable를 이용할 수 있다. 이에 따른 확률은 다음과 같다.  \n  $P(X=x) \\propto x^{\\alpha-1}(1-x)^{\\beta-1}$  \n  이에 대한 의미를 이해하자면, 확률에 대한 확률 분포이다. 각 $\\alpha - 1$와 $\\beta - 1$를 성공 횟수, 실패 횟수라고 하자.  이는 이미 알고 있는 모집단(전체 집합)의 계산 결과이다. 그리고 random variable을 특정 event의 확률이라고 하자. 예를들면, 동전 던지기를 할 때, 앞면이 나올 확률이 $1\\over2$이라는 것을 이미 알고 있다. 따라서, 우리는 $\\alpha - 1$ = $\\beta - 1$ 라는 것을 알고 있는 것이다. 하지만, 실제로 동전 던지기를 5번 수행했을 때, 4번 앞면이 나왔다고 하자. 그렇다면, 우리가 추측한 해당 event의 확률은 $4\\over5$이 된다. 그렇다면, 실제로 해당 확률이 $4\\over5$일 확률을 얼마나 될까?  \n  이를 측정하기 위한 것이 Beta distribution인 것이다. 이에 따라, Beta distribution을 PDF로 표현하면 ${\\alpha\\over\\alpha+\\beta}$에서 높은 확률값을 가지는 것을 볼 수 있다.\n  - 평균  \n    $E[X] = {\\alpha\\over{\\alpha+\\beta}}$\n  - 분산  \n    $Var[X] = {\\alpha\\beta\\over{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}}$\n  \n  위의 평균과 분산을 보면 알 수 있듯이, 만약 이전 모집단에서의 평균값에 대한 믿음이 크다면, 각각  $\\alpha, \\beta$의 비율은 유지하면서 상수배를 수행하여 평균은 동일하지만 분산 값을 더 적게 만들어 뾰족한 형태의 분포를 완성할 수도 있다. 이 경우에는 평균과 맞지 않는 표본집합에서의 평균을 굉장히 확률이 낮은 확률로 식별하는 것이다.\n- **Gaussian Distribution**  \n  $\\mu, \\sigma^2$를 parameter로 갖는 probability distribution이다.\n    \n  이는 $[-\\infin, \\infin]$를 구간으로 continuous한 random varible을 이용한다. 이에 따른 확률은 다음과 같다. (단일 random variable인 경우)\n  \n  $P(X) = {1\\over{\\sqrt{2\\pi\\sigma^2}}}\\exp(-{1\\over{2\\sigma^2}}(X-\\mu)^2)$\n\n  이것이 가지는 의미는 다소 복잡하다. (Lindeberg-Levy) Central Limit Theoriem에 따르면, 표본에서 얻은 표본 평균을 구하면 구할 수록 점점 Gaussian Distribution을 따라간다. 즉, $n \\rarr \\infin$이면, 표본 평균이 이루는 분포가 Gaussian이라는 것이다.\n  \n  추가적으로 알아볼 것은 바로 여러 개의 Random Variable로 Gaussian Distribution을 더 높은 차원으로 구성할 수 있다는 것이다. 이를 수행하고, 해당 Random Variable 간의 Covarince, Correlation coefficient를 구하면, 이 변수 간의 상관성을 얻을 수 있다.\n  \n  - 유의 사항 : correlation은 어디까지나 상관성이다. correlation이 높다고 해당 Random Variable이 상관성이 높은 Random Variable 발생의 원인이 될 수는 없다.\n\n## Calculus\n\n일명 미적분학으로, 대게의 미적분 법칙은 모두 알고 있을 것이라고 생각하고 넘어간다.\n\n### Optimization\n\n정말 모두가 알고 있을 거 같지만, 그럼에도 중요하기 때문에 정리하고 넘어가자. 일반적으로 Optimization이란 최적값을 찾는 과정이다. 이 과정에서 대게 사용되는 것이 최솟값 또는 최댓값이다. 우리는 최솟값/최댓값을 미분을 통해서 구할 수 있다.\n\n\u003e **최대/최소 구하기**\n\n모두가 알다시피 함수의 미분은 기울기를 의미한다. 만약 함수의 미분에 특정 값을 대입할 경우 이는 그 지점에서의 기울기를 의미한다.\n\n먼저, 함수의 미분에 대입한 값이 0인 경우에 해당 값(극값)이 가지는 성질을 기억해야 한다. 특정 지점에서 기울기가 0이라는 것은 만약 해당 함수가 직선이 smooth한 경우, 즉 연속인 경우 해당 함수의 기울기의 부호가 바뀌었다는 의미로 받아들일 수 있다.\n\n여기서 다시 한 번 함수의 기울기의 부호가 바뀌었다는 의미를 살펴보아야 한다. 이는 함수의 값이 구간 내에서 가장 작은 값(극소) 또는 구간 내에서 가장 큰 값(극대)라는 것을 의미한다. 왜냐하면, 직관적으로 기울기가 0이 되기 전까지는 계속해서 증가/감소해왔다는 것을 알기 때문이다. 따라서, 우리는 기울기가 0인 지점을 모두 찾으면, 그 안에서 최대/최소를 찾을 수 있을 것이다.\n\n그런데 어떻게 하면, 기울기가 0인 지점이 극대인지 극소인지를 구분할 수 있을까? 이것은 바로 직전의 값을 미분 함수에 대입해보면 쉽게 알 수 있다. 하지만, 이것이 매번 그렇게 쉽게 판별되는 것은 아니다. 따라서, 우리는 이중 미분을 사용한다. 이중 미분 함수에 극값을 대입했을 때 양수라면 이는 극소를 의미하고, 음수인 경우는 극대를 의미한다. 이 또한, 직관적으로 기울기의 변화량이라는 이중 미분의 정의를 알면, 직관적으로 와닿을 수 있다.\n\n이렇게 해서 극대와 극소를 골라내고, 이중에서 가장 큰 값과 가장 작은 값을 찾아내면, 우리는 이것을 함수의 최적화를 수행했다고 한다.\n\n### Constraint Optimization\n\n여기서는 특별한 case를 위한 예시이다. 특정 조건이 주어졌을 때, 이를 만족하면서 특정 함수의 optimization을 수행하는 것이다.\n\n그러면 우리가 최적화하고자 하는 목적함수($\\mathcal{J}(\\bold{x})$)와 등식 제약 조건($h_{j}(\\bold{x})$), 부등식 제약 조건($g_{i}(\\bold{x})$)을 살펴보자.\n\n우리는 모든 최적화 문제를 다음과 같은 형태로 묘사할 수 있다.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 \\mathcal{J}(\\bold{x}) \u0026\\\\\n  \\text{subject to} \\quad \u0026 g_{i}(\\bold{x}) \\leq 0, \u0026 i = 1, ..., M \\\\\n                          \u0026 h_{j}(\\bold{x}) = 0, \u0026 j = 1, ..., L\n\\end{align*}\n$$\n\nmaximization인 경우는 음수를 취해서 결과를 구한 후 변환 시에 다시 음수를 취해주면 된다. 그리고 부등호가 반대인 제약 조건인 경우에도 양변에 음수를 취해서 간단하게 뒤집는 것이 가능하다.\n\n이러한 문제를 풀기 위해서는 우리는 식을 **Lagrangian** 형태로 변환해야 한다.\n\n\u003e **Lagrangian**\n\n이는 조건부식에 있는 조건에 변수($\\nu$, $\\lambda$)를 곱한 값의 합과 원래 목적 함수($\\mathcal{J}(\\bold{x})$)의 합이다.\n\n$$\n\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) = \\mathcal{J}(\\bold{x}) + \\sum_{i=1}^{M}{\\nu_{i}g_{i}(\\bold{x})} + \\sum_{j=1}^{K}{\\lambda_{j}h_{j}(\\bold{x})}\n$$\n\n여기서 **Lagrangian** 함수의 optimization이 곧 목적함수의 optimization이다. 증명은 수행하지 않는다. 이에 대한 추가적인 설명이 필요한 경우에는 직접 찾아보아야할 것이다.  \n여기서 만약 등식만 있는 식인 경우에 우리는 간단히 모든 변수에 대해서 편미분이 0이 되는 등식을 이용해서, 최적 $\\bold{x}$를 찾을 수 있다. 위에 식에서 부등식 조건($g_{i}(\\bold{x})$)이 사라진다면, 우리는 미분을 통해서 처리해야하는 값은 총 x의 크기(N)와 L이다. 이는 우리가 편미분해서 구할 수 있는 식의 갯수와 똑같다. 즉, 우리가 모르는 변수는 N+M개 우리가 가진 등식은 N+M개이므로 연립해서 각 값을 구할 수 있는 것이다.\n\n하지만, 부등식인 경우에는 추가적으로 고려해줘야할 것이 있다.\n\n\u003e **KKT Condition**\n\n이는 우리가 최적값($\\bold{x}_{*}$)를 찾았을 때, 다음과 같은 $\\boldsymbol{\\nu_{*}}$, $\\boldsymbol{\\lambda_{*}}$가 존재해야 한다는 정리이다.\n\n1. Optimality  \n   $\\nabla\\mathcal{L} = \\nabla\\mathcal{J}(\\bold{x}_{*}) + \\sum_{i=1}^{M}{\\nu_{*(i)}\\nabla g_{i}(\\bold{x}_{*})} + \\sum_{j=1}^{L}{\\lambda_{*(j)}\\nabla h_{j}(\\bold{x}_{*})} = 0$  \n   위에서 보았던 최적화를 수행하는 식이다.\n2. Feasibility  \n   $g_{i}(\\bold{x}_{*}) \\leq 0,  i = 1, ..., M$  \n   $h_{j}(\\bold{x}_{*}) = 0,  j = 1, ..., L$  \n   조건이 만족하는지를 확인하는 것이다.\n3. Complementary slackness  \n   $\\nu_{*(i)}g_{i}(\\bold{x}_{*}) = 0, i = 1, ..., M\\quad(\\nu_{*(i)} \\geq 0)$  \n   위의 식은 다소 헷갈릴 수 있는데 가장 알아듣기 쉬운 형태는 아래이다.  \n   $g_{i}(\\bold{x}_{*}) \\lt 0\\text{, then } \\nu_{*(i)} = 0$  \n   $g_{i}(\\bold{x}_{*}) = 0\\text{, then } \\nu_{*(i)} \u003e 0$\n\n위의 식을 만족하는 $\\bold{x}_{*}$, $\\boldsymbol{\\nu_{*}}$, $\\boldsymbol{\\lambda_{*}}$를 찾으면, 그것이 최적값에서의 $\\bold{x}_{*}$이다.\n\n\u003e **Lagrangian Dual Problem**\n\n여기서 한 발 더 나아가면, Lagrangian에 다시 한번 Lagrangian을 취할 수 있다.\n\n우리가 만약 Lagrangian의 하한을 $\\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda})$이라 하고, \n\n$$\n\\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) = \\inf_{\\bold{x} \\in \\mathcal{X}}\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda})\n$$\n\n$\\boldsymbol{f}^{*}$을 최적값이라고 한다면, 아래 식이 성립한다.\n\n$$\n\\boldsymbol{f}^{*} \\geq \\min_{\\bold{x}}\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) := \\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda})\n$$\n\n따라서, 우리는 $\\mathcal{G}$의 최댓값을 찾으면 해당 값이 최적해에 근사한다는 것을 알 수 있다.\n\n이는 우리가 풀고자 하는 문제의 형식을 다시 한 번 바꾸게 된다.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 \\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\boldsymbol{\\nu}_{i} \\geq 0, \u0026 i = 1, ..., M\n\\end{align*}\n$$\n\n이 식을 KKT condition을 이용하여 푸는 것이 기존 식보다 쉽게 풀 수 있는 경우가 많다. 따라서, 이러한 형태로 문제를 풀이할 수도 있다.\n\n## Information Theory\n\n### Entropy\n\n수학에서 정보의 불확실성(uncertainty)을 표현하기 위해서 물리의 entropy 라는 개념을 도입한 것이다. 즉 정보가 가지는 \"surprise\" 정도가 크다면, entropy가 큰 정보를 의미하고, 일반적인 정보라면 이는 entropy가 작은 정보인 것이다.\n\n수학적으로 다시 정의하자면, 다음과 같다.  \nsample space $\\Omega$에서 정의된 random variable $X$와 확률 $p_{X}(x)$이 주어질 때, Entropy를 $H(x)$라 하자.\n\n$$\nH(X) = -\\sum_{x \\in \\Omega}p(x)\\log_{2}p(x)\n$$\n\n위 식에서 log의 밑이 2인 것을 알 수 있는데 computer science에서는 정보가 bit단위로 저장되기 때문에 기본적으로는 2를 사용한다. 하지만, 상황에 따라서는 다른 밑을 사용할 수도 있다.\n\n헷갈릴 수 있는데 표기법이 굉장히 다양하니 유의해서 보도록 하자.\n\n$$\nH(X) = H_{p}(X) = H(p) = H_{X}(p) = H(p_{X})\n$$\n\n\u003e **최댓값과 최솟값**\n\nEntropy는 정보의 불확실성을 나타낸다고 했다. 즉, 정보가 확실할 수록 Entrophy는 0으로 수렴하며, 확실히 아는 정보의 경우 Entropy가 최솟값인 0이 된다.  \n반대로 Entropy의 최댓값의 경우 $|\\Omega| = n$이라고 할 때, $\\log_{2}{n}$이다. 이는 uniform distribution(모든 Random Variable의 확률이 같은 분포)일 경우이다.\n\n$$\n0 \\leq H(x) \\leq log_{2}{|\\Omega|}\n$$\n\n\u003e **$\\bold{\\log_{2}({1 \\over p_{X}(x)})}$의 평균**\n\nEntropy를 random variable x의 확률의 역수의 log를 취한 값으로 해석할 수도 있다.\n\n$$\n\\begin{align*}\nE[\\log_{2}(({1 \\over p_{X}(x)})] \u0026= \\sum_{x \\in X}p_{X}(x)\\log_{2}({1 \\over p_{X}(x)}) \\\\\n\u0026= -\\sum_{x \\in X}p_{X}(x)\\log_{2}(p_{X}(x)) \\\\\n\u0026= H(p_{X})\n\\end{align*}\n$$\n\n여기서 우리는 $\\log_{2}({1 \\over p_{X}(x)})$을 **정보량**이라고 정의한다. 식에서도 알 수 있지만, 정보량과 해당 정보량을 얻을 확률은 반비례한다.\n\n\u003e **Joint, Conditional Entropy**\n\nRandom Variable이 두 개 이상일 때, 이를 적용할 수 있다. 유도 과정은 $H(X)$가 Expectation과 어떤 관계였는지를 떠올려 보면 알 수 있다.\n\n- **Joint Entropy** : $H(X, Y) = -\\sum_{x \\in \\Omega_{X}}\\sum_{y \\in \\Omega_{Y}}p(x, y)\\log_{2}p(x, y)$\n- **Conditional Entropy** : $H(Y|X) = -\\sum_{x \\in \\Omega_{X}}\\sum_{y \\in \\Omega_{Y}}p(x, y)\\log_{2}p(y|x)$\n\n\u003e **properties**\n\n1. Chain Rule  \n   $H(X, Y) = H(Y|X) + H(X)$  \n   $H(X, Y) = H(X|Y) + H(Y)$\n2. Conditional Entropy's Maximum  \n   $H(Y|X) \\leq H(Y)$  \n   독립일 때 같아지며 그 외에는 항상 Conditional의 Entropy가 더 낮다. 의미를 이해하면 쉽다. 한 마디로 X라는 정보가 Y라는 정보가 발생할 확률에 대한 티끌이라도의 힌트를 준다면, 해당 불확실성은 감소하게 되는 것이다.\n3. Joint Entropy's Maximum  \n   $H(X, Y) \\leq H(X) + H(Y)$  \n   동일하게 독립일 때 같아지며, 그 외에는 항상 Joint의 Entropy가 더 낮다. 이 또한, 두 사건이 티끌이라도 겹치는 Event가 있다면, 각 Entropy를 더하는 것보다 당연히 작을 수 밖에 없는 것이다.\n4. Concave  \n   Entropy의 그래프는 항상 concave하다. \n\n\u003e **Coding**\n\n정보 이론이 활발하게 사용되는 예시 중에 하나가 바로 데이터의 Encoding/Decoding을 수행하여 bit data로 mapping할 때이다. variable length encoding을 알고 있다면, 이에 대한 이해가 쉬울 것이다. 쉽게 설명하면, 데이터를 bit sequence로 mapping할 때 모든 데이터에게 동일한 bit sequence의 길이만큼을 할당하는 게 아니라 빈도가 높은 데이터부터 짧은 bit sequence 길이를 할당하는 방식이다. 이때 bit sequence의 길이를 Entropy를 이용해서 구할 수 있다. 이 길이는 항상 해당 데이터의 Entropy보다는 커야 한다. 따라서, 해당 Entropy보다 큰 가장 작은 자연수가 해당 데이터의 Bit Sequence 길이의 최적값이다.\n\n### KL divergence(Relative Entropy)\n\nKullback-Leibler Divergence의 약자로, 우리가 구하고자하는 실제 확률(p)과 추측 확률(q) 사이의 오차를 계산할 때 사용하는 지표이다. 따라서, 동일한 Sample Space와 Random Variable에 대한 서로 다른 확률 분포를 비교한다고 생각할 수 있다.\n\n$$\nD(p||q) = KL(p||q) = \\sum_{x \\in \\Omega}p(x)\\log_{2}(p(x)/q(x)) = E_{p}[\\log_{2}({p(x) \\over q(x)})]\n$$\n\n아쉽게도 KL divergence는 거리와 같은 연산을 적용할 수 없다. 즉, 둘 사이의 역연산은 같지 않을 뿐만 아니라($D(p||q) \\neq D(q||p)$), 서로 다른 Random Variable의 KL divergence의 합이 Random Variable의 합의 KL divergence와는 다르다. \n\n### Mutual Information\n\nKL divergence를 활용하여 서로 다른 Random Variable X,Y의 연관성을 유추하는 것도 가능하다.\n\n$$\nI(X, Y) = D(p(x,y) || p(x)p(y))\n$$\n\n$I$값이 의미하는 것은 Y를 아는 것이 X의 추측을 얼마나 쉽게하는지에 대한 지표로 작용한다.\n\n증명 과정은 생략하지만, 위의 식을 정리하면 결국은 아래와 같아지기 때문이다.\n\n$$\n\\begin{align*}\n  I(X, Y) \u0026= H(X) - H(X|Y) \\\\\n  \u0026= H(Y) - H(Y|X)\n\\end{align*}\n$$\n\n### Cross Entropy\n\n우리가 특정 corpus를 통해서 확률을 추정했다고 해보자. 그렇다면, 우리는 이 가설을 증명하기 위해서 다른 data에 대해서 해당 추측이 적절한지를 확인하여 정당성을 증명할 수 있다. 그러기 위해서 우리가 만든 확률에서 정보량을 추출하고, 이를 우리가 알고 있는 data의 공간에 넣었을 때의 확률을 추정하기 위해서 Cross Entropy를 사용할 수 있다.\n\n$$\nH_{q}(p) = - \\sum_{x \\in \\Omega}q(x)\\log_{2}p(x)\n$$\n\n간단하게 요약하면, 위 식은 틀릴 수 있는 정보를 갖고 구한 최적의 Entropy로, 정보량에 추측 정보량을 넣고, 확률에는 실제 확률을 넣는 방식이다.\n\n또한, 이는 다음과 같이 변형되기도 한다.\n\n$$\nH_{q}(p) = H(q) + D(q || p)\n$$\n\n또한, 특정 조건이 주어졌을 때의 Conditional Cross Entropy는 다음과 같다.\n\n$$\nH_{q}(p) = - \\sum_{y \\in \\Omega_{Y}}\\sum_{x \\in \\Omega_{X}}q(y,x)\\log_{2}p(y|x)\n$$\n\n하지만, 실제 구현 level에서는 이러한 Cross Entropy를 정석으로 구하는 것은 비용이 크다. 따라서, 이와 근사하는 식을 사용한다.\n\n$$\n\\begin{align*}\n  H_{q}(p) \u0026= - \\sum_{y \\in \\Omega_{Y}}\\sum_{x \\in \\Omega_{X}}q(y,x)\\log_{2}p(y|x) \\\\\n  \u0026= {1\\over{|T_{Y}|}}\\sum_{i=1..|T_{Y}|}{\\log_{2}p(y_{i}|x_{i})}\n\\end{align*}\n$$\n\n위 식을 이용할 때에는 대신 실제 데이터와 비교에는 사용되어서는 안된다. 대신 두 개의 서로 다른 p,q가 있을 때, s라는 실제 분포에 어떤 것이 더 적절한지를 판명할 때 사용할 수 있다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"ml-base-knowledge","date":"2022-10-14 19:28","title":"[ML] 0. Base Knowledge","category":"AI","tags":["ML","Probability","Calculus","InformationTheory"],"desc":"Machine Learning은 data들로 부터 특정 pattern을 나타내는 function을 만드는 것이라고 할 수 있다. 즉, pattern은 data에 대한 간단한 요약본이라고 볼 수 있다.확률/통계 이론 및 선형대수/미적분 관련 기본을 해당 포스팅에 정리한다. 여기서 다루는 내용은 대게 많이 추상적인 내용이며, 키워드 중심의 내용이다. 만약, 추가적인 설명이 필요하다면 키워드를 기반으로 더 검색을 하는 것이 좋을 것이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nMachine Learning은 특정 목표를 달성하기 위해서 데이터로 부터 pattern 또는 가정 등을 유도해내는 방법이다.\n이를 위한 가장 일반적인 방법은 여러 개의 확률분포와 이것의 parameter의 조합(probabilistic model)들 중에서 측정된 데이터들을 가장 잘 나타내는 하나를 찾아내는 것이다.\n그 중에서, 확률 분포를 결정한 상태에서 parameter를 찾아나가는 형태의 접근법을 우리는 Parametric Estimation이라고 한다. 그 외에도 Nonparametric, Semi-parametric 방식도 존재하지만 이는 여기서는 다루지 않는다.\n\n## Small Example\n\n간단한 예시를 통해서 Parametric Estimation의 흐름을 익혀보자.\n\n한 학급에서 학생들의 형제자매 수에 대한 예측을 하고 싶다고 하자.  \n그렇다면, 우리는 먼저 조사(관측)를 수행해야 한다. 이를 통해서 다음과 같은 데이터를 얻게 되었다고 하자.\n\n| x        | 1    | 2    | 3    | 4    | 5    | 6    | x$\\geq$7 |\n| :------- | :--- | :--- | :--- | :--- | :--- | :--- | :------- |\n| $p(X=x)$ | 17   | 59   | 15   | 6    | 2    | 0    | 1        |\n\n여기서 우리는 여러 사전 지식을 활용하여 해당 데이터를 보았을 때, 해당 분포가 Poisson 분포의 형태라는 것을 알 수 있다.  \n따라서, 우리는 해당 분포를 Poisson이라고 가정한 다음에는 단순히 해당 분포에 대입하며, 가장 적절한 parameter만 찾으면 된다.  \n\n이 과정과 단순히 각 x에서의 확률값을 구하는 방식이랑 무엇이 다른지를 알아야지 해당 과정의 의의를 알 수 있다.\n먼저, 우리가 하고자 하는 일이 형제자매의 평균 수를 구한다고 하자. 이때의 평균 값과 Poisson 분포에서의 확률값은 다를 수 밖에 없다.\n\n이렇게 확률 분포를 구하는 것의 의미는 이것말고도 보지 않은 데이터(unseen data)를 처리함에 있다. 우리가 만약 모든 가능한 경우의 수를 모두 알고 있고, 이를 저장할 공간이 충분하다면,\n이러한 확률 분포를 구할 필요가 없다. 하지만, 우리가 원하는 추측은 unseen data에 대해서도 그럴사해야 한다. 이를 위해서는 결국 확률 분포가 필요하다.\n\n위의 예시에서 만약, 형제자매가 3명인 경우의 데이터가 없다고 하자. 이 경우에도 확률분포를 통한 추측을 한다면, 우리는 유의미한 값을 구할 수 있는 것이다.\n\n## Parametric Estimation\n\n\u003e **정의**\n\nsample space $\\Omega$에서 통계 실험의 관측 결과를 통해서 얻은 sample $X_1$, $X_2$, ... , $X_n$이 있다고 하자. 각 sample에 대한 확률 분포를 우리는 $p_\\theta$라고 한다.\n여기서 $\\theta$는 특정 확률 분포에서의 parameter를 의미한다. 만약, bernoulli 라면, 단일 시행에 대한 확률이 될 것이고, binomial이라면, 단일 시행의 확률과 횟수가 해당 값이 될 것이다.\n\n\u003e **성능 평가**\n\n여기서 우리가 찾기를 원하는 것은 전체 sample space $\\Omega$를 모두 잘 표현할 수 있는 $\\theta_{*}$(실제 true $\\theta$)를 찾는 것이다.(이미 확률 분포의 형태(함수, ex. Bernoulli, Binomial)는 이미 정의되어 있다.)  \n그렇다면, 실제 $\\theta_*$와 추측을 통해 만든 $\\hat{\\theta}$ 사이의 비교를 위한 지표도 필요할 것이다. 즉, 우리가 만든 확률 분포의 예측 성능평가가 필요하다는 것이다. 이를 측정하기 위해서 우리는 **Risk**라는 것을 사용한다.  \n간단하게도 실제 $\\theta_*$와 $\\hat{\\theta}$의 Mean Square Error를 계산한다.\n\n$$ \n\\begin{align*}\nRisk \u0026= E[(\\hat{\\theta} - \\theta_*)^2] = E[\\hat{\\theta}^2 - 2\\hat{\\theta}\\theta_* + \\theta_*^2] \\\\\n\u0026= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 \\\\\n\u0026= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 + (E^2[\\hat{\\theta}] - E^2[\\hat{\\theta}]) \\\\\n\u0026= (E[\\hat{\\theta}] - \\theta_*)^2 + E[\\hat{\\theta}^2] - E^2[\\hat{\\theta}] \\\\\n\u0026= {Bias}^2 + Var[\\hat{\\theta}]\n\\end{align*}\n$$\n\n해당 식을 분석해보면, 이와 같은 의미로 해석하는 것이 가능하다. 우리가 특정 확률 분포의 파라미터를 단 하나로 단정하고 Risk를 계산하는 경우는 Variance 값은 0이다. 즉, 해당 확률 분포가 가지는 Risk는 단순히 해당 parameter와 실제 parameter가 얼마나 찾이가 나는가를 의미한다.\n\n하지만, parameter를 특정하지 않고, 범위로 지정한다면, (예를 들어, 주사위를 던져 3이 나올 확률은 1/6 ~ 1/3이다.) 해당 확률의 평균과 Variance가 영향을 미칠 것이다.  \n다소 처음에는 헷갈릴 수 있지만, 해당 식에서 평균이 의미는 잘 확인하자. 특정 확률 분포를 가지도록 하는 $\\theta$가 $\\theta_*$ 에 얼마나 근접한지를 확인하기 위한 식이라는 것을 다시 한 번 기억하자.\n\n\u003e **Estimation**\n\n이제부터는 앞에서 살펴보았던, parameteric estimation에서 어떻게 $\\hat{\\theta}$를 구할 수 있는지를 다룰 것이다. 확률/통계 이론에서는 크게 3가지로 나눌 수 있다고 볼 수 있다. 각 각을 살펴보도록 하자.\n\n\u003cmark\u003e**1. MLE**\u003c/mark\u003e\n\nMaximum Likelihood Estimation의 약자이다. 여기서, Likelihood는 가능성이라는 뜻을 가지며, 확률/통계 이론에서 이는 확률을 해당 사건이 발생할 가능성으로 해석하는 것이다. 이를 이용해서 우리가 풀고자 하는 문제, 우리가 추측한 $\\theta$가 우리가 가진 Dataset를 만족시킬 가능성을 확인하기 위해 사용한다. 아래 수식을 보자.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta;\\mathcal{D}) \u0026= p(\\mathcal{D}|\\theta) = p(x_1, x_2, ..., x_n|\\theta) \\\\\n\u0026= \\prod_{i=1}^{n}{p(x_i|\\theta)}\n\\end{align*}\n$$\n\n(위 식을 이해하려면, 먼저 Dataset의 각 data들은 서로 independent하다는 사실을 기억하자.)  \n결국 $\\theta$가 주어졌을 때, Dataset일 확률을 구하는 것이다. 이를 다시 생각하면, $\\theta$가 얼마나 데이터셋의 확률을 잘 표현할 수 있는가와 같다.\n\n이것을 직관적으로 이해하려면 하나의 예시를 보면 좋다.\n\n![MLE example](/images/MLE-example.png)\n\n첫 번째 그래프는 같은 가우시안 분포 함수를 쓰면서, parameter만 다르게 한 경우이고, 아래는 실제 데이터의 분포라고 하자.(빨간색 선 하나 하나가 데이터를 의미)  \n이때, Likelihood를 각 각 구하면 각 x에서의 확률분포의 확률값을 모두 곱하면 된다. 그 경우 어떤 것이 제일 클지는 분명하다. 바로 파란색 분포일 것이다.  \n\n그렇다면, 우리가 원하는 것은 무엇인가? 바로 가장 높은 가능성을 가지게 하는 $\\theta$를 찾는 것이다. 따라서, 이를 식으로 표시하면 아래와 같다.\n\n$$\n\\hat{\\theta}_{MLE} = \\argmax_{\\theta}\\mathcal{L}(\\theta;\\mathcal{D})\n$$\n\n여기서 하나 문제가 있을 수 있다. 바로, 컴퓨터로 연산하게 되면 underflow가 발생하는 것이다. 특정 언어가 계산할 수 있는 소수점 범위를 벗어난다면, 제대로 된 결과를 얻을 수 없다. 이와 같은 문제를 **vanishing likelihood**라고 한다.  \n따라서, 우리는 log를 취했을 때와 log를 취하지 않았을 때의 경향성이 같음을 바탕으로 likelihood에 log를 취한 값을 이용하여 MLE를 구하는 것이 일반적이다. 이 방식을 maxmum log likelihood estimation 이라고 부른다.\n\n$$\n\\mathcal{l}(\\theta;\\mathcal{D}) = \\sum_{i=1}^{n}{\\log{(p(x_i|\\theta))}}\n$$\n\n이 방식을 이용하게 되면, 곱셈이 모두 덧셈으로 바뀌기 때문에 계산에서도 용이하다.\n\n여기까지 살펴보면, 하나의 의문이 들 수도 있다. 바로, $p(\\theta|\\mathcal{D})$도 측정 기준으로 사용할 수 있지 않냐는 것이다. 이 역시도 Dataset이 주어질 때, $\\theta$일 확률이라고 볼 수 있다.  \n어찌보면, 사람의 생각으로는 이게 더 당연하게 느껴질 수도 있다. 이는 바로 다음 MAP에서 다룰 것이다. 우선 MLE를 먼저한 이유는 이것이 더 구하기 쉽기 때문임을 기억해두자. \n\n```plaintext\n 🤔 증명\n\n (*해당 내용은 정보 이론에 기반한 MLE에 대한 추가적인 이해를 위한 내용입니다. 해당 내용은 자세히 알 필요까지는 없습니다.)\n\n 두 확률 분포 간 information Entropy의 차이를 나타내는 KL divergence의 최솟값을 구하는 것이 우리의 목표라고 정의할 수 있다.  \n 따라서, 우리가 결국 얻고자 하는 것은 확률 분포 함수가 주어졌을 때,  \n n이 무한대로 갈 때, 경험적 확률(empirical probability)에 가장 근사하는 parameter를 찾는 것이다.  \n 따라서, 우리는 KL divergence의 최솟값을 구하면 된다.\n```\n\n$$\n\\begin{align*}\n\\argmin_\\theta KL(\\tilde{p}||p_\\theta) \u0026= \\argmin_\\theta \\int\\tilde{p}(x)\\log{\\tilde{p}(x)\\over{p_\\theta(x)}}dx \\\\ \n\u0026=\\argmin_\\theta[-\\int\\tilde{p}(x)\\log{\\tilde{p}(x)dx} - \\int\\tilde{p}(x)\\log{p_\\theta(x)dx}] \\\\\n\u0026= \\argmax_\\theta\\int{\\tilde{p}(x)\\log{p_\\theta(x)}dx} \\\\\n\u0026= \\argmax_\\theta\\sum_{i=1}^{n}{\\log{p_\\theta(x_i)}} \\\\\n\u0026= \\theta_{MLE}\n\\end{align*} \n$$\n\n\u003cmark\u003e**2. MAP**\u003c/mark\u003e\n\nMaximum A Posteriori의 약자이다. Posteriori는 사후 확률이라고도 부르며, dataset이 주어졌을 때, $\\theta$일 확률을 구하는 것이다.  \n이를 바로 구하는 것은 다소 어렵다. 왜냐하면, Dataset이 조건으로 들어가는 형태이기 때문이다. ($p(\\theta|\\mathcal{D})$)  \n따라서, 우리는 Bayes' Theorem에 따라서 이전에 배운 Likelihood와 parameter의 확률, 그리고 Dataset의 확률을 활용히여 풀어낼 것이다.\n\n$$\np(\\theta|\\mathcal{D}) = {p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}}\n$$\n\n여기서 주의해서 볼 것은 바로 $p(\\theta|\\mathcal{D})$와 $p(\\theta)$의 관계이다. dataset이 주어질 때의 parameter의 확률을 구하기 위해서 원래 parameter의 확률이 필요하다는 것이다.  \n어찌보면 굉장히 모순되어 보일 수 있지만, 우리가 이것을 사전 확률(priori)로 본다면 다르게 볼 여지가 있다.  \n예를 들면, 우리가 수상한 주사위로 하는 게임에 참가한다고 하자. 이때, 우리는 수상한 주사위의 실제 확률은 알 수 없지만, 주사위 자체의 확률은 모두 1/6이라는 것을 알고 있다. 따라서, $p(\\theta={1\\over6}) = \\alpha, p(\\theta\\neq{1\\over6}) = \\beta$ 라고 할 수 있다. 만약 정말 수상해보인다면, 우리는 $\\alpha$가 점점 작아진다는 식으로 표현할 수 있고, 하나도 수상해보이지 않는 일반 주사위라면, $\\alpha=1, \\beta=0$으로 할 수도 있다. 이 경우에는 likelihood 값에 상관없이 다른 모든 값이 0이기 때문에 결국은 $p(\\theta|\\mathcal{D}) = p(\\theta)$ 가 되는 것을 알 수 있다.\n\n최종적으로, MAP도 결국은 Dataset을 얼마나 parameter가 잘 표현하는가에 대한 지표로 사용할 수 있다. \n따라서, 이를 최대로 만드는 parameter는 $\\theta_*$와 굉장히 근접할 것이다.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{MAP} \u0026= \\argmax_{\\theta}p(\\theta|\\mathcal{D}) \\\\\n\u0026= \\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}} \\\\\n\u0026=\\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)} \\\\\n\u0026=\\argmax_\\theta{[\\red{\\log{p(\\mathcal{D}|\\theta)}} + \\blue{\\log{p(\\theta)}}]}\n\\end{align*}\n$$\n\nMLE와 마찬가지로 이 또한 연산 및 **vanishing**을 막기 위해서 log를 취한다. 사실상 likelihood와 사전 확률의 합을 최대로 하는 $\\theta$를 찾는 것이다.\n\n\u003cmark\u003e**3. Bayesian Inference**\u003c/mark\u003e\n\n이제 마지막 방법으로 제시되는 Bayesian Inference이다. 이는 대게 Bayesian Estimation이라고 많이 불리는 것 같다. 이전까지 MLE, MAP는 결국 주어진 식을 최대로 하는 확정적 $\\theta$ 하나를 구하는 것을 목표로 했다.\n\nBayesian Inference는 Dataset이 주어졌을 때, $\\theta$의 평균값을 활용한다. 더 자세히 말하면, Posteriori(사후 확률)의 평균을 구하는 것이다.  \n이를 구하는 과정을 살펴보면 이해하는데 도움이 될 것이다. 한 번 살펴보자.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{BE}\u0026= E[\\theta|\\mathcal{D}] \\\\\n\u0026= {\\int_{0}^{1}{{\\theta}p(\\theta|\\mathcal{D})}d\\theta} \\\\\n\u0026= {\\int_{0}^{1}{\\theta}{{p(\\mathcal{D}|\\theta)p(\\theta)}\\over{p(\\mathcal{D})}}d\\theta} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)p(\\mathcal{D}|\\theta)}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\mathcal{D}|\\theta)p(\\theta)}d\\theta}} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\theta)\\prod_{i=1}^{n}p(x_i|\\theta)}d\\theta}} \\\\\n\\end{align*}\n$$\n\n이를 구하는 과정은 이전과는 다르게 상대값이 아닌 평균을 구해야하기 때문에 posteriori(사후 확률,$p(\\theta|\\mathcal{D})$)를 구해야 한다.\n\n하지만, 여기서 잡기술이 하나 존재한다. 바로 **Conjugate Prior**이다.\n\n바로 두 확률 분포 함수(likelihood, prior)에 의한 posterior의 형태가 정해진 경우가 있기 때문이다.\n\n| Prior $p(\\theta \\mid \\alpha)$  | Likelihood $p(\\mathcal{D} \\mid \\theta)$                 | Posterior $p(\\theta \\mid \\mathcal{D}, \\alpha)$                                                                                                                                                                                                                      | Expectation of Posterior                                                                                                                                                         |\n| :----------------------------- | :------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Beta ($\\alpha, \\beta$)         | Benoulli ($\\sum _{i=1}^{n}x_{i}$)                       | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +n-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                                               | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + n}}$                                                                                                                     |\n| Beta ($\\alpha, \\beta$)         | Binomial ($\\sum _{i=1}^{n}N_{i}, \\sum _{i=1}^{n}x_{i}$) | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                            | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + \\sum _{i=1}^{n}N_{i}}}$                                                                                                  |\n| Gaussian ($\\mu_0, \\sigma_0^2$) | Gaussian ($\\mu, \\sigma^2$)                              | Gaussian (${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu _{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum _{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right),\\left({\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}\\right)^{-1}}$) | ${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu _{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum _{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right)}$ |\n\n이를 이용하면, 우리는 간단하게 Posteriori의 평균을 구할 수 있다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n  ","slug":"ml-parametric-estimation","date":"2022-10-15 11:25","title":"[ML] 1. Parametric Estimation","category":"AI","tags":["ML","MLE","MAP","Bayesian"],"desc":"Machine Learning은 특정 목표를 달성하기 위해서 데이터로 부터 pattern 또는 가정 등을 유도해내는 방법이다.이를 위한 가장 일반적인 방법은 여러 개의 확률분포와 이것의 parameter의 조합(probabilistic model)들 중에서 측정된 데이터들을 가장 잘 나타내는 하나를 찾아내는 것이다.그 중에서, 확률 분포를 결정한 상태에서 parameter를 찾아나가는 형태의 접근법을 우리는 Parametric Estimation이라고 한다. 그 외에도 Nonparametric, Semi-parametric 방식도 존재하지만 이는 여기서는 다루지 않는다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nRegression(회귀)이라는 단어는 \"원래의 상태로 돌아간다\"로 돌아간다는 의미를 가진다. 결국 어떤 일련의 Event로 인해서 데이터에 Noise가 발생할 수 있어도 결국은 하나의 \"보편\"으로 시간이 지나면 수렴(회귀)할 것이라는 생각에 기반하는 것이다.  \n따라서, 우리는 이러한 \"보편\"을 찾기 위해서 우리가 알고 있는 독립 데이터 X를 통해서 알고자 하는 값 Y를 보편적으로 추론할 수 있다. 이 과정을 우리는 Regression이라고 부른다. 또한, X에 의해 독립적이지 않고 종속적인 Y의 관계가 Linear하게 표현될 때 이를 우리는 Linear Regression이라고 한다.  \n따라서, 해당 Posting에서는 Linear Regression을 바탕으로 Machine Learning이 어떻게 동작하는지를 이해하는 것이 목표이다.\n\n## Regression\n\n\u003e **정의**\n\n독립 변수 X로 부터 종속 변수 Y에 대응되는 함수 f를 생성하는 과정을 의미한다.\n\n$$\n\\bold{y} = f(\\bold{x}) + \\epsilon\n$$\n\n$$\nf(\\bold{x}) = \\bold{w}^{\\top}\\bold{x}, \n(\\bold{w} = \\begin{bmatrix} w_{0} \\\\ w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{N} \\\\ \\end{bmatrix}, \\bold{x} = \\begin{bmatrix} 1 \\\\ x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{N} \\\\ \\end{bmatrix} )\n$$\n\n여기서 각 변수 $x$, $y$, $\\epsilon$, $w$은 다음과 같이 여러 이름으로 불려진다.\n\n- $x$ : input, 독립 변수, predictor, regressor, covariate\n- $y$ : output, 종속 변수, response\n- $\\epsilon$ : noise, 관측되지 않은 요소\n- $w$ : weight, 가중치, parameter \n\n\u003e \u003cmark\u003e**성능 평가(MSE)**\u003c/mark\u003e\n\n우리가 만든 Regression이 얼마나 데이터를 잘 반영하는지를 알고 싶을 때, 즉 평가하고자 할 때, 우리는 Mean Squared Error(MSE)를 사용한다. 이는 이전 포스팅인 [Parametric Estimation](/posts/ml-parametric-estimation)에서도 살펴보았었다. \n\n그렇다면, MSE를 최소로 하는 f(x)는 무엇일까? 이를 통해서 또, 하나의 식견을 넓힐 수 있다. 한 번 MSE 식을 정리해보자.\n\n$$\n\\begin{align*}\n\\Epsilon(f) \u0026= E[||\\bold{y}_*-f(\\bold{x})||^2] \\\\\n\u0026= \\int\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{x}, \\bold{y}_*)d\\bold{x}d\\bold{y}_* \\\\\n\u0026= \\int\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{x})p(\\bold{y}_* | \\bold{x})d\\bold{y}_*d\\bold{x} \\\\\n\u0026= \\int p(\\bold{x}) \\red{\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}d\\bold{x}\n\\end{align*}\n$$\n\n여기서 중요한 것은 바로 빨간색으로 색칠한 부분이다. 우리가 바꿀 수 있는 값은 f(x)를 구성하는 w밖에 없다 즉, 위 식을 최소화하는 것은 빨간색 부분을 최소화하는 것과 같아진다.  \n따라서, 이 부분을 미분해서 최솟값을 구할 수 있는데 이를 확인해보자.\n\n$$\n\\begin{align*}\n\u0026{\\partial\\over{\\partial{f(\\bold{x})}}}({\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}) = 0 \\\\\n\u0026{\\partial\\over{\\partial{f(\\bold{x})}}}({\\int||\\bold{y}_*^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} - 2f(\\bold{x}){\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} + f(\\bold{x})^2{\\int p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}) = 0 \\\\\n\u0026-2{\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} + 2f(\\bold{x}) = 0 \\\\\n\u0026f(\\bold{x}) = {\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} = E[\\bold{y}_*|\\bold{x}]\n\\end{align*}\n$$\n\n즉, 우리가 구하고자 하는 Linear Regression 함수는 data x에 따른 실제 y 값의 평균을 의미한다. Regression의 정의를 생각했을 때, 어느정도 합리적이라는 것을 알 수 있다. \"보편\"적이다는 의미에서 \"평균\"을 쓰는 경우가 많이 있기 때문이다.\n\n위에서는 MSE를 이용해서 분석하였지만, MAE(Mean Absolute Error)를 활용하여 구할 수 있는데 이 경우에는 Regression의 형태가 또 달라진다. 즉, MSE를 최소화하는 방식은 우리가 \"보편\"적인 답을 구하는데 있어 \"평균\"을 활용한 것이고, MAE를 사용한다면, 또 다른 방식을 사용한다는 것을 알게 될 것이다.\n\n## MLE of Linear Regression\n\n이제 Linear Regression에서 $\\bold{w}$를 어떻게 찾아 나갈지에 대해서 살펴볼 것이다. 순서는 이전 Posting [Parametric Estimation](/posts/ml-parametric-estimation)에서 살펴봤던 것과 마찬가지로 MLE, MAP 순으로 살펴볼 것이다. 그리고 이것이 왜 MLE고, MAP랑 관련이 있는지도 살펴볼 것이다.\n\n들어가기에 앞 서, 표기법과 용어를 몇 개 정리할 필요가 있다.\n\n- $\\bold{x}, \\bold{y}, \\bold{w}$ 등 굵은 선 처리되어 있는 변수는 vector를 의미한다.\n- $\\bold{X}$ 등 굵고 대문자로 처리되어 있는 변수는 Matrix를 의미한다.\n- $\\bold{w^{\\top}}$, $\\bold{X}^{\\top}$ 에서 T는 Transpose를 의미한다.\n- feature : input 데이터의 각 각 분류 기준들을 의미한다. 수식으로는 $x_1, x_2, x_3$ 이런 식으로 표현된 input들 중에 각 각의 input을 feature라고 하며, 실제 예시로는 데이터 수집 시에 각 데이터의 column(나이, 성별, 등 등)이 될 것이다.\n\n위의 용어 정리에 의해서 다음과 같은 사실을 다시 한 번 확인하자.\n\n먼저, 단일 Linear Regression이다.\n\n$$\n\\hat{y} = f(\\bold{x}) = \\bold{w}^{\\top}\\bold{x} = \\bold{x}^{\\top}\\bold{w}\n$$\n\n이번에는 여러 개의 데이터를 한 번에 추측한 결과값 $\\hat{\\bold{y}}$ 이다.\n\n$$\n\\hat{\\bold{y}} = \\bold{X}\\bold{w}\n$$\n\n각 의미를 곱씹어보면 어떻게 생겼을지 어렵풋이 짐작이 올 것이다.\n\n\u003e **basis function**\n\n여기서 또 하나 짚어볼 것은 바로 $\\bold{x}$를 변형하는 방법이다. 바로, 우리는 데이터로 입력 받은 데이터를 바로 사용할 수도 있지만, 해당 input 값을 제곱해서 사용해도 되고, 서로 더해서 사용해도 되고, 나누어서 사용할 수도 있다. 예를 들어서 우리가 구하고 싶은 값이 대한민국 인구의 평균 나이라고 하자. 이때, 우리가 사용하는 데이터의 값이 가구 단위로 조사되어 부,모,자식1, 자식2, ... 로 분류되어 나이가 적혀있다고 하자. 이때 우리가 필요한 것은 결국 전체 인구의 나이 데이터임으로 모두 하나의 feature로 합쳐버릴 수도 있다.\n\n이러한 과정을 위해서 우리는 basis function($\\phi(\\bold{x})$)이라는 것을 이용한다. 단순히 input data를 합성해서 하나의 input을 생성하는 것이다.\n\n따라서, 우리는 필요에 따라 input data를 가공하여 사용하며 여러 $\\phi$를 적용하여 나타낼 경우 linear regression은 다음과 같은 형태가 된다.\n\n$$\nf(\\bold{x}) = \\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x})\n$$\n\n대표적인 Basis function을 살펴보자.\n\n- Polynomial basis : 하나의 input feature에 대해서 n-제곱형태의 vector로 변환하는 형식이다. 따라서, 다음과 같이 표기 된다.  \n  $\\boldsymbol{\\phi}(\\bold{x}) = \\begin{bmatrix} 1 \\\\ x \\\\ x^{2} \\\\ \\vdots \\\\ x^{n} \\\\ \\end{bmatrix}$, $\\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x}) = w_{0} + w_{1}x + w_{2}x^{2} + ... + w_{n}x^{n}$  \n  대게 이러한 형태로 변형한 Linear Regression을 Polinomial Regression이라고 부르는데, 이를 통한 결과 값이 마치 다항식의 형태를 띄기 때문이다. 하지만, feature의 값이 polynomial이 되었더라도 $\\bold{w}$가 선형임을 잊어서는 안된다.  \n  이를 사용하게 되면, 우리는 1차원의 input 공간에서 선형으로는 나눌 수 없던 분류를 수행할 수 있다.\n- Gaussian basis : 가우시안 분포로 변환하는 것으로 특정 feature를 gaussian으로 변환하게 되면, 데이터의 경향성이 파악된다. 이는 후에 더 자세히 다룰 기회가 온다.  \n- Spline basis: 특정 구간마다 다른 Polynomial 형태의 feature를 적용하도록 하는 방식이다. 대게 구간마다 다른 확률 분포를 적용하고자 할 때 사용한다.\n- Fourier basis, Hyperbolic tangent basis, wavelet basis 등 여러 가지 방식이 존재한다.\n\n\u003e **Design Matrix**\n\n마지막으로, 이렇게 만들어진 $\\phi(\\bold{x})$를 하나의 Matrix로 합친 것을 Design Matrix라고 한다. N개의 데이터를 L개의 서로 다른 basis function으로 변환한 데이터를 행렬로 표현하면, 다음과 같다.\n\n$$\n\\Phi = \n  \\begin{bmatrix} \n    \\phi_1({\\bold{x_1}})  \u0026 \\phi_2(\\bold{x_1})  \u0026 \\cdots  \u0026 \\phi_L(\\bold{x_1})  \\\\\n    \\phi_1({\\bold{x_2}})  \u0026 \\phi_2(\\bold{x_2})  \u0026 \\cdots  \u0026 \\phi_L(\\bold{x_2})  \\\\\n    \\vdots                \u0026 \\vdots              \u0026 \\ddots  \u0026 \\vdots              \\\\\n    \\phi_1({\\bold{x_N}})  \u0026 \\phi_2(\\bold{x_N})  \u0026 \\cdots  \u0026 \\phi_L(\\bold{x_N})  \\\\\n  \\end{bmatrix}\n$$\n\n이를 통해서 표현한 모든 데이터에 대한 Linear Regression은 다음과 같다.\n\n$$\n\\hat{\\bold{y}} = \\Phi\\bold{w}\n$$\n\n자 이제부터 우리는 본론으로 들어와서 우리의 Linear Regression의 Weight(Parameter, $\\bold{w}$)를 어떻게 추정할 수 있을지를 알아보자.\n\n우리는 최종적으로 우리의 Linear Regression이 정답과 매우 유사한 값을 내놓기를 원한다. 따라서, 이때 우리는 Least Square Error를 사용할 수 있다. 이는 모든 데이터에서 얻은 예측값(Linear Regression의 output)과 실제 y의 값의 Square Error의 합을 최소화하는 것이다.\n\n$$\n\\varepsilon_{LS}(\\bold{w}) = {1\\over2}\\sum_{n=1}^{N}(y_n - \\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x_n}))^2 = {1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2\n$$\n\n이제 $\\argmin_{\\bold{w}}\\varepsilon_{LS}(\\bold{w})$을 풀기 위해서 미분을 해보자.\n\n$$\n\\begin{align*}\n\u0026{\\partial\\over\\partial\\bold{w}}{1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2 = 0 \\\\\n\u0026\\Phi^{\\top}(\\bold{y}_* - \\Phi\\bold{w}) = 0 \\\\\n\u0026\\Phi^{\\top}\\Phi\\bold{w} = \\Phi^{\\top}\\bold{y_*} \\\\\n\u0026\\bold{w} = (\\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y_*} \\\\\n\u0026\\bold{w} = \\Phi^{\\dagger}\\bold{y_*}\n\\end{align*}\n$$\n\n이를 통해서, 위와 같은 식을 얻을 수 있다.\n\n---\n\n그럼 이 식이 왜 MLE랑 관련이 있는 것일까? 그것은 다음의 과정을 통해서 증명할 수 있다.\n\n우리는 각 data마다 존재하는 error(noise, $y_* - \\hat{y}$, $\\varepsilon$)가 그 양이 많아짐에 따라 정규 분포를 따른다는 것을 알 수 있다. (Central Limit Theorem)\n\n$$\n\\begin{align*}\n\\varepsilon \u0026= y_* - \\hat{y} = y_*-\\phi(\\bold{x}) \\\\\ny_* \u0026= \\phi(\\bold{x}) + \\varepsilon\n\\end{align*}\n$$\n\n이를 좌표 평면 상에서 나타내면 다음과 같다고 할 수 있다.\n\n![gaussian-error](/images/gaussian-error.jpeg)\n\n또한, $\\varepsilon$의 확률을 정의하면 다음과 같은 확률을 얻을 수 있다.\n\n$$\n\\begin{align*}\np(\\varepsilon) \u0026= {1\\over{\\sqrt{2\\pi}\\sigma}}\\exp{[-{\\varepsilon^2\\over{2\\sigma^2}}]} \\\\\np(\\varepsilon) \u0026= {1\\over{\\sqrt{2\\pi}\\sigma}}\\exp{[-{(y_*-\\phi(\\bold{x}))^2\\over{2\\sigma^2}}]}\n\\end{align*}\n$$\n\n여기서 우리는 $p(\\varepsilon)$을 $p(y_*|\\bold{x}; \\theta)$라고 볼 수 있다. ($\\theta = (\\bold{w}, \\phi, \\sigma)$)\n\n우리는 이를 이용해서 Likelihood를 구할 수 있다.\n\n$$\n\\begin{align*}\n\\mathcal{L} \u0026= \\log{p(\\bold{y}_*|\\bold{X}; \\theta)} = \\sum_{i=1}^{N}{\\log{p(y_{*(i)}|\\bold{x}_{(i)}; \\theta)}} \\\\\n\u0026= N\\log{{1\\over{\\sqrt{2\\pi}\\sigma}}} + \\sum_{i=1}^{N}{-{(y_*-\\phi(\\bold{x}))^2\\over{2\\sigma^2}}} \\\\\n\u0026= N\\log{{1\\over{\\sqrt{2\\pi}\\sigma}}} - {1\\over{\\sigma^2}}\\red{{1\\over{2}}\\sum_{i=1}^{N}{(y_*-\\phi(\\bold{x}))^2}}\n\\end{align*}\n$$\n\n우리가 변경할 수 있는 데이터는 $\\phi(\\bold{x})$ 밖에 없다. 따라서, 빨간색을 제외한 부분은 Likelihood의 최댓값을 구할 때, 고려하지 않아도 되는 상수로 볼 수 있다. 그렇다면, 우리는 Likelihood의 최댓값을 구하기 위해서 빨간색 표시된 부분을 최소화해야 한다는 것을 알 수 있다. 그리고, 이는 우리가 앞에서 살펴봤던, Least Squared Error와 같다.\n\n\u003cmark\u003e즉, $\\bold{w}_{LS}=\\bold{w}_{MLE}$ 라는 것이다.\u003c/mark\u003e\n\n## MAP of Linear Regression\n\n이번에는 Linear Regression에서 $\\bold{w}$를 찾아나가는 과정에서 MAP를 활용하는 과정을 알아볼 것이다.\n\n\u003e **overfitting**\n\n우리가 MLE를 통해서 Linear Regression을 찾는 것이 충분하다고 생각할 수 있다. 하지만, 우리는 어쩔 수 없이 **overfitting**이라는 문제에 직면하게 된다. \n\n![over-fitting-example](/images/over-fitting-example.jpg)\n\n**overfitting**이란 데이터를 통해서 구할 수 있는 분포가 학습에 사용된 데이터에 대해서는 에러가 거의 없는 형태로 예측하지만, 그 외에 데이터에 대해서는 에러가 크게 발생하는 경우를 의미한다. 위의 예시에서 처럼 데이터가 전체 Sample space보다 턱없이 적은 경우에 발생하기 쉽다.\n\n이러한 문제는 사실 basis function을 잘 선택하면 해결할 수 있다. 하지만, 우리가 어떻게 매번 적절한 basis function을 찾기 위해서 iteration을 반복하는 것이 올바를까? 그리고 이는 실제 적합한 값을 찾기 위한 수학적 식도 존재하지 않는다.\n\n\u003e **Regularization**\n\n따라서, 우리는 **regularization**을 수행한다. 위의 overfitting된 그래프를 보면 하나의 insight(번뜩이는 idea?)를 얻을 수 있다. 바로, 급격한 기울기의 변화는 overfitting과 유사한 의미로 볼 수 있다는 것이다. 즉, 그래프의 형태가 smooth 해야한다는 것이다.\n\n따라서, 우리는 하나의 error에 대해서 다음과 같이 재정의해서 smoothing(regularization)을 수행할 수 있다.\n\n$$\n\\varepsilon = {1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2 + {\\lambda\\over{2}}||\\bold{w}||^2\n$$\n\n$\\bold{w}$의 L2 norm을 error에 추가하여 $\\bold{w}$의 크기가 작아지는 방향으로 예측을 할 수 있도록 하는 것이다. (물론 L1 norm을 사용할 수도 있다. 이 또한, 후에 다룰 것이니 여기서는 넘어가겠다. 추가로 이렇게 L2 norm을 이용하면 **Ridge Regression**, L1 norm을 이용하면 **Lasso Regression**이라고 한다.)\n\n자 이제 위의 식을 미분해서 최소값이 되게 하는 $\\bold{w}$를 찾아보자. 과정은 연산이 그렇게 어렵지 않으므로 넘어가고 결과는 아래와 같다.\n\n$$\n\\bold{w}_{ridge} = (\\lambda I + \\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y}_*\n$$\n\n---\n\n그럼 이 역시 MAP를 통해서 해석해보도록 하자.\n\n위에서 살펴본 바와 같이 우리는 w값이 작을 확률이 높을 수록 좋은 성능을 가질 것이라는 Prior를 얻을 수 있다.\n\n즉,$p(\\bold{w})$가 zero-mean gaussian(표준정규분포)형태를 이루기를 바랄 것이다.\n\n$$\np(\\bold{w}) = \\mathcal{N}(\\bold{w}|0, \\Sigma)\n$$\n\n그리고, 이전에 MLE를 구할 때, Likelihood를 다음과 같이 정의했다.\n\n$$\n\\begin{align*}\np(\\bold{y}_*|\\bold{X}; \\theta) \u0026= \\mathcal{N}(\\bold{y}_*-\\Phi\\bold{w}, \\sigma I) \\\\\np(\\bold{y}_*|\\Phi, \\bold{w}) \u0026= \\mathcal{N}(\\bold{y}_*-\\Phi\\bold{w}, \\sigma I)\n\\end{align*}\n$$\n\n따라서, 우리는 이를 이용해서 posterior를 추론할 수 있다.\n\n$$\np(\\bold{w}|\\bold{y}_*, \\Phi) = {{p(\\bold{y}_*| \\Phi, \\bold{w})p(\\bold{w})}\\over{p(\\bold{y}_*|\\Phi)}}\n$$\n\n여기서 MAP를 구할 때에는 Lemma 정리(두 정규분포의 conditional Probability를 구하는 공식)를 이용하면 편하다. 따로 연산은 수행하지 않지만 결과 값은 아래와 같다.\n\n$$\n\\bold{w}_{MAP} = (\\sigma^2\\Sigma^{-1}+\\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y}\n$$\n\n여기서 만약 우리가 $\\Sigma = {\\sigma^2\\over{\\lambda}}I$라고 가정하면, 위의 MAP 식은 Ridge Regression과 동일해지는 것을 알 수 있다.\n\n즉, Ridge Regression은 MAP의 한 종류라고 볼 수 있는 것이다.\n\n$$\n\\bold{w}_{ridge} \\in {(\\bold{w}_{MAP}, \\Sigma)}\n$$\n\n## Gradient Descent\n\n여태까지 우리는 Loss를 정의하고, 이 Loss가 최솟값을 갖는 $\\bold{w}$를 찾는 것을 목표로 하였다. 하지만, 우리가 다루는 모든 Loss가 미분이 항상 쉬운 것은 아니다. 뿐만 아니라, Loss의 미분 값이 5차원 이상의 식으로 이루어진다면, 우리는 이를 풀 수 없을 수도 있다. 5차원 이상의 polynomial에서는 선형대수적인 해결법(근의 방정식)이 없다는 것이 증명되어있다.(Abel-Ruffini theorem)\n\n따라서, 우리는 Loss가 0이 되는 지점을 찾기 위해서, w의 값을 점진적으로 업데이트하는 방식을 활용한다. 이때, 우리는 w의 값이 계속해서 Loss를 감소시키기를 원한다. 따라서, 우리는 현재 $\\bold{w}$에서 Gradient를 현재 $\\bold{w}$에 빼준다. 이를 우리는 **Gradient Descent**라고 한다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\gamma((\\nabla L)(\\bold{w}_{t}))^{\\top}\n$$\n\n여기서 $\\gamma$는 step size(learning rate)라고 하며, 기울기값을 얼마나 반영할지를 의미한다.\n\n---\n\n이제부터는 Gradient Descent를 더 효과적으로 진행하기 위한 3가지의 기술들을 추가적으로 제시한다.\n\n\u003e **1. optimize stepsize**\n\nstepsize($\\gamma$)가 특정 상수로 제시된 게 아니라 변수로 표현된 이유는 linear regression마다 적절한 $\\gamma$가 다르기 때문이다. 하나의 예시를 들어보자.\n\n![loss-divergence](/images/loss-divergence.jpg)\n\n위는 Loss function이 convex할 때, 최솟값을 찾아나가는 과정이다. 만약, $\\gamma$가 크다면, Loss가 특정값으로 수렴하는 것이 아니라 발산하는 것을 알 수 있다. 이를 막기 위해 $\\gamma$를 굉장히 작은 수로 하는 경우에는 Loss의 최솟값을 찾기도 전에 특정 지점에서 멈춰버릴 수도 있다. 또한, Loss의 graph형태는 data마다 달라지기 때문에 절대적인 $\\gamma$역시 존재하지 않는다.\n\n따라서, 우리는 매 update마다 적절한 $\\gamma$를 찾을려고 노력한다. 여기서는 자세히 다루지 않지만 후에 더 다룰 기회가 있을 것이다. 간단히 프로그래밍적으로(systemical) 생각하면, 업데이트 이후 loss가 만약 그전 Loss보다 커진다면, 이를 취소하고 더 작은 $\\gamma$를 사용하도록 하고, 업데이트 된 후의 Loss와 그전 Loss가 같다면, 진짜 수렴하는지를 확인하기 위해서 $\\gamma$를 키워볼 수도 있다.\n\n\u003e **2. momentum**\n\n우리가 Gradient Descent를 진행하다보면, 다음과 같은 현상을 자주 마주하게 된다.\n\n![momentum-example-1](/images/momentum-example-1.jpg)\n\n우리가 찾고자 하는 Loss를 찾아가는 과정에서 매 업데이트마다 반대방향으로 기울기가 바뀌는 경우이다.(진동한다) 이는 최종으로 찾고자 하는 값을 찾는 과정이 더 오래 걸리게 한다. 따라서, 우리는 이러한 진동을 막기 위해서 Momentum을 사용한다. 즉, 이전 차시에서의 gradient를 저장해두고, 이를 더해서 진동하는 것을 막는 것이다.\n\n$$\n\\bold{w}_{i+1} = \\bold{w}_{i} - \\gamma_{i}((\\nabla L)(\\bold{w}_{i}))^{\\top} + \\alpha \\Delta \\bold{w}_i ,( \\alpha \\in [0, 1] )\n$$\n\n$$\n\\Delta \\bold{w}_i = \\bold{w}_{i} - \\bold{w}_{i-1} = \\alpha \\Delta \\bold{w}_{i-1} - \\gamma_{i-1}((\\nabla L)(\\bold{w}_{i-1}))^{\\top}\n$$\n\n즉, 그림으로 표현하면, 다음과 같다.\n\n![momentum-example-2](/images/momentum-example-2.jpg)\n\n이전 변화량과 현재 변화량을 합하여 이동하기 때문에 위에 새로 추가된 것처럼 진동하지 않고, 진행하는 것을 볼 수 있다.\n\n\u003e **3. Stochastic Gradient Descent**\n\n우리의 Gradient Descent의 가장 큰 문제는 바로 Global Minimum을 찾을 거라는 확신을 줄 수 없다는 것이다. 아래 그림을 보자.\n\n![gradient-descent-example](/images/gradient-descent-example.jpg)\n\n여기서 우리는 초기 w 값을 어떻게 정하냐에 따라서, **local minimum**을 얻게 되거나 **global minimum**을 얻게 된다. 즉, 초기값이 결과에 굉장히 큰 영향을 준다는 것이다.\n\n이를 해결할 수 있으며, 학습 효율도 높일 수 있는 것이 Stochastic Gradient Descent이다. 원리는 Loss를 구하기 위해서 전체 데이터(모집단)를 사용했었는데 그러지말고 일부 데이터를 랜덤하게 추출(sampling)해서(표본 집단) 이들을 통해서 Loss function을 구하기를 반복하자는 것이다.\n\n이 방식을 통해서 구한 Gradient의 평균이 결국은 전체 batch의 평균과 같다는 것은 Central Limit Theorem(중심 극한 정리)에 의해 증명이 된다. 따라서, 우리는 이를 통한 gradient descent도 특정 minimum을 향해 나아가고 있음을 알 수 있다.\n\n그렇지만, 표본 집단을 이용한 평균을 구했을 때에 우리는 noise에 의해서 local minimum으로만 수렴하는 현상을 막을 수 있다. 즉, gradient descent를 반복하다보면, 다른 local minimum으로 튀어나가기도 하며 global minimum을 발견할 확률을 높일 수 있는 것이다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- [Probabilistic interpretation of linear regression clearly explained](https://towardsdatascience.com/probabilistic-interpretation-of-linear-regression-clearly-explained-d3b9ba26823b), Lily Chen","slug":"ml-linear-regression","date":"2022-10-17 09:46","title":"[ML] 2. Linear Regression","category":"AI","tags":["ML","LinearRegression","BasisFunction","Regularization","GradientDescent","Momentum","StochasticGradientDescent"],"desc":"Regression(회귀)이라는 단어는 \"원래의 상태로 돌아간다\"로 돌아간다는 의미를 가진다. 결국 어떤 일련의 Event로 인해서 데이터에 Noise가 발생할 수 있어도 결국은 하나의 \"보편\"으로 시간이 지나면 수렴(회귀)할 것이라는 생각에 기반하는 것이다.  따라서, 우리는 이러한 \"보편\"을 찾기 위해서 우리가 알고 있는 독립 데이터 X를 통해서 알고자 하는 값 Y를 보편적으로 추론할 수 있다. 이 과정을 우리는 Regression이라고 부른다. 또한, X에 의해 독립적이지 않고 종속적인 Y의 관계가 Linear하게 표현될 때 이를 우리는 Linear Regression이라고 한다.  따라서, 해당 Posting에서는 Linear Regression을 바탕으로 Machine Learning이 어떻게 동작하는지를 이해하는 것이 목표이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n우리는 Classification을 하기 위해서 Logistic Regression을 수행하였다. 그 결과 결국 Classification도 결국은 선을 긋는 것이라는 결론을 내리게 되었다. 하지만, 여기서 그치지 않고 하나 더 고민해 볼 수 있는 것이 있다. 바로 주어진 데이터에 대해서 완벽하게 구분하는 decision boundary가 여러 개 있을 때, 어떤 것이 가장 좋은 것일까? 이것에 대한 아이디어를 제시하는 것이 SVM이다. 해당 Posting에서는 이에 대해서 살펴보도록 하겠다.\n\n## (Hard Margin) SVM\n\nSoft Vector Machine의 약자로, 위에서 제시한 문제를 해결하기 위해서 Margin이라는 것을 도입하였다.\n\n\u003e **Margin**\n\n**Margin**이란 decison boundary와 가장 가까운 각 class의 두 점 사이의 거리를 2로 나눈 값이다.\n\n![svm-1](/images/svm-1.jpg)\n\n위의 그림은 똑같은 데이터 분포에서 대표적인 decision boundary 두 개를 제시한 것이다. 여기서 우리는 굉장히 많은 decision boundary를 그릴 수 있다. 그 중에서도 파란색 실선이 직관적으로 가장 적절한 decision boundary가 될 것이라고 짐작할 수 있다. 그 이유는 필연적으로 data는 noise에 의한 오차가 발생할 수 있는데 실제 데이터의 오차의 허용 범위를 우리는 **margin**(=capability of unexpected noise)만큼 확보할 수 있다는 의미로 이를 해석할 수 있다. 따라서, 이 margin을 크게 하면 할 수록 좋은 성능을 가지는 선을 그을 수 있을 것이라는 결론을 내릴 수 있다.\n\n이것이 SVM의 핵심 아이디어이다.\n\n그렇다면, margin을 수학적으로 정의해보자. 우리가 decision boundary를 $f(\\bold{x}) := \\bold{w}^{\\top}\\bold{x} + b = 0$이라고 한다면, 점($\\bold{x}_{i}$)과 vector 직선 vector 사이의 거리 공식을 통해서 ${{|f(\\bold{x}_{i})|}\\over{||\\bold{w}||}}$라는 것을 알 수 있다.\n\n따라서 margin은 수학적으로 다음과 같다.\n\n$$\n\\min_{i}{{|f(\\bold{x}_{i})|}\\over{||\\bold{w}||}}\n$$\n\n```plaintext\n 🤔 Canonical(법칙까지는 아니지만 사실상 표준화된) SVM\n\n SVM에서는 f(x) = 0인 등식 형태를 같는다. 즉 f(x)에 어떤 값을 곱해도 똑같다는 것이다.\n 그런데 margin의 크기를 구할 때에는, w와 b에 어떤 값이 곱해진다면 이 값이 굉장히 달라지게 된다.\n 따라서, 일반적으로 우리는 margin에서의 |f(x)| = 1이 될 수 있도록 설정한다. \n 이렇게 하면 계산이 굉장히 쉬워진다.\n```\n\n![svm-2](/images/svm-2.jpg)\n\n따라서, 우리는 위의 그림과 같은 형태로 $\\bold{x}^{-}$와 $\\bold{x}^{+}$를 찾을 수 있는 것이다.\n\n이제 마지막으로 margin을 정의해보자.\n\n$$\n\\begin{align*}\n\\rho \u0026= {1\\over2}\\{ {{|f(\\bold{x}^{+})|}\\over{||\\bold{w}||}} - {{|f(\\bold{x}^{-})|}\\over{||\\bold{w}||}}  \\} \\\\\n\u0026= {1\\over2}{1\\over{||\\bold{w}||}}\\{\\bold{w}^{\\top}\\bold{x}^{+} - \\bold{w}^{\\top}\\bold{x}^{-}\\} \\\\\n\u0026= {1\\over{||\\bold{w}||}}\n\\end{align*}\n$$\n\n\u003e **Optimization**\n\n그렇다면, 이제 우리는 문제를 해결할 준비가 된 것이다. 우리가 하고자 하는 것은 margin을 최대화하면서도, 모든 data를 오류없이 분류하는 것이다. 이는 다음과 같은 Constraint Optimization 형태로 변환할 수 있다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 {1\\over{||\\bold{w}||}} \u0026\\\\\n  \\text{subject to} \\quad \u0026 y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\geq 1, \u0026 i = 1, ..., N\n\\end{align*}\n$$\n\nConditional Optimization은 이전 Posting([[ML] 0. Base Knowledge](/posts/ml-base-knowledge))에서 다룬바 있다. 해당 내용에 대해 미숙하다면 한 번 살펴보고 오도록 하자.\n\n위 내용을 숙지하였다면, 위의 폼이 다소 바뀌어야 한다는 것을 알 것이다. 해당 형태를 바꾸면서, minimize 형태를 미분이 간편할 수 있도록 바꾸도록 하겠다.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 {1\\over2}||\\bold{w}|| \u0026\\\\\n  \\text{subject to} \\quad \u0026 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\leq 0, \u0026 i = 1, ..., N\n\\end{align*}\n$$\n\n우선 lagrangian은 다음과 같다.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}\\alpha_{i}(1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b))\n$$\n\n이것에 KKT Condition을 적용하여 정리하면 다음과 같은 등식을 얻을 수 있다.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n이를 $\\mathcal{L}$에 대입하여 식을 정리하면, 다음과 같다.\n\n$$\n\\mathcal{L} = -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i}\n$$\n\n이제 이것을 이용해서 Dual Problem을 정의하면 다음과 같다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, \u0026 \\\\\n  \u0026 \\alpha_{i} \\geq 0, \u0026 i = 1, ..., N \n\\end{align*}\n$$\n\n이 식에서 눈여겨 볼점은 바로 constraint 부분이다. 이 과정을 통해서 결론적으로 constraint 부분이 부등식에서 등식이 되었다. 이는 연산 과정을 매우 간단하게 한다. 뿐만 아니라 $\\bold{x}_{i}^{\\top}\\bold{x}_{j}$는 한 번 계산하면, 전체 과정에서 계속해서 재사용할 수 있기 때문에 컴퓨팅 시에는 굉장한 이점을 발휘할 수 있다. 따라서, 실제로 값을 구할 때에는 이것을 이용하여 값을 구하는 것이 가장 일반적이다.\n\n## (Soft Margin) SVM\n\nSVM의 모든 절차를 살펴본 것 같지만, 우리가 간과한 사실이 하나 있다. 바로 그것은 우리는 data가 하나의 선을 통해서 완벽하게 나뉘어진다고 가정했다. 하지만, 실제 데이터는 그렇지 않을 가능성이 크다. 따라서, 우리는 어느 정도의 오차를 허용할 수 있도록 해야 한다. 이를 slack($\\zeta$)이라고 한다.\n\n![svm-2](/images/svm-2.jpg)\n\n이를 적용하면, 우리의 목적함수와 제약 조건을 변경해야 한다. 이를 변경하는 방법은 두 가지가 존재하는데 각 각 slack variable의 L2-norm을 목적함수에 더하는 방식과 L1-norm을 더하는 방식이다.\n\n\u003e **L2-norm Optimization**\n\n먼저 L2-norm을 더하는 방식을 알아보자\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 {1\\over2}||\\bold{w}|| + C\\sum_{i=1}^{N}\\zeta_{i}^{2} \u0026\\\\\n  \\text{subject to} \\quad \u0026 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, \u0026 i = 1, ..., N\n\\end{align*}\n$$\n\n여기서 $C$는 margin 최대화와 slackness 정도의 상대값을 의미한다. 만약, slackness보다 margin의 최대화가 중요하다면, C값은 커지고 반대라면 이 값은 작아진다.\n\n우선 lagrangian을 먼저 구하면 다음과 같다.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}|| + {C\\over2}\\sum_{i=1}^{N}\\zeta_{i}^{2} + \\sum_{i=1}^{N}\\alpha_{i}(1 - \\zeta_{i} y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b))\n$$\n\nKKT condition을 이용하여 주요 값들을 구하면 다음과 같은 등식을 얻을 수 있다.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n$$\n\\boldsymbol{\\zeta} = {\\alpha\\over{C}}\n$$\n\n마지막으로 이를 Dual Problem으로 재정의하면 다음과 같아진다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\bold{x}_{i}^{\\top}\\bold{x}_{j} + {1\\over{C}}\\delta_{ij}) + \\sum_{i=1}^{N}\\alpha_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, \u0026 \\\\\n  \u0026 \\alpha_{i} \\geq 0, \u0026 i = 1, ..., N \n\\end{align*}\n$$\n\n여기서 $\\delta_{ij}$는 단위행렬이다. 기존 hard margin svm과 비교했을 때, ${1\\over{C}}\\delta_{ij}$ 외에는 바뀌지 않는 것을 알 수 있다.\n\n\u003e **L1-norm Optimization**\n\n그 다음은 L1-norm이다.\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 {1\\over2}||\\bold{w}|| + C\\sum_{i=1}^{N}\\zeta_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, \u0026 \\\\\n  \u0026 \\zeta_{i} \\geq 0 \u0026 i = 1, ..., N\n\\end{align*}\n$$\n\n여기서는 slack variable이 반드시 0보다 크거나 같다는 것을 주의하자.\n\nlagrangian은 다음과 같다.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}|| + C\\sum_{i=1}^{N}\\zeta_{i} + \\sum_{i=1}^{N}\\alpha_{i}(1 - \\zeta_{i} y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b)) -  \\sum_{i=1}^{N}\\beta_{i}\\zeta_{i}\n$$\n\nKKT condition을 이용하여 주요 값들을 구하면 다음과 같은 등식을 얻을 수 있다.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n$$\n\\sum_{i=1}^{N}\\beta_{i} = C\n$$\n\n마지막으로 이를 Dual Problem으로 재정의하면 다음과 같아진다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, \u0026 \\\\\n  \u0026 0 \\leq \\alpha_{i} \\leq C, \u0026 i = 1, ..., N \n\\end{align*}\n$$\n\n결국 기존 Hard margin과 비교했을 대는 마지막 constraint에 $\\alpha_{i} \\leq C$가 추가된 것 밖에 없다.\n\n---\n\n마지막으로 여기서 하나의 insight를 더 얻을 수 있다.  \nL1-norm의 optimization으로 돌아가보자.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 {1\\over2}||\\bold{w}|| + C\\sum_{i=1}^{N}\\zeta_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, \u0026 \\\\\n  \u0026 \\zeta_{i} \\geq 0 \u0026 i = 1, ..., N\n\\end{align*}\n$$\n\n목적 함수의 slack variable에 constraint의 값을 대입하여, 다음과 같이 변환이 가능하다.\n\n$$\n\\min {C^{\\prime}\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}max\\{ 0, 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\}\n$$\n\n이 형태는 logistric regression에 regularization을 수행한 것과 동일한 형태를 가지게 된다. 즉, 이전 logistic regression에서 regularization을 다루지 않았는데, 결국은 soft margin svm의 L1-norm 목적함수가 logistic regression 중에서도 hinge function이라는 것을 이용했을 때의 regularization이 되는 것이다.\n\n## Generalization\n\n여태까지 살펴본 Regression을 통해서 우리는 General한 Classification 방식을 지정할 수 있다. 우선 아래 식을 살펴보자.\n\n- Linear Regression(Quadratic Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}{1\\over2}(1 - y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})) )^{2}$\n- Logit Regresion(Log Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}\\log( 1 + \\exp[-y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})]) )$\n- Binary SVM(Hinge Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}max\\{ 0, 1 - y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})) \\}$\n\n여태까지 나온 식들을 살펴보면 위와 같다. 우리는 여기서 아래와 같은 일반적인 형태의 Classification을 제시할 수 있다. \n\n- General Classification  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}\\varepsilon\\log( 1 + \\exp[-y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})]) )$\n\n여기서 $\\varepsilon$이 1이면 바로 logistic regression이 되고, $\\varepsilon$이 0에 수렴할 수록 SVM이 된다. 아래 그림을 보면 이를 알 수 있다.\n\n![compare-regressions](/images/compare-regressions.jpg)\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)","slug":"ml-svm","date":"2022-10-18 17:29","title":"[ML] 4. SVM","category":"AI","tags":["ML","SVM","GeneralClassifier"],"desc":"우리는 Classification을 하기 위해서 Logistic Regression을 수행하였다. 그 결과 결국 Classification도 결국은 선을 긋는 것이라는 결론을 내리게 되었다. 하지만, 여기서 그치지 않고 하나 더 고민해 볼 수 있는 것이 있다. 바로 주어진 데이터에 대해서 완벽하게 구분하는 decision boundary가 여러 개 있을 때, 어떤 것이 가장 좋은 것일까? 이것에 대한 아이디어를 제시하는 것이 SVM이다. 해당 Posting에서는 이에 대해서 살펴보도록 하겠다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n이전 Posting에서는 SVM에 대해서 알아보았다. 일반적인 Logistic Regression에서는 softmax function을 통해서 여러 class를 구분할 수 있었지만, SVM의 경우 구분 선이 결국은 hyperplane으로만 표현 가능하다. 이를 해결하기 위한 SVM에서의 여러 해결책을 알아보자.\n\n## Multiclass in SVM\n\n가장 쉽게 생각할 수 있는 것은 SVM을 결합해서 Multiclass를 구분할 수 있다는 idea이다. 아래에서 곧바로 제시할 아이디어들이 이에 대한 내용이다.\n\n\u003e **1. OvR SVM**\n\nOne vs Rest 의 약자로 다양한 별명이 존재한다. (One vs All, OVA, One Against All, OAA)  \n이름에서부터 느껴지다시피 하나의 class와 그 외에 모든 class를 하나로 묶어서 SVM을 총 class 갯수만큼 만들어서 각 decision boundary로 부터 거리가 양의 방향으로 가장 큰 class를 선택하는 방식이다.\n\n$$\n\\argmax_{k \\in [K]}(\\bold{w}_{(k)}^{\\top}\\phi(\\bold{x})+ b_{(k)})\n$$\n\n![svm-ovr](/images/svm-ovr.jpg)\n\n이 방식은 하나의 큰 문제를 갖고 있는데, 그것은 과도한 데이터 불균형을 유발한다는 것이다. 이러한 문제는 class의 수가 많아질 수록 더 심해진다.\n\n\u003e **2. OvO SVM**\n\nOne vs (Another) One의 약자로, 해당 방식은 1대1로 비교하면서 각 SVM에서 선택한 class 중에서 가장 많은 선택을 받은 class를 최종한다. OvR과는 다르게 각 각의 class를 1대1로 비교하기 때문에 데이터의 불균형에 대한 위협은 덜하다. 하지만, 해당 과정을 수행하기 위해서는 총 K(K-1)/2개의 SVM이 필요하다.\n\n![svm-ovo](/images/svm-ovo.jpg)\n\n또한, 그림에서 \"?\"로 표시된 부분을 어떤 class로 선택할지에 대한 기준이 없다. 왜냐하면, 각 영역에서 한 표씩만 받기 때문이다.\n\n\u003e **3. DAG SVM**\n\n앞 서 보았던 OvO와 OvR의 문제를 해결하기 위해서 장단점을 취하기 위해서 둘을 결합한 방식이다. 계층 형태로 SVM을 구성하기 때문에 OvO보다는 적은 SVM을 사용하면서, OvO에서의 과도한 데이터 불균형을 해결한다.\n\n![svm-multiclass-comparing](/images/svm-multiclass-comparing.jpg)\n\n\u003e **4. WW SVM**\n\nmulticlass 구분을 SVM 최적화 과정에 적용하기 위해서 목적 함수의 형태를 변형하여 구현한 방법으로 자세히 다루지 않지만, 궁금하다면 해당 [🔗 link](https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf)를 통해서 확인할 수 있다.\n\n## Kernel Method\n\n이전까지는 실제로 SVM의 형태를 변형시키거나 SVM을 여러 개 활용하여 multiclass classification을 수행하기 위한 방법을 보았다.\n\n또 다른 방법이 존재한다. 바로 input 공간을 확장하는 것이다. 즉, 더 많은 유의미한 feature를 수집하거나 기존 feature를 가공하여 새로운 feature로 활용하는 것이다. 시스템적으로 해결할 수 있는 방법은 기존 feature를 가공하여 새로운 feature를 활용하는 것이다. 아래의 예시를 보자.\n\n![feature-transposing](/images/feature-transposing.jpg)\n\n왼쪽 공간에서는 SVM은 decision vector를 적절하게 선택하는 것이 어렵다. 하지만, 기존 x 데이터에 절대값을 취하여 나타내어 데이터에 추가하면, 쉽게 decision boundary를 결정하는 것을 볼 수 있다. 그렇다면, 이러한 여러 변환 함수를 적용해보며 여러 feature를 더 추출하는 것이 좋은 해결책을 가져다 줄 것이다.\n\n그렇다면, 우리의 Soft margin SVM의 Dual Problem을 다시 한 번 살펴보자.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, \u0026 \\\\\n  \u0026 0 \\leq \\alpha_{i} \\leq C, \u0026 i = 1, ..., N \n\\end{align*}\n$$\n\n이것을 feature 변환(basis function을 취한다.)을 통해서 다음과 같이 변형한다는 것이다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\red{\\boldsymbol{\\phi}^{\\top}(\\bold{x}_{i})\\boldsymbol{\\phi}(\\bold{x}_{j})} + \\sum_{i=1}^{N}\\alpha_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, \u0026 \\\\\n  \u0026 0 \\leq \\alpha_{i} \\leq C, \u0026 i = 1, ..., N \n\\end{align*}\n$$\n\n하지만, 우리가 새로운 feature를 생성할 수록, 그리고 기존 feature를 복잡하게 사용할 수록 $\\boldsymbol{\\phi}(\\bold{x}_{i})$를 연산하는 비용이 커질 수 밖에 없다.  \n\n따라서, 우리는 일종의 trick을 하나 사용하도록 한다. 바로, 매 bayese update 마다 변하지 않고 재사용되는 값인 $\\boldsymbol{\\phi}^{\\top}(\\bold{x}_{i})\\boldsymbol{\\phi}(\\bold{x}_{j})$를 다른 값으로 대체하면 어떨까? 그렇게 하면 우리는 $\\boldsymbol{\\phi}(\\bold{x}_{i})$를 계산하고 구성하는 수고를 덜 수 있다.\n\n이것이 kernel method(trick)의 핵심 아이디어이다.\n\n가장 대표적인 예시로 아래와 같은 복잡한 $\\phi$ 가 주어졌을 때,\n\n$$\n\\boldsymbol{\\phi}(x) = \\exp({{-x^{2}}\\over{2\\sigma^{2}}})[1, \\sqrt{1\\over{1!\\sigma^{2}}}x, \\sqrt{1\\over{2!\\sigma^{4}}}x^{2}, \\sqrt{1\\over{3!\\sigma^{6}}}x^{3}, \\cdots]\n$$\n\n아래의 (RBF) kernel로 대체가 가능해진다.\n\n$$\n\\kappa(x,x^{\\prime}) = \\exp(-{{(x - x^{\\prime})}\\over{2\\sigma^{2}}}) = \\boldsymbol{\\phi}^{\\top}(x)\\boldsymbol{\\phi}(x^{\\prime})\n$$\n\n대게 우리가 표현하고자 하는 형태의 $\\boldsymbol{\\phi}$는 이미 특정 kernel 함수로 매핑되고 있으니 직접 $\\boldsymbol{\\phi}$를 계산하기 전에 찾아보는 것이 도움이 될 것이다.[🔗 link](https://dataaspirant.com/svm-kernels/#t-1608054630726)\n\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- A Comparison of Methods for Multi-class Support Vector Machines, Chih-Wei Hsu and Chih-Jen Lin, https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf\n- SEVEN MOST POPULAR SVM KERNELS, https://dataaspirant.com/svm-kernels/#t-1608054630726","slug":"ml-multiclass-classification-in-svm","date":"2022-10-18 23:19","title":"[ML] 5. Multiclass Classification in SVM","category":"AI","tags":["ML","SVM","KernelMethod"],"desc":"이전 Posting에서는 SVM에 대해서 알아보았다. 일반적인 Logistic Regression에서는 softmax function을 통해서 여러 class를 구분할 수 있었지만, SVM의 경우 구분 선이 결국은 hyperplane으로만 표현 가능하다. 이를 해결하기 위한 SVM에서의 여러 해결책을 알아보자.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n우리는 Linear Regression, Logistic Regression, SVM을 거치며 data로 부터 유의미한 pattern을 발견하는 과정을 알아보았다. 이 과정은 우리에게 명확한 식 하나를 제시하였고, 모든 과정을 우리가 제어할 수 있게 하였다. 하지만, 실제 데이터를 우리가 모두 명확하게 이해할 수 있는 형태로 분류할 수 있는 것인지는 의문이 들 수 있다. 그렇다면, 우리가 이해하지는 못하지만, 알아서 최적의 결과를 가져오게 할 수 있는 방법이 있을까? 이런 마법같은 일에 대한 아이디어를 제시하는 것이 Neural Network이다.\n  게 알지 못하지만 input이 들어왔을 때, 이를 처리해서 output을 전달하는 시스템을 우리의 신체에서 찾게 된다. 바로 우리 몸을 이루는 신경망이다. 예시로 우리는 눈을 통해 빛이라는 input을 받으면, 우리 눈과 뇌에서 무슨 일이 발생하는지는 모르지만 결과적으로 우리는 물체를 볼 수 있다. 이 과정을 추측의 과정에 도입하면 어떻게 될까?\n\n## Perceptron\n\nPerception(인지 능력) + Neuron(신경 세포)의 합성어이다. 중고등학교 생명 수업을 들었다면, 우리의 모든 신경은 뉴런이라는 단위 세포로 이루어진다는 것을 배웠을 것이다. 즉, 우리의 신경 세포를 컴퓨터 공학에서 활용하기 위해서, 수학적으로 변환한 것이다. 형태를 먼저 살펴보자.\n\n$$\ny = sign(\\bold{w}^{\\top}\\bold{x} + b)\n$$\n\n![nn-perceptron-1](/images/nn-perceptron-1.jpg)\n\n대단한 것을 기대했다면 실망하겠지만, simple한 것이 최고라는 연구의 진리에 따라서 위의 식은 꽤나 합리적이다. 우리가 Linear Regression과 Logistic Regression을 배웠으니 알 것이다. 이는 사실 Linear Regression을 이용해서 우리가 Classification을 수행할 때 사용했던 식이다. 즉, perceptron 하나는 input을 선형으로 구분할 수 있도록 하는 decision boundary를 찾는 것과 같다.\n\n\u003e **Optimization**\n\n그렇다면, 해당 perceptron을 통해서 모든 데이터를 구분하기 위해서는 다음을 만족하는 $\\bold{w}$를 찾아야 한다.\n\n$$\ny_{n} = \n\\begin{cases}\n1  \u0026\\text{ if  } \\bold{x}_{n} \\in \\mathcal{C}_{1} \\\\\n-1 \u0026\\text{ if  } \\bold{x}_{n} \\in \\mathcal{C}_{2} \\\\\n\\end{cases}\n$$\n$$\ny_{n}\\bold{w}^{\\top}\\bold{x}_{n} \\gt 0, \\forall n\n$$\n\n결국 Loss 함수는 perceptron의 잘못된 classification 결과를 최소화하는 것이다.\n\n$$\n\\mathcal{J}(\\bold{w}) = - \\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{w}^{\\top}\\bold{x}_{n}} \\quad( \\mathcal{M}(\\bold{w}) = \\{ n : y_{n}\\bold{w}^{\\top}\\bold{x}_{n} \\} )\n$$\n$$\n\\nabla_{\\bold{w}}\\mathcal{J}(\\bold{w}) = - \\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{x}_{n}}\n$$\n\n따라서, 우리가 사용할 수 있는 Gradient Descent식은 다음과 같다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} + \\alpha\\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{x}_{n}}\n$$\n\n간단한 예시로 AND, OR Gate를 percentron을 통해 표현해보자.\n\n![nn-and-gate](/images/nn-and-gate.jpg)\n![nn-or-gate](/images/nn-or-gate.jpg)\n\n하지만, 우리가 다루는 데이터는 항상 완벽하게 선으로 나뉘어지지는 않는다. 하나의 perceptron으로는 아래의 XOR조차도 구분해낼 수 없다.\n\n![nn-multi-line-example](/images/nn-multi-line-example.jpg)\n\n## Multilayer Perceptron\n\n위의 문제를 해결하기 위해서 나온 것이 perceptron을 다층으로 쌓아서 해결하는 방법이다. 이제는 하나의 신경세포였던 perceptron을 진짜 신경망처럼 연결해보자는 것이다.\n\n먼저 추상적인 예시를 생각해보자. 우리가 XOR Gate를 만들기 위해서는 어떤 Gate를 결합해야할까?\n\n$$\na \\oplus b = ab + \\bar{a}\\bar{b}\n$$\n\n우리는 AND Gate 2개 연산을 수행하고, 해당 결과값을 이용해서 OR Gate 연산을 수행하면 XOR Gate를 표현할 수 있다는 것을 알고 있다. 그렇다면, 각 Gate는 우리가 perceptron으로 나타낼 수 있었는데 그냥 이것을 gate로 표현하듯이 똑같이 나타내면 풀 수 있지 않을까?\n\n그래서 직접 수행해보면 다음과 같은 값을 구할 수 있다.\n\n![nn-xor-gate](/images/nn-xor-gate.jpg)\n\n```plaintext\n 🤔 Insight\n\n 위의 과정을 보다보면 놀라운 것을 하나 발견할 수 있다. 바로 왼 쪽 그림의 변화이다. \n 첫번째, 두 개의 perceptron을 통해서 만들어진 output이 이루는 결과값의 형태로 feature를 변환하면, \n 하나의 perceptron으로 decision boundary를 그릴 수 있다는 것이다. \n 이는 마치 이전 linear regression에서 배웠던 basis function(ϕ)이 했던 역할이다.\n\n 그렇다면, 이를 더욱 확장해보자. \n 만약 해당 Layer가 더 깊어진다고 해도, 출력 직전의 layer는 단순히 이전 모든 layer는 입력 데이터를 가공해서\nfeature를 변환하는 하나의 basis function(ϕ)를 취한 것으로 이해할 수 있다.\n```\n\n결론적으로 우리는 더 복잡하고, 어려운 문제의 경우에도 더 깊게 신경망을 구성하면 결국은 문제를 풀 수 있다는 것이다.\n\n\u003e **Universal Approximation Theorem**\n\n위와 같은 깊은 신경망 구조를 이용하자는 주장도 있지만, 이와 유사하게 넓은 신경망을 쓰자는 주장도 존재했다.  \n\n![nn-universal-approx-theorem-1](/images/nn-universal-approx-theorem-1.jpg)\n\n만약, 우리가 하나의 Layer와 output에서 최종 output perceptron만 갖고 처리를 한다면, 결국 여러 perceptron의 weighted 합으로 볼 수 있다. 그 경우 우리는 계단 함수의 weighted 합으로 생각할 수 있는데 perceptron이 많아질 수록 촘촘해지며 정답과 유사한 추론이 가능해진다.(마치 적분의 개념과 유사하다. 물론 이는 추상적인 설명이기 때문에 실제로는 계단함수의 합이기 때문에 좀 다르다.)\n\n![nn-universal-approx-theorem-2](/images/nn-universal-approx-theorem-2.jpg)\n\n위의 그림을 보면 이해할 수 있다. 하지만, 이 방식은 결국 모든 함수 형태를 기억하는 것이다.(**memorizer**) 이것은 input data가 많아질 수록 복잡도가 급격하게 증가하기 때문에 학습과 예측과정에 굉장히 많은 시간을 소모한다.\n\n\u003e **Multilayer Optimization(Backpropagation)**\n\n그렇다면, 넓은 신경망이 한계가 있으니 선택지는 input과 output 사이의 layer(**hidden layer**)의 갯수를 늘려서 깊은 신경망을 만드는 것이다. 하지만, 우리가 사용하고 있는 perceptron은 sign함수로 감싸져있기 때문에 미분 시에 기울기가 0이라는 문제를 갖는다. 또한, 그렇다고 정답의 갯수를 이용하기에는 각 perceptron의 영향을 전달하기에 부족하다는 것이 명확하다. 따라서, 우리는 perceptron에 있는 정답을 판별하는 함수 sign을 다른 함수로 대체하기로 한다.\n\n![nn-perceptron-2](/images/nn-perceptron-2.jpg)\n\n여기서 이 함수를 우리는 **activation function**이라고 부르고 대표적으로는 같은 종류가 있다.\n\n- **sigmoid**  \n  우리가 가장 쉽게 생각할 수 있는 함수이다. logistic regression에서 사용해본만큼 기울기값을 효과적으로 가질 수 있다.\n- **tanh**  \n  sigmod와 굉장히 유사한 함수이다. 따라서, 비슷한 용도로 사용될 수 있다.\n- **ReLU**  \n  출력 시점에서는 사용하지 않지만, 각 각의 hidden layer에서 이를 사용하는 경우가 많다. 왜냐하면, sigmoid 함수는 출력값의 형태가 [0, 1], tanh는 [-1, 1]이기 때문에 반복해서 적용하면, gradient가 사라지는 현상이 발생할 수 있다. 따라서, 기울기를 있는 그대로 적용할 수 있는 이러한 형태를 출력 이전에는 많이 사용한다.\n- **Leaky ReLU**  \n  ReLU가 음수값을 완전히 무시하는데 Leaky ReLU는 이러한 데이터가 조금이라도 의미 있는 경우에 사용할 수 있다.\n- **ELU**  \n  Leaky ReLU와 비슷한 이유이다.\n\n  ![activation-functions](/images/activation-functions.png)\n  \n자료가 보이지 않는다면 [🔗 wikipedia](https://en.wikipedia.org/wiki/Activation_function)를 참고하자.\n\n---\n\n자 이제 실제로 어떻게 optimization을 수행할지를 알아보도록 하자.\n\n먼저, Loss는 가장 마지막 layer(output layer)의 output과 실제 값과의 차이가 될 것이다. 따라서, 다음과 같이 정의할 수 있다.\n\n(아래서 $\\bold{h}_{L}$은 L번째 layer의 output을 의미한다.)\n\n$$\n\\begin{align*}\n\\mathcal{L} \u0026= \\sum_{n=1}^{N}{\\ell(y_n, \\bold{h}_L)} \\\\\n\u0026= \\sum_{n=1}^{N}{(y_{n} - \\bold{h}_{L})^2} \\\\\n\u0026= \\sum_{n=1}^{N}{(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))^2} \\\\\n\u0026= \\sum_{n=1}^{N}{(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\sigma(\\bold{w}_{L-1}^{\\top}\\bold{h}_{L-2} + b_{L-2}) + b_{L-1}))^2} \\\\\n\u0026= ...\n\\end{align*}\n$$\n\n여기서 중요한 것은 우리는 전체 $\\bold{W}$를 학습시켜야 한다는 것이다. 우리는 출력층만 학습하는 게 아니라 전체 모든 layer의 $\\bold{w}_{i}$를 업데이트해야 한다는 것이다.\n\n그러기 위해서는 우리는 ${{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{i}}}$를 모두 구해야 한다는 것이다. 아마 가장 습관적으로 하는 행위는 숫자가 작은 값부터 편미분하면서 진행하는 것이다. 하지만, 그렇게 하지말고 반대 순서로 미분을 하라는 것이 **backpropagation**의 main idea이다.\n\n$$\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L}}} = \\sum_{n=1}^{N}\\{({\\partial \\bold{h}_L \\over \\partial \\bold{w}_{L}} )\\times\\red{-2(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))}\\}\n$$\n\n$$\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-1}}} = \\sum_{n=1}^{N}\\{({\\partial \\bold{h}_{L} \\over \\partial \\bold{w}_{L-1} })\\times\\red{-2(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))}\\}\n$$\n\n즉, 다음과 같은 chain rule을 이용하는 것이다.\n\n$$\n\\begin{align*}\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L}}} \u0026= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L}}} \\\\\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-1}}} \u0026= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n\u0026= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{h}_{L-1}}} {{\\partial\\bold{h}_{L-1}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n\u0026= \\blue{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L-1}}} } {{\\partial\\bold{h}_{L-1}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-2}}} \u0026= \\blue{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L-1}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L-2}}} \\\\\n\\end{align*}\n$$\n\n우리는 빨간색과 파란색 부분의 연산을 재활용할 수 있다는 것이다. 또한, ${{\\partial\\bold{h}_{l}}\\over{\\partial \\bold{w}_{l}}}$은 굉장히 쉬운 연산이기에 우리가 신경 써서 계산해야 할 값은 매단계를 연결해줄 $ {{\\partial\\bold{h}_{l}}\\over{\\partial \\bold{h}_{l-1}}}$이다.\n\n![ml-backpropagation](/images/ml-backpropagation.jpg)\n\n## Loss Function\n\n우선 KL-Divergence, Entropy, Cross Entropy에 대한 약간의 이해가 필요하니 이전 Posting([🔗 Base Knowledge](posts/ml-base-knowledge))을 살펴보고 오자.\n\n위에서는 자연스럽게 Loss를 계산할 때, Squared Error를 사용하였다. 하지만 경우에 따라서는 다양한 함수를 사용할 수 있다. multiclass classification에서는 **Cross Entropy Loss**를 사용한다. \n\n우선 Cross Entropy Loss는 대게 L2 Loss(Squared Error)와 같이 비교되어진다. 우선 우리가 이전 [🔗 Parametric Estimation](posts/ml-parametric-estimation)에서 MLE를 다룰 때, KL-Divergence를 통해서 MLE가 최적 parameter를 찾을 것이라는 걸 증명한 적이 있다. 그렇다면, 우리가 [🔗 Logistic Regression](/posts/ml-logistic-regression)에서 Squared Error를 통해서 Loss를 구했던 공식을 확인해보자.(Gradient Asecent Part)\n\n여기서 우리는 다음과 공식을 봤었다.\n\n$$\n\\argmax_{\\bold{w}} \\sum_{n=1}^{N}y_{n}\\log{\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}) + (1-y_{n})\\log{(1-\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}))} }\n$$\n\n이 공식을 Cross Entropy를 통해서 설명할 수 있다.\n\n$$\n\\begin{align*}\nH_{q}(p) \u0026= - \\sum_{x \\in \\Omega}q(x)\\log_{2}p(x) \\\\\n\u0026= \\sum_{n=1}^{N}{[-y_{n}\\log\\hat{y}_{n} - (1- y_{n})\\log(1-\\hat{y}_{n})]}\n\\end{align*}\n$$\n\n즉, 여기서 우리가 얻을 수 있는 insight는 Cross Entropy는 sigmoid를 취한 binary classification에서 Squared Error와 같고, 이러한 Cross Entropy를 Squared Error가 할 수 없는 Multiclass에는 적용할 수 있을 것이라는 점이다. 왜냐하면, multiclass classification에 사용되는 Softmax Function을 이용해서 Sigmoid function을 유도하기 때문이다. 잠시 까먹었을까봐 Softmax 함수를 다시 적는다.\n\n$$\n\\hat{y}_{k} = {{\\exp(\\bold{w}_{k}^{\\top}\\bold{x})}\\over{\\sum_{i=1}^{K}{\\exp(\\bold{w}_{i}^{\\top}\\bold{x})}}}\n$$\n\n따라서, Cross Entropy Loss를 대입하여 다음과 같은 Loss를 얻을 수 있다.\n\n$$\n\\mathcal{L} = \\sum_{n=1}^{N}\\sum_{k=1}^{K}[-y_{k,n}\\log\\hat{y}_{k,n}],\\quad y_{k,n} = p(x_{n} \\in C_{k}| x_{n}) \n$$\n\n여기서 $y_{k,n}$은 one-hot encoding된 데이터로, 정답인 class만 1이고 나머지는 모두 0으로 되어 있다. 따라서, multiclass classification에서는 위와 같은 Loss를 주로 사용한다.\n\n이 두가지 뿐만 아니라 여러가지 Loss Function이 이미 존재한다. 예전에 잠깐 설명했던 L1 Loss부터 시작해서 NLLLoss, KLDivLoss 등등 존재하며, data의 특성과 output의 형태에 따라서 우리는 스스로 Loss Function을 새로 정의할 수도 있다.\n\n\n\n\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- activation function, wikipedia, https://en.wikipedia.org/wiki/Activation_function","slug":"ml-nn","date":"2022-10-20 09:00","title":"[ML] 6. Neural Network","category":"AI","tags":["ML","NeuralNetwork","Perceptron","Backpropagation","CrossEntropyLoss"],"desc":"우리는 Linear Regression, Logistic Regression, SVM을 거치며 data로 부터 유의미한 pattern을 발견하는 과정을 알아보았다. 이 과정은 우리에게 명확한 식 하나를 제시하였고, 모든 과정을 우리가 제어할 수 있게 하였다. 하지만, 실제 데이터를 우리가 모두 명확하게 이해할 수 있는 형태로 분류할 수 있는 것인지는 의문이 들 수 있다. 그렇다면, 우리가 이해하지는 못하지만, 알아서 최적의 결과를 가져오게 할 수 있는 방법이 있을까? 이런 마법같은 일에 대한 아이디어를 제시하는 것이 Neural Network이다.  게 알지 못하지만 input이 들어왔을 때, 이를 처리해서 output을 전달하는 시스템을 우리의 신체에서 찾게 된다. 바로 우리 몸을 이루는 신경망이다. 예시로 우리는 눈을 통해 빛이라는 input을 받으면, 우리 눈과 뇌에서 무슨 일이 발생하는지는 모르지만 결과적으로 우리는 물체를 볼 수 있다. 이 과정을 추측의 과정에 도입하면 어떻게 될까?","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}]},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"ml-logistic-regression"},"buildId":"eGkZYbzPXM35ApK3V1pre","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>