<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Network 분야에 관심이 많은 개발자로 Computer Engineering 관련 Posting을 주로 다룹니다."/><meta property="og:description" content="Network 분야에 관심이 많은 개발자로 Computer Engineering 관련 Posting을 주로 다룹니다."/><meta property="og:type" content="blog"/><meta property="og:site_name" content="JustLog"/><meta property="og:image" content="https://euidong.github.io/logo192.png"/><title>AI | JustLog</title><meta property="og:title" content="AI | JustLog"/><link rel="canonical" href="https://euidong.github.io/tags/AI"/><meta property="og:url" content="https://euidong.github.io/tags/AI"/><meta name="next-head-count" content="11"/><link rel="icon" href="https://euidong.github.io/favicon.png"/><link rel="apple-touch-icon" href="https://euidong.github.io/logo192.png"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7452732177557701" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/css/d4ec5c8b3df09443.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d4ec5c8b3df09443.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6dc16d084a5153e5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6dc16d084a5153e5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-81da43a8dcd978d9.js" defer=""></script><script src="/_next/static/chunks/main-7b6c38cbad60dfcf.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8aec970175275c86.js" defer=""></script><script src="/_next/static/chunks/675-ae8e8a351ce30ae2.js" defer=""></script><script src="/_next/static/chunks/pages/categories/%5Bsubject%5D-9b00d940d6a3d32e.js" defer=""></script><script src="/_next/static/IY7V1zkqy9YrwU6Mfg7tn/_buildManifest.js" defer=""></script><script src="/_next/static/IY7V1zkqy9YrwU6Mfg7tn/_ssgManifest.js" defer=""></script><script src="/_next/static/IY7V1zkqy9YrwU6Mfg7tn/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_wrapper__dKJSz root"><header class="Layout_header__XosLl" style="position:sticky"><div><button tabindex="1" class="SideBarToggler_search_bar_toggler__CEuUg"><svg stroke="currentColor" fill="none" stroke-width="0" viewBox="0 0 24 24" height="35px" width="35px" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></button><nav class="SideBar_side_bar__wrapper--close__8Nwnr"><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/">Home</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/tags">Tags</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Algorithm">Algorithm<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Computer%20Architecture">Computer Architecture<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->7<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Tech">Tech<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->17<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Web">Web<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Network">Network<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/AI">AI<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->12<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Paper">Paper<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->1<!-- -->)<!-- --></span></a></nav></div><a class="Logo_logo___yD0t" tabindex="1" href="/"></a><div><button class="SearchBarToggler_search_bar_toggler__3dHbA" tabindex="2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg></button></div></header><section><div class="RowCard_row_card__list__background___xFj5"><h1 class="RowCard_row_card__list__title__t4a2h">AI</h1><label class="RowCard_row_card__list__select__wrapper__TZ4_9"><select class="RowCard_row_card__list__select__dxkxA"><option class="RowCard_row_card__list__select__option__GRKZU">최신순<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">AtoZ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">ZtoA<!-- --></option></select></label><ul class="RowCard_row_card__list__wrapper__5Gtgi"><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-hmm"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 5. Hidden Markov Model" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 5. Hidden Markov Model" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-hmm">[NLP] 5. Hidden Markov Model</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 21일 21시 55분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/MarkovModel"># <!-- -->MarkovModel<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/HMM"># <!-- -->HMM<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/HiddenMarkovModel"># <!-- -->HiddenMarkovModel<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-classification"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 4. Classification" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 4. Classification" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-classification">[NLP] 4. Classification</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 21일 21시 53분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Classification"># <!-- -->Classification<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/SpamFiltering"># <!-- -->SpamFiltering<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/F1Score"># <!-- -->F1Score<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-language-modeling"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 3. Language Modeling" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 3. Language Modeling" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-language-modeling">[NLP] 3. Language Modeling</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 21일 12시 15분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NoisyChannel"># <!-- -->NoisyChannel<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Ngram"># <!-- -->Ngram<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/LanguageModeling"># <!-- -->LanguageModeling<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Smoothing"># <!-- -->Smoothing<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/WordClass"># <!-- -->WordClass<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-nn"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 6. Neural Network" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 6. Neural Network" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-nn">[ML] 6. Neural Network</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 20일 09시 00분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NeuralNetwork"># <!-- -->NeuralNetwork<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Perceptron"># <!-- -->Perceptron<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Backpropagation"># <!-- -->Backpropagation<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/CrossEntropyLoss"># <!-- -->CrossEntropyLoss<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-text-processing"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 2. Text Processing" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 2. Text Processing" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-text-processing">[NLP] 2. Text Processing</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 19일 21시 59분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Regex"># <!-- -->Regex<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Tokenization"># <!-- -->Tokenization<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Collocation"># <!-- -->Collocation<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/MinimumEditDistance"># <!-- -->MinimumEditDistance<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-linguistics"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 1. Linguistics" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 1. Linguistics" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-linguistics">[NLP] 1. Linguistics</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 19일 09시 03분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Languagistics"># <!-- -->Languagistics<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-multiclass-classification-in-svm"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 5. Multiclass Classification in SVM" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 5. Multiclass Classification in SVM" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-multiclass-classification-in-svm">[ML] 5. Multiclass Classification in SVM</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 18일 23시 19분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/SVM"># <!-- -->SVM<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/KernelMethod"># <!-- -->KernelMethod<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-svm"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 4. SVM" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 4. SVM" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-svm">[ML] 4. SVM</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 18일 17시 29분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/SVM"># <!-- -->SVM<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/GeneralClassifier"># <!-- -->GeneralClassifier<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-logistic-regression"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 3. Logistic Regression" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 3. Logistic Regression" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-logistic-regression">[ML] 3. Logistic Regression</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 18일 09시 58분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/LogisticRegression"># <!-- -->LogisticRegression<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Classification"># <!-- -->Classification<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/SigmoidFunction"># <!-- -->SigmoidFunction<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/SoftmaxFunction"># <!-- -->SoftmaxFunction<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NewtonMethod"># <!-- -->NewtonMethod<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-linear-regression"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 2. Linear Regression" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 2. Linear Regression" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-linear-regression">[ML] 2. Linear Regression</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 17일 09시 46분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/LinearRegression"># <!-- -->LinearRegression<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/BasisFunction"># <!-- -->BasisFunction<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Regularization"># <!-- -->Regularization<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/GradientDescent"># <!-- -->GradientDescent<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Momentum"># <!-- -->Momentum<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/StochasticGradientDescent"># <!-- -->StochasticGradientDescent<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-parametric-estimation"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 1. Parametric Estimation" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 1. Parametric Estimation" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-parametric-estimation">[ML] 1. Parametric Estimation</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 15일 11시 25분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/MLE"># <!-- -->MLE<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/MAP"># <!-- -->MAP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Bayesian"># <!-- -->Bayesian<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-base-knowledge"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 0. Base Knowledge" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 0. Base Knowledge" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-base-knowledge">[ML] 0. Base Knowledge</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 14일 19시 28분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Probability"># <!-- -->Probability<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Calculus"># <!-- -->Calculus<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/InformationTheory"># <!-- -->InformationTheory<!-- --></a></ul></div></div></ul></div></section><footer class="Layout_footer__EL5v8"><div class="Layout_footer__copyright__r5baC"><span>Copyright © euidong</span><br/><span>모든 컨텐츠에 대한 저작권은 작성자에게 존재합니다. <!-- --><br/>불법 복제를 통한 상업적 사용을 절대적으로 금지합니다. <!-- --><br/>단, 비상업적 이용의 경우 출처 및 링크를 적용한다면 자유롭게 사용가능 합니다.<!-- --></span><span>Also I use photos by<!-- --> <!-- --><a href="https://unsplash.com/@lorenzoherrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Lorenzo Herrera</a> <!-- -->on<!-- --> <!-- --><a href="https://unsplash.com/s/photos/tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Unsplash</a></span></div><div class="Layout_footer__contents__YZWSm"><a class="Layout_footer__contents__link__K_TKH" href="https://github.com/euidong" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>github</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://euidong.github.io/portfolio" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M6 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm-5 6s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H1zM11 3.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 0 1h-4a.5.5 0 0 1-.5-.5zm.5 2.5a.5.5 0 0 0 0 1h4a.5.5 0 0 0 0-1h-4zm2 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2zm0 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2z"></path></svg><span>portfolio</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://chrome.google.com/webstore/detail/bonfire/nkooidijgbppkojdgkoafcoppnohdfka?hl=ko" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 16c-1.066-2.219-0.498-3.49 0.321-4.688 0.897-1.312 1.129-2.61 1.129-2.61s0.706 0.917 0.423 2.352c1.246-1.387 1.482-3.598 1.293-4.445 2.817 1.969 4.021 6.232 2.399 9.392 8.631-4.883 2.147-12.19 1.018-13.013 0.376 0.823 0.448 2.216-0.313 2.893-1.287-4.879-4.468-5.879-4.468-5.879 0.376 2.516-1.364 5.268-3.042 7.324-0.059-1.003-0.122-1.696-0.649-2.656-0.118 1.823-1.511 3.309-1.889 5.135-0.511 2.473 0.383 4.284 3.777 6.197z"></path></svg><span>chat</span></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"content":"\n## Intro\n\n이전까지 특정 word를 기반으로 하여 modeling을 수행하는 방법을 알아보았다. 하지만, 우리가 특정 word의 sequence를 통해서 각 word에 대한 classification을 한 번에 하고 싶은 경우는 어떻게 할까?(예를 들어, 각 단어의 품사를 지정하는 일) 일반적으로 각 단어가 특정 해당 class일 확률로 구하는 방법이 일반적일 것이다. 하지만, 문맥을 고려하여 확률을 구할 방법은 없을까? 그 방법은 바로 bigram을 이용하면 될 것이다. 그렇다면, 사실 우리가 사용하는 문맥이 단어 자체보다는 이전 class가 더 영향이 크다면, 이는 어떻게 해야할까? 이를 위한 해결책이 HMM이다. NLP 뿐만 아니라 여러 분야에서 넓게 사용되고 있지만, 여기서는 NLP 분야에서 어떻게 이를 사용하는지를 알아볼 것이다.\n\n## Markov Model\n\nHMM을 알아보기전에 Markov Model을 알아야 한다. 이는 특정 sequence의 확률을 추정하는 방법이다. 즉 우리에게 state sequence ($S= {s_{0}, s_{1}, ..., s_{N}}$)가 주어질 때, 각 state에서 다음 state로 전이(이동)할 확률을 이용해서 state sequence의 확률을 구하는 방법이다.\n\n![nlp-markov-model-1](/images/nlp-markov-model-1.jpg)\n\n위의 그림이 state 각 각에서 다음 state로 전이할 확률을 나타낸 것이라면, 우리는 아래 그림과 같은 그림으로 sequence의 확률을 추론할 수 있는 것이다.\n\n![nlp-markov-model-2](/images/nlp-markov-model-2.jpg)\n\n따라서, 위의 그림에서 우리가 만약 $(s_{0}, s_{1}, s_{0}, s_{2})$으로 이루어진 sequence의 확률을 얻기를 바란다면, 그 확률은 아래와 같아진다.\n$$\n\\begin{align*}\np(s_{0}, s_{1}, s_{0}, s_{2}) \u0026= p(s_{0}| \\text{start}) \\times p(s_{1}|s_{0}) \\times p(s_{0}|s_{1}) \\times p(s_{2}|s_{1}) \\times p(end|s_{2}) \\\\\n\u0026= \\pi_{0} \\times p_{01} \\times p_{10} \\times p_{12} \\times 1 \n\\end{align*}\n$$\n\n이를 잘 살펴보니 bigram에서의 Likelihood를 구하는 공식과 똑같다. 즉, state 각 각을 word라고 본다면, Markov Model을 통해서 구할 수 있는 확률은 bigram의 Likelihood인 것이다.\n\n그리고 이를 일반화하면 다음과 같다.\n\n$$\np(seq) = \\prod_{i=1}^{N}p(seq_{i}|seq_{i-1})\n$$\n\n그런데, 여기서 n이 3 이상인 ngram을 적용하고 싶다면, 각 state를 n-1 gram으로 설정하면 된다.\n\n$$\n\\begin{align*}\nX_{i} \u0026= (Q_{i-1}, Q_{i}) \\text{라면, }\\\\\nP(X_{i} | X_{i-1}) \u0026= P(Q_{i-1}, Q_{i} | Q_{i-2}, Q_{i-1}) \\\\\n\u0026= P(Q_{i} | Q_{i-2}, Q_{i-1})\n\\end{align*}\n$$\n\n따라서, trigram을 적용해보면 아래와 같다.\n\n$$\n\\begin{align*}\np((start, w_{0}), (w_{0}, w_{1}), (w_{1}, w_{0}), (w_{0}, w_{2})) \u0026= p(w_{0}| \\text{start}, \\text{start}) \\times p(w_{1}|\\text{start}, w_{0}) \\times p(w_{0}|w_{0}, w_{1}) \\times p(w_{2}|w_{1}, w_{0}) \\times p(end|w_{0}, w_{2}) \\\\\n\u0026= \\pi_{0} \\times p_{01} \\times p_{10} \\times p_{12} \\times 1\n\\end{align*}\n$$\n\n\n## Hidden Markov Model\n\nHidden Markov Model은 state를 하나 더 만든다는 것이 핵심이다. 그래서, 우리가 직접 관측하는 state(**observed state**)와 직접적으로 관측하지 않지만, 관측한 state들에 의존하는 state(**hidden state**) 총 두 개의 state를 사용한다. 일반적인 예시가 text가 입력되었을 때 우리는 각 단어를 observed state라고 한다면, 각 단어의 품사를 hidden state라고 정의할 수 있다.\n\n![nlp-markov-model-3](/images/nlp-markov-model-3.jpg)\n\n위의 예시는 우리가 관측하는 데이터($O$)가 3개의 state를 가지고, 이 사건에 의존적인 또 다른 사건($H$)이 3개의 state를 가지는 경우이다. 이를 이용해서 기존 Markov Model보다 복잡한 작업을 수행하는 것이 가능하다.\n\n### Estimation\n\n우리가 할 수 있는 작업은 크게 두 가지이다. 일반적인 Markov Model에서 할 수 있던 방식이 **Trellis** 방식이고, 또 다른 방식이 **Viterbi** 방식이다.\n\n1. $(o_{0}, o_{1}, o_{0}, o_{2})$의 확률이 궁금할 때(**Trellis**)\n2. $(o_{0}, o_{1}, o_{0}, o_{2})$가 주어질 때, 이것의 hidden state의 sequence 중 가장 유력한 sequence를 찾고자할 때(**Viterbi**)\n\n위의 경우를 각각 풀어보도록 하자.\n\n\u003e \u003cmark\u003e**1. Trellis**\u003c/mark\u003e\n\n우리가 직접 관측한 데이터의 sequence 자체의 확률이 궁금할 때이다. 따라서, 이에 대한 분석은 $(o_{0}, o_{1}, o_{0}, o_{2})$의 확률을 분석해보면서 설명하겠다.\n\n$$\n\\begin{align*}\np(o_{0}, o_{1}, o_{0}, o_{2}) \u0026= p(o_{0}, o_{1}, o_{0}) \\times p(o_{2} | o_{0}, o_{1}, o_{0}) \\\\\n\u0026= p(o_{0}, o_{1}, o_{0}) \\times \\{p(o_{2} | h_{0})p(h_{0} | o_{0}, o_{1}, o_{0}) + p(o_{2} | h_{1})p(h_{1} | o_{0}, o_{1}, o_{0}) + p(o_{2} | h_{2})p(h_{2} | o_{0}, o_{1}, o_{0})\\} \\\\\n\u0026= p(o_{0}, o_{1}, o_{0}) \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n\u0026= p(o_{0}, o_{1}) \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n\u0026= p(o_{0}) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n\u0026= \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) }\n\\end{align*}\n$$\n\n이를 그림으로 표현하면 다음과 같다.\n\n![nlp-hidden-markov-model-1](/images/nlp-hidden-markov-model-1.jpg)\n\n또한, 이 식을 다음과 같이 축소가 가능하다.\n\n$$\n\\begin{align*}\n  \u0026\\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =\u0026 \\sum_{i=0}^{2}\\alpha_{0 i} \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =\u0026 \\sum_{i=0}^{2}{\\alpha_{1 i} } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =\u0026 \\sum_{i=0}^{2}{\\alpha_{2 i} } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =\u0026 \\sum_{i=0}^{2}{\\alpha_{3 i} }\n\\end{align*}\n$$\n\n우리는 이를 통해서, Markov Model의 특징을 하나 배울 수 있다. 그것은 바로 복잡한 sequence 전체의 확률에서 벗어나서 바로 직전의 확률값만 으로 다음 확률을 추론할 수 있다는 것이다. 이것이 Markov Chain이라는 이론이고, 이를 이용했기 때문에 Markov Model라고 부르는 것이기도 하다.\n\n따라서, $\\alpha$는 다음과 같이 정의할 수 있다.\n\n$$\n\\alpha(t, i) = \\sum_{k=1}^{N}{\\alpha(t-1, k)p(h_{i}|h_{k})p(o = s_{t}|h_{i})} \\quad (s_{t} = \\text{input으로 들어온 sequence의 t번째 값})\n$$\n\n또, 이를 반대로 할 경우에는 다음과 같은 식을 얻을 수 있다.\n\n![nlp-hidden-markov-model-2](/images/nlp-hidden-markov-model-2.jpg)\n\n$$\n\\begin{align*}\n  \u0026\\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =\u0026 \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{\\beta_{3i}} \\\\\n  =\u0026 \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{\\beta_{2i}} \\\\\n  =\u0026 \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{\\beta_{1i}} \\\\\n  =\u0026 \\sum_{i=0}^{2}{\\beta_{0i}} \\\\\n\\end{align*}\n$$\n\n$$\n\\beta(t, i) = \\sum_{k=1}^{N}{\\beta(t+1, k)p(h_{k}|h_{i})p(o = s_{t}|h_{i})} \\quad (s_{t} = \\text{input으로 들어온 sequence의 t번째 값})\n$$\n\n위의 처럼 앞에서부터 풀이를 해나가면서, $\\alpha$의 합으로 끝이 나도록 푸는 방법을 forwarding 방식이라하고, 반대로 뒤에서부터 풀이하면서 $\\beta$의 합으로 푸는 방법을 backwarding 방식이라고 한다. 사실 이 경우는 HMM이 굳이 아니더라도, MM으로 구할 수 있으니 굳이 필요는 없다. 하지만, 이것은 후에 modeling 단계에서 사용하기 때문에 알아두어야 한다.\n\n\u003e \u003cmark\u003e**2. Viterbi**\u003c/mark\u003e\n\n이는 observed state의 sequence에 의해서 파생되는 가장 적절한 hidden sequence를 구하는 것이 목표이다. 이를 통해서 할 수 있는 대표적인 것이 sequence classification이다. \n\n그렇다면 가장 유력한 hidden state의 sequence를 $\\hat{s}^{(H)}$라고 하자. 이는 다음과 같다.\n\n$$\n\\begin{align*}\n\\hat{s}^{(H)} \u0026= \\argmax_{s^{(H)} \\in S^{(H)}}P(s^{(H)}|s^{(O)}) \\\\\n\u0026= \\argmax_{s^{(H)} \\in S^{(H)}}P(s^{(O)}|s^{(H)})P(s^{(H)}) \\\\\n\u0026= \\argmax_{{h_{1}, h_{2}, ..., h_{N}} \\in S^{(H)}}\\underbrace{P(o_{1}, o_{2}, ... , o_{N}|h_{1}, h_{2}, ... , h_{N})}_{\\text{Markov Model}}\\underbrace{P(h_{1}, h_{2}, ... , h_{N})}_{\\text{Markov Model}} \\\\\n\u0026= \\argmax_{{h_{1}, h_{2}, ..., h_{N}} \\in S^{(H)}}\\prod_{i=1}^{N}p(o_{i}|h_{i})p(h_{i}|h_{i-1})\n\\end{align*}\n$$\n\n![nlp-hidden-markov-model-3](/images/nlp-hidden-markov-model-3.jpg)\n\n즉, 각 layer에서 단 하나의 가장 큰 output만 살아남을 수 있게 되는 것이다. 이 과정이 사실상 HMM의 본질적인 목표이다. sequence를 입력해서 sequence 형태의 classification 결과를 얻는 것이다.\n\n### Modeling\n\n여태까지 HMM을 활용하여 sequential class를 어떻게 estimation 하는지 알아보았다. 그렇다면, 이제는 이를 위해서 사용되는 확률값을 구해야한다. 필요한 확률값은 다음과 같다.\n\n- $p(h_{i}|h_{i-1})$ : Hidden State에서 Hidden State로 넘어가기 위한 확률이다.\n- $p(o_{i}|h_{i})$ : 방출 확률로 특정 Hidden State에서 다음 State의 Observed State로 넘어가는 방법이다. \n- $\\pi_{i}$\n\nTrelli 방식에서 만들었던, $\\alpha$와 $\\beta$의 의미를 이해해야 한다. 각 각은 해당 과정까지 오면서 누적해온 확률이라고 할 수 있다. 그리고, 우리가 원하는 것은 입력으로 주어진 데이터를 잘 반영할 수 있는 확률 값을 찾는 것이다. 그렇다면, 우리가 생각할 수 있는 방법은 평균을 활용하는 것이다. 이를 구하는 과정을 먼저 살펴보자.\n\n$$\n\\begin{align*}\n  c(i, j, k) \u0026= h_{i}\\text{에서 } h_{j}\\text{로 넘어가고, } o_{k}\\text{가 관측될 확률의 합} \\\\\n  \u0026= \\sum_{t=2}^{T} \\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j) \\\\\n  \\\\\n  c(i,j) \u0026= h_{i}\\text{에서 } h_{j}\\text{로 넘어갈 확률의 합} \\\\\n  \u0026= \\sum_{k=1}^{K}\\sum_{t=2}^{T}{\\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j)} \\\\\n  \\\\\n  c(i) \u0026= h_{i}\\text{에서 상태를 변경하는 확률의 합} \\\\\n  \u0026= \\sum_{j=1}^{N}\\sum_{k=1}^{K}\\sum_{t=2}^{T}{\\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j)} \\\\\n\\end{align*}\n$$\n\n위의 값을 통해서 우리는 우리가 가지고 있던 확률을 업데이트할 수 있다.\n\n$$\n\\begin{align*}\np(h_{j}|h_{i}) \u0026= {c(i,j)\\over c(i)} \\\\\np(o_{k}|h_{i}) \u0026= {c(i,j,k)\\over c(i,j)}\n\\end{align*}\n$$\n\n즉, 우리는 다음 과정을 수행하여 Modeling을 수행할 수 있는 것이다.\n\n1. 초기값 ($p(h_{i}|h_{i-1})$, $p(o_{i}|h_{i})$, $\\pi_{i}$)을 초기화 한다.  \n2. Trelli를 통해서 $\\alpha$, $\\beta$를 계산한다.\n3. $p(h_{i}|h_{i-1})$, $p(o_{i}|h_{i})$를 업데이트 한다.  \n   ($pi_{i}$같은 경우는 발생 빈도로 업데이트 한다.)\n4. 임계치에 도달할 때까지 2,3번을 반복한다.\n\n이 과정을 대게 10번 정도만 하면 수렴하게 되고, 이를 확률로 사용하는 것이다.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"nlp-hmm","date":"2022-10-21 21:55","title":"[NLP] 5. Hidden Markov Model","category":"AI","tags":["NLP","MarkovModel","HMM","HiddenMarkovModel"],"desc":"이전까지 특정 word를 기반으로 하여 modeling을 수행하는 방법을 알아보았다. 하지만, 우리가 특정 word의 sequence를 통해서 각 word에 대한 classification을 한 번에 하고 싶은 경우는 어떻게 할까?(예를 들어, 각 단어의 품사를 지정하는 일) 일반적으로 각 단어가 특정 해당 class일 확률로 구하는 방법이 일반적일 것이다. 하지만, 문맥을 고려하여 확률을 구할 방법은 없을까? 그 방법은 바로 bigram을 이용하면 될 것이다. 그렇다면, 사실 우리가 사용하는 문맥이 단어 자체보다는 이전 class가 더 영향이 크다면, 이는 어떻게 해야할까? 이를 위한 해결책이 HMM이다. NLP 뿐만 아니라 여러 분야에서 넓게 사용되고 있지만, 여기서는 NLP 분야에서 어떻게 이를 사용하는지를 알아볼 것이다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n이전 Posting에서는 sentence의 적절성을 확인한다든지 다음 단어를 유추한다든지 오타를 정정하는 등에 필요한 기본적인 Language Modeling 방식을 살펴보았다. 이번에는 실제로 가장 많이 사용되는 예제인 Classification을 Language Model을 이용하여 할 수 있는지를 배우며, 이를 직접 Spam Filtering에서 어떻게 사용하는지 살펴본다. 주의할점은 해당 Posting은 Naive Bayes 방식을 기반으로 진행하는 Classification이다. 더 다양한 방법론을 원한다면 해당 포스팅은 도움을 주기 어렵다. 만약 지금 말이 이해가 안된다면, 그냥 계속 읽으시면 되겠다.\n\n## Classification\n\n기본적으로 input이 들어왔을 때, 이를 알맞은 분류로 나누는 기능을 하는 것이다. 단순히 사전 지식에 기반해서 이를 수행할 수도 있지만, Language Modeling을 이용하면, 더 정확도 높은 분류를 수행할 수 있다.\n\n일반적으로 많이 사용하는 Classification 도구는 아래와 같다.\n\n1. Naive Bayes\n2. Hidden Markov Model(HMM)\n3. Maximum Entropy Model(MaxEnt)\n   1. Logistic Regression\n   2. Support Vector Machine\n   3. Neural Network(Deep Learning)\n4. K Nearest Neighbors\n\n해당 Posting에서는 **Naive Bayes Classifier**를 이용한 분류를 수행할 것이다. 3번 방법은 기본적으로 각 단어를 Random Variable로 치환해서 처리하는 과정이 필요한데 이는 후에 더 자세히 다룰 것이기 때문에 여기서는 기본적인 Classifier를 활용하는 방법을 배우고 후에 가서 word를 vector로 변환하는 과정을 거친 후에 더 훌륭한 기술들을 활용해보겠다.  \n따라서, 앞으로 3개의 Posting 동안은 Naive Bayes, HMM, MaxEnt 방식에 대해서 알아볼 것이고, 그 후에는 word를 vector 데이터로 치환하여 처리를 하는 방식을 배워볼 것이다.\n\n기본적으로 Classification은 데이터가 주어졌을 때, 해당 데이터가 특정 class에 속할 확률을 제시하는 것이다. 따라서, 특정 class에서 해당 데이터가 얼마나 자주 발생되는지와 실제로 해당 class의 빈도가 가장 중요하다.\n\n이를 수식적으로 표현하기 위해서 다음 변수들을 먼저 살펴보자.\n\n- **documents($D$)**: 여러 개의 Document를 의미하며, 하나의 Document는 대게 여러 개의 words를 포함한다. 각 document는 $d_{i} \\in D$의 형태로 표현한다.\n- **classes($C$)**: class는 두 개 이상을 가진다. 각 클래스는 $c_{i} \\in C$의 형태로 표현된다.\n- **labeled dataset**: 이는 (document($d_{i}$), class($c_{i}$))가 하나씩 mapping된 형태로 존재한다. 우리가 가지는 dataset으로 학습, 평가 시에 사용한다. 대게 평가에 사용되는 데이터는 학습 시에 사용하는 것을 금지하기 때문에 별도로 분리하여 사용한다.\n- **word($w$)**: 하나의 word를 의미하며 NLP 학습 시에 사용하는 가장 작은 단위이다. 대게 document 하나에 있는 단어의 수는 N으로 표기하고, unique한 단어의 수는 V(size of vocabulary)로 표시한다.\n\n따라서, 우리가 찾고자 하는 가장 높은 확률을 가진 class는 다음을 통해서 구할 수 있다.\n\n$$\n\\begin{align*}\nc_{MAP} \u0026= \\argmax_{c \\in C}{P(c|d)} \\\\\n\u0026= \\argmax_{c \\in C}{p(d|c)p(c)\\over p(d)} \\\\\n\u0026= \\argmax_{c \\in C}{p(d|c)p(c)} \\\\\n\u0026= \\argmax_{c \\in C}{p(w_{1}, w_{2}, ... , w_{N} | c)p(c)} \\\\\n\u0026= \\argmax_{c \\in C}{\\prod_{i=1}^{N}p(w_{i})p(c)} \\\\\n\u0026= \\argmax_{c \\in C}{\\log(\\prod_{i=1}^{N}p(w_{i})p(c))} \\\\\n\u0026= \\argmax_{c \\in C}{\\sum_{i=1}^{N}\\log p(w_{i}) + \\log{p(c)}} \\\\\n\\end{align*}\n$$\n\n여기서 우리가 language model을 무엇으로 정했는지가 중요하다. 위에서는 uni-gram이라고 가정해서 풀이했지만, bi-gram인 경우 document의 형태가 $d={(w_{1}, w_{2}), (w_{2}, w_{3}), ... , (w_{N-1}, w_{N})}$이다. 따라서, 전체적인 크기와 vocabulary자체도 바뀌게 된다.\n\n즉, 우리는 train set을 통해서 vocabulary를 완성한다. 그리고, 각 word의 count 및 필요에 따라 필요한 word sequence의 count를 수집하여 $p(w_i)$를 구한 후 위에 방법을 통해서 특정 class를 추측할 수 있는 것이다.\n\n## Evaluation\n\nbinary classificaiton의 결과는 아래와 같이 4개 중 하나로 결정된다.\n\n| prediction\\answer | True           | False          |\n| :---------------- | :------------- | :------------- |\n| Positive          | true positive  | false positive |\n| Negative          | false negative | true negative  |\n\n이를 쉽게 이해할려면, 병(코로나)의 양성/음성 판정이 row에 해당하고, 실제 병의 여부를 column으로 생각하면 쉽다. 또한, 각 cell의 값이 헷갈릴 수 있는데, 우리가 원하는 것이 예측의 정확도를 확인하는 것이기 때문에 예측 결과는 그대로 보여주면서, 이것이 틀렸는지 맞았는지를 앞에 true/false로 표현했다고 생각하면 쉽다.\n\n\nclassification의 성능을 측정하는 지표는 대표적으로 4 가지가 있다.\n\n1. **Accuracy(정확도)**  \n   가장 쉽게 그리고 일반적으로 생각하는 지표다. 위의 표에서는 전체 경우의 수를 더하여 옳게 예측한 것(true postive, true negative)의 합을 나누는 것이다. \n   $tp + fn \\over tp + fp + fn + tn$  \n   하지만, 이 방식은 한계가 있다. 바로, 데이터가 한쪽으로 치우쳐져있을 때이다. 예를 들어, 우리가 진짜를 진짜라고 맞출확률은 높지만, 가짜를 가짜라고 맞출 확률이 낮다고 할 때, 이를 제대로 반영하기가 어렵다. 그런데 데이터에서 진짜가 가짜보다 압도적으로 많을 경우 정확도는 좋은 지표로 쓰기 어렵다는 것이다.\n2. **Precision(정밀도, 정답률)**  \n   쉽게 정답 자체를 맞힐 확률입니다.  \n   $tp \\over tp + fn$\n3. **Recall(재현율)**  \n   예측이 맞을 확률을 의미합니다.  \n   $tp \\over tp + fp$\n4. **F1 Score**  \n   좀 더 세분화된 평가지표이다. 조화 평균에 기반하여 모델의 성능을 정확하게 평가할 때 사용한다.  \n   $2 \\times {\\text{Precision} \\times \\text{Recall} \\over \\text{Precision} + \\text{Recall}}$\n\n여기까지 봤으면, 슬슬 multi class의 경우에는 어떻게 해야할지 궁금할 것이다. 대게 두 가지 방법을 통해서 수행할 수 있다.\n\n\u003e **1. Micro Average**\n\n전체 class를 하나의 binary table로 합치는 것이다. 즉, 클래스가 A, B, C 3개가 있다면, 각 클래스 별로 예측 성공도를 binary로 표시하고, 이를 하나의 테이블로 합치는 것이다. 그 후에는 binary에서 계산하는 식을 그대로사용할 수 있다.  \n\n\u003e **2. Macro Average**\n\nmulti class의 경우에도 별로 다를 것은 없다. 단지 Precision과 Recall 그리고 Accuracy가 어떻게 바뀌는지만 알면 쉽게 이해할 수 있을 것이다.  \n\n| prediction\\answer | c1            | c2            | c3            | c4            |\n| :---------------- | :------------ | :------------ | :------------ | :------------ |\n| c1                | true positive | x             | x             | x             |\n| c2                | x             | true positive | x             | x             |\n| c3                | x             | x             | true positive | x             |\n| c4                | x             | x             | x             | true positive |\n\n- Precision: $c_{ii} \\over \\sum_{j}c_{ij}$\n- Recall: $c_{ii} \\over \\sum_{j}c_{ji}$\n- Accuracy: $c_{ii} \\over \\sum_{i}\\sum_{j}c_{ij}$\n\n## Case Study. Spam Filtering\n\n초기 NLP가 가장 많이 사용되었던 예시 중에 하나이다. 여러 개의 메일에 spam인지 ham인지를 labeling한 데이터를 갖고 후에 input으로 mail 데이터가 들어왔을 때, 이를 filtering하는 것이다. 위에서 살펴보았던 확률을 그대로 적용하면 된다. 예측에 필요한 확률을 습득하고, 예측하는 방법과 이를 평가하는 방법의 순으로 설명하겠다.\n\n### 0. Preprocessing\n\n사실 mail data의 형태가 이상할 수도 있다. Subject부터 시작하여 날짜 데이터 그리고 특수 문자 등이 존재할 수 있는데, 이를 먼저 처리해서 후에 있을 Modeling 단계에서 잘 사용할 수 있도록 형태를 변형해주어야 한다.\n\n[🔗 이전 Posting(Text Processing)](/posts/nlp-text-processing)에서 배웠던 기술들을 활용하여 이를 해결할 수 있다.\n\n대표적으로 해줄 수 있는 작업들은 다음과 같다.\n\n1. 대소문자 통일\n2. alphabet이 하나라도 들어있지 않은 데이터는 삭제\n3. date, 참조 등을 의미하는 데이터 삭제\n\n### 1. Modeling\n\nParameter Estimation / Learning / Modeling 등으로 불리는 단계이다. 일단 우리는 train set으로부터 우리가 원하는 확률을 추출해야 한다. 그 전에 우리가 어떤 language model을 이용할지 선택해야 한다. 먼저 uni-gram인 경우에는 다음과 같은 방법으로 train set이 정의된다.\n$$\n\\text{TrainSet} = {(d_{1}, c_{1}),  (d_{2}, c_{2}), ..., (d_{N}, c_{N})}\n$$\n$$\nd_{i} = \\begin{cases}\n  {w_{1}, w_{2}, ... , w_{M_{i}}} \\quad\u0026\\text{unigram} \\\\\n  {(\u003cs\u003e, w_{1}), (w_{1}, w_{2}), ... , (w_{M_{i}}, \u003c/s\u003e)} \\qquad\u0026\\text{bigram}\n\\end{cases}\n$$\n\n이제 우리가 원하는 parameter, 즉 확률은 다음과 같은 데이터이다.\n\n\u003e **unigram**\n\n$$\n\\begin{align*}\np(w_{i}|c_{j}) \u0026= {\\text{count}(w_{i}, c_{j}) \\over \\sum_{w \\in V} \\text{count}(w, c_{j})} \\\\\np(c_{j}) \u0026= {\\sum_{i = 1}^{N}{1[c_{i} = c_{j}]} \\over N}\n\\end{align*}\n$$\n\n\u003e **bigram**\n\n$$\n\\begin{align*}\np(w_{i}|w_{i-1},c_{j}) \u0026= {\\text{count}((w_{i-1}, w_{i}), c_{j}) \\over \\sum_{(w^{(1)}, w^{(2)}) \\in V} \\text{count}((w^{(1)}, w^{(2)}), c_{j})} \\\\\np(c_{j}) \u0026= {\\sum_{i = 1}^{N}{1[c_{i} = c_{j}]} \\over N}\n\\end{align*}\n$$\n\n여기서 우리는 반드시 Smoothing을 해주어야 한다. 왜냐하면, spam mail에서 안 본 단어가 나올 가능성이 너무나 높기 때문이다. 따라서, 실제 $p(w_{i}|c_{j})$는 아래와 같이 변경된다. (간단한 예시를 들기 위해서 Add-1 방식을 사용했다. - 해당 내용이 기억이 나지 않는다면, [🔗 이전 포스팅](/posts/nlp-language-modeling)을 다시 보고 오자.)\n\n$$\np(w_{i}|c_{j}) = {\\text{count}(w_{i}, c_{j}) + 1 \\over \\sum_{w \\in V} \\text{count}(w, c_{j}) + |V|}\n$$\n\n주의할 점은 다시 한 번 강조하지만, $V$는 후에 Estimation에서 input으로 사용하는 단일 document까지 포함한 Vocabulary이다.\n\n### 2. Estimation\n\n이제 우리가 얻은 parameter를 이용해서 실제 input data에 대한 estimation을 수행할 수 있다.\n\n이 경우 다음과 같은 과정을 수행할 수 있다.\n\n$$\n\\hat{c} = \\argmax_{c \\in C} p(c)\\prod_{w \\in d_{\\text{input}}}p(w|c)\n$$\n\n물론 어떤 n-gram을 쓰냐에 따라 $d_{\\text{input}}$도 형태가 달라질 것이다.\n\n### 3. Evaluation\n\n이제 평가를 수행할 것이다. 평가는 우리가 알아봤던 Accuracy와 F1 Score를 추출할 수 있다. Binary Classification이기 때문에 쉽게 구할 수 있을 것이다.\n\n\n| prediction\\answer | True                                                                       | False                                                                     |\n| :---------------- | :------------------------------------------------------------------------- | :------------------------------------------------------------------------ |\n| Positive          | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} = c, c = \\text{spam}]$    | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} \\neq c, c = \\text{ham}]$ |\n| Negative          | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} \\neq c, c = \\text{spam}]$ | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} = c, c = \\text{ham}]$    |\n\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"nlp-classification","date":"2022-10-21 21:53","title":"[NLP] 4. Classification","category":"AI","tags":["NLP","Classification","SpamFiltering","F1Score"],"desc":"이전 Posting에서는 sentence의 적절성을 확인한다든지 다음 단어를 유추한다든지 오타를 정정하는 등에 필요한 기본적인 Language Modeling 방식을 살펴보았다. 이번에는 실제로 가장 많이 사용되는 예제인 Classification을 Language Model을 이용하여 할 수 있는지를 배우며, 이를 직접 Spam Filtering에서 어떻게 사용하는지 살펴본다. 주의할점은 해당 Posting은 Naive Bayes 방식을 기반으로 진행하는 Classification이다. 더 다양한 방법론을 원한다면 해당 포스팅은 도움을 주기 어렵다. 만약 지금 말이 이해가 안된다면, 그냥 계속 읽으시면 되겠다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n이제 통계적인 관점에서 NL을 input으로 하는 문제를 해결할 방법을 찾을 것이다. 이를 Language Modeling이라고 하며, 이를 위해서 또는 이를 더 효과적으로 하기 위한 방법들을 소개할 것이다.\n\n## Noisy Channel\n\n일반적으로 우리는 원하는 결과가 있다. 그 결과를 얻기 위해서 우리는 말을 하거나 행동을 하거나 글을 쓴다. 그 과정은 우리가 갖고 싶은 A라는 것을 얻기 위해서 B라는 행동을 대신하는 것과 같다. 즉, 우리는 이를 A에 noise가 껴서 B라는 것이 생성되었다고 생각하는 것이다. 그리고 우리가 관측할 수 있는 것은 B밖에 없는 것이다.\n\n이는 우리가 사용하는 NL에서도 동일하다. 우리가 원하는 결과값 A를 얻기 위해서 우리는 B라는 문장, 음성을 제시한다. 그 결과가 원하는 결과로 될 수 있는 확률을 얻어서 최종 결과를 예측하는 것이 우리의 목표인 것이다.\n\n이 과정을 수식으로 표현하면 다음과 같아진다.\n\n$$\nP(A|B) = {P(B|A)P(A)\\over{P(B)}}\\quad(\\text{Bayes Rule})\n$$\n\n우리가 얻고 싶은 $P(A|B)$ 를 얻기 위해서, $P(A)$와 $P(B|A)$ 를 통해서 구할 수 있는 것이다. 이에 대한 더 자세한 내용은 ML을 이해하는 것이 좋을 것이다. 추천하는 Posting은 [🔗 [ML] Parametric Estimation](/posts/ml-parametric-estimation)이다.\n\n## Language Modeling\n\n결국 우리가 얻고 싶은 것은 특정 문장의 할당된 확률인 것이다. 따라서, Language Model은 input으로 word sequence과 들어왔을 때, 확률을 계산하는 것이다.\n그리고, 이 계산을 수행하기 위해서 필요한 parameter를 찾는 과정을 Language Modeling이라고 한다.\n\n## Input(N-gram)\n\n대게 이러한 모델은 문장 또는 word의 배열이 다음과 같이 주어질 때, $W = w_{1}\\ w_{2}\\ w_{3}\\ ...\\ w_{n}$ 아래와 같은 형태로 표현하는 것이 일반적이다.\n\n- **Single word probability**  \n  하나의 단어의 확률을 나타낼 때 단순하게 아래와 같이 표현한다.  \n  $P(w_{i})\\quad(w_{i} \\in W)$\n- **Sequence of Words probability**  \n  일반적으로 sentence의 확률을 나타낼 때, 여러 문장을 한꺼번에 가지는 확률이므로 아래와 같이 표현하는 것이 일반적이다.  \n  $P(W) = P(w_{1}, w_{2}, w_{3}, ..., w_{n})$\n- **single word probability with context**  \n  일반적으로 우리는 이전에 사용한 단어가 문맥이라고 이해할 수 있다. 따라서, 구체적인 단어들 이후에 특정 단어가 나오는 것은 문맥을 반영한 확률이라고 생각할 수 있다.  \n  $P(W) = P(w_{5}| w_{1}, w_{2}, w_{3}, w_{4})$\n\n위의 식을 보게 되면, 우리는 다시 한번 sentence의 확률을 다시 정리할 수 있다.\n\n$$\nP(W) = P(w_{1}) \\times P(w_{2}|w_{1}) \\times P(w_{3}|w_{1},w_{2}) \\times ... \\times P(w_{n}| w_{1},w_{2},..., w_{n-1})\n$$\n\n위의 식을 보게되면, W가 짧다고 하더라도 굉장히 많은 처리가 필요하고, 저장을 위해 많은 공간이 필요하다는 것을 알 수 있다. 따라서, 우리는 현재 단어를 기준으로 너무 오래된 단어에 대해서는 무시를 하도록 하는 방법을 취하는 것이다.(**Markov Chain**) 이를 \"n 번째까지 허락\"했을 때, 이를 n-gram 이라고 부른다.\n\n$$\np(W) = \\prod_{i=1}^{n}{p(w_{i}|w_{i-n+1},w_{i-n+2},...,w_{i-1})}\n$$\n\n그렇다면, n-gram에서 적절한 n이란 무엇일까? 일반적으로는 n이 크다는 것은 context를 많이 받아들일 수 있다는 의미로 받아들여질 수 있다. 따라서, n이 클수록 성능의 최적화 가능성이 더 높다. 하지만, Vocabulary의 사이즈가 커지는 경우를 예를 들어보자. 여기서는 $|V| = 60$k 라고 해보자.\n\n| n-gram          | p(w_{i})                         | # of parameters   |\n| :-------------- | :------------------------------- | :---------------- |\n| 0-gram(uniform) | ${1\\over\\vert V\\vert}$           | 1                 |\n| 1-gram(unigram) | $p(w_{i})$                       | $6\\times10^4$     |\n| 2-gram(bigram)  | $p(w_{i}\\vert w_{i-1})$          | $3.6\\times10^9$   |\n| 3-gram(trigram) | $p(w_{i}\\vert w_{i-2}, w_{i-1})$ | $2.16\\times10^14$ |\n\nn이 커질 수록 가능한 조합의 수는 굉장히 커지기 때문에 우리가 보지 못하는 경우의 수도 굉장히 증가하게 되어 data자체의 빈도가 적어지는 현상(sparse)이 발생한다. 따라서, 대게의 경우 최대 n의 크기는 3정도로 하는 것이 일반적이다.\n\n```plaintext\n 🤔 주의\n\n 실제 데이터를 가공할 때에는 bigram부터는 문장의 시작과 끝을 표시해주어야 한다. \n 그렇지 않으면, 첫번째 문자의 확률을 구할 때, 이전 단어의 영향을 받을 수 없다.\n 정해진 규칙은 없지만, 대게 \u003cs\u003e\u003c/s\u003e를 이용한다.\n ex.  bigram : \u003cs\u003e w1 w2 w3 w4 \u003c/s\u003e\n     trigram : \u003cs\u003e \u003cs\u003e w1 w2 w3 w4 \u003c/s\u003e \u003c/s\u003e\n```\n\n```plaintext\n 🤔 Google N-gram\n\n 구글에서 2006년에 N-gram을 직접 구성한 것이 있다. \n 총 1,024,908,257,229개의 단어가 존재하고, 40회 이상 등장하는 5-gram이 1,176,470,663개 존재한다. \n 총 Vocabulary의 size는 200번 이하로 등장하는 것은 제외하면, 13,588,391개이다. \n```\n\n## Estimation\n\nML에서는 Estimation을 수행할 때, continuous하게 추정하였다. 즉, 보지 못한 데이터에 대한 처리를 수행하기 위해서 continuous한 분포의 parameter만 추정하면 되었다. 하지만, NLP에서는 다르다. NL를 continuous하게 표현할 마땅한 방법이 없다. 따라서, 우리는 결국 모든 확률을 discrete하게 구해야 한다. 따라서, 우리는 특정 단어의 확률을 구하는 방법은 단 하나가 된다.\n\n$$\n|T| = \\text{count of observed tokens}\n$$\n$$\nc(w_{i}) = \\text{count of observed } w_{i}\n$$\n$$\n\\begin{align*}\n\u0026P(w_{i}) = {{c(w_{i})}\\over{|T|}} \\\\\n\u0026P(w_{i}| w_{i-1}) = {{c(w_{i-1}, w_{i})}\\over{c(w_{i-1})}} \\\\\n\u0026P(w_{i}| w_{i-2}, w_{i-1}) = {{c(w_{i-2}, w_{i-1}, w_{i})}\\over{c(w_{i-2}, w_{i-1})}} \\\\\n\\end{align*}\n$$\n\n이때, 반드시 sequence의 순서를 유의하도록 하자. 순서가 바뀌면 다른 종류이다.\n\n\u003e **Small Example**\n\n데이터가 다음과 같이 주어진다고 하자.\n\n```plaintext\n He can buy the can of soda.\n```\n\n이때 각 n-gram을 이용한 model의 확률들을 살펴보자.\n\n| model    | probability                                                                                                                                                     |\n| :------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| uni-gram | $p(He)=p(buy)=p(the)=p(of)=p(soda)=p(.) = 0.125$\u003cbr /\u003e $p(can)=0.25$                                                                                            |\n| bi-gram  | $p(He\\vert \u003cs\u003e)=p(can\\vert He)=p(the\\vert buy)=p(can\\vert the)=p(soda\\vert of)=p(.\\vert sode) =p(\u003c/s\u003e \\vert .) = 1$\u003cbr /\u003e $p(buy\\vert can)=p(of\\vert can)= 0.5$ |\n| tri-gram | $p(He\\vert \u003cs\u003e, \u003cs\u003e)=p(can\\vert \u003cs\u003e, He)=p(the\\vert He, buy)=...=p(\u003c/s\u003e\\vert ., \u003c/s\u003e) =1$                                                                       |\n\n\n## Evaluation\n\n평가할 때는 ML과 결국은 동일하다. 우리가 확률분포를 구할 때, 사용한 데이터 외에 데이터를 이용해서 잘 적용이 되었는지를 확인할 수 있다. 하지만, word의 갯수와 데이터의 수가 굉장히 많은 NL의 특성상 이 Evaluation 단계에만 굉장히 많은 시간을 소모할 수 있다. 따라서, 즉각적인 평가를 위해서 사용하는 척도가 있다.\n\n\u003e **Perplexity**\n\ntrain set을 통해 학습을 하고, test set을 통해서 평가를 수행할 때, train set을 통해 구한 확률이 실제 test set에서 어느정도의 Entropy를 발생시키는지를 확인하는 것이다. 원래의 식은 \n$PP = 2^{H}$이지만, 이를 변형하여 다음과 같이 나타낼 수 있다.\n\n$$\n\\begin{align*}\nPP(W) \u0026= \\sqrt[N]{1 \\over P(w_1, w_2, ..., w_N)} \\\\\n\u0026= \\sqrt[N]{\\prod_{i=1}^{N}{P(w_i|w_{i-n+1}, w_{i-n+2}, ..., w_{i-1})}}\n\\end{align*}\n$$\n\n이를 통해서, 실제로 해당 문제가 너무 어렵지는 않은지, 선택한 model이 잘못되지는 않았는지를 판단한다.\n\n| model    | probability                                                                                                                                                     | entropy |\n| :------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------ |\n| uni-gram | $p(He)=p(buy)=p(the)=p(of)=p(soda)=p(.) = 0.125$\u003cbr /\u003e $p(can)=0.25$                                                                                            | 2.75    |\n| bi-gram  | $p(He\\vert \u003cs\u003e)=p(can\\vert He)=p(the\\vert buy)=p(can\\vert the)=p(soda\\vert of)=p(.\\vert sode) =p(\u003c/s\u003e \\vert .) = 1$\u003cbr /\u003e $p(buy\\vert can)=p(of\\vert can)= 0.5$ | 0.25    |\n| tri-gram | $p(He\\vert \u003cs\u003e, \u003cs\u003e)=p(can\\vert \u003cs\u003e, He)=p(the\\vert He, buy)=...=p(\u003c/s\u003e\\vert ., \u003c/s\u003e) =1$                                                                       | 0       |\n\n위의 예시를 가져와서 봐보자. 물론 동일한 dataset에서 perplexity를 측정하기는 했지만, n이 커질 수록 점점 entropy가 작아지는 것을 볼 수 있다. 그렇다면, 이런 data가 좋은 걸까? 이는 좋은 게 아니다. 왜냐하면, 해당 dataset에서만 잘 작동하도록 되어있기 때문이다. 일명 **overfitting**이다.\n\n## Smooting\n\n위에서 말한 overfitting을 어느정도 해소할 뿐만 아니라 정말 큰 문제가 될 수 있는 probability가 0이 되는 문제(우리가 trainset을 통해 학습한 확률 분포에서 testset에 들어오는 데이터에 해당하는 확률값이 없을 때, 즉 해당 확률이 0일때)를 해결하기 위해서는 smoothing이 필수적이다. probability가 0이 된다는 것은 후에 위에서 구한 확률로 Prediction을 할 때 모든 예측을 망치는 요인이 된다. 왜냐하면, 추정확률의 최적 Entropy를 의미하는 Cross Entropy를 $\\infin$로 만들기 때문이다. (최적 Entropy가 무한대라는 것은 추정이 불가능하다는 것이다.)\n\n따라서, 우리는 probability가 0이 되지 않게 하는 방법으로 기존의 확률의 일부를 나누어주도록 하는 방법을 제시한다. 이것이 smoothing이다.\n\n이때 반드시 유의할 점은 smoothing을 하건 안하건 각 확률의 총합은 1이 되도록 보장해야 한다는 것이다.\n\n$$\n\\sum_{w \\in \\Omega}p(w) = 1\n$$\n\n대략 6가지의 대표적인 smoothing 방식들을 소개하겠다.\n\n\u003e \u003cmark\u003e**1. Add-1(Laplace)**\u003c/mark\u003e\n\n가장 간단한 방법의 smoothing 방법이지만, 실용적인면은 다소 떨어지는 방법이다. 아이디어는 간단하다. prediction을 수행할 때, 현재 들어온 input까지 포함하여 만든 $|V|$를 분모에 더하고, 분자에 1을 더해주는 방법이다. 이 방법을 사용하면, 설사 count가 0이 더라도 확률이 0이 되지는 않는다.\n\n$$\np(w) = {X \\over Y} \\rArr p^{\\prime}(w) = {X + 1 \\over Y + |V|}\n$$\n\n여기서, $|V|$ 값이 정말 헷갈렸는데, 아무도 잘 설명을 안하는 것 같아서 짚고 넘어가면, 우리가 확률값을 얻기 위해서 사용했던 dataset과 현재 prediction을 하기 위해서 들어온 input 둘에서 발생한 모든 unique한 단어의 수를 의미한다. (# of vocabulary)\n\n그렇게 해야만 $\\sum_{w \\in \\Omega}p(w) = 1$을 만족하는 값이 나온다.\n\n따라서, 이를 각 각의 n-gram에 대입하면 다음과 같다.\n\n| n    | $p(w_{i})$                                                 | $p^{\\prime}(w_{i})$                                                            |\n| :--- | :--------------------------------------------------------- | :----------------------------------------------------------------------------- |\n| $1$  | $c(w_{i}) \\over \\vert T \\vert$                             | $c(w_{i}) + 1 \\over \\vert T \\vert + \\vert V \\vert$                             |\n| $2$  | ${{c(w_{i-1}, w_{i})}\\over{c(w_{i-1})}}$                   | ${{c(w_{i-1}, w_{i}) + 1}\\over{c(w_{i-1})} + \\vert V \\vert} $                  |\n| $3$  | ${{c(w_{i-2}, w_{i-1}, w_{i})}\\over{c(w_{i-2}, w_{i-1})}}$ | ${{c(w_{i-2}, w_{i-1}, w_{i}) + 1}\\over{c(w_{i-2}, w_{i-1})} + \\vert V \\vert}$ |\n\n\u003e \u003cmark\u003e**2. Add-k**\u003c/mark\u003e\n\n1이라는 숫자가 경우에 따라서는 굉장히 큰 영향을 줄 때가 있다. 특히 기존 데이터의 수가 적은 경우에 더욱 그렇다. 따라서, 이를 해결하기 위해서 1보다 작은 임의의 값(k)을 쓰는 방법이다.\n\n$$\np(w) = {X \\over Y} \\rArr p^{\\prime}(w) = {X + 1 \\over Y + k|V|}\n$$\n\n하지만, 위와 같은 방식은 결국 어떤 확률 값이든지 분자에 1을 더하기 때문에 불평등하게 값을 나눠준다고 할 수 있다. 왜냐하면, 애초에 count(분자)가 큰 데이터에게 1은 별로 영향을 안주겠지만, 분자가 처음부터 작았던 경우에는 이로 인해서 받는 영향이 굉장히 크기 때문이다. 따라서, 이러한 한계점을 극복할 수 있는 방법들이 아래와 같은 방법들이다.\n\n\u003e \u003cmark\u003e**3.Good Turing**\u003c/mark\u003e\n\n이를 이해하기 위해서는 우리는 새로운 feature의 데이터를 가져와야 한다. 바로 word의 frequency의 frequency이다. \n\n$$\nN_{k} = \\sum_{i=1}^{n}1[c(w_{i}) = k]\n$$\n\n아마 예시를 봐야 이해가 빠를테니 하나의 예시를 보자.\n\n```plaintext\n sam I am I am sam I do not eat\n```\n\n이 경우 우리는 다음과 같이 count를 구할 수 있다.\n\n$$\n\\begin{align*}\n  \u0026c(I) \u0026=\\ 3 \\\\\n  \u0026c(sam) \u0026=\\ 2\\\\\n  \u0026c(am) \u0026=\\ 2\\\\\n  \u0026c(do) \u0026=\\ 1\\\\\n  \u0026c(not) \u0026=\\ 1\\\\\n  \u0026c(eat) \u0026=\\ 1\\\\\n\\end{align*}\n\\quad\\rArr\\quad\n\\begin{align*}\n  \u0026 N_{1} = 3 \\\\\n  \u0026 N_{2} = 2 \\\\\n  \u0026 N_{3} = 1\n\\end{align*}\n$$\n\n여기서 Good Turing은 한 번도 안본 데이터에게는 한 번만 보는 경우의 수를 전체 경우의 수로 나눈값만큼의 확률을 나누어주고, 기존 데이터들에게는 laplace처럼 1을 더해주는 것이 아니라 비례하는 만큼을 곱해주어 적절한 확률을 가져갈 수 있게하였다.\n\n따라서, 식은 다음과 같다.\n$$\np(w_{i}) = {(c(w_{i})+1)N_{c(w_{i})+1}\\over{N_{c(w_{i})}}} \\times {1\\over |T|},\\quad (N_{0} = 1)\n$$\n\n대게 ${(c(w_{i})+1)N_{c(w_{i})+1}\\over{N_{c(w_{i})}}}$이 부분을 $c^{*}$라고도 부른다.\n\n\u003e \u003cmark\u003e**4. Kneser-Ney**\u003c/mark\u003e\n\n가장 널리 쓰이는 Smoothing 방식으로 기억해두는 것이 좋다. 이를 이해하기 위해서는 먼저, Absolute Discounting을 먼저 이해해야 한다.  \nGood-turing 방식을 사용했을 때 $c$와 $c^{*}$사이에 차이가 경험적으로 특정 상수만큼씩 차이가 난다는 것을 발견하여, \n\n$$\nc^{*} = c - d\n$$\n\nChurch과 Gale은 이를 Absolute Discounting 확률이라며 다음 식을 제시한다.\n\n$$\nP(w_{i}|w_{i-1}) = {c(w_{i-1}, w_{i}) -d \\over c(w_{i-1})} + \\lambda(w_{i-1})P(w) \n$$\n\n여기서 뒷에 부분 $\\lambda(w_{i-1})P(w)$은 discounting으로 발생한 오차를 매꾸기 위한 값이다.\n\n여기서 Kneser-Ney problem은 더 넓은 범위로 확장시킬 수 있는 범위로 확장시킨 것이다. 기존에는 bigram으로 제한되어 있던 Absolute Discounting의 식은 다음과 같이 변형된다.\n\n$$\nP_{KN}(w_{i}|w_{i-n+1}^{i-1}) = {\\max(c(w_{i-1}, w_{i}) -d, 0) \\over c(w_{i-n+1}^{i-1})} + \\lambda(w_{i-n+1}^{i-1})P_{KN}(w_{i}|w_{i-n+2}^{i-1}) \n$$\n\n(위의 식에 대해서 정확하게 이해를 하지 않았지만, 그렇구나 하고 넘어가도 충분할 것 같다.)\n\n\u003e \u003cmark\u003e**5. Backoff \u0026 Interpolation**\u003c/mark\u003e\n\n상황에 따라서 unigram, bigram, trigram을 가중치만큼 더해서 사용하는 방식이다. 결국 n-gram에서 n이 작아질 수록 detail을 신경쓸 수 없지만, 신뢰도 자체는 늘어날 수 있다. 따라서, 이를 적절히 섞어쓰면 좋은 결과가 나온다는 이론이다. 하지만, 어떤 것을 더 중점으로 직접 정해주어야 한다. \n\n$$\np^{\\prime}(w_{i}|w_{i-2}, w_{i-1}) = \\lambda_{3}p(w_{i}|w_{i-2}, w_{i-1}) + \\lambda_{2}p(w_{i}|w_{i-1}) + \\lambda_{1}p(w_{i}) + {\\lambda_{0}\\over|V|}\n$$\n\n이를 정할 때는 대게 held-out data를 활용해서 구한다.(validation set이라고 부른다.) 즉, 전체 corpus를 (train, validation, test)로 적절히 나누어 쓰라는 것이다. 그래서 성능을 측정할 때는 testset을 쓰고, $\\lambda$를 추정할 때에는 validation(heldout)set을 사용하라는 것이다.\n\n\n## Word Class\n\nSmoothing 방식을 이용해서 unseen data를 처리해주었는데 좀 더 효과적으로 이를 처리하는 방법을 고려한 것이다. class단위로 word를 grouping하는 것이다. 그래서 존재하지 않는 단어였다고 하더라도 특정 group에 속한다면, 이를 활용해서 어느정도 확률을 부여할 수 있다는 것이다. 이 방식을 활용하면, 실제로 보지 않은 데이터에 대해서도 현실적인 추정이 가능하지만 detail에 대한 성능은 감소할 수 있다.\n\n$$\np(w_{i}|w_{i-1}) \\rArr p(w_{i}|c_{i-1}) ={ c(c_{i}, w_{i}) \\over c(c_{i-1}) }\n$$\n\n위의 식을 보면, 이전 단어의 context를 보는 것이 아니라 이제는 class를 보고 다음 단어를 probability를 계산하도록 바뀐 것이다. 그리고 이 확률은 class내부에서 해당 단어의 빈도를 이전 class의 빈도로 나누었다고 보면 되겠다.\n\n$$\np(w_{i}|c_{i-1}) = p(w_{i}|c_{i}) \\times p(c_{i}|h_{i})\n$$\n\n즉, class 단위로 단어를 묶고 class에서 단어의 발생 확률에 class에서의 n-gram을 곱한 값이 되는 것이다. 일반적인 Bayes Decison Rule에 기반하여 선택한다고 보면 되겠다.\n\n## Example. Spelling Correction\n\n여태까지 배운 내용을 활용하여 Spelling 오류를 정정해주는 application을 제작한다고 해보자. 먼저, Spelling Error의 종류부터 알아보도록 하자.\n\n- **Non word Error**  \n  잘못된 spelling에 의해서 전혀 뜻이 없는 단어가 만들어진 경우이다.  \n  해결을 위해서는 사전에서 유사한 단어를 찾아서 가장 가능성이 높은 것 또는 이전에 배웠던 shortest edit distance를 찾는 것이다.\n- **Real word Error**  \n  잘못된 spelling 또는 유사한 발음 때문에 뜻이 있는 단어가 만들어졌지만, 오류가 의심되는 경우이다.  \n  해결책은 비슷한 발음 또는 spelling의 모든 단어를 찾아서 해당 단어와 함께 language model에 넣어서 가장 높은 가능성을 가지는 값을 찾는 것이다.\n\n먼저, **Non Word Error** 같은 경우는 오타 데이터에 원래 쓰려고 했던 값을 labeling해서 모아두고 다음 값을 학습시키는 것이다. \n(*x=오타데이터, w=사전에있는단어)\n- $P(x|w)$ = x가 w일 가능성\n- $p(w)$ = w의 확률\n\n그리고 나서, 다음을 실행시켜서 가장 적절한 $\\hat{w}$를 찾으면 된다.\n$$\n\\begin{align*}\n\\hat{w} \u0026= \\argmax_{w \\in V}P(w|x) \\\\\n\u0026= \\argmax_{w \\in V}{P(x|w)P(w)}\n\\end{align*}\n$$\n\n**Real Word Error**의 경우에는 결국 이전 단어 sequence를 활용해야 한다. 전체 corpus를 학습해서 tri-gram을 추출해놓고, 번갈아가면서 후보 단어들을 집어넣어서 가장 높은 확률이 나오는 단어를 사용하는 것이다. 예를 들어, 후보 단어가 다음과 같이 정해졌다고 하자. ($\\bold{w}_{3} = {w_3^{(1)}, w_3^{(2)}, w_3^{(3)}, ...}$) 이때 우리가 원하는 w는 다음과 같이 구할 수 있다.\n\n$$\n\\hat{w}_{3} = \\argmax_{w_{3}^{(i)} \\in \\bold{w}_{3}} P(w_{3}^{(i)}| w_{1}, w_{2}) \n$$\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)","slug":"nlp-language-modeling","date":"2022-10-21 12:15","title":"[NLP] 3. Language Modeling","category":"AI","tags":["NLP","NoisyChannel","Ngram","LanguageModeling","Smoothing","WordClass"],"desc":"이제 통계적인 관점에서 NL을 input으로 하는 문제를 해결할 방법을 찾을 것이다. 이를 Language Modeling이라고 하며, 이를 위해서 또는 이를 더 효과적으로 하기 위한 방법들을 소개할 것이다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n우리는 Linear Regression, Logistic Regression, SVM을 거치며 data로 부터 유의미한 pattern을 발견하는 과정을 알아보았다. 이 과정은 우리에게 명확한 식 하나를 제시하였고, 모든 과정을 우리가 제어할 수 있게 하였다. 하지만, 실제 데이터를 우리가 모두 명확하게 이해할 수 있는 형태로 분류할 수 있는 것인지는 의문이 들 수 있다. 그렇다면, 우리가 이해하지는 못하지만, 알아서 최적의 결과를 가져오게 할 수 있는 방법이 있을까? 이런 마법같은 일에 대한 아이디어를 제시하는 것이 Neural Network이다.\n  게 알지 못하지만 input이 들어왔을 때, 이를 처리해서 output을 전달하는 시스템을 우리의 신체에서 찾게 된다. 바로 우리 몸을 이루는 신경망이다. 예시로 우리는 눈을 통해 빛이라는 input을 받으면, 우리 눈과 뇌에서 무슨 일이 발생하는지는 모르지만 결과적으로 우리는 물체를 볼 수 있다. 이 과정을 추측의 과정에 도입하면 어떻게 될까?\n\n## Perceptron\n\nPerception(인지 능력) + Neuron(신경 세포)의 합성어이다. 중고등학교 생명 수업을 들었다면, 우리의 모든 신경은 뉴런이라는 단위 세포로 이루어진다는 것을 배웠을 것이다. 즉, 우리의 신경 세포를 컴퓨터 공학에서 활용하기 위해서, 수학적으로 변환한 것이다. 형태를 먼저 살펴보자.\n\n$$\ny = sign(\\bold{w}^{\\top}\\bold{x} + b)\n$$\n\n![nn-perceptron-1](/images/nn-perceptron-1.jpg)\n\n대단한 것을 기대했다면 실망하겠지만, simple한 것이 최고라는 연구의 진리에 따라서 위의 식은 꽤나 합리적이다. 우리가 Linear Regression과 Logistic Regression을 배웠으니 알 것이다. 이는 사실 Linear Regression을 이용해서 우리가 Classification을 수행할 때 사용했던 식이다. 즉, perceptron 하나는 input을 선형으로 구분할 수 있도록 하는 decision boundary를 찾는 것과 같다.\n\n\u003e **Optimization**\n\n그렇다면, 해당 perceptron을 통해서 모든 데이터를 구분하기 위해서는 다음을 만족하는 $\\bold{w}$를 찾아야 한다.\n\n$$\ny_{n} = \n\\begin{cases}\n1  \u0026\\text{ if  } \\bold{x}_{n} \\in \\mathcal{C}_{1} \\\\\n-1 \u0026\\text{ if  } \\bold{x}_{n} \\in \\mathcal{C}_{2} \\\\\n\\end{cases}\n$$\n$$\ny_{n}\\bold{w}^{\\top}\\bold{x}_{n} \\gt 0, \\forall n\n$$\n\n결국 Loss 함수는 perceptron의 잘못된 classification 결과를 최소화하는 것이다.\n\n$$\n\\mathcal{J}(\\bold{w}) = - \\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{w}^{\\top}\\bold{x}_{n}} \\quad( \\mathcal{M}(\\bold{w}) = \\{ n : y_{n}\\bold{w}^{\\top}\\bold{x}_{n} \\} )\n$$\n$$\n\\nabla_{\\bold{w}}\\mathcal{J}(\\bold{w}) = - \\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{x}_{n}}\n$$\n\n따라서, 우리가 사용할 수 있는 Gradient Descent식은 다음과 같다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} + \\alpha\\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{x}_{n}}\n$$\n\n간단한 예시로 AND, OR Gate를 percentron을 통해 표현해보자.\n\n![nn-and-gate](/images/nn-and-gate.jpg)\n![nn-or-gate](/images/nn-or-gate.jpg)\n\n하지만, 우리가 다루는 데이터는 항상 완벽하게 선으로 나뉘어지지는 않는다. 하나의 perceptron으로는 아래의 XOR조차도 구분해낼 수 없다.\n\n![nn-multi-line-example](/images/nn-multi-line-example.jpg)\n\n## Multilayer Perceptron\n\n위의 문제를 해결하기 위해서 나온 것이 perceptron을 다층으로 쌓아서 해결하는 방법이다. 이제는 하나의 신경세포였던 perceptron을 진짜 신경망처럼 연결해보자는 것이다.\n\n먼저 추상적인 예시를 생각해보자. 우리가 XOR Gate를 만들기 위해서는 어떤 Gate를 결합해야할까?\n\n$$\na \\oplus b = ab + \\bar{a}\\bar{b}\n$$\n\n우리는 AND Gate 2개 연산을 수행하고, 해당 결과값을 이용해서 OR Gate 연산을 수행하면 XOR Gate를 표현할 수 있다는 것을 알고 있다. 그렇다면, 각 Gate는 우리가 perceptron으로 나타낼 수 있었는데 그냥 이것을 gate로 표현하듯이 똑같이 나타내면 풀 수 있지 않을까?\n\n그래서 직접 수행해보면 다음과 같은 값을 구할 수 있다.\n\n![nn-xor-gate](/images/nn-xor-gate.jpg)\n\n```plaintext\n 🤔 Insight\n\n 위의 과정을 보다보면 놀라운 것을 하나 발견할 수 있다. 바로 왼 쪽 그림의 변화이다. \n 첫번째, 두 개의 perceptron을 통해서 만들어진 output이 이루는 결과값의 형태로 feature를 변환하면, \n 하나의 perceptron으로 decision boundary를 그릴 수 있다는 것이다. \n 이는 마치 이전 linear regression에서 배웠던 basis function(ϕ)이 했던 역할이다.\n\n 그렇다면, 이를 더욱 확장해보자. \n 만약 해당 Layer가 더 깊어진다고 해도, 출력 직전의 layer는 단순히 이전 모든 layer는 입력 데이터를 가공해서\nfeature를 변환하는 하나의 basis function(ϕ)를 취한 것으로 이해할 수 있다.\n```\n\n결론적으로 우리는 더 복잡하고, 어려운 문제의 경우에도 더 깊게 신경망을 구성하면 결국은 문제를 풀 수 있다는 것이다.\n\n\u003e **Universal Approximation Theorem**\n\n위와 같은 깊은 신경망 구조를 이용하자는 주장도 있지만, 이와 유사하게 넓은 신경망을 쓰자는 주장도 존재했다.  \n\n![nn-universal-approx-theorem-1](/images/nn-universal-approx-theorem-1.jpg)\n\n만약, 우리가 하나의 Layer와 output에서 최종 output perceptron만 갖고 처리를 한다면, 결국 여러 perceptron의 weighted 합으로 볼 수 있다. 그 경우 우리는 계단 함수의 weighted 합으로 생각할 수 있는데 perceptron이 많아질 수록 촘촘해지며 정답과 유사한 추론이 가능해진다.(마치 적분의 개념과 유사하다. 물론 이는 추상적인 설명이기 때문에 실제로는 계단함수의 합이기 때문에 좀 다르다.)\n\n![nn-universal-approx-theorem-2](/images/nn-universal-approx-theorem-2.jpg)\n\n위의 그림을 보면 이해할 수 있다. 하지만, 이 방식은 결국 모든 함수 형태를 기억하는 것이다.(**memorizer**) 이것은 input data가 많아질 수록 복잡도가 급격하게 증가하기 때문에 학습과 예측과정에 굉장히 많은 시간을 소모한다.\n\n\u003e **Multilayer Optimization(Backpropagation)**\n\n그렇다면, 넓은 신경망이 한계가 있으니 선택지는 input과 output 사이의 layer(**hidden layer**)의 갯수를 늘려서 깊은 신경망을 만드는 것이다. 하지만, 우리가 사용하고 있는 perceptron은 sign함수로 감싸져있기 때문에 미분 시에 기울기가 0이라는 문제를 갖는다. 또한, 그렇다고 정답의 갯수를 이용하기에는 각 perceptron의 영향을 전달하기에 부족하다는 것이 명확하다. 따라서, 우리는 perceptron에 있는 정답을 판별하는 함수 sign을 다른 함수로 대체하기로 한다.\n\n![nn-perceptron-2](/images/nn-perceptron-2.jpg)\n\n여기서 이 함수를 우리는 **activation function**이라고 부르고 대표적으로는 같은 종류가 있다.\n\n- **sigmoid**  \n  우리가 가장 쉽게 생각할 수 있는 함수이다. logistic regression에서 사용해본만큼 기울기값을 효과적으로 가질 수 있다.\n- **tanh**  \n  sigmod와 굉장히 유사한 함수이다. 따라서, 비슷한 용도로 사용될 수 있다.\n- **ReLU**  \n  출력 시점에서는 사용하지 않지만, 각 각의 hidden layer에서 이를 사용하는 경우가 많다. 왜냐하면, sigmoid 함수는 출력값의 형태가 [0, 1], tanh는 [-1, 1]이기 때문에 반복해서 적용하면, gradient가 사라지는 현상이 발생할 수 있다. 따라서, 기울기를 있는 그대로 적용할 수 있는 이러한 형태를 출력 이전에는 많이 사용한다.\n- **Leaky ReLU**  \n  ReLU가 음수값을 완전히 무시하는데 Leaky ReLU는 이러한 데이터가 조금이라도 의미 있는 경우에 사용할 수 있다.\n- **ELU**  \n  Leaky ReLU와 비슷한 이유이다.\n\n  ![activation-functions](/images/activation-functions.png)\n  \n자료가 보이지 않는다면 [🔗 wikipedia](https://en.wikipedia.org/wiki/Activation_function)를 참고하자.\n\n---\n\n자 이제 실제로 어떻게 optimization을 수행할지를 알아보도록 하자.\n\n먼저, Loss는 가장 마지막 layer(output layer)의 output과 실제 값과의 차이가 될 것이다. 따라서, 다음과 같이 정의할 수 있다.\n\n(아래서 $\\bold{h}_{L}$은 L번째 layer의 output을 의미한다.)\n\n$$\n\\begin{align*}\n\\mathcal{L} \u0026= \\sum_{n=1}^{N}{\\ell(y_n, \\bold{h}_L)} \\\\\n\u0026= \\sum_{n=1}^{N}{(y_{n} - \\bold{h}_{L})^2} \\\\\n\u0026= \\sum_{n=1}^{N}{(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))^2} \\\\\n\u0026= \\sum_{n=1}^{N}{(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\sigma(\\bold{w}_{L-1}^{\\top}\\bold{h}_{L-2} + b_{L-2}) + b_{L-1}))^2} \\\\\n\u0026= ...\n\\end{align*}\n$$\n\n여기서 중요한 것은 우리는 전체 $\\bold{W}$를 학습시켜야 한다는 것이다. 우리는 출력층만 학습하는 게 아니라 전체 모든 layer의 $\\bold{w}_{i}$를 업데이트해야 한다는 것이다.\n\n그러기 위해서는 우리는 ${{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{i}}}$를 모두 구해야 한다는 것이다. 아마 가장 습관적으로 하는 행위는 숫자가 작은 값부터 편미분하면서 진행하는 것이다. 하지만, 그렇게 하지말고 반대 순서로 미분을 하라는 것이 **backpropagation**의 main idea이다.\n\n$$\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L}}} = \\sum_{n=1}^{N}\\{({\\partial \\bold{h}_L \\over \\partial \\bold{w}_{L}} )\\times\\red{-2(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))}\\}\n$$\n\n$$\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-1}}} = \\sum_{n=1}^{N}\\{({\\partial \\bold{h}_{L} \\over \\partial \\bold{w}_{L-1} })\\times\\red{-2(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))}\\}\n$$\n\n즉, 다음과 같은 chain rule을 이용하는 것이다.\n\n$$\n\\begin{align*}\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L}}} \u0026= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L}}} \\\\\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-1}}} \u0026= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n\u0026= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{h}_{L-1}}} {{\\partial\\bold{h}_{L-1}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n\u0026= \\blue{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L-1}}} } {{\\partial\\bold{h}_{L-1}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-2}}} \u0026= \\blue{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L-1}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L-2}}} \\\\\n\\end{align*}\n$$\n\n우리는 빨간색과 파란색 부분의 연산을 재활용할 수 있다는 것이다. 또한, ${{\\partial\\bold{h}_{l}}\\over{\\partial \\bold{w}_{l}}}$은 굉장히 쉬운 연산이기에 우리가 신경 써서 계산해야 할 값은 매단계를 연결해줄 $ {{\\partial\\bold{h}_{l}}\\over{\\partial \\bold{h}_{l-1}}}$이다.\n\n![ml-backpropagation](/images/ml-backpropagation.jpg)\n\n## Loss Function\n\n우선 KL-Divergence, Entropy, Cross Entropy에 대한 약간의 이해가 필요하니 이전 Posting([🔗 Base Knowledge](posts/ml-base-knowledge))을 살펴보고 오자.\n\n위에서는 자연스럽게 Loss를 계산할 때, Squared Error를 사용하였다. 하지만 경우에 따라서는 다양한 함수를 사용할 수 있다. multiclass classification에서는 **Cross Entropy Loss**를 사용한다. \n\n우선 Cross Entropy Loss는 대게 L2 Loss(Squared Error)와 같이 비교되어진다. 우선 우리가 이전 [🔗 Parametric Estimation](posts/ml-parametric-estimation)에서 MLE를 다룰 때, KL-Divergence를 통해서 MLE가 최적 parameter를 찾을 것이라는 걸 증명한 적이 있다. 그렇다면, 우리가 [🔗 Logistic Regression](/posts/ml-logistic-regression)에서 Squared Error를 통해서 Loss를 구했던 공식을 확인해보자.(Gradient Asecent Part)\n\n여기서 우리는 다음과 공식을 봤었다.\n\n$$\n\\argmax_{\\bold{w}} \\sum_{n=1}^{N}y_{n}\\log{\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}) + (1-y_{n})\\log{(1-\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}))} }\n$$\n\n이 공식을 Cross Entropy를 통해서 설명할 수 있다.\n\n$$\n\\begin{align*}\nH_{q}(p) \u0026= - \\sum_{x \\in \\Omega}q(x)\\log_{2}p(x) \\\\\n\u0026= \\sum_{n=1}^{N}{[-y_{n}\\log\\hat{y}_{n} - (1- y_{n})\\log(1-\\hat{y}_{n})]}\n\\end{align*}\n$$\n\n즉, 여기서 우리가 얻을 수 있는 insight는 Cross Entropy는 sigmoid를 취한 binary classification에서 Squared Error와 같고, 이러한 Cross Entropy를 Squared Error가 할 수 없는 Multiclass에는 적용할 수 있을 것이라는 점이다. 왜냐하면, multiclass classification에 사용되는 Softmax Function을 이용해서 Sigmoid function을 유도하기 때문이다. 잠시 까먹었을까봐 Softmax 함수를 다시 적는다.\n\n$$\n\\hat{y}_{k} = {{\\exp(\\bold{w}_{k}^{\\top}\\bold{x})}\\over{\\sum_{i=1}^{K}{\\exp(\\bold{w}_{i}^{\\top}\\bold{x})}}}\n$$\n\n따라서, Cross Entropy Loss를 대입하여 다음과 같은 Loss를 얻을 수 있다.\n\n$$\n\\mathcal{L} = \\sum_{n=1}^{N}\\sum_{k=1}^{K}[-y_{k,n}\\log\\hat{y}_{k,n}],\\quad y_{k,n} = p(x_{n} \\in C_{k}| x_{n}) \n$$\n\n여기서 $y_{k,n}$은 one-hot encoding된 데이터로, 정답인 class만 1이고 나머지는 모두 0으로 되어 있다. 따라서, multiclass classification에서는 위와 같은 Loss를 주로 사용한다.\n\n이 두가지 뿐만 아니라 여러가지 Loss Function이 이미 존재한다. 예전에 잠깐 설명했던 L1 Loss부터 시작해서 NLLLoss, KLDivLoss 등등 존재하며, data의 특성과 output의 형태에 따라서 우리는 스스로 Loss Function을 새로 정의할 수도 있다.\n\n\n\n\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- activation function, wikipedia, https://en.wikipedia.org/wiki/Activation_function","slug":"ml-nn","date":"2022-10-20 09:00","title":"[ML] 6. Neural Network","category":"AI","tags":["ML","NeuralNetwork","Perceptron","Backpropagation","CrossEntropyLoss"],"desc":"우리는 Linear Regression, Logistic Regression, SVM을 거치며 data로 부터 유의미한 pattern을 발견하는 과정을 알아보았다. 이 과정은 우리에게 명확한 식 하나를 제시하였고, 모든 과정을 우리가 제어할 수 있게 하였다. 하지만, 실제 데이터를 우리가 모두 명확하게 이해할 수 있는 형태로 분류할 수 있는 것인지는 의문이 들 수 있다. 그렇다면, 우리가 이해하지는 못하지만, 알아서 최적의 결과를 가져오게 할 수 있는 방법이 있을까? 이런 마법같은 일에 대한 아이디어를 제시하는 것이 Neural Network이다.  게 알지 못하지만 input이 들어왔을 때, 이를 처리해서 output을 전달하는 시스템을 우리의 신체에서 찾게 된다. 바로 우리 몸을 이루는 신경망이다. 예시로 우리는 눈을 통해 빛이라는 input을 받으면, 우리 눈과 뇌에서 무슨 일이 발생하는지는 모르지만 결과적으로 우리는 물체를 볼 수 있다. 이 과정을 추측의 과정에 도입하면 어떻게 될까?","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nNLP를 수행할 때, 우리는 sequence of character를 처리하는 방식을 제대로 알아야 제대로 된 전처리와 후처리 등이 가능하다. 따라서 해당 chapter에서는 어떻게 원본 문자/단어/문장을 처리하는 방식을 다룰 것이다.\n\n## Regular Express\n\n아주 기본적인 문자열 처리 방법이다. 이를 알고 있어야 실질적인 처리가 가능하다. 해당 내용은 별도의 Posting으로 분리하여 다루었다. ([🔗 Regex](/posts/regex))를 살펴보도록 하자.\n\n## Text Normalization\n\n우리가 사용할 NL은 정제되어 있지 않아서 여러 전처리를 수행해야 한다. 그 중에서 대중적으로 좋다고 알려진 방법들을 살펴볼 것이다. 기본적으로는 아래 단계를 처리하는 것이 일반적이다.\n\n1. Word Tokenization  \n   말 그대로 NL 데이터가 입력되었을 때, 이를 단어 단위로 쪼개는 것이다.\n2. Word Reformating  \n   단어를 나누었다면, 각 단어의 형태를 처리하기 쉬운 형태로 Normalizing하는 것이다.  \n3. Sentence Segmentation  \n   문장 단위로 구분해는 과정이다.\n\n이제 각 단계를 세부적으로 다뤄보겠다.\n\n### 1. Word Tokenization\n\n우선 쉽게 생각할 수 있는 것은 단순히 띄어쓰기를 기준으로 구분하는 것이다. 그렇게 하면, 우리는 입력으로 주어진 Corpus에서 token을 추출할 수 있다. \n\n하지만, \"San Francisco\"와 같은 단어가 두 개의 token으로 나누는 것이 아니라 하나의 token으로 처리되기를 원할 수 있다. 뿐만 아니라 일부 언어들(특히 중국어와 일본어)의 경우 띄어쓰기 없이 작성하는 언어들의 경우 문제는 더 커질 수 있다. 이 경우에는 **Word Segmenting**이라는 알고리즘을 활용할 수도 있는데, 원리는 매우 간단하다. 언어의 모든 단어를 포함하는 사전을 기반으로 문장에서 사전에 일치하는 가장 긴 문자열을 찾을 수 있을 때까지 token을 연장해서 만드는 방식이다.\n\n그러나 이 방식도 결국은 특정 언어(중국어, 등)에서는 잘 작동하지만, 일부 언어(영어 등)에서는 잘 작동하지 않는 경우가 많다. 따라서, 최근에는 확률에 기반하여 같이 등장하는 횟수가 많을 경우 하나의 token으로 묶는 형태의 tokenization을 더 선호한다.\n\n이 과정에서 우리가 추가적으로 수행하는 것이 바로 word의 갯수를 추출하는 것이다. 대게 우리가 관심 있어하는 수는 총 3가지이다.\n\n1. **number of tokens**  \n   즉, 띄어쓰기로 나뉘어지는 token들의 총 갯수를 의미한다.\n2. **number of types(Vocabulary)**  \n   띄어쓰기로 나뉘어진 token들의 중복을 제거한 종류들의 갯수를 의미한다. 대게 이러한 type들의 모음을 Vocabulary라고 한다.\n3. **number of each type's tokens**  \n   각 종류의 token이 얼마나 많이 등장했는지를 의미한다.\n\n여기서 이러한 token이나 type이 서로 같나는 것을 어떻게 구분할 수 있을까? 이를 위해서 Word Reformating을 수행하여 좀 더 일반적인 형태로 변형하여 위의 수들을 파악하기도 한다.\n\n### 2. Word Normalization and Stemming(Word Reformating)\n\n대게의 언어는 word의 형태가 여러 개로 존재한다. 이 과정에서 우리가 고려해야 할 것이 정말 많다. 그 중에서 가장 기본적으로 수행되어야 할 내용은 다음과 같다. 해당 내용은 영어에 중심을 둔 설명이다.\n\n1. **Uppercase**  \n   영어에서 첫 글자는 대문자로 시작한다는 규칙이 있다. 또는 강조하고 싶은 단어를 대문자로 표현하기도 한다. 그 결과 token의 종류를 추출하는 과정에서 문제를 일으키기도 한다. 따라서, 이를 모두 lowercase로 바꿔버리는 것이다. 하지만, 모든 경우에 이를 적용할 수 잇는 것은 아니다. 가장 대표적인 예시로 US와 us의 의미가 다르다는 것이다. 또, 고유 명사인 General Motors와 같은 경우도 다르게 처리하는 것이 좋다. 따라서, 이를 고려해서 먼저 처리한 이후에 전체 데이터를 lowercase로 변환하는 방식을 수행한다.  \n2. **Lemmatization**  \n   Lemma(기본형, 사전형)로 단어를 변환하는 것이다. 가장 기본적인 것은 am, are, is와 같은 be동사를 모두 be로 변환하거나 car, cars, car's를 모두 기본형태인 car로 바꾸는 것이다. 대게의 경우에는 이 과정에서 의미를 일부 잃어버리기 때문에 lemma + tag로 기존 token을 복구할 수 있도록 하는 tag를 포함하는 것이 좋다.\n3. **Stemming**  \n   morpheme(형태소)은 중심 의미를 가지는 stem과 핵심 의미는 아니지만 stem에 추가 의미를 더해주는 affixes로 나누어 word를 나눌 수 있다. 따라서, 각 token을 가장 core의 의미를 가지는 stem으로 나타내는 방식이다. 대표적인 예시가 automate, automatic, automation을 automat으로 변환하는 것이다. 이는 lemmatization보다 넓은 범위의 word를 하나로 묶기 때문에 세부의미가 더 손실될 수 있다. 따라서, 기존 의미로 복구할 수 있는 tag를 포함하는 것이 좋다. \n\n### 3. Sentence Segmentation\n\n문장을 구분할 수 있는 도구로 우리는 \"?\", \"!\", \".\"을 활용한다. \"?\"와 \"!\" 같은 경우는 문장의 끝을 의미하는것이 대게 자명하다. 하지만, \".\"은 꽤나 애매할 수 있다. 소수점, Abbreviation(Mr., Dr., Inc.)와 같은 경우에 빈번하게 사용되기 때문이다. 따라서, 이를 판단하기 위해서 Decision Tree를 만들어서 이를 수행한다. 아래와 같이 사람이 직접 규칙을 정할 수도 있지만 현재는 대게 통계 기반으로 수행한다.\n\n![nlp-sentence-segmentation](/images/nlp-sentence-segmentation.jpg)\n\n## Collocation(연어) processing\n\nText Normalization을 통해서 우리는 sentence를 구분하고, word를 추출할 수 있었다. 하지만, 단순히 하나의 word를 기반으로 처리하는 것이 아니라 주변 단어를 활용하여 처리해야만 얻을 수 있는 정보들이 있다. 우리는 이를 Collocation(연어)를 활용하여 수행한다. 이는 특정 단어쌍이 높은 빈도로 같이 붙어 사용되는 현상을 말한다. **\"모든 단어는 이를 동반하는 주변 단어에 의해 특성 지어진다.\"** 따라서, 우리는 이 collocation을 co-ocurrence로 생각할 수 있다. 이를 통해서 우리는 다음과 같은 것들을 할 수 있다.\n\n1. **lexicography(사전 편찬)** : 같가니 유사한 뜻을 가지는 단어는 빈번하게 붙어서 사용되는데 이를 이용해서 하나의 단어의 뜻을 안다면, 이를 통해서 다른 단어의 뜻을 추론하며 확장해나갈 수 있다.\n2. **language modeling** : NL를 통해서 원하는 결과를 얻기 위해서 특정 parameter를 추정해내는 것을 language modeling이라고 하는데 이 과정에서 collocation을 활용하는 것이 단일 단어를 활용하는 것보다 context를 활용할 수 있다는 점에서 장점을 발휘할 수 있다.\n3. **NL generation** : 우리는 문맥상 매끄러운 문장을 원한다. 즉, \"감을 잡다\"를 \"감을 붙잡다\"라고 했을 때, 뜻을 이해할 수는 있지만 어색하다고 느낀다. 따라서, 이 관계를 활용해서 NL을 생성해야 하기 때문에 collocation을 고려해야 한다.\n\n그렇다면, 이러한 Collocation을 어떻게 찾을 수 있을까?\n\n1. **Frequency**  \n   가장 간단하게 단순히 동시 발생 빈도를 확인하는 것이다. 정확한 파악을 위해서는 빈번하게 등장하는 의미 없는 단어를 먼저 filtering할 필요가 있다. 대표적인 예시로 a, the, and 등이 있다.\n2. **Hypothesis Testing**  \n   가설 검증으로 우리가 가정한 collocation을 지정하고, 이 사건이 일어날 가능성을 굉장히 낮게 하는 가설을 반대로 가정한 후에 이것이 불가능하다는 것을 증명하는 Null Hypothesis를 이용한 증명으로 타당성을 확보하는 것이다. 따라서, 우리가 보이고자 하는 것은 word1, word2가 있을 때, 두 단어가 서로 의존적이라는 것을 증명하고 싶은 것이다. 따라서, Null Hypothesis로 두 단어는 독립이다라고 지정하면, 우리는 다음 식을 얻을 수 있다.  \n   $p(w_{1}, w_{2}) = p(w_{1})p(w_{2})$  \n   이를 바탕으로 t-검증을 다음과 같이 수행할 수 있다.  \n   $t = {{p(w_{1}, w_{2}) - p(w_{1})p(w_{2})} \\over \\sqrt{p(w_{1}, w_{2})\\over{N}}} $  \n   t값이 이제 커질 수록 우리는 해당 가설이 틀렸음을 증명하여 collocation임을 주장할 수 있다.\n\n## Minimum Edit Distance\n\n단어 또는 문장 간 유사도를 측정할 때, 사전을 기반으로 수행할 수도 있지만 참고할 corpus가 마땅하지 않거나 더 추가적인 수치가 필요하다면, Minimum Edit Distance로 유사도를 측정하기도 한다. 즉, 두 문자열이 같아지기 위해서 어느정도의 수정이 필요한지를 수치화한 것이다. 여기서 연산은 새로운 문자 추가, 삭제, 대체만 가능하다.\n\n\n```plaintext\nS - N O W Y  | - S N O W - Y\nS U N N - Y  | S U N - - N Y\ndistance : 3 | distance : 5\n```\n\n다음과 같이 표현이 가능하다.\n\n1. 문자열 x, y가 있을 때, $E(i, j)$는 x의 0\\~i까지를 포함하는 문자열과 y의 0~j까지를 포함하는 문자열의 distance라고 하자. \n2. 이렇게 되면, $E(i,j)$에서 우리는 끝문자의 규칙을 볼 수 있다.\n    \n    오른쪽 끝 문자가 가질 수 있는 조합은 3가지 밖에 없다.\n    \n    ```plaintext\n    x[i]        | -           | x[i]\n    -           | y[j]        | y[j]\n    distance: 1 | distance: 1 | distance: 0 or 1\n    ```\n    \n3. 그렇다면 우리는 하나의 사실을 알게 된다.\n    \n    $E(i,j)$는 다음 경우의 수 중 하나여야만 한다.\n    \n    - $E(i-1, j) + 1$\n    - $E(i, j-1) + 1$\n    - $E(i-1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )$\n4. 따라서, 다음과 같은 식을 유도할 수 있다.\n    \n    $E(i, j) =\\min( \\\\\n      \\quad E(i-1, j) + 1, \\\\\n      \\quad E(i, j-1) + 1,  \\\\\n      \\quad E(i - 1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )\\\\\n    )$\n\n만약, 각 연산의 비용이 다를 경우라면, 1 대신에 그 값을 넣어주면 충분히 풀 수 있으며, 추가적으로 최적의 이동형태를 알고 싶다면, back pointer 하나를 추가하는 것으로 충분하다.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)","slug":"nlp-text-processing","date":"2022-10-19 21:59","title":"[NLP] 2. Text Processing","category":"AI","tags":["NLP","Regex","Tokenization","Collocation","MinimumEditDistance"],"desc":"NLP를 수행할 때, 우리는 sequence of character를 처리하는 방식을 제대로 알아야 제대로 된 전처리와 후처리 등이 가능하다. 따라서 해당 chapter에서는 어떻게 원본 문자/단어/문장을 처리하는 방식을 다룰 것이다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nNatural Language(자연어, 사람이 사용하는 통상 언어)를 input으로 활용하고자 하는 노력은 컴퓨터의 등장부터 시작하여 여러 번 시도되어 왔다. 지금까지도 완벽하게 이를 처리하는 것은 힘들다. 왜 Natural Language를 다루는 것은 어렵고, 이를 해결하기 위해서 NLP에서는 어떤 방식을 활용할지에 대한 개략적인 overview를 제시한다. 또한, Natural Language의 특성과 분석 단계를 이해하기 위해서 Linguistics(언어학)을 간략하게 정리한다. \n\n## NLP\n\nNatural Language Processing의 약자로 사람이 사용하는 언어를 input으로 하여 원하는 값을 추출해내는 것이 목표이다. 이를 위해서 우리는 사람의 언어를 이해하거나 다룰 수 있는 능력을 컴퓨터에게 부여해야 한다.\n\n먼저, 이러한 필요가 있는 대표적인 usecase를 살펴보면 다음과 같다.\n\n### Usecase\n\n- **Spam Detection**  \n  가장 간단한 예시로 mail에서 spam 여부를 확인하는 기능이다.\n- **POS tagging / NER**  \n  특정 단어 단위의 처리를 수행하게 되는데 단어의 품사와 대략적인 의미를 가진 category로 분류로 tagging하는 과정이다. 이를 기반으로 하여 다른 usecase에서 활용하는 경우가 많다. 품사와 category는 단어의 뜻을 추론하는데 큰 도움을 주며, 이것이 문장의 이해 등에 도움을 주기 때문이다.\n- **Sentiment Analysis**  \n  감정/여론 분석 등의 영역을 의미하며, 텍스트 또는 대화에서의 긍정/부정 여부를 판단하거나 평점 등을 추출하는 기능이다.\n- **Conference Resolution**  \n  \"he\", \"she\" 등 대명사, 생략 단어 등을 원래의 단어로 대체하거나 채우는 과정을 수행한다. 이 역시도 여러 영역에서 이를 기반으로 추가적인 작업을 할 수 있다.  \n- **Word Sense Disambiguation(WSD)**  \n  특정 단어가 주어졌을 때, 동의어, 동음이의어 등에서 가르키는 진짜 의미를 헷갈리지 않게 명확하게 다시 한 번 처리한다. 이 역시도 다른 NLP usecase에서 두루 사용된다. \n- **Parsing**  \n  문장에서 단어들을 의미를 가지는 단위(구, 절, 문장)로 다시 grouping한다. 이 과정을 잘 수행하기 위해서는 이전 단계에서 WSD와 Conference Resolution, POS tagging, NER이 이루어지면 좋다. 이 과정을 통해서 문장의 개략적인 의미를 파악할 수 있다.\n- **Machine Translation(MT)**  \n  특정 언어를 또 다른 Natural Language로 변경하는 기능이다.\n- **Information Extraction(IE)**  \n  특정 문장에서 사용자에게 의미있을만한 데이터를 추출하는 것이다.\n- **Q\u0026A**  \n  특정 사용자가 질문을 하였을 때, 이 뜻을 이해하고, 이에 적절한 대답을 수행하는 방식이다.\n- **Paraphrase**  \n  문장의 뜻을 이해하고, 더 쉬운 형태의 표현으로 변환하는 기능이다.\n- **Summarization**  \n  여러 문장으로 이루어진 글의 의미를 이해하고, 적절한 내용으로 요약하는 기능이다.\n- **Dialog**  \n  Natural Language를 사용하는 사람과 1:1로 담화를 주고 받는 것이다. 의미를 이해할 뿐만 아니라 자신이 내보낼 output에 대해서도 적절하게 생성할 수 있는 능력이 필요하다.\n\n위와 같이 많은 usecase가 있는데 이를 구현하는 것은 지금까지도 굉장히 challenge한 부분이다. 그것은 Natural Language가 가지는 몇몇 특징 때문이다.\n\n### Why is NLP difficult?\n\n여기서는 Natural Language 중에서 영어를 기반으로 한 설명이지만, 한국어도 매우 유사하다.\n\n- **non-standard** : Natural Language를 사용하는 사람들이 표준을 항상 따르지는 않는다는 것이다. 우리는 약어를 사용하거나 문법에 맞지 않는 비문을 사용하여 의사소통을 하기 때문에 이것을  시스템이 이해하게 하는 것은 어렵다.\n- **segmentation** issues : 의미를 가지는 단어 단위로 묶는 것이 어렵다는 것이다. 우리는 문장의 띄어쓰기를 어디로 받아들이냐에 따라서 의미가 달라지는 것을 본 경우가 있을 것이다.\n- **idioms** : 관용구의 사용은 NLP에서 예외처리로 해주어야 하는 것이다. 단어 그대로의 의미와 다른 의미를 가지기 때문이다.\n- **neologisms** : 신조어는 계속해서 생겨나기 때문에 이를 계속해서 업데이트 해주는 것도 부담이 된다.\n- **world knowledge** : 사전 지식을 알고 있어야 이해할 수 있는 단어, 문장이 존재한다. 즉, 어떤 지식을 가지고 있느냐에 따라서 해석이 달라진다는 것이다.\n- **tricky entity names** : 고유 명사 중에서 특히 contents(노래, 그림, 소설) 등의 제목이 해석 시에 헷갈리게 한다. 예를 들면, \"Let it be\"라는 비틀즈의 노래는 문장 중간에 들어가면, 하나의 문장으로 받아들여지게 되는데 이를 잘 해결할 수 있도록 해야 한다.\n\n위의 내용을 요약하자면, 다양한 단어가 다양한 현상과 다양한 법칙(Grammer)의 영향을 받기에 어려우며, 단어가 가지는 모호성이 문제를 야기한다는 것이다.\n\n### Solutions\n\n이러한 문제를 해결하기 위해서 크게 두 가지 방식을 사용할 수 있다.\n\n- Rule based approach  \n  Gammer와 같은 법칙을 모두 적용해서 prgoramming을 하는 것이다. 하지만, 이 방식은 비문과 같은 문장을 제대로 처리할 수 없을 뿐만 아니라 정확한 형태의 문장이라도 여러 의미로 해석되는 문장에서 경향성과 문맥을 전혀 파악할 수 없다. \n- Statistic based approach  \n  그래서 최근에는 경향성과 문맥을 파악할 수 있도록 AI 기술, ML, Deep Learning을 이용하여 NLP를 수행하는 것이 하나의 trend로 자리 잡았다. 그렇다면, 어떻게 통계적인 접근법이 경향성과 문맥을 포함할 수 있을까? 이는 통계가 가지는 경향성이라는 특징과 conditional probability를 사용할 때의 문맥을 포함한 경향성을 파악할 수 있다는 점을 활용해서 가능하다. \n\n## Linguistics\n\n결국 앞으로 통계적인 방식을 활용하더라도 우리는 최소한의 언어학적인 기본이 필요하다. 왜냐하면, 통계에 사용할 데이터를 처리하기 위해서이다. 우리가 사용할 데이터는 text 또는 음성이다. 이를 적절하게 처리하여 통계에 사용할 유의미한 데이터로 변환하는 과정이 필요하다. 이를 위해서 언어학에 대한 이해가 필요한 것이다.\n\n일반적으로 언어를 분석할 때, 사용할 수 있는 도구는 **Grammar**이다. 이는 특정 language에서 허용되는 규칙의 집합을 정리한 것이다. 이것의 종류는 크게 두 가지로 나뉜다.\n\n- **Classic Grammar**  \n  사람이 실제로 언어를 사용함에 있어 발생하는 이상한 습관과 같은 언어 표현이다. 이러한 법칙들은 대게 예제들을 통해서 정의되는데 이런 것을 명확하게 구분할 수 있는 명백한 도구가 존재하지는 않는다. 예를 들면, 감탄사와 같은 것들이 여기에 포함되겠다. 이는 이러한 변칙적인 형태 때문에 programming적으로 표현하는 것이 불가능하다. \n- **Explicit Grammar**  \n  명백하게 정의되어 있는 언어 규칙을 의미한다. 이는 Programm으로 구현할 수 있으며, 여러 Grammar 정리 내용이 이미 정리되어 있다. (CFG, LFG, GPSG, HPSG, ....)  \n  이를 문법적으로 분석하기 위해서 우리는 6단계의 순차적인 처리가 필요하다.\n\n### 6 Layers in Language\n\n각 단계는 input과 output을 가진다. 단계적으로 진행되기 때문에 이전 단계의 output이 다음 단계의 input이 되며, 때때로 몇 단계는 생략될 수 있기에 유연하게 생각하도록 하자.\n\n각 단계에서 실제로 특정 문장이 처리되는 과정을 이해하기 위해서 \"Astronomers saw stars with telescope\"라는 문장이 음성 또는 text로 들어왔을 때를 가정하여 각 단계에는 무엇을 하고 이를 통해서 어떻게 이 문장을 바꿀 수 있는지를 확인해보겠다.\n\n\u003e **1. Phonetics/Orthography(음성학/맞춤법)**\n\n먼저 Orthography는 맞춤법 검사를 의미하며, character sequence로 input이 들어오면, 이를 맞춤법에 맞는지를 확인하여 이것이 수정된 sequence로 반환한다.  \n예시 문장에 있는 \"telescope\"는 문법에 맞지 않으므로 \"telescopes\"로 바뀌어야 한다.\n\n| input                                 | output                                 |\n| :------------------------------------ | :------------------------------------- |\n| Astronomers saw stars with telescope. | Astronomers saw stars with telescopes. |\n\nPhonetics는 음성학을 의미하며, 혀와 음성의 영향을 주는 다양한 근육의 위치 형태, 모양, 빈도를 활용하여 자음과 모음을 분류하는 작업을 수행한다. Orthoography와는 달리 억양이라는 것을 추가적으로 활용할 수 있다.\n\n| input                                                       | output                                 |\n| :---------------------------------------------------------- | :------------------------------------- |\n| Astronomers saw stars with telescopes.를 의미하는 음성 신호 | əsˈtrɒnəməz sɔː stɑːz wɪð ˈtɛlɪskəʊps. |\n\n*https://tophonetics.com/ 을 통해서 변환하여 얻을 수 있다.\n\n\u003e **2. Phonology/Lexicalization(음운론/어휘화)**\n\nPhonology은 음운론으로 소리와 phonemes(음소)사이의 관계를 이용하여, 음소를 특정 word로 변환하고, Lexicalization에서는 해당 단어를 사전에서의 형태로 변환하는 과정을 수행한다.\n\n| input                                  | output                                 |\n| :------------------------------------- | :------------------------------------- |\n| əsˈtrɒnəməz sɔː stɑːz wɪð ˈtɛlɪskəʊps. | Astronomers saw stars with telescopes. |\n\n\u003e **3. Morphology(어형론)**\n\nMorphology는 어형론으로 음소의 구성을 기본형(lemma)의 형태로 변환하며, 각 단어들을 형태학적인 의미를 갖는 카테고리(category, tag)로 분류한다. \n여기서 사용되는 lemma와 category가 무엇인지 좀 더 자세히 살펴보자.\n\n- **lemma**  \n  - 사전에 표기되는 단어의 기본형으로, 사전에서 word를 찾는 pointer가 된다.  \n  - 동음이의어의 경우 특정 뜻을 지칭하고 싶은 경우에는 numbering을 수행하기도 한다.\n  - 더 나아가서는 형태소(morpeheme)까지 구분하기도 한다. 이는 혼자서 쓰일 수 있는 자립 형태소(root)와 의존 형태소(stem)으로 나눌 수 있다.  \n    - 예를 들면, quotations -\u003e quote[root] + -ation[stem] + -s[stem]\n    - 위와 같은 형태로 세분화할 수도 있지만, 대게는 lemma 단위에서 그친다.\n- **categorizing**  \n  - category는 정하기 나름이며, 이미 정해져있는 tagset들도(Brown, Penn, Multext) 많이 존재하고, 억양이나 실제 분류 등을 수행하는 것도 가능하다.\n  - **POS tagging**은 category를 분류하는 방법 중에서 가장 유명한데, 이는 여러 언어에서 거의 호환되기 때문에 이 방식을 활용하여 분석하는 것이 가장 안정적인 방법이라고 할 수 있다. 이는 별도의 Posting에서 더 자세히 다루도록 하겠다.\n\n또한, 단어의 형태는 언어마다 다양하기 때문에 어느정도 언어마다 다른 작업을 해주어야 한다. 크게 구분되는 형태로 언어를 3개의 종류로 나눌 수 있다.\n\n1. **Analytical Language(고립어)**  \n   하나의 단어가 대게 하나의 morpheme을 가진다. 따라서, 하나 이상의 category로 구분되어질 수 있다.  \n   ex. English, Chinese, Italian\n2. **Inflective Fusional Language(굴절어)**  \n   prefix/suffix/infix가 모두 morpheme에 영향을 미치며, morpheme의 정의 자체가 애매해지는 언어 형태  \n   ex. Czech, Russian, Polish, ...\n3. **Agglutinative Language(교착어)**  \n   하나의 단어에는 morpheme이 명확하게 구분되고, prefix/suffix/infix 또한 명확하게 구분 가능하다. 따라서, 각 morpheme에 명확한 category를 mapping하는 것이 가능하다.  \n   ex. Korean, ...\n\n\n| input                                  | output(based on Brown tagset)                                         |\n| :------------------------------------- | :-------------------------------------------------------------------- |\n| Astronomers saw stars with telescopes. | (astronomer/NNS) (see/VBD) (star/NNS) (with/IN) (telescope/NNS) (./.) |\n\n\u003e **4. Syntax(통사론)**\n\nlemma나 morpheme을 구문의 요소인 S(Subject, 주어), V(Verb, 동사), O(Object, 목적어)와 같은 요소로 분류한다. 이 분류를 수행할 때에는 문장의 구성요소를 알아야 한다. 이를 bottom-up으로 살펴보자.\n \n- **Word(단어)**  \n  사전에 명시된 하나의 단위라고 볼 수 있다. 이는 관용어(dark horse)를 포함한다.\n- **Phrase(구)**  \n  둘 이상의 단어 또는 구의 결합으로 만들어진다. 대게 하나의 문법적인 의미로 변환되어진다.  \n  - 대표적인 예시\n    - Noun : a new book\n    - Adjective : brand new\n    - Adverbial : so much\n    - Prepositional : in a class\n    - Verb : catch a ball\n  - **Elipse(생략)**  \n    대게 단어 또는 구가 생략되는 경우가 많다. 특히 담화의 경우 더욱 그렇다.  \n    이를 추론을 통해서 추가할 수도 있다.\n- **Clause(절)**  \n  절은 주어와 서술어를 갖춘 하나의 문장과 유사하지만, 문장 요소로서 더 상위 문장에 속하는 경우이다.  \n  또한, 영어에서는 특히 접속사로 연결된 절이 아닌 경우에는 해당 절이 지칭하는 대상이 절 내부에서 생략된다. 이를 gap이라고 한다.\n- **Sentense**  \n  하나 이상의 절로 이루어지고, 영어에서는 시작 시에 대문자로 표기하며 종료 시에는 구분자로 .?!로 끝난다.\n\n결국 우리는 이러한 요소를 적절하게 표시해야 하는데, 이를 위해서 tree 구조를 사용하는 것이 일반적이다. 대표적으로 두 가지의 구조가 있다.\n\n1. **phrase structure(derivation tree)**    \n   문장을 기점으로 절, 구, 단어로 top-down으로 내려가는 구조를 가진다.  \n   각 단위를 묶을 때에는 ()를 이용하고, 그 뒤애 해당하는 내용이 무슨 구, 절인지를 표기한다.   \n2. **dependency structure**  \n   단어 간의 관계에 더 집중하여 나타낸다. 따라서, 사람이 보기에는 불명확해 보일 수 있지만 특정 usecase에서는 유용하다.\n\n| input                                                                 | output (phrase structure)                                                              |\n| :-------------------------------------------------------------------- | :------------------------------------------------------------------------------------- |\n| (astronomer/NNS) (see/VBD) (star/NNS) (with/IN) (telescope/NNS) (./.) | ((astronomer/NNS)NP ((see/VBD)V ((star/NNS)NP ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S |\n\n![nlp-phase-structure](/images/nlp-phase-structure.jpg)\n\n\u003e **5. Semantics(Meaning, 의미론)**\n\n간단하게는 주어, 목적어와 같은 tag나 \"Agent\"나 \"Effect\"와 같은 tag를 적용하며, 전체적인 의미를 유추해낸다.\n\n| input                                                                                  | output                                                                                                |\n| :------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------- |\n| ((astronomer/NNS)NP ((see/VBD)V ((star/NNS)NP ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S | ((astronomer/NNS)NP/agent ((see/VBD)V ((star/NNS)NP/affected ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S |\n\n\u003e **6. Discourse/Pragmatics(담화/화용론)**\n\n실제 대화 등과 같은 목표를 해결하기 위해서 앞서 보았던 문장 구조를 이용한다.\n\n만약, 해당 데이터를 통해서 하고자 하는 것이 이 이야기를 한 사람이 식당 내부에 있는지를 판단하고자 한다고 가정해보자.\n\n| input                                                                                                 | output |\n| :---------------------------------------------------------------------------------------------------- | :----- |\n| ((astronomer/NNS)NP/agent ((see/VBD)V ((star/NNS)NP/affected ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S | False  |\n\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- text to phonetic converter, https://tophonetics.com","slug":"nlp-linguistics","date":"2022-10-19 09:03","title":"[NLP] 1. Linguistics","category":"AI","tags":["NLP","Languagistics"],"desc":"Natural Language(자연어, 사람이 사용하는 통상 언어)를 input으로 활용하고자 하는 노력은 컴퓨터의 등장부터 시작하여 여러 번 시도되어 왔다. 지금까지도 완벽하게 이를 처리하는 것은 힘들다. 왜 Natural Language를 다루는 것은 어렵고, 이를 해결하기 위해서 NLP에서는 어떤 방식을 활용할지에 대한 개략적인 overview를 제시한다. 또한, Natural Language의 특성과 분석 단계를 이해하기 위해서 Linguistics(언어학)을 간략하게 정리한다. ","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n이전 Posting에서는 SVM에 대해서 알아보았다. 일반적인 Logistic Regression에서는 softmax function을 통해서 여러 class를 구분할 수 있었지만, SVM의 경우 구분 선이 결국은 hyperplane으로만 표현 가능하다. 이를 해결하기 위한 SVM에서의 여러 해결책을 알아보자.\n\n## Multiclass in SVM\n\n가장 쉽게 생각할 수 있는 것은 SVM을 결합해서 Multiclass를 구분할 수 있다는 idea이다. 아래에서 곧바로 제시할 아이디어들이 이에 대한 내용이다.\n\n\u003e **1. OvR SVM**\n\nOne vs Rest 의 약자로 다양한 별명이 존재한다. (One vs All, OVA, One Against All, OAA)  \n이름에서부터 느껴지다시피 하나의 class와 그 외에 모든 class를 하나로 묶어서 SVM을 총 class 갯수만큼 만들어서 각 decision boundary로 부터 거리가 양의 방향으로 가장 큰 class를 선택하는 방식이다.\n\n$$\n\\argmax_{k \\in [K]}(\\bold{w}_{(k)}^{\\top}\\phi(\\bold{x})+ b_{(k)})\n$$\n\n![svm-ovr](/images/svm-ovr.jpg)\n\n이 방식은 하나의 큰 문제를 갖고 있는데, 그것은 과도한 데이터 불균형을 유발한다는 것이다. 이러한 문제는 class의 수가 많아질 수록 더 심해진다.\n\n\u003e **2. OvO SVM**\n\nOne vs (Another) One의 약자로, 해당 방식은 1대1로 비교하면서 각 SVM에서 선택한 class 중에서 가장 많은 선택을 받은 class를 최종한다. OvR과는 다르게 각 각의 class를 1대1로 비교하기 때문에 데이터의 불균형에 대한 위협은 덜하다. 하지만, 해당 과정을 수행하기 위해서는 총 K(K-1)/2개의 SVM이 필요하다.\n\n![svm-ovo](/images/svm-ovo.jpg)\n\n또한, 그림에서 \"?\"로 표시된 부분을 어떤 class로 선택할지에 대한 기준이 없다. 왜냐하면, 각 영역에서 한 표씩만 받기 때문이다.\n\n\u003e **3. DAG SVM**\n\n앞 서 보았던 OvO와 OvR의 문제를 해결하기 위해서 장단점을 취하기 위해서 둘을 결합한 방식이다. 계층 형태로 SVM을 구성하기 때문에 OvO보다는 적은 SVM을 사용하면서, OvO에서의 과도한 데이터 불균형을 해결한다.\n\n![svm-multiclass-comparing](/images/svm-multiclass-comparing.jpg)\n\n\u003e **4. WW SVM**\n\nmulticlass 구분을 SVM 최적화 과정에 적용하기 위해서 목적 함수의 형태를 변형하여 구현한 방법으로 자세히 다루지 않지만, 궁금하다면 해당 [🔗 link](https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf)를 통해서 확인할 수 있다.\n\n## Kernel Method\n\n이전까지는 실제로 SVM의 형태를 변형시키거나 SVM을 여러 개 활용하여 multiclass classification을 수행하기 위한 방법을 보았다.\n\n또 다른 방법이 존재한다. 바로 input 공간을 확장하는 것이다. 즉, 더 많은 유의미한 feature를 수집하거나 기존 feature를 가공하여 새로운 feature로 활용하는 것이다. 시스템적으로 해결할 수 있는 방법은 기존 feature를 가공하여 새로운 feature를 활용하는 것이다. 아래의 예시를 보자.\n\n![feature-transposing](/images/feature-transposing.jpg)\n\n왼쪽 공간에서는 SVM은 decision vector를 적절하게 선택하는 것이 어렵다. 하지만, 기존 x 데이터에 절대값을 취하여 나타내어 데이터에 추가하면, 쉽게 decision boundary를 결정하는 것을 볼 수 있다. 그렇다면, 이러한 여러 변환 함수를 적용해보며 여러 feature를 더 추출하는 것이 좋은 해결책을 가져다 줄 것이다.\n\n그렇다면, 우리의 Soft margin SVM의 Dual Problem을 다시 한 번 살펴보자.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, \u0026 \\\\\n  \u0026 0 \\leq \\alpha_{i} \\leq C, \u0026 i = 1, ..., N \n\\end{align*}\n$$\n\n이것을 feature 변환(basis function을 취한다.)을 통해서 다음과 같이 변형한다는 것이다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\red{\\boldsymbol{\\phi}^{\\top}(\\bold{x}_{i})\\boldsymbol{\\phi}(\\bold{x}_{j})} + \\sum_{i=1}^{N}\\alpha_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, \u0026 \\\\\n  \u0026 0 \\leq \\alpha_{i} \\leq C, \u0026 i = 1, ..., N \n\\end{align*}\n$$\n\n하지만, 우리가 새로운 feature를 생성할 수록, 그리고 기존 feature를 복잡하게 사용할 수록 $\\boldsymbol{\\phi}(\\bold{x}_{i})$를 연산하는 비용이 커질 수 밖에 없다.  \n\n따라서, 우리는 일종의 trick을 하나 사용하도록 한다. 바로, 매 bayese update 마다 변하지 않고 재사용되는 값인 $\\boldsymbol{\\phi}^{\\top}(\\bold{x}_{i})\\boldsymbol{\\phi}(\\bold{x}_{j})$를 다른 값으로 대체하면 어떨까? 그렇게 하면 우리는 $\\boldsymbol{\\phi}(\\bold{x}_{i})$를 계산하고 구성하는 수고를 덜 수 있다.\n\n이것이 kernel method(trick)의 핵심 아이디어이다.\n\n가장 대표적인 예시로 아래와 같은 복잡한 $\\phi$ 가 주어졌을 때,\n\n$$\n\\boldsymbol{\\phi}(x) = \\exp({{-x^{2}}\\over{2\\sigma^{2}}})[1, \\sqrt{1\\over{1!\\sigma^{2}}}x, \\sqrt{1\\over{2!\\sigma^{4}}}x^{2}, \\sqrt{1\\over{3!\\sigma^{6}}}x^{3}, \\cdots]\n$$\n\n아래의 (RBF) kernel로 대체가 가능해진다.\n\n$$\n\\kappa(x,x^{\\prime}) = \\exp(-{{(x - x^{\\prime})}\\over{2\\sigma^{2}}}) = \\boldsymbol{\\phi}^{\\top}(x)\\boldsymbol{\\phi}(x^{\\prime})\n$$\n\n대게 우리가 표현하고자 하는 형태의 $\\boldsymbol{\\phi}$는 이미 특정 kernel 함수로 매핑되고 있으니 직접 $\\boldsymbol{\\phi}$를 계산하기 전에 찾아보는 것이 도움이 될 것이다.[🔗 link](https://dataaspirant.com/svm-kernels/#t-1608054630726)\n\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- A Comparison of Methods for Multi-class Support Vector Machines, Chih-Wei Hsu and Chih-Jen Lin, https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf\n- SEVEN MOST POPULAR SVM KERNELS, https://dataaspirant.com/svm-kernels/#t-1608054630726","slug":"ml-multiclass-classification-in-svm","date":"2022-10-18 23:19","title":"[ML] 5. Multiclass Classification in SVM","category":"AI","tags":["ML","SVM","KernelMethod"],"desc":"이전 Posting에서는 SVM에 대해서 알아보았다. 일반적인 Logistic Regression에서는 softmax function을 통해서 여러 class를 구분할 수 있었지만, SVM의 경우 구분 선이 결국은 hyperplane으로만 표현 가능하다. 이를 해결하기 위한 SVM에서의 여러 해결책을 알아보자.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n우리는 Classification을 하기 위해서 Logistic Regression을 수행하였다. 그 결과 결국 Classification도 결국은 선을 긋는 것이라는 결론을 내리게 되었다. 하지만, 여기서 그치지 않고 하나 더 고민해 볼 수 있는 것이 있다. 바로 주어진 데이터에 대해서 완벽하게 구분하는 decision boundary가 여러 개 있을 때, 어떤 것이 가장 좋은 것일까? 이것에 대한 아이디어를 제시하는 것이 SVM이다. 해당 Posting에서는 이에 대해서 살펴보도록 하겠다.\n\n## (Hard Margin) SVM\n\nSoft Vector Machine의 약자로, 위에서 제시한 문제를 해결하기 위해서 Margin이라는 것을 도입하였다.\n\n\u003e **Margin**\n\n**Margin**이란 decison boundary와 가장 가까운 각 class의 두 점 사이의 거리를 2로 나눈 값이다.\n\n![svm-1](/images/svm-1.jpg)\n\n위의 그림은 똑같은 데이터 분포에서 대표적인 decision boundary 두 개를 제시한 것이다. 여기서 우리는 굉장히 많은 decision boundary를 그릴 수 있다. 그 중에서도 파란색 실선이 직관적으로 가장 적절한 decision boundary가 될 것이라고 짐작할 수 있다. 그 이유는 필연적으로 data는 noise에 의한 오차가 발생할 수 있는데 실제 데이터의 오차의 허용 범위를 우리는 **margin**(=capability of unexpected noise)만큼 확보할 수 있다는 의미로 이를 해석할 수 있다. 따라서, 이 margin을 크게 하면 할 수록 좋은 성능을 가지는 선을 그을 수 있을 것이라는 결론을 내릴 수 있다.\n\n이것이 SVM의 핵심 아이디어이다.\n\n그렇다면, margin을 수학적으로 정의해보자. 우리가 decision boundary를 $f(\\bold{x}) := \\bold{w}^{\\top}\\bold{x} + b = 0$이라고 한다면, 점($\\bold{x}_{i}$)과 vector 직선 vector 사이의 거리 공식을 통해서 ${{|f(\\bold{x}_{i})|}\\over{||\\bold{w}||^{2}}}$라는 것을 알 수 있다.\n\n따라서 margin은 수학적으로 다음과 같다.\n\n$$\n\\min_{i}{{|f(\\bold{x}_{i})|}\\over{||\\bold{w}||^{2}}}\n$$\n\n```plaintext\n 🤔 Canonical(법칙까지는 아니지만 사실상 표준화된) SVM\n\n SVM에서는 f(x) = 0인 등식 형태를 같는다. 즉 f(x)에 어떤 값을 곱해도 똑같다는 것이다.\n 그런데 margin의 크기를 구할 때에는, w와 b에 어떤 값이 곱해진다면 이 값이 굉장히 달라지게 된다.\n 따라서, 일반적으로 우리는 margin에서의 |f(x)| = 1이 될 수 있도록 설정한다. \n 이렇게 하면 계산이 굉장히 쉬워진다.\n```\n\n![svm-2](/images/svm-2.jpg)\n\n따라서, 우리는 위의 그림과 같은 형태로 $\\bold{x}^{-}$와 $\\bold{x}^{+}$를 찾을 수 있는 것이다.\n\n이제 마지막으로 margin을 정의해보자.\n\n$$\n\\begin{align*}\n\\rho \u0026= {1\\over2}\\{ {{|f(\\bold{x}^{+})|}\\over{||\\bold{w}||^{2}}} - {{|f(\\bold{x}^{-})|}\\over{||\\bold{w}||^{2}}}  \\} \\\\\n\u0026= {1\\over2}{1\\over{||\\bold{w}||^{2}}}\\{\\bold{w}^{\\top}\\bold{x}^{+} - \\bold{w}^{\\top}\\bold{x}^{-}\\} \\\\\n\u0026= {1\\over{||\\bold{w}||^{2}}}\n\\end{align*}\n$$\n\n\u003e **Optimization**\n\n그렇다면, 이제 우리는 문제를 해결할 준비가 된 것이다. 우리가 하고자 하는 것은 margin을 최대화하면서도, 모든 data를 오류없이 분류하는 것이다. 이는 다음과 같은 Constraint Optimization 형태로 변환할 수 있다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 {1\\over{||\\bold{w}||^{2}}} \u0026\\\\\n  \\text{subject to} \\quad \u0026 y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\geq 1, \u0026 i = 1, ..., N\n\\end{align*}\n$$\n\nConditional Optimization은 이전 Posting([[ML] 0. Base Knowledge](/posts/ml-base-knowledge))에서 다룬바 있다. 해당 내용에 대해 미숙하다면 한 번 살펴보고 오도록 하자.\n\n위 내용을 숙지하였다면, 위의 폼이 다소 바뀌어야 한다는 것을 알 것이다. 해당 형태를 바꾸면서, minimize 형태를 미분이 간편할 수 있도록 바꾸도록 하겠다.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 {1\\over2}||\\bold{w}||^{2} \u0026\\\\\n  \\text{subject to} \\quad \u0026 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\leq 0, \u0026 i = 1, ..., N\n\\end{align*}\n$$\n\n우선 lagrangian은 다음과 같다.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}\\alpha_{i}(1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b))\n$$\n\n이것에 KKT Condition을 적용하여 정리하면 다음과 같은 등식을 얻을 수 있다.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n이를 $\\mathcal{L}$에 대입하여 식을 정리하면, 다음과 같다.\n\n$$\n\\mathcal{L} = -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i}\n$$\n\n이제 이것을 이용해서 Dual Problem을 정의하면 다음과 같다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, \u0026 \\\\\n  \u0026 \\alpha_{i} \\geq 0, \u0026 i = 1, ..., N \n\\end{align*}\n$$\n\n이 식에서 눈여겨 볼점은 바로 constraint 부분이다. 이 과정을 통해서 결론적으로 constraint 부분이 부등식에서 등식이 되었다. 이는 연산 과정을 매우 간단하게 한다. 뿐만 아니라 $\\bold{x}_{i}^{\\top}\\bold{x}_{j}$는 한 번 계산하면, 전체 과정에서 계속해서 재사용할 수 있기 때문에 컴퓨팅 시에는 굉장한 이점을 발휘할 수 있다. 따라서, 실제로 값을 구할 때에는 이것을 이용하여 값을 구하는 것이 가장 일반적이다.\n\n## (Soft Margin) SVM\n\nSVM의 모든 절차를 살펴본 것 같지만, 우리가 간과한 사실이 하나 있다. 바로 그것은 우리는 data가 하나의 선을 통해서 완벽하게 나뉘어진다고 가정했다. 하지만, 실제 데이터는 그렇지 않을 가능성이 크다. 따라서, 우리는 어느 정도의 오차를 허용할 수 있도록 해야 한다. 이를 slack($\\zeta$)이라고 한다.\n\n![svm-2](/images/svm-2.jpg)\n\n이를 적용하면, 우리의 목적함수와 제약 조건을 변경해야 한다. 이를 변경하는 방법은 두 가지가 존재하는데 각 각 slack variable의 L2-norm을 목적함수에 더하는 방식과 L1-norm을 더하는 방식이다.\n\n\u003e **L2-norm Optimization**\n\n먼저 L2-norm을 더하는 방식을 알아보자\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i}^{2} \u0026\\\\\n  \\text{subject to} \\quad \u0026 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, \u0026 i = 1, ..., N\n\\end{align*}\n$$\n\n여기서 $C$는 margin 최대화와 slackness 정도의 상대값을 의미한다. 만약, slackness보다 margin의 최대화가 중요하다면, C값은 커지고 반대라면 이 값은 작아진다.\n\n우선 lagrangian을 먼저 구하면 다음과 같다.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}||^{2} + {C\\over2}\\sum_{i=1}^{N}\\zeta_{i}^{2} + \\sum_{i=1}^{N}\\alpha_{i}(1 - \\zeta_{i} - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b))\n$$\n\nKKT condition을 이용하여 주요 값들을 구하면 다음과 같은 등식을 얻을 수 있다.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n$$\n\\boldsymbol{\\zeta} = {\\alpha\\over{C}}\n$$\n\n마지막으로 이를 Dual Problem으로 재정의하면 다음과 같아진다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\bold{x}_{i}^{\\top}\\bold{x}_{j} + {1\\over{C}}\\delta_{ij}) + \\sum_{i=1}^{N}\\alpha_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, \u0026 \\\\\n  \u0026 \\alpha_{i} \\geq 0, \u0026 i = 1, ..., N \n\\end{align*}\n$$\n\n여기서 $\\delta_{ij}$는 단위행렬이다. 기존 hard margin svm과 비교했을 때, ${1\\over{C}}\\delta_{ij}$ 외에는 바뀌지 않는 것을 알 수 있다.\n\n\u003e **L1-norm Optimization**\n\n그 다음은 L1-norm이다.\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, \u0026 \\\\\n  \u0026 \\zeta_{i} \\geq 0 \u0026 i = 1, ..., N\n\\end{align*}\n$$\n\n여기서는 slack variable이 반드시 0보다 크거나 같다는 것을 주의하자.\n\nlagrangian은 다음과 같다.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i} + \\sum_{i=1}^{N}\\alpha_{i}(1 - \\zeta_{i} - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b)) -  \\sum_{i=1}^{N}\\beta_{i}\\zeta_{i}\n$$\n\nKKT condition을 이용하여 주요 값들을 구하면 다음과 같은 등식을 얻을 수 있다.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n$$\n\\sum_{i=1}^{N}\\beta_{i} = C\n$$\n\n마지막으로 이를 Dual Problem으로 재정의하면 다음과 같아진다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, \u0026 \\\\\n  \u0026 0 \\leq \\alpha_{i} \\leq C, \u0026 i = 1, ..., N \n\\end{align*}\n$$\n\n결국 기존 Hard margin과 비교했을 대는 마지막 constraint에 $\\alpha_{i} \\leq C$가 추가된 것 밖에 없다.\n\n---\n\n마지막으로 여기서 하나의 insight를 더 얻을 수 있다.  \nL1-norm의 optimization으로 돌아가보자.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i} \u0026\\\\\n  \\text{subject to} \\quad \u0026 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, \u0026 \\\\\n  \u0026 \\zeta_{i} \\geq 0 \u0026 i = 1, ..., N\n\\end{align*}\n$$\n\n목적 함수의 slack variable에 constraint의 값을 대입하여, 다음과 같이 변환이 가능하다.\n\n$$\n\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}max\\{ 0, 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\}\n$$\n\n이 형태는 logistric regression에 regularization을 수행한 것과 동일한 형태를 가지게 된다. 즉, 이전 logistic regression에서 regularization을 다루지 않았는데, 결국은 soft margin svm의 L1-norm 목적함수가 logistic regression 중에서도 hinge function이라는 것을 이용했을 때의 regularization이 되는 것이다.\n\n## Generalization\n\n여태까지 살펴본 Regression을 통해서 우리는 General한 Classification 방식을 지정할 수 있다. 우선 아래 식을 살펴보자.\n\n- Linear Regression(Quadratic Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}{1\\over2}(1 - y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})) )^{2}$\n- Logit Regresion(Log Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}\\log( 1 + \\exp[-y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})]) )$\n- Binary SVM(Hinge Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}max\\{ 0, 1 - y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})) \\}$\n\n여태까지 나온 식들을 살펴보면 위와 같다. 우리는 여기서 아래와 같은 일반적인 형태의 Classification을 제시할 수 있다. \n\n- General Classification  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}\\varepsilon\\log( 1 + \\exp[-y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})]) )$\n\n여기서 $\\varepsilon$이 1이면 바로 logistic regression이 되고, $\\varepsilon$이 0에 수렴할 수록 SVM이 된다. 아래 그림을 보면 이를 알 수 있다.\n\n![compare-regressions](/images/compare-regressions.jpg)\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)","slug":"ml-svm","date":"2022-10-18 17:29","title":"[ML] 4. SVM","category":"AI","tags":["ML","SVM","GeneralClassifier"],"desc":"우리는 Classification을 하기 위해서 Logistic Regression을 수행하였다. 그 결과 결국 Classification도 결국은 선을 긋는 것이라는 결론을 내리게 되었다. 하지만, 여기서 그치지 않고 하나 더 고민해 볼 수 있는 것이 있다. 바로 주어진 데이터에 대해서 완벽하게 구분하는 decision boundary가 여러 개 있을 때, 어떤 것이 가장 좋은 것일까? 이것에 대한 아이디어를 제시하는 것이 SVM이다. 해당 Posting에서는 이에 대해서 살펴보도록 하겠다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n이전까지 우리는 input data가 들어왔을 때, continuos한 output을 얻는 것을 목표로 했다. 하지만 현실에서는 대게 정확한 수치보다는 특정 분류로 나누는 것이 효과적인 경우가 많다. 예를 들어, spam 필터링, object detection, 등 등. 따라서, 해당 포스팅에서는 classification을 위해서 사용할 수 있는 logistic regression에 대해서 살펴볼 것이다.\n\n## Classification\n\n**Classification**이란 결국 특정 input이 들어왔을 때, 이를 하나의 Class라는 output을 내보내는 것이다. 즉, output은 연속적이지 않고, descret하다. 대게 Classification에서는 Class의 갯수를 K라고 표기하고, $C_k$는 k 번째 Class라는 의미로 사용되어진다.\n\n그렇다면, 어떻게 Class를 나눌 수 있는 것일까? 매우 단순하게도 이는 **Decision Boundary**라는 선을 그어서 해결 할 수 있다.\n\n![decision-boundary](/images/decision-boundary.jpg)\n\n위의 예시처럼 우리는 선을 하나 그어서 $\\red{\\text{x}}$와 $\\blue{\\text{o}}$를 구분할 수 있다. 이를 통해서 우리는 Class 1에 해당할 것이라고 예측하는 구간 $R_1$이 만들어지고, Class 2라고 예측하는 구간 $R_2$를 구성할 수 있다.\n\n즉, classification을 수행하기 위해서 해야할 일은 기존의 Regression 과정과 마찬가지로 선을 찾는 것이다.\n\n결국 찾고자 하는 것이 선이라면, 이것을 Linear Regression으로 해결할 수 있을 것이다. 따라서, 우리는 다음과 같은 식으로 간단히 Linear Regression을 바꿔서 생각할 수 있다.\n\n- 예측값($\\hat{y}$, $h(\\bold{x})$)  \n  $h(\\bold{x}) = \\text{sign}(\\bold{w}^{\\top}\\bold{x}) = \\begin{cases} +1 \u0026 \\bold{w}^{\\top}\\bold{x} \\geq 0 \\\\ -1 \u0026 \\text{otherwise}\\end{cases}$\n- Least Squared Error(LS, MLE)  \n  실제로 parameter를 구할 때에는 sign을 취하지 않는데, sign을 취하게 되면 모두 LS는 결국 오답의 갯수 정도로 취급된다. 즉, 얼마나 예측이 잘못되었는지를 반영할 수 없다는 것이다. 따라서, 이는 기존 Linear Regression의 LS를 구하는 방법과 동일하게 수행한다.  \n  $\\argmin_{w} {1\\over2}\\sum_{n=1}^{N}{(y_n - (\\bold{w}^{\\top}\\bold{x}))^2}$\n\n이렇게 Linear Regression을 적용하면 문제가 없을 거 같다. 하지만, 실제로는 문제가 있다. 바로, 데이터가 불균형할 때이다. 만약 데이터가 decision boundary를 기준으로 대칭(symmetric)인 형태로 존재한다면, 문제가 없다. 하지만, 비대칭(asymmetric)인 경우 제대로 동작하지 않는다. 왜냐하면, linear regression은 최적에서 데이터의 평균을 반영하는데 불균형한 경우 데이터의 평균이 Decision Boundary가 되는 것은 문제가 있다.\n\n![linear-in-classification](/images/linear-in-classification.jpg)\n\n## Logistic Regression\n\n위에서 제시한 문제를 해결하기 위해서 Classification에서는 Linear Regression이 아닌 Logistic Regression을 활용한다. 이를 이해하기 위해서 기반이 될 요소들을 먼저 살펴보자.\n\n\u003e **Discriminant Function**\n\n판별함수(Discriminant Function, Score Function) 등으로 불리는 해당 함수는 특정 data가 특정 class에 속할 가능성(likelihood, probability, score)을 나타내는 함수이다. 즉, input으로 data를 받고, output으로 class에 속할 확률을 내보낸다.\n\n이를 통해서 우리는 다음과 같은 과정을 할 수 있다.\n\n만약, $f_k(\\bold{x}) \\gt f_j(\\bold{x})$이라면, $\\bold{x}$의 class는 $C_k$이다.\n\n따라서, 우리는 다음과 같은 식으로 여러 개의 Class가 있는 공간에서 data를 분류할 수 있다.\n\n$$\nh(\\bold{x}) = \\argmax_{k}f_{k}(\\bold{x})\n$$\n\n그렇다면, Discriminant Function으로 어떤 값을 쓰면 좋을까? 이에 대한 해결책을 Bayes Decision Rule에서 제시한다.\n\n\u003e **Bayes Decision Rule**\n\n만약 우리가 특정 data가 특정 Class에 속할 확률을 구한다고 하자. 우리는 먼저 Likelihood를 생각할 수 있다. $P(x|C = k), P(x|C = j)$를 구하여 각 Class에 속할 확률을 비교할 수 있을까?  \n물론 비교는 가능하다 하지만, 반쪽짜리 비교라고 할 수 있다. 만약, class k에 속하는 데이터보다 class j에 속하는 데이터가 훨씬 많다고 하자. 그러면, 일반적으로 class j가 발생할 확률 자체가 높다. 하지만, likelihood는 이러한 경향을 반영하지 않는다. 간단한 예시를 들어보자.\n\n```plaintext\n 🤔 어떤 동물의 털에 존재하는 색의 갯수가 주어졌을 때, 고양이일 확률과 호랑이일 확률이라고 하자.\n\n  그리고, input data는 털에 존재하는 색의 수라고 하자. (호랑이는 대게 3가지 색, 백호 = 2가지 색, 고양이는 매우 다양)\n  그렇다면, P(털의 색 = 3|C = 호랑이), P(털의 색 = 3|C = 고양이)를 비교했을 때, 우리는 당연히 전자가 크다고 생각할 것이다.\n  하지만, 여기서 우리가 고려하지 않은 것이 있다. 바로 전체 고양이와 호랑이의 비율이다. \n  상대적으로 고양이가 호랑이보다 압도적으로 많다는 것을 고려했을 때, 고양이의 확률이 더 높을 수도 있다. \n\n  즉, 어떤 동물의 털에 존재하는 색의 갯수가 주어졌을 때, 고양이일 확률은 \n  P(C=고양이|털의 색=3) =  P(털의 색 = 3|C = 고양이)P(C=고양이)이다. (분모는 생략함.)\n```\n\n즉, Bayes Rule에 기반하여 우리가 원하는 output은 Posterior라는 것을 명확히 알 수 있다.\n\n$$\n\\begin{align*}\np(C_{k}|\\bold{x}) \u0026= {{p(\\bold{x}| C_{k}) p(C_{k})}\\over{\\sum_{j=1}^{K}{p(\\bold{x}|C_{j})p(C_{j})}}} \\\\\n\u0026\\propto p(\\bold{x}| C_{k}) p(C_{k})\n\\end{align*}\n$$\n\n위의 경우 Class간의 상대 비교에 사용하는 지표로 이를 사용하기 때문에, 분모(Normalization Factor, 확률의 총합이 1이 되도록 하는 역할)를 제외하여도 상관없기에 대게 복잡한 분모 계산을 제외하고 표현하는 것이 일반적이다.\n\n또한, 앞선 예시에서 얻을 수 있는 insight는 편향된 데이터일수록 MLE를 사용할 수 없다는 것이다. 위에서 Linear Regression이 Classification에 부적함한 경우도 데이터의 편향이 있을 경우이다. 이 역시 Linear Regression이 결국은 MLE에 기반하기 때문인 것이다.\n\n우리는 각 Class 자체의 확률(Prior)과 Likelihood를 이용할 수 있는 Discriminant Function을 구해야 한다는 것이다.\n\n\u003e **Logistic Regression**\n\n자 이제 드디어 Logistric Regression을 시작해보자. 우리는 Discriminant Function을 먼저 지정해야 한다. 여러 가지 방법이 있지만, 가장 대표적으로 사용되는 방법은 **Softmax**를 활용하는 것이다. **Softmax**를 활용하여 식을 나타내면 아래와 같다.\n\n$$\np(y_n = k | \\bold{x}_n, \\bold{w}) = {{\\exp(\\bold{w}_{k}^{\\top}\\bold{x}_n)}\\over{\\sum_{j=1}^{K}{\\exp(\\bold{w}_{j}^{\\top}\\bold{x}_n)}}} \n$$\n\n만약, class가 2개인 Binary Classification인 경우에 **Softmax**는 다음과 같아진다. 특히 이를 **Sigmoid**(**Logit**)라고 정의한다.\n\n$$\np(y_n = k | \\bold{x}_n, \\bold{w}) = {1\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}\n$$\n\n이를 유도하는 과정은 생략하지만, 여타 다른 블로그를 더 참고하면 좋다.\n\n이를 Linear Regression과 비교해서 살펴보자.\n\n![logistic-vs-linear](/images/logistic-vs-linear.jpg)\n\nLinear Regression은 특정값을 향해 나아가고 있다. 해당 방식을 보면 x가 대상의 특성을 강하게 가지고 있다면, 명확하게 구분할 수 있는데, 이는 **sigmoid**($\\sigma$) 함수가 [0, 1] 범위 내에서 정의되기 때문에 Regression 과정에서 극단 데이터(outlier)가 가지는 영향력이 Linear Regression보다 극단적으로 적다는 것을 알 수 있다.\n\n자 이것이 가지는 의미를 이전에 살펴본 **Bayes Decision Rule**에 기반해서 생각해보자. **sigmoid**($\\sigma$)는 결국 극단적인 데이터이든, 애매한 데이터이든 거의 비슷한 값으로 변환한다. 그렇다는 것은 기존에는 평균을 구하는데에 input(x)의 값이 큰 영향을 미쳤다면, **sigmoid**($\\sigma$)에서는 특정 class에 속하는 x의 갯수가 많은 영향을 주는 것을 알 수 있다. 이를 통해서 **sigmoid**($\\sigma$)가 완벽하지는 않지만, **Bayes Decision Rule**을 반영했다는 것을 알 수 있다.\n\n마지막으로, MLE를 통해서 Logistic Regression의 parameter를 추정해보자. (MAP는 기존에 살펴본 Linear Regression과 동일하게 regularizer를 더해주는 방식이기 때문에 생략한다.)\n\n$$\n\\begin{align*}\n\\argmax_{w}\\log{p(\\mathcal{D}|\\bold{w})} \u0026= \\argmax_{w}\\sum_{n=1}^{N}{\\log p(y_{n}|\\bold{x}_{n}, \\bold{w})} \\\\\n\u0026= \\argmax_{w}\\sum_{n=1}^{N}{\\log ({1\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}) } \\\\\n\u0026= \\argmax_{w}\\sum_{n=1}^{N}{-\\log (1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})) } \\\\\n\u0026= \\argmin_{w}\\sum_{n=1}^{N}{\\log (1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})) } \\\\\n\\end{align*}\n$$\n\n## Gradient Descent/Ascent\n\n위의 복잡한 식을 봤으면 알겠지만, 안타깝게도 일반식으로 $\\bold{w}_{MLE}, \\bold{w}_{MAP}$ 등을 구할 수는 없다. 따라서, 우리가 믿을 것은 Gradient를 이용한 방식이다.\n\n\u003e **Gradient Descent**\n\n먼저, 위에서 봤겠지만, Loss는 다음과 같다.\n\n$$\n\\mathcal{L} = \\sum_{n=1}^{N}{\\log(1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n}))}\n$$\n\n이제 이를 미분해서 Gradient를 구하면 다음과 같다.\n\n$$\n\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}) = \\sum_{n=1}^{N}{{{-y_{n}\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}\\bold{x}_{n}}\n$$\n\n따라서, Gradient Descent 방식은 다음과 같이 진행된다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\alpha\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}_{t})\n$$\n\n\u003e **Gradient Ascent**\n\n위의 방식이 가장 일반적이지만, 우리가 sigmoid의 class값으로 $y \\in \\{-1, 1\\}$ 대신 $y \\in \\{0, 1\\}$을 사용했을 경우 다른 식으로도 접근이 가능하다.\n\n이 경우에는 Loss라기 보기 어렵지만, 다른 형태의 optimization 형태가 만들어진다. (여기서 $\\sigma$는 sigmoid 함수를 의미한다.)\n\n$$\n\\argmax_{\\bold{w}} \\sum_{n=1}^{N}y_{n}\\log{\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}) + (1-y_{n})\\log{(1-\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}))} }\n$$\n\n이를 똑같이 미분하여 사용하지만, 반대로 이 경우에는 maximization 이기 때문에 Gradient Ascent를 수행해야 한다. \n\n우선 미분 결과 얻는 Gradient는 다음과 같다.\n\n$$\n\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}) = \\sum_{n=1}^{N}{[y_{n} - \\sigma(\\bold{w}^{\\top}\\bold{x}_{n})]\\bold{x}_{n}}\n$$\n\n굉장히 간단하게 정리가 되어지는 것을 볼 수 있다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\alpha\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}_{t})\n$$\n\n따라서, 아래와 같이 Gradient Ascent를 활용하여 계산하는 것도 충분히 가능하다.\n\n\u003e **Newton Method**\n\n이러한 형태로 넘어오게 되면, 굉장히 많은 연산이 각 update마다 필요하다는 것을 알 수 있다. 따라서, 우리는 이 과정을 축약할 방법을 찾게 된다. 그 아이디어는 바로 gradient를 업데이트 할 때, linear 하게 update하는 것이 아니라 Quadratic하게 update하는 것이다. 이를 위한 방법론이 **Newton Method**이다. 이 방식을 Logistic Regression에 적용하였을 때, 이를 IRLS(Iterative Re-weighted Least Squared) Algorithm 이라고 부른다.\n\n![newton-method](/images/newton-method.jpg)\n\n위 그래프에서 f(x)가 Loss 라고 할 때, 우리는 $x_k$에서 직선형의 gradient를 사용하는 것보다 quadratic 형태를 사용하는 것이 더 빠르게 수렴값을 찾을 수 있다는 것을 알 수 있다.\n\n이를 사용하기 위해서는 다음 2가지에 대한 사전 이해가 필요하다.\n\n- Taylor Series  \n  smooth한 형태를 가진 x에 대한 함수를 x에 대한 급수의 형태로 변환한 것이다. 따라서 이를 식으로 나타내면 다음과 같다.  \n  $T_{\\infin}(x) = \\sum_{k=0}^{\\infin}{f^{(k)}(x_{0})\\over{k\\!}}(x-x_{0})^{k} $  \n  즉, sine 함수와 같은 형태의 그래프도 x의 급수 형태로 변환이 가능하다는 것이다. Newton Method에서는 무한대까지는 사용하지 않고, 대게 K=2까지를 쓴다.\n- Hessian Matrix  \n  특정 함수 $f(\\bold{x})$를 각 feature에 대해서 이중 편미분한 결과를 저장한 행렬이다. 식은 다음과 같다.  \n  $\n  H = \\nabla^{2}f(x) =\n  \\left[\n    \\begin{array}{ccc}\n      \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{1}^{2}} \u0026 \\cdots \u0026 \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{1} \\partial x_{D}} \\\\ \n      \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\n      \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{D} \\partial x_{1}} \u0026 \\cdots \u0026 \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{n}^{2}}\n    \\end{array}\n  \\right]\n  $\n\n이를 이용해서, Newton Method의 결과값을 정리하면 결과는 다음과 같다.\n\n$$\n\\bold{w}^{(k+1)} = \\bold{w}^{(k)} - [\\nabla^{2}\\mathcal{J}(\\bold{w}^{(k)})]^{-1}\\nabla\\mathcal{J}(\\bold{w}^{(k)})\n$$\n\n자 이제 이것을 실제로 Logistic Regression 식에 대입해보자.\n\n$$\n\\begin{align*}\n  \\nabla\\mathcal{J}(w) \u0026= - \\sum_{n=1}^{N}(y_{n}-\\hat{y}_{n})x_{n} \\\\\n  \\nabla^{2}\\mathcal{J}(w) \u0026= \\sum_{n=1}^{N}\\hat{y}_{n}(1-\\hat{y}_{n})\\bold{x}_{n}\\bold{x}_{n}^{\\top}\n\\end{align*}\n$$\n\n여기서, 아래와 같이 변수를 정의하면,\n\n$$\nS = \n  \\begin{bmatrix}\n    \\hat{y}_{1}(1-\\hat{y}_1)  \u0026 \\cdots  \u0026 0                         \\\\\n    \\vdots                    \u0026 \\ddots  \u0026 \\vdots                     \\\\\n    0                         \u0026 \\cdots  \u0026 \\hat{y}_{N}(1-\\hat{y}_N)  \\\\\n  \\end{bmatrix},\n\n\\bold{b} = \n  \\begin{bmatrix}\n    {{y_{1} - \\hat{y}_{1}}\\over{\\hat{y}_{1}(1-\\hat{y}_{1})}} \\\\\n    \\vdots \\\\\n    {{y_{N} - \\hat{y}_{N}}\\over{\\hat{y}_{N}(1-\\hat{y}_{N})}}\n  \\end{bmatrix}\n$$\n\n결과적으로 다음과 같은 형태를 얻을 수 있다.\n\n$$\n\\begin{align*}\n\\bold{w}_{k+1} \u0026= \\bold{w}_{k} + (XS_{k}X^{\\top})^{-1}XS_{k}\\bold{b}_{k} \\\\\n\u0026= (XS_{k}X^{\\top})^{-1}[(XS_{k}X^{\\top})\\bold{w}_{k} + XS_{k}\\bold{b}_{k}] \\\\\n\u0026= (XS_{k}X^{\\top})^{-1}XS_{k}[X^{\\top}\\bold{w}_{k} + \\bold{b}_{k}]\n\\end{align*}\n$$\n\n이는 결코 계산 과정이 단순하다고는 할 수 없지만, 빠르게 수렴할 수 있기 때문에 가치있는 방법이다.\n\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)","slug":"ml-logistic-regression","date":"2022-10-18 09:58","title":"[ML] 3. Logistic Regression","category":"AI","tags":["ML","LogisticRegression","Classification","SigmoidFunction","SoftmaxFunction","NewtonMethod"],"desc":"이전까지 우리는 input data가 들어왔을 때, continuos한 output을 얻는 것을 목표로 했다. 하지만 현실에서는 대게 정확한 수치보다는 특정 분류로 나누는 것이 효과적인 경우가 많다. 예를 들어, spam 필터링, object detection, 등 등. 따라서, 해당 포스팅에서는 classification을 위해서 사용할 수 있는 logistic regression에 대해서 살펴볼 것이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nRegression(회귀)이라는 단어는 \"원래의 상태로 돌아간다\"로 돌아간다는 의미를 가진다. 결국 어떤 일련의 Event로 인해서 데이터에 Noise가 발생할 수 있어도 결국은 하나의 \"보편\"으로 시간이 지나면 수렴(회귀)할 것이라는 생각에 기반하는 것이다.  \n따라서, 우리는 이러한 \"보편\"을 찾기 위해서 우리가 알고 있는 독립 데이터 X를 통해서 알고자 하는 값 Y를 보편적으로 추론할 수 있다. 이 과정을 우리는 Regression이라고 부른다. 또한, X에 의해 독립적이지 않고 종속적인 Y의 관계가 Linear하게 표현될 때 이를 우리는 Linear Regression이라고 한다.  \n따라서, 해당 Posting에서는 Linear Regression을 바탕으로 Machine Learning이 어떻게 동작하는지를 이해하는 것이 목표이다.\n\n## Regression\n\n\u003e **정의**\n\n독립 변수 X로 부터 종속 변수 Y에 대응되는 함수 f를 생성하는 과정을 의미한다.\n\n$$\n\\bold{y} = f(\\bold{x}) + \\epsilon\n$$\n\n$$\nf(\\bold{x}) = \\bold{w}^{\\top}\\bold{x}, \n(\\bold{w} = \\begin{bmatrix} w_{0} \\\\ w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{N} \\\\ \\end{bmatrix}, \\bold{x} = \\begin{bmatrix} 1 \\\\ x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{N} \\\\ \\end{bmatrix} )\n$$\n\n여기서 각 변수 $x$, $y$, $\\epsilon$, $w$은 다음과 같이 여러 이름으로 불려진다.\n\n- $x$ : input, 독립 변수, predictor, regressor, covariate\n- $y$ : output, 종속 변수, response\n- $\\epsilon$ : noise, 관측되지 않은 요소\n- $w$ : weight, 가중치, parameter \n\n\u003e \u003cmark\u003e**성능 평가(MSE)**\u003c/mark\u003e\n\n우리가 만든 Regression이 얼마나 데이터를 잘 반영하는지를 알고 싶을 때, 즉 평가하고자 할 때, 우리는 Mean Squared Error(MSE)를 사용한다. 이는 이전 포스팅인 [Parametric Estimation](/posts/ml-parametric-estimation)에서도 살펴보았었다. \n\n그렇다면, MSE를 최소로 하는 f(x)는 무엇일까? 이를 통해서 또, 하나의 식견을 넓힐 수 있다. 한 번 MSE 식을 정리해보자.\n\n$$\n\\begin{align*}\n\\Epsilon(f) \u0026= E[||\\bold{y}_*-f(\\bold{x})||^2] \\\\\n\u0026= \\int\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{x}, \\bold{y}_*)d\\bold{x}d\\bold{y}_* \\\\\n\u0026= \\int\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{x})p(\\bold{y}_* | \\bold{x})d\\bold{y}_*d\\bold{x} \\\\\n\u0026= \\int p(\\bold{x}) \\red{\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}d\\bold{x}\n\\end{align*}\n$$\n\n여기서 중요한 것은 바로 빨간색으로 색칠한 부분이다. 우리가 바꿀 수 있는 값은 f(x)를 구성하는 w밖에 없다 즉, 위 식을 최소화하는 것은 빨간색 부분을 최소화하는 것과 같아진다.  \n따라서, 이 부분을 미분해서 최솟값을 구할 수 있는데 이를 확인해보자.\n\n$$\n\\begin{align*}\n\u0026{\\partial\\over{\\partial{f(\\bold{x})}}}({\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}) = 0 \\\\\n\u0026{\\partial\\over{\\partial{f(\\bold{x})}}}({\\int||\\bold{y}_*^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} - 2f(\\bold{x}){\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} + f(\\bold{x})^2{\\int p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}) = 0 \\\\\n\u0026-2{\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} + 2f(\\bold{x}) = 0 \\\\\n\u0026f(\\bold{x}) = {\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} = E[\\bold{y}_*|\\bold{x}]\n\\end{align*}\n$$\n\n즉, 우리가 구하고자 하는 Linear Regression 함수는 data x에 따른 실제 y 값의 평균을 의미한다. Regression의 정의를 생각했을 때, 어느정도 합리적이라는 것을 알 수 있다. \"보편\"적이다는 의미에서 \"평균\"을 쓰는 경우가 많이 있기 때문이다.\n\n위에서는 MSE를 이용해서 분석하였지만, MAE(Mean Absolute Error)를 활용하여 구할 수 있는데 이 경우에는 Regression의 형태가 또 달라진다. 즉, MSE를 최소화하는 방식은 우리가 \"보편\"적인 답을 구하는데 있어 \"평균\"을 활용한 것이고, MAE를 사용한다면, 또 다른 방식을 사용한다는 것을 알게 될 것이다.\n\n## MLE of Linear Regression\n\n이제 Linear Regression에서 $\\bold{w}$를 어떻게 찾아 나갈지에 대해서 살펴볼 것이다. 순서는 이전 Posting [Parametric Estimation](/posts/ml-parametric-estimation)에서 살펴봤던 것과 마찬가지로 MLE, MAP 순으로 살펴볼 것이다. 그리고 이것이 왜 MLE고, MAP랑 관련이 있는지도 살펴볼 것이다.\n\n들어가기에 앞 서, 표기법과 용어를 몇 개 정리할 필요가 있다.\n\n- $\\bold{x}, \\bold{y}, \\bold{w}$ 등 굵은 선 처리되어 있는 변수는 vector를 의미한다.\n- $\\bold{X}$ 등 굵고 대문자로 처리되어 있는 변수는 Matrix를 의미한다.\n- $\\bold{w^{\\top}}$, $\\bold{X}^{\\top}$ 에서 T는 Transpose를 의미한다.\n- feature : input 데이터의 각 각 분류 기준들을 의미한다. 수식으로는 $x_1, x_2, x_3$ 이런 식으로 표현된 input들 중에 각 각의 input을 feature라고 하며, 실제 예시로는 데이터 수집 시에 각 데이터의 column(나이, 성별, 등 등)이 될 것이다.\n\n위의 용어 정리에 의해서 다음과 같은 사실을 다시 한 번 확인하자.\n\n먼저, 단일 Linear Regression이다.\n\n$$\n\\hat{y} = f(\\bold{x}) = \\bold{w}^{\\top}\\bold{x} = \\bold{x}^{\\top}\\bold{w}\n$$\n\n이번에는 여러 개의 데이터를 한 번에 추측한 결과값 $\\hat{\\bold{y}}$ 이다.\n\n$$\n\\hat{\\bold{y}} = \\bold{X}\\bold{w}\n$$\n\n각 의미를 곱씹어보면 어떻게 생겼을지 어렵풋이 짐작이 올 것이다.\n\n\u003e **basis function**\n\n여기서 또 하나 짚어볼 것은 바로 $\\bold{x}$를 변형하는 방법이다. 바로, 우리는 데이터로 입력 받은 데이터를 바로 사용할 수도 있지만, 해당 input 값을 제곱해서 사용해도 되고, 서로 더해서 사용해도 되고, 나누어서 사용할 수도 있다. 예를 들어서 우리가 구하고 싶은 값이 대한민국 인구의 평균 나이라고 하자. 이때, 우리가 사용하는 데이터의 값이 가구 단위로 조사되어 부,모,자식1, 자식2, ... 로 분류되어 나이가 적혀있다고 하자. 이때 우리가 필요한 것은 결국 전체 인구의 나이 데이터임으로 모두 하나의 feature로 합쳐버릴 수도 있다.\n\n이러한 과정을 위해서 우리는 basis function($\\phi(\\bold{x})$)이라는 것을 이용한다. 단순히 input data를 합성해서 하나의 input을 생성하는 것이다.\n\n따라서, 우리는 필요에 따라 input data를 가공하여 사용하며 여러 $\\phi$를 적용하여 나타낼 경우 linear regression은 다음과 같은 형태가 된다.\n\n$$\nf(\\bold{x}) = \\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x})\n$$\n\n대표적인 Basis function을 살펴보자.\n\n- Polynomial basis : 하나의 input feature에 대해서 n-제곱형태의 vector로 변환하는 형식이다. 따라서, 다음과 같이 표기 된다.  \n  $\\boldsymbol{\\phi}(\\bold{x}) = \\begin{bmatrix} 1 \\\\ x \\\\ x^{2} \\\\ \\vdots \\\\ x^{n} \\\\ \\end{bmatrix}$, $\\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x}) = w_{0} + w_{1}x + w_{2}x^{2} + ... + w_{n}x^{n}$  \n  대게 이러한 형태로 변형한 Linear Regression을 Polinomial Regression이라고 부르는데, 이를 통한 결과 값이 마치 다항식의 형태를 띄기 때문이다. 하지만, feature의 값이 polynomial이 되었더라도 $\\bold{w}$가 선형임을 잊어서는 안된다.  \n  이를 사용하게 되면, 우리는 1차원의 input 공간에서 선형으로는 나눌 수 없던 분류를 수행할 수 있다.\n- Gaussian basis : 가우시안 분포로 변환하는 것으로 특정 feature를 gaussian으로 변환하게 되면, 데이터의 경향성이 파악된다. 이는 후에 더 자세히 다룰 기회가 온다.  \n- Spline basis: 특정 구간마다 다른 Polynomial 형태의 feature를 적용하도록 하는 방식이다. 대게 구간마다 다른 확률 분포를 적용하고자 할 때 사용한다.\n- Fourier basis, Hyperbolic tangent basis, wavelet basis 등 여러 가지 방식이 존재한다.\n\n\u003e **Design Matrix**\n\n마지막으로, 이렇게 만들어진 $\\phi(\\bold{x})$를 하나의 Matrix로 합친 것을 Design Matrix라고 한다. N개의 데이터를 L개의 서로 다른 basis function으로 변환한 데이터를 행렬로 표현하면, 다음과 같다.\n\n$$\n\\Phi = \n  \\begin{bmatrix} \n    \\phi_1({\\bold{x_1}})  \u0026 \\phi_2(\\bold{x_1})  \u0026 \\cdots  \u0026 \\phi_L(\\bold{x_1})  \\\\\n    \\phi_1({\\bold{x_2}})  \u0026 \\phi_2(\\bold{x_2})  \u0026 \\cdots  \u0026 \\phi_L(\\bold{x_2})  \\\\\n    \\vdots                \u0026 \\vdots              \u0026 \\ddots  \u0026 \\vdots              \\\\\n    \\phi_1({\\bold{x_N}})  \u0026 \\phi_2(\\bold{x_N})  \u0026 \\cdots  \u0026 \\phi_L(\\bold{x_N})  \\\\\n  \\end{bmatrix}\n$$\n\n이를 통해서 표현한 모든 데이터에 대한 Linear Regression은 다음과 같다.\n\n$$\n\\hat{\\bold{y}} = \\Phi\\bold{w}\n$$\n\n자 이제부터 우리는 본론으로 들어와서 우리의 Linear Regression의 Weight(Parameter, $\\bold{w}$)를 어떻게 추정할 수 있을지를 알아보자.\n\n우리는 최종적으로 우리의 Linear Regression이 정답과 매우 유사한 값을 내놓기를 원한다. 따라서, 이때 우리는 Least Square Error를 사용할 수 있다. 이는 모든 데이터에서 얻은 예측값(Linear Regression의 output)과 실제 y의 값의 Square Error의 합을 최소화하는 것이다.\n\n$$\n\\varepsilon_{LS}(\\bold{w}) = {1\\over2}\\sum_{n=1}^{N}(y_n - \\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x_n}))^2 = {1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2\n$$\n\n이제 $\\argmin_{\\bold{w}}\\varepsilon_{LS}(\\bold{w})$을 풀기 위해서 미분을 해보자.\n\n$$\n\\begin{align*}\n\u0026{\\partial\\over\\partial\\bold{w}}{1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2 = 0 \\\\\n\u0026\\Phi^{\\top}(\\bold{y}_* - \\Phi\\bold{w}) = 0 \\\\\n\u0026\\Phi^{\\top}\\Phi\\bold{w} = \\Phi^{\\top}\\bold{y_*} \\\\\n\u0026\\bold{w} = (\\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y_*} \\\\\n\u0026\\bold{w} = \\Phi^{\\dagger}\\bold{y_*}\n\\end{align*}\n$$\n\n이를 통해서, 위와 같은 식을 얻을 수 있다.\n\n---\n\n그럼 이 식이 왜 MLE랑 관련이 있는 것일까? 그것은 다음의 과정을 통해서 증명할 수 있다.\n\n우리는 각 data마다 존재하는 error(noise, $y_* - \\hat{y}$, $\\varepsilon$)가 그 양이 많아짐에 따라 정규 분포를 따른다는 것을 알 수 있다. (Central Limit Theorem)\n\n$$\n\\begin{align*}\n\\varepsilon \u0026= y_* - \\hat{y} = y_*-\\phi(\\bold{x}) \\\\\ny_* \u0026= \\phi(\\bold{x}) + \\varepsilon\n\\end{align*}\n$$\n\n이를 좌표 평면 상에서 나타내면 다음과 같다고 할 수 있다.\n\n![gaussian-error](/images/gaussian-error.jpeg)\n\n또한, $\\varepsilon$의 확률을 정의하면 다음과 같은 확률을 얻을 수 있다.\n\n$$\n\\begin{align*}\np(\\varepsilon) \u0026= {1\\over{\\sqrt{2\\pi}\\sigma}}\\exp{[-{\\varepsilon^2\\over{2\\sigma^2}}]} \\\\\np(\\varepsilon) \u0026= {1\\over{\\sqrt{2\\pi}\\sigma}}\\exp{[-{(y_*-\\phi(\\bold{x}))^2\\over{2\\sigma^2}}]}\n\\end{align*}\n$$\n\n여기서 우리는 $p(\\varepsilon)$을 $p(y_*|\\bold{x}; \\theta)$라고 볼 수 있다. ($\\theta = (\\bold{w}, \\phi, \\sigma)$)\n\n우리는 이를 이용해서 Likelihood를 구할 수 있다.\n\n$$\n\\begin{align*}\n\\mathcal{L} \u0026= \\log{p(\\bold{y}_*|\\bold{X}; \\theta)} = \\sum_{i=1}^{N}{\\log{p(y_{*(i)}|\\bold{x}_{(i)}; \\theta)}} \\\\\n\u0026= N\\log{{1\\over{\\sqrt{2\\pi}\\sigma}}} + \\sum_{i=1}^{N}{-{(y_*-\\phi(\\bold{x}))^2\\over{2\\sigma^2}}} \\\\\n\u0026= N\\log{{1\\over{\\sqrt{2\\pi}\\sigma}}} - {1\\over{\\sigma^2}}\\red{{1\\over{2}}\\sum_{i=1}^{N}{(y_*-\\phi(\\bold{x}))^2}}\n\\end{align*}\n$$\n\n우리가 변경할 수 있는 데이터는 $\\phi(\\bold{x})$ 밖에 없다. 따라서, 빨간색을 제외한 부분은 Likelihood의 최댓값을 구할 때, 고려하지 않아도 되는 상수로 볼 수 있다. 그렇다면, 우리는 Likelihood의 최댓값을 구하기 위해서 빨간색 표시된 부분을 최소화해야 한다는 것을 알 수 있다. 그리고, 이는 우리가 앞에서 살펴봤던, Least Squared Error와 같다.\n\n\u003cmark\u003e즉, $\\bold{w}_{LS}=\\bold{w}_{MLE}$ 라는 것이다.\u003c/mark\u003e\n\n## MAP of Linear Regression\n\n이번에는 Linear Regression에서 $\\bold{w}$를 찾아나가는 과정에서 MAP를 활용하는 과정을 알아볼 것이다.\n\n\u003e **overfitting**\n\n우리가 MLE를 통해서 Linear Regression을 찾는 것이 충분하다고 생각할 수 있다. 하지만, 우리는 어쩔 수 없이 **overfitting**이라는 문제에 직면하게 된다. \n\n![over-fitting-example](/images/over-fitting-example.jpg)\n\n**overfitting**이란 데이터를 통해서 구할 수 있는 분포가 학습에 사용된 데이터에 대해서는 에러가 거의 없는 형태로 예측하지만, 그 외에 데이터에 대해서는 에러가 크게 발생하는 경우를 의미한다. 위의 예시에서 처럼 데이터가 전체 Sample space보다 턱없이 적은 경우에 발생하기 쉽다.\n\n이러한 문제는 사실 basis function을 잘 선택하면 해결할 수 있다. 하지만, 우리가 어떻게 매번 적절한 basis function을 찾기 위해서 iteration을 반복하는 것이 올바를까? 그리고 이는 실제 적합한 값을 찾기 위한 수학적 식도 존재하지 않는다.\n\n\u003e **Regularization**\n\n따라서, 우리는 **regularization**을 수행한다. 위의 overfitting된 그래프를 보면 하나의 insight(번뜩이는 idea?)를 얻을 수 있다. 바로, 급격한 기울기의 변화는 overfitting과 유사한 의미로 볼 수 있다는 것이다. 즉, 그래프의 형태가 smooth 해야한다는 것이다.\n\n따라서, 우리는 하나의 error에 대해서 다음과 같이 재정의해서 smoothing(regularization)을 수행할 수 있다.\n\n$$\n\\varepsilon = {1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2 + {\\lambda\\over{2}}||\\bold{w}||^2\n$$\n\n$\\bold{w}$의 L2 norm을 error에 추가하여 $\\bold{w}$의 크기가 작아지는 방향으로 예측을 할 수 있도록 하는 것이다. (물론 L1 norm을 사용할 수도 있다. 이 또한, 후에 다룰 것이니 여기서는 넘어가겠다. 추가로 이렇게 L2 norm을 이용하면 **Ridge Regression**, L1 norm을 이용하면 **Lasso Regression**이라고 한다.)\n\n자 이제 위의 식을 미분해서 최소값이 되게 하는 $\\bold{w}$를 찾아보자. 과정은 연산이 그렇게 어렵지 않으므로 넘어가고 결과는 아래와 같다.\n\n$$\n\\bold{w}_{ridge} = (\\lambda I + \\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y}_*\n$$\n\n---\n\n그럼 이 역시 MAP를 통해서 해석해보도록 하자.\n\n위에서 살펴본 바와 같이 우리는 w값이 작을 확률이 높을 수록 좋은 성능을 가질 것이라는 Prior를 얻을 수 있다.\n\n즉,$p(\\bold{w})$가 zero-mean gaussian(표준정규분포)형태를 이루기를 바랄 것이다.\n\n$$\np(\\bold{w}) = \\mathcal{N}(\\bold{w}|0, \\Sigma)\n$$\n\n그리고, 이전에 MLE를 구할 때, Likelihood를 다음과 같이 정의했다.\n\n$$\n\\begin{align*}\np(\\bold{y}_*|\\bold{X}; \\theta) \u0026= \\mathcal{N}(\\bold{y}_*-\\Phi\\bold{w}, \\sigma I) \\\\\np(\\bold{y}_*|\\Phi, \\bold{w}) \u0026= \\mathcal{N}(\\bold{y}_*-\\Phi\\bold{w}, \\sigma I)\n\\end{align*}\n$$\n\n따라서, 우리는 이를 이용해서 posterior를 추론할 수 있다.\n\n$$\np(\\bold{w}|\\bold{y}_*, \\Phi) = {{p(\\bold{y}_*| \\Phi, \\bold{w})p(\\bold{w})}\\over{p(\\bold{y}_*|\\Phi)}}\n$$\n\n여기서 MAP를 구할 때에는 Lemma 정리(두 정규분포의 conditional Probability를 구하는 공식)를 이용하면 편하다. 따로 연산은 수행하지 않지만 결과 값은 아래와 같다.\n\n$$\n\\bold{w}_{MAP} = (\\sigma^2\\Sigma^{-1}+\\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y}\n$$\n\n여기서 만약 우리가 $\\Sigma = {\\sigma^2\\over{\\lambda}}I$라고 가정하면, 위의 MAP 식은 Ridge Regression과 동일해지는 것을 알 수 있다.\n\n즉, Ridge Regression은 MAP의 한 종류라고 볼 수 있는 것이다.\n\n$$\n\\bold{w}_{ridge} \\in {(\\bold{w}_{MAP}, \\Sigma)}\n$$\n\n## Gradient Descent\n\n여태까지 우리는 Loss를 정의하고, 이 Loss가 최솟값을 갖는 $\\bold{w}$를 찾는 것을 목표로 하였다. 하지만, 우리가 다루는 모든 Loss가 미분이 항상 쉬운 것은 아니다. 뿐만 아니라, Loss의 미분 값이 5차원 이상의 식으로 이루어진다면, 우리는 이를 풀 수 없을 수도 있다. 5차원 이상의 polynomial에서는 선형대수적인 해결법(근의 방정식)이 없다는 것이 증명되어있다.(Abel-Ruffini theorem)\n\n따라서, 우리는 Loss가 0이 되는 지점을 찾기 위해서, w의 값을 점진적으로 업데이트하는 방식을 활용한다. 이때, 우리는 w의 값이 계속해서 Loss를 감소시키기를 원한다. 따라서, 우리는 현재 $\\bold{w}$에서 Gradient를 현재 $\\bold{w}$에 빼준다. 이를 우리는 **Gradient Descent**라고 한다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\gamma((\\nabla L)(\\bold{w}_{t}))^{\\top}\n$$\n\n여기서 $\\gamma$는 step size(learning rate)라고 하며, 기울기값을 얼마나 반영할지를 의미한다.\n\n---\n\n이제부터는 Gradient Descent를 더 효과적으로 진행하기 위한 3가지의 기술들을 추가적으로 제시한다.\n\n\u003e **1. optimize stepsize**\n\nstepsize($\\gamma$)가 특정 상수로 제시된 게 아니라 변수로 표현된 이유는 linear regression마다 적절한 $\\gamma$가 다르기 때문이다. 하나의 예시를 들어보자.\n\n![loss-divergence](/images/loss-divergence.jpg)\n\n위는 Loss function이 convex할 때, 최솟값을 찾아나가는 과정이다. 만약, $\\gamma$가 크다면, Loss가 특정값으로 수렴하는 것이 아니라 발산하는 것을 알 수 있다. 이를 막기 위해 $\\gamma$를 굉장히 작은 수로 하는 경우에는 Loss의 최솟값을 찾기도 전에 특정 지점에서 멈춰버릴 수도 있다. 또한, Loss의 graph형태는 data마다 달라지기 때문에 절대적인 $\\gamma$역시 존재하지 않는다.\n\n따라서, 우리는 매 update마다 적절한 $\\gamma$를 찾을려고 노력한다. 여기서는 자세히 다루지 않지만 후에 더 다룰 기회가 있을 것이다. 간단히 프로그래밍적으로(systemical) 생각하면, 업데이트 이후 loss가 만약 그전 Loss보다 커진다면, 이를 취소하고 더 작은 $\\gamma$를 사용하도록 하고, 업데이트 된 후의 Loss와 그전 Loss가 같다면, 진짜 수렴하는지를 확인하기 위해서 $\\gamma$를 키워볼 수도 있다.\n\n\u003e **2. momentum**\n\n우리가 Gradient Descent를 진행하다보면, 다음과 같은 현상을 자주 마주하게 된다.\n\n![momentum-example-1](/images/momentum-example-1.jpg)\n\n우리가 찾고자 하는 Loss를 찾아가는 과정에서 매 업데이트마다 반대방향으로 기울기가 바뀌는 경우이다.(진동한다) 이는 최종으로 찾고자 하는 값을 찾는 과정이 더 오래 걸리게 한다. 따라서, 우리는 이러한 진동을 막기 위해서 Momentum을 사용한다. 즉, 이전 차시에서의 gradient를 저장해두고, 이를 더해서 진동하는 것을 막는 것이다.\n\n$$\n\\bold{w}_{i+1} = \\bold{w}_{i} - \\gamma_{i}((\\nabla L)(\\bold{w}_{i}))^{\\top} + \\alpha \\Delta \\bold{w}_i ,( \\alpha \\in [0, 1] )\n$$\n\n$$\n\\Delta \\bold{w}_i = \\bold{w}_{i} - \\bold{w}_{i-1} = \\alpha \\Delta \\bold{w}_{i-1} - \\gamma_{i-1}((\\nabla L)(\\bold{w}_{i-1}))^{\\top}\n$$\n\n즉, 그림으로 표현하면, 다음과 같다.\n\n![momentum-example-2](/images/momentum-example-2.jpg)\n\n이전 변화량과 현재 변화량을 합하여 이동하기 때문에 위에 새로 추가된 것처럼 진동하지 않고, 진행하는 것을 볼 수 있다.\n\n\u003e **3. Stochastic Gradient Descent**\n\n우리의 Gradient Descent의 가장 큰 문제는 바로 Global Minimum을 찾을 거라는 확신을 줄 수 없다는 것이다. 아래 그림을 보자.\n\n![gradient-descent-example](/images/gradient-descent-example.jpg)\n\n여기서 우리는 초기 w 값을 어떻게 정하냐에 따라서, **local minimum**을 얻게 되거나 **global minimum**을 얻게 된다. 즉, 초기값이 결과에 굉장히 큰 영향을 준다는 것이다.\n\n이를 해결할 수 있으며, 학습 효율도 높일 수 있는 것이 Stochastic Gradient Descent이다. 원리는 Loss를 구하기 위해서 전체 데이터(모집단)를 사용했었는데 그러지말고 일부 데이터를 랜덤하게 추출(sampling)해서(표본 집단) 이들을 통해서 Loss function을 구하기를 반복하자는 것이다.\n\n이 방식을 통해서 구한 Gradient의 평균이 결국은 전체 batch의 평균과 같다는 것은 Central Limit Theorem(중심 극한 정리)에 의해 증명이 된다. 따라서, 우리는 이를 통한 gradient descent도 특정 minimum을 향해 나아가고 있음을 알 수 있다.\n\n그렇지만, 표본 집단을 이용한 평균을 구했을 때에 우리는 noise에 의해서 local minimum으로만 수렴하는 현상을 막을 수 있다. 즉, gradient descent를 반복하다보면, 다른 local minimum으로 튀어나가기도 하며 global minimum을 발견할 확률을 높일 수 있는 것이다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- [Probabilistic interpretation of linear regression clearly explained](https://towardsdatascience.com/probabilistic-interpretation-of-linear-regression-clearly-explained-d3b9ba26823b), Lily Chen","slug":"ml-linear-regression","date":"2022-10-17 09:46","title":"[ML] 2. Linear Regression","category":"AI","tags":["ML","LinearRegression","BasisFunction","Regularization","GradientDescent","Momentum","StochasticGradientDescent"],"desc":"Regression(회귀)이라는 단어는 \"원래의 상태로 돌아간다\"로 돌아간다는 의미를 가진다. 결국 어떤 일련의 Event로 인해서 데이터에 Noise가 발생할 수 있어도 결국은 하나의 \"보편\"으로 시간이 지나면 수렴(회귀)할 것이라는 생각에 기반하는 것이다.  따라서, 우리는 이러한 \"보편\"을 찾기 위해서 우리가 알고 있는 독립 데이터 X를 통해서 알고자 하는 값 Y를 보편적으로 추론할 수 있다. 이 과정을 우리는 Regression이라고 부른다. 또한, X에 의해 독립적이지 않고 종속적인 Y의 관계가 Linear하게 표현될 때 이를 우리는 Linear Regression이라고 한다.  따라서, 해당 Posting에서는 Linear Regression을 바탕으로 Machine Learning이 어떻게 동작하는지를 이해하는 것이 목표이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nMachine Learning은 특정 목표를 달성하기 위해서 데이터로 부터 pattern 또는 가정 등을 유도해내는 방법이다.\n이를 위한 가장 일반적인 방법은 여러 개의 확률분포와 이것의 parameter의 조합(probabilistic model)들 중에서 측정된 데이터들을 가장 잘 나타내는 하나를 찾아내는 것이다.\n그 중에서, 확률 분포를 결정한 상태에서 parameter를 찾아나가는 형태의 접근법을 우리는 Parametric Estimation이라고 한다. 그 외에도 Nonparametric, Semi-parametric 방식도 존재하지만 이는 여기서는 다루지 않는다.\n\n## Small Example\n\n간단한 예시를 통해서 Parametric Estimation의 흐름을 익혀보자.\n\n한 학급에서 학생들의 형제자매 수에 대한 예측을 하고 싶다고 하자.  \n그렇다면, 우리는 먼저 조사(관측)를 수행해야 한다. 이를 통해서 다음과 같은 데이터를 얻게 되었다고 하자.\n\n| x        | 1    | 2    | 3    | 4    | 5    | 6    | x$\\geq$7 |\n| :------- | :--- | :--- | :--- | :--- | :--- | :--- | :------- |\n| $p(X=x)$ | 17   | 59   | 15   | 6    | 2    | 0    | 1        |\n\n여기서 우리는 여러 사전 지식을 활용하여 해당 데이터를 보았을 때, 해당 분포가 Poisson 분포의 형태라는 것을 알 수 있다.  \n따라서, 우리는 해당 분포를 Poisson이라고 가정한 다음에는 단순히 해당 분포에 대입하며, 가장 적절한 parameter만 찾으면 된다.  \n\n이 과정과 단순히 각 x에서의 확률값을 구하는 방식이랑 무엇이 다른지를 알아야지 해당 과정의 의의를 알 수 있다.\n먼저, 우리가 하고자 하는 일이 형제자매의 평균 수를 구한다고 하자. 이때의 평균 값과 Poisson 분포에서의 확률값은 다를 수 밖에 없다.\n\n이렇게 확률 분포를 구하는 것의 의미는 이것말고도 보지 않은 데이터(unseen data)를 처리함에 있다. 우리가 만약 모든 가능한 경우의 수를 모두 알고 있고, 이를 저장할 공간이 충분하다면,\n이러한 확률 분포를 구할 필요가 없다. 하지만, 우리가 원하는 추측은 unseen data에 대해서도 그럴사해야 한다. 이를 위해서는 결국 확률 분포가 필요하다.\n\n위의 예시에서 만약, 형제자매가 3명인 경우의 데이터가 없다고 하자. 이 경우에도 확률분포를 통한 추측을 한다면, 우리는 유의미한 값을 구할 수 있는 것이다.\n\n## Parametric Estimation\n\n\u003e **정의**\n\nsample space $\\Omega$에서 통계 실험의 관측 결과를 통해서 얻은 sample $X_1$, $X_2$, ... , $X_n$이 있다고 하자. 각 sample에 대한 확률 분포를 우리는 $p_\\theta$라고 한다.\n여기서 $\\theta$는 특정 확률 분포에서의 parameter를 의미한다. 만약, bernoulli 라면, 단일 시행에 대한 확률이 될 것이고, binomial이라면, 단일 시행의 확률과 횟수가 해당 값이 될 것이다.\n\n\u003e **성능 평가**\n\n여기서 우리가 찾기를 원하는 것은 전체 sample space $\\Omega$를 모두 잘 표현할 수 있는 $\\theta_{*}$(실제 true $\\theta$)를 찾는 것이다.(이미 확률 분포의 형태(함수, ex. Bernoulli, Binomial)는 이미 정의되어 있다.)  \n그렇다면, 실제 $\\theta_*$와 추측을 통해 만든 $\\hat{\\theta}$ 사이의 비교를 위한 지표도 필요할 것이다. 즉, 우리가 만든 확률 분포의 예측 성능평가가 필요하다는 것이다. 이를 측정하기 위해서 우리는 **Risk**라는 것을 사용한다.  \n간단하게도 실제 $\\theta_*$와 $\\hat{\\theta}$의 Mean Square Error를 계산한다.\n\n$$ \n\\begin{align*}\nRisk \u0026= E[(\\hat{\\theta} - \\theta_*)^2] = E[\\hat{\\theta}^2 - 2\\hat{\\theta}\\theta_* + \\theta_*^2] \\\\\n\u0026= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 \\\\\n\u0026= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 + (E^2[\\hat{\\theta}] - E^2[\\hat{\\theta}]) \\\\\n\u0026= (E[\\hat{\\theta}] - \\theta_*)^2 + E[\\hat{\\theta}^2] - E^2[\\hat{\\theta}] \\\\\n\u0026= {Bias}^2 + Var[\\hat{\\theta}]\n\\end{align*}\n$$\n\n해당 식을 분석해보면, 이와 같은 의미로 해석하는 것이 가능하다. 우리가 특정 확률 분포의 파라미터를 단 하나로 단정하고 Risk를 계산하는 경우는 Variance 값은 0이다. 즉, 해당 확률 분포가 가지는 Risk는 단순히 해당 parameter와 실제 parameter가 얼마나 찾이가 나는가를 의미한다.\n\n하지만, parameter를 특정하지 않고, 범위로 지정한다면, (예를 들어, 주사위를 던져 3이 나올 확률은 1/6 ~ 1/3이다.) 해당 확률의 평균과 Variance가 영향을 미칠 것이다.  \n다소 처음에는 헷갈릴 수 있지만, 해당 식에서 평균이 의미는 잘 확인하자. 특정 확률 분포를 가지도록 하는 $\\theta$가 $\\theta_*$ 에 얼마나 근접한지를 확인하기 위한 식이라는 것을 다시 한 번 기억하자.\n\n\u003e **Estimation**\n\n이제부터는 앞에서 살펴보았던, parameteric estimation에서 어떻게 $\\hat{\\theta}$를 구할 수 있는지를 다룰 것이다. 확률/통계 이론에서는 크게 3가지로 나눌 수 있다고 볼 수 있다. 각 각을 살펴보도록 하자.\n\n\u003cmark\u003e**1. MLE**\u003c/mark\u003e\n\nMaximum Likelihood Estimation의 약자이다. 여기서, Likelihood는 가능성이라는 뜻을 가지며, 확률/통계 이론에서 이는 확률을 해당 사건이 발생할 가능성으로 해석하는 것이다. 이를 이용해서 우리가 풀고자 하는 문제, 우리가 추측한 $\\theta$가 우리가 가진 Dataset를 만족시킬 가능성을 확인하기 위해 사용한다. 아래 수식을 보자.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta;\\mathcal{D}) \u0026= p(\\mathcal{D}|\\theta) = p(x_1, x_2, ..., x_n|\\theta) \\\\\n\u0026= \\prod_{i=1}^{n}{p(x_i|\\theta)}\n\\end{align*}\n$$\n\n(위 식을 이해하려면, 먼저 Dataset의 각 data들은 서로 independent하다는 사실을 기억하자.)  \n결국 $\\theta$가 주어졌을 때, Dataset일 확률을 구하는 것이다. 이를 다시 생각하면, $\\theta$가 얼마나 데이터셋의 확률을 잘 표현할 수 있는가와 같다.\n\n이것을 직관적으로 이해하려면 하나의 예시를 보면 좋다.\n\n![MLE example](/images/MLE-example.png)\n\n첫 번째 그래프는 같은 가우시안 분포 함수를 쓰면서, parameter만 다르게 한 경우이고, 아래는 실제 데이터의 분포라고 하자.(빨간색 선 하나 하나가 데이터를 의미)  \n이때, Likelihood를 각 각 구하면 각 x에서의 확률분포의 확률값을 모두 곱하면 된다. 그 경우 어떤 것이 제일 클지는 분명하다. 바로 파란색 분포일 것이다.  \n\n그렇다면, 우리가 원하는 것은 무엇인가? 바로 가장 높은 가능성을 가지게 하는 $\\theta$를 찾는 것이다. 따라서, 이를 식으로 표시하면 아래와 같다.\n\n$$\n\\hat{\\theta}_{MLE} = \\argmax_{\\theta}\\mathcal{L}(\\theta;\\mathcal{D})\n$$\n\n여기서 하나 문제가 있을 수 있다. 바로, 컴퓨터로 연산하게 되면 underflow가 발생하는 것이다. 특정 언어가 계산할 수 있는 소수점 범위를 벗어난다면, 제대로 된 결과를 얻을 수 없다. 이와 같은 문제를 **vanishing likelihood**라고 한다.  \n따라서, 우리는 log를 취했을 때와 log를 취하지 않았을 때의 경향성이 같음을 바탕으로 likelihood에 log를 취한 값을 이용하여 MLE를 구하는 것이 일반적이다. 이 방식을 maxmum log likelihood estimation 이라고 부른다.\n\n$$\n\\mathcal{l}(\\theta;\\mathcal{D}) = \\sum_{i=1}^{n}{\\log{(p(x_i|\\theta))}}\n$$\n\n이 방식을 이용하게 되면, 곱셈이 모두 덧셈으로 바뀌기 때문에 계산에서도 용이하다.\n\n여기까지 살펴보면, 하나의 의문이 들 수도 있다. 바로, $p(\\theta|\\mathcal{D})$도 측정 기준으로 사용할 수 있지 않냐는 것이다. 이 역시도 Dataset이 주어질 때, $\\theta$일 확률이라고 볼 수 있다.  \n어찌보면, 사람의 생각으로는 이게 더 당연하게 느껴질 수도 있다. 이는 바로 다음 MAP에서 다룰 것이다. 우선 MLE를 먼저한 이유는 이것이 더 구하기 쉽기 때문임을 기억해두자. \n\n```plaintext\n 🤔 증명\n\n (*해당 내용은 정보 이론에 기반한 MLE에 대한 추가적인 이해를 위한 내용입니다. 해당 내용은 자세히 알 필요까지는 없습니다.)\n\n 두 확률 분포 간 information Entropy의 차이를 나타내는 KL divergence의 최솟값을 구하는 것이 우리의 목표라고 정의할 수 있다.  \n 따라서, 우리가 결국 얻고자 하는 것은 확률 분포 함수가 주어졌을 때,  \n n이 무한대로 갈 때, 경험적 확률(empirical probability)에 가장 근사하는 parameter를 찾는 것이다.  \n 따라서, 우리는 KL divergence의 최솟값을 구하면 된다.\n```\n\n$$\n\\begin{align*}\n\\argmin_\\theta KL(\\tilde{p}||p_\\theta) \u0026= \\argmin_\\theta \\int\\tilde{p}(x)\\log{\\tilde{p}(x)\\over{p_\\theta(x)}}dx \\\\ \n\u0026=\\argmin_\\theta[-\\int\\tilde{p}(x)\\log{\\tilde{p}(x)dx} - \\int\\tilde{p}(x)\\log{p_\\theta(x)dx}] \\\\\n\u0026= \\argmax_\\theta\\int{\\tilde{p}(x)\\log{p_\\theta(x)}dx} \\\\\n\u0026= \\argmax_\\theta\\sum_{i=1}^{n}{\\log{p_\\theta(x_i)}} \\\\\n\u0026= \\theta_{MLE}\n\\end{align*} \n$$\n\n\u003cmark\u003e**2. MAP**\u003c/mark\u003e\n\nMaximum A Posteriori의 약자이다. Posteriori는 사후 확률이라고도 부르며, dataset이 주어졌을 때, $\\theta$일 확률을 구하는 것이다.  \n이를 바로 구하는 것은 다소 어렵다. 왜냐하면, Dataset이 조건으로 들어가는 형태이기 때문이다. ($p(\\theta|\\mathcal{D})$)  \n따라서, 우리는 Bayes' Theorem에 따라서 이전에 배운 Likelihood와 parameter의 확률, 그리고 Dataset의 확률을 활용히여 풀어낼 것이다.\n\n$$\np(\\theta|\\mathcal{D}) = {p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}}\n$$\n\n여기서 주의해서 볼 것은 바로 $p(\\theta|\\mathcal{D})$와 $p(\\theta)$의 관계이다. dataset이 주어질 때의 parameter의 확률을 구하기 위해서 원래 parameter의 확률이 필요하다는 것이다.  \n어찌보면 굉장히 모순되어 보일 수 있지만, 우리가 이것을 사전 확률(priori)로 본다면 다르게 볼 여지가 있다.  \n예를 들면, 우리가 수상한 주사위로 하는 게임에 참가한다고 하자. 이때, 우리는 수상한 주사위의 실제 확률은 알 수 없지만, 주사위 자체의 확률은 모두 1/6이라는 것을 알고 있다. 따라서, $p(\\theta={1\\over6}) = \\alpha, p(\\theta\\neq{1\\over6}) = \\beta$ 라고 할 수 있다. 만약 정말 수상해보인다면, 우리는 $\\alpha$가 점점 작아진다는 식으로 표현할 수 있고, 하나도 수상해보이지 않는 일반 주사위라면, $\\alpha=1, \\beta=0$으로 할 수도 있다. 이 경우에는 likelihood 값에 상관없이 다른 모든 값이 0이기 때문에 결국은 $p(\\theta|\\mathcal{D}) = p(\\theta)$ 가 되는 것을 알 수 있다.\n\n최종적으로, MAP도 결국은 Dataset을 얼마나 parameter가 잘 표현하는가에 대한 지표로 사용할 수 있다. \n따라서, 이를 최대로 만드는 parameter는 $\\theta_*$와 굉장히 근접할 것이다.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{MAP} \u0026= \\argmax_{\\theta}p(\\theta|\\mathcal{D}) \\\\\n\u0026= \\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}} \\\\\n\u0026=\\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)} \\\\\n\u0026=\\argmax_\\theta{[\\red{\\log{p(\\mathcal{D}|\\theta)}} + \\blue{\\log{p(\\theta)}}]}\n\\end{align*}\n$$\n\nMLE와 마찬가지로 이 또한 연산 및 **vanishing**을 막기 위해서 log를 취한다. 사실상 likelihood와 사전 확률의 합을 최대로 하는 $\\theta$를 찾는 것이다.\n\n\u003cmark\u003e**3. Bayesian Inference**\u003c/mark\u003e\n\n이제 마지막 방법으로 제시되는 Bayesian Inference이다. 이는 대게 Bayesian Estimation이라고 많이 불리는 것 같다. 이전까지 MLE, MAP는 결국 주어진 식을 최대로 하는 확정적 $\\theta$ 하나를 구하는 것을 목표로 했다.\n\nBayesian Inference는 Dataset이 주어졌을 때, $\\theta$의 평균값을 활용한다. 더 자세히 말하면, Posteriori(사후 확률)의 평균을 구하는 것이다.  \n이를 구하는 과정을 살펴보면 이해하는데 도움이 될 것이다. 한 번 살펴보자.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{BE}\u0026= E[\\theta|\\mathcal{D}] \\\\\n\u0026= {\\int_{0}^{1}{{\\theta}p(\\theta|\\mathcal{D})}d\\theta} \\\\\n\u0026= {\\int_{0}^{1}{\\theta}{{p(\\mathcal{D}|\\theta)p(\\theta)}\\over{p(\\mathcal{D})}}d\\theta} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)p(\\mathcal{D}|\\theta)}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\mathcal{D}|\\theta)p(\\theta)}d\\theta}} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\theta)\\prod_{i=1}^{n}p(x_i|\\theta)}d\\theta}} \\\\\n\\end{align*}\n$$\n\n이를 구하는 과정은 이전과는 다르게 상대값이 아닌 평균을 구해야하기 때문에 posteriori(사후 확률,$p(\\theta|\\mathcal{D})$)를 구해야 한다.\n\n하지만, 여기서 잡기술이 하나 존재한다. 바로 **Conjugate Prior**이다.\n\n바로 두 확률 분포 함수(likelihood, prior)에 의한 posterior의 형태가 정해진 경우가 있기 때문이다.\n\n| Prior $p(\\theta \\mid \\alpha)$  | Likelihood $p(\\mathcal{D} \\mid \\theta)$                 | Posterior $p(\\theta \\mid \\mathcal{D}, \\alpha)$                                                                                                                                                                                                                      | Expectation of Posterior                                                                                                                                                         |\n| :----------------------------- | :------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Beta ($\\alpha, \\beta$)         | Benoulli ($\\sum _{i=1}^{n}x_{i}$)                       | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +n-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                                               | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + n}}$                                                                                                                     |\n| Beta ($\\alpha, \\beta$)         | Binomial ($\\sum _{i=1}^{n}N_{i}, \\sum _{i=1}^{n}x_{i}$) | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                            | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + \\sum _{i=1}^{n}N_{i}}}$                                                                                                  |\n| Gaussian ($\\mu_0, \\sigma_0^2$) | Gaussian ($\\mu, \\sigma^2$)                              | Gaussian (${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu _{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum _{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right),\\left({\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}\\right)^{-1}}$) | ${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu _{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum _{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right)}$ |\n\n이를 이용하면, 우리는 간단하게 Posteriori의 평균을 구할 수 있다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n  ","slug":"ml-parametric-estimation","date":"2022-10-15 11:25","title":"[ML] 1. Parametric Estimation","category":"AI","tags":["ML","MLE","MAP","Bayesian"],"desc":"Machine Learning은 특정 목표를 달성하기 위해서 데이터로 부터 pattern 또는 가정 등을 유도해내는 방법이다.이를 위한 가장 일반적인 방법은 여러 개의 확률분포와 이것의 parameter의 조합(probabilistic model)들 중에서 측정된 데이터들을 가장 잘 나타내는 하나를 찾아내는 것이다.그 중에서, 확률 분포를 결정한 상태에서 parameter를 찾아나가는 형태의 접근법을 우리는 Parametric Estimation이라고 한다. 그 외에도 Nonparametric, Semi-parametric 방식도 존재하지만 이는 여기서는 다루지 않는다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nMachine Learning은 data들로 부터 특정 pattern을 나타내는 function을 만드는 것이라고 할 수 있다. 즉, pattern은 data에 대한 간단한 요약본이라고 볼 수 있다.\n확률/통계 이론 및 선형대수, 미적분, 정보 이론 관련 기본 내용을 해당 포스팅에 정리한다. 여기서 다루는 내용은 대게 많이 추상적인 내용이며, 키워드 중심의 내용이다. 만약, 추가적인 설명이 필요하다면 키워드를 기반으로 더 검색을 하는 것이 좋을 것이다.\n\n## Probability/Statisics\n\n확률과 통계는 대게 거의 동의어처럼 사용되지만, Statistics는 대게 과거를 의미할 때 사용하는 반면 Probability는 미래를 의미하는 용도로 많이 사용되어진다. \n\n### Probability Space\n\n확률 공간을 정의하는 것은 확률을 이해하는 토대가 된다. 확률을 적용하기 위한 공간을 먼저 살펴보자.\n\n- Sample Space($\\Omega$)  \n  가능한 모든 결과값의 집합이다.  \n  ex. 동전을 두 번을 던져서 나올 수 있는 모든 결과값은 $\\Omega = $ $\\{ hh, ht, th, tt \\}$\n- Event($E$)  \n  Sample Space의 Subset이다. Sample Space에서 발생할 수 있는 event라는 의미로 볼 수 있다.  \n  ex. 동전을 두 번을 던져서 모두 같은 면이 나오는 Event는 $E = $ $\\{ hh, tt \\}$\n- Field($\\mathcal{F}$)  \n  Sample Space에서 발생 가능한 모든 Event들의 집합이다.  \n  ex 동전을 두 번 던져서 나오는 결과값의 Field는 $\\mathcal{F} = $ $\\{$ $\\emptyset$, $\\{hh\\}$, $\\{ht\\}$, $\\{th\\}$, $\\{tt\\}$, $\\{hh, ht\\}$, $\\{hh, th\\}$, $\\{hh, tt\\}$, $\\{ht, th\\}$, $\\{ht, tt\\}$, $\\{th, tt\\}$, $\\{hh, ht, th\\}$, $\\{hh, ht, tt\\}$, $\\{hh, th, tt\\}$, $\\{ht, th, tt\\}$, $\\{hh, ht, th, tt\\}$ $\\}$\n- $\\sigma$-field  \n  자신 내부의 원소를 포함하는 합집합을 모두 포함하는 셀 수 있는 field를 sigma field라고 한다.  \n  이 $\\sigma$-field는 일반적인 확률과 특정 domain에서의 확률을 정의하는데 필요하다.  \n  우리가 sample space($\\Omega$)와 $\\sigma$-field $\\mathcal{F} \\subset 2^{\\Omega}$가 주어질 때, 확률P가 다음과 같이 mapping한다고 하자. $P: \\mathcal{F} \\mapsto [0, 1]$ 이때 P는 다음과 같은 특징을 가진다.\n  - $A \\in \\mathcal{F}$인 모든 A에 대해서 $P(A) \\leq 0$ 이다.  \n    $P(\\emptyset) = 0, P(\\Omega) = 1$\n  - $\\{A_i\\}_{i \\in I}$이고, 서로 다른 모든 i, j에 대해 $ A_{i}\\cup A_{j} = \\emptyset$이라면, 아래 식을 만족한다.  \n    $$P(\\cup_{i \\in I}A_i) = \\sum_{i \\in I}P(A_i)$$\n\n### Important properties of Probability\n\n- **Joint Probability**  \n  두 Event의 Joint Probability는 두 Event의 합집합의 확률을 의미한다.\n  $P(A, B) = P(A \\cap B)$\n- **Marginal Probability**  \n  대게 두 개 이상의 Event가 있을 때, 각 각의 Event의 확률을 특정할 때 사용한다.\n  $P(A), P(B)$\n- **Independence**  \n  두 Event가 독립이라는 의미는 서로의 Event가 서로 영향을 받지 않는다는 의미이다. \u003cmark\u003e**주의할 것은 이것이 의미하는 것이 두 Event의 교집합이 없다는 의미가 아니다.**\u003c/mark\u003e  \n  예를 들어보면 다음과 같다. 우리가 위에서 예시로 사용한 두 개의 동전을 던진 결과를 보자. 두 개의 동전이 모두 앞면이 나오는 경우와 모두 뒷면이 나오는 경우는 서로 독립일까? 이는 독립이 아니다. 왜냐하면, 동전이 모두 앞면이 나오는 사건은 필연적으로 모두 뒷면이 나오는 사건은 반드시 일어나지 않을 것이라는 증거가 되기 때문이다. 반대로, 모두 앞면이 나오는 사건과 한 번만 앞면이 나오는 사건을 생각해보자. 하나의 사건이 일어났다고, 반드시 그 사건이 일어났거나 안일어났다는 관계를 밝혀낼 수 없다. 따라서, 이러한 경우 두 사건이 독립적이라고 한다.  \n  이를 수학적으로 표현하면, 다음과 같이 표현할 수 있다.  \n  $P(A, B)=P(A)P(B)$  \n  즉 위 공식이 성립하면 독립이며, 독립이라면 위의 식이 성립한다.\n- **Conditional Probability**  \n  두 Event가 있을 때, 하나의 Event가 발생했을 때 다른 하나의 Event가 발생할 확률을 의미한다. 따라서, 이는 다음과 같이 수식으로 표현할 수 있다.  \n  $P(A|B) = {{P(A, B)}\\over{P(B)}}, (P(B) \\neq 0)$  \n  여기서 independence 특성을 더 명확하게 확인할 수 있는데, 만약 A와 B가 독립이라면, $P(A|B) = P(A)$이다.  \n  즉, B가 발생했는지 여부는 A의 결과에 영향을 안준다는 것이다.\n- **Partition**  \n  Sample Space($\\Omega$)를 겹치지 않고, 모두 포함하는 Event의 집합을 의미한다. 따라서, 이를 식으로 다음과 같이 표현할 수 있다.  \n  $\\cup_{i=1}^{n}{P_i} = \\Omega$ 이고, $\\cap_{i=1}^{n}{P_i} = \\emptyset$\n- **Marginalization**  \n  전체 Sample space($\\Omega$)에 대하여 **B**가 이에 대한 partition일 때, 아래 공식이 성립한다.  \n  $P(A) = \\sum_{i=1}^{n}{P(A,B_i)} = \\sum_{i=1}^{n}{P(A|B_i)P(B_i)}$\n- **Bayes' Theorem**  \n  만약 $P(B) \\neq 0$라면, 아래 공식이 성립한다. 간단히 conditional probability를 풀어주면 아래 식을 얻을 수 있다.  \n  $P(A|B) = {P(B|A)P(A)\\over{P(B)}}$  \n  해당 식은 단순히 Joint Probability로 변환하고, 다시 반대 확률로 변경했을 뿐이다. 이 공식이 중요하다기 보다는 이 공식이 가지는 의미를 이해하는 것이 중요하다. 확률을 사건의 발생의 빈도로 이해하는 Frequentist Approach에서는 관측을 통해서 특정 데이터가 발생할 확률을 얻는다. 만약 우리가 원하는 확률이 관측을 통해서는 얻을 수 없는 데이터라고 하자. 이 경우에 우리는 확률의 역연산이 필요하다. 위의 공식을 보면 특이한 것이 보이는데, 바로 $P(A|B)$와 $P(A)$이다. 이는 전체 확률을 통해서 **Conditional Probability**를 찾는 것이다. 그렇기에 우리는 이를 역연산이라고 부르며, 우리가 가지고 있는 기존 **사전 확률**(Priority, 이전까지 맞을 거라고 생각한 확률)을 통해서 데이터가 주어졌을 때의 사건의 확률을 다시 계산해보는 것이다. 이 과정을 **Bayesian Update**라고 하는데 이 과정을 통해서 얻은 $P(A|B)$를 다시 다음 데이터에 대해서는 $P(A)$로써 활용하는 것이다. 이렇게 해서 우리는 점진적으로 $P(A)$를 찾아나갈 수 있다.\n\n### Random Variable\n\nRandom Variable이라는 것은 특정 사건을 수학적으로 표현하기 위해서 변형하는 과정을 의미한다. 우리는 이전 예시에서 두 개의 동전을 동시에 던져서 나온 결과를 Sample Space로 두었고, 이를 $\\Omega = $ $\\{ hh, ht, th, tt \\}$라고 표현했다. 하지만, 이와 같은 표기 방식은 수학적인 연산을 적용하기 어렵다. 따라서, 우리는 앞면이 나온 경우를 $X=1$, 뒷면이 나온 경우를 $X=-1$ 라고 하는 형태로 치환하는 것이다. 여기서 만들어진 X를 우리는 Random Variable이라고 부른다. 이런 치환을 통해서 우리는 확률을 Random Variable에 대한 함수로 표현할 수 있다.\n\n또, Random Variable을 정의하여 다음과 같은 값을 연속적으로 정의할 수 있다.\n\n- **Mean**  \n  Random Variable의 평균 또는 기댓값이라고 부른다.  \n  $\\mu_{X} = E[X] = \\sum_{x}{xP(X=x)}$\n- **Variance**  \n  평균에서 데이터가 떨어진 정도를 표현하는 값으로 분산이라고 부른다.  \n  $\\sigma_{X}^{2} = E[(X-\\mu_{X})^2] = E[X^2] -\\mu_{X}^{2}$\n- **Covariance**  \n  Random Variable X와 Y의 상관관계(Correlation)을 확인하는 척도로 사용한다.  \n  $cov(X, Y) = E[(X-\\mu_{X})(Y-\\mu_{Y})] = E[XY] -\\mu_{X}\\mu_{Y}$  \n  만약, 두 X와 Y가 서로 전혀 상관이 없다(Independent)면, $cov(X, Y) = 0$이다. 그 반대는 성립하지 않지만, 그럴 가능성이 굉장히 높아진다.\n- **Correlation Coefficient**  \n  Covariance보다 더 엄격한 상관관계를 확인하는 척도로 사용되는데, 단순히 Covariance를 각 표준편차($\\sigma$)로 나눈 것이다. 이로 인해 결과 값은 [-1, 1] 사이 값이 된다.  \n  $corr(X, Y) = {cov(X,Y)\\over{\\sigma_{X}\\sigma_{Y}}}$  \n  따라서, \u003cmark\u003e1일 수록 두 Random Variable의 상관성이 높으며 비례하는 관계라는 것을 의미하며, -1일 경우에는 상관이 높지만 반비례하는 관계라는 것을 의미한다. 반대로, 0인 경우는 상관 관계가 아주 낮음으로 독립일 가능성이 높다.\u003c/mark\u003e 그렇다고 100%는 아니지만, 단지 그럴 확률이 굉장히 높다는 것이다. 주의할 점은 Correlation Coefficient가 1이라고 X가 Y의 원인이 되는 것은 아니라는 것을 유의해야 한다. 단순히 X가 일어났을 때, Y가 일어날 확률이 높다는 것이다.  \n\n### Law of Large Numbers\n\n경험적 확률과 수학적 확률 사이의 관계를 나타내는 법칙으로, 전체 경우의 수와 이에 따른 확률(모집단)이 있을 때, 관측한 경우의 수와 이에 따른 확률(표본 집단)은 관측 데이터의 크기가 커질 수록 표본 평균이 모평균에 가까워짐을 의미한다.\n\n### 자주 사용되는 Probability Distribution Function\n\n특정 task의 경우 이미 정의된 확률 분포를 통해서 표현할 수 있는 경우가 있다. 따라서, 아래와 같은 대표적인 확률 분포는 알아두는 것은 중요하다.\n\n- **Bernoulli distribution**  \n  하나의 사건이 일어날 확률을 의미한다. 발생하는 경우를 X=1, 그렇지 않은 경우를 X=0으로 random variable로 치환하여 나타낸 확률 분포(probability distribution)이다. 대표적인 사건은 동전 던지기와 같은 두 개의 결과만 갖는 binary event을 표현할 때이다.  \n  따라서, 사건이 일어날 확률을 p라고 할 때, 다음과 같이 Random Variable에 대한 확률을 정의할 수 있다.  \n  $P(X=x) = p^{x}(1-p)^{1-x}$ \n  복잡해보이지만, 실상은 X가 0 또는 1이므로, $P(X=0)=1-p$이고, $P(X=1)=p$이다.\n  - 평균      \n    $E[X] = p$\n  - 분산  \n    $Var[X] = E[X^2] - \\mu_{X}^2 = p - p^2 = p(1-p)$\n- **Binomial Distribution**  \n  확률이 p인 사건을 n번 수행했을 때, x번 발생할 확률을 의미한다. 따라서, Random Variable X의 범위는 {0, 1, …, n}이 된다. 대표적인 사건은 동전 던지기를 여러 번 던졌을 때, 앞 면이 x번 나올 경우의 수이다.  \n  이에 따라 Random Variable에 대한 확률을 정의하면 다음과 같다.  \n  $P(X=x) = {n \\choose x}p^x(1-p)^{n-x}$  \n  이 또한 복잡해 보이지만, 사실은 독립적인 Bernoulli의 연속 수행으로 볼 수 있다.  \n  - 평균  \n    $E[X] = np$\n  - 분산  \n    $Var[X] = Var[\\sum_{i}X_i]=\\sum_iVar[X_i]=np(1-p)$\n- **Beta Distribution**  \n  $\\alpha, \\beta \u003e 0$를 만족하는 두 parameter를 이용한 probability distribution이다.  \n  이는 [0, 1]에서 continuous한 random variable를 이용할 수 있다. 이에 따른 확률은 다음과 같다.  \n  $P(X=x) \\propto x^{\\alpha-1}(1-x)^{\\beta-1}$  \n  이에 대한 의미를 이해하자면, 확률에 대한 확률 분포이다. 각 $\\alpha - 1$와 $\\beta - 1$를 성공 횟수, 실패 횟수라고 하자.  이는 이미 알고 있는 모집단(전체 집합)의 계산 결과이다. 그리고 random variable을 특정 event의 확률이라고 하자. 예를들면, 동전 던지기를 할 때, 앞면이 나올 확률이 $1\\over2$이라는 것을 이미 알고 있다. 따라서, 우리는 $\\alpha - 1$ = $\\beta - 1$ 라는 것을 알고 있는 것이다. 하지만, 실제로 동전 던지기를 5번 수행했을 때, 4번 앞면이 나왔다고 하자. 그렇다면, 우리가 추측한 해당 event의 확률은 $4\\over5$이 된다. 그렇다면, 실제로 해당 확률이 $4\\over5$일 확률을 얼마나 될까?  \n  이를 측정하기 위한 것이 Beta distribution인 것이다. 이에 따라, Beta distribution을 PDF로 표현하면 ${\\alpha\\over\\alpha+\\beta}$에서 높은 확률값을 가지는 것을 볼 수 있다.\n  - 평균  \n    $E[X] = {\\alpha\\over{\\alpha+\\beta}}$\n  - 분산  \n    $Var[X] = {\\alpha\\beta\\over{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}}$\n\n  위의 평균과 분산을 보면 알 수 있듯이, 만약 이전 모집단에서의 평균값에 대한 믿음이 크다면, 각각  $\\alpha, \\beta$의 비율은 유지하면서 상수배를 수행하여 평균은 동일하지만 분산 값을 더 적게 만들어 뾰족한 형태의 분포를 완성할 수도 있다. 이 경우에는 평균과 맞지 않는 표본집합에서의 평균을 굉장히 확률이 낮은 확률로 식별하는 것이다.\n- **Gaussian Distribution**  \n  $\\mu, \\sigma^2$를 parameter로 갖는 probability distribution이다.\n    \n  이는 $[-\\infin, \\infin]$를 구간으로 continuous한 random varible을 이용한다. 이에 따른 확률은 다음과 같다. (단일 random variable인 경우)\n  \n  $P(X) = {1\\over{\\sqrt{2\\pi\\sigma^2}}}\\exp(-{1\\over{2\\sigma^2}}(X-\\mu)^2)$\n\n  이 분포는 굉장히 많은 곳에 사용되는데 \u003cmark\u003e우리가 생각할 수 있는 대게의 자연 발생에 의한 현상들(ex. 사람 키의 분포)이 이 분포를 따르기 때문이다.\u003c/mark\u003e 그렇기에 다양한 환경에서 많이 사용되는 분포이다. 뿐만 아니라 (Lindeberg-Levy) **Central Limit Theoriem**에 따르면, 표본에서 얻은 표본 평균을 구하면 구할 수록 점점 Gaussian Distribution을 따라간다. 즉, $n \\rarr \\infin$이면, 표본 평균이 이루는 분포가 Gaussian이라는 것이다.\n  \n  추가적으로 알아볼 것은 바로 여러 개의 Random Variable로 Gaussian Distribution을 더 높은 차원으로 구성할 수 있다는 것이다. 이를 수행하면, Gaussian 분포가 평균과 분산을 포함하기 때문에 두 데이터의 경향성(Covariance)를 어느정도 파악할 수 있다.\n\n## Calculus\n\n일명 미적분학으로, 대게의 미적분 법칙은 모두 알고 있을 것이라고 생각하고 넘어간다.\n\n### Optimization\n\n정말 모두가 알고 있을 거 같지만, 그럼에도 중요하기 때문에 정리하고 넘어가자. 일반적으로 Optimization이란 최적값을 찾는 과정이다. 이 과정에서 대게 사용되는 것이 최솟값 또는 최댓값이다. 우리는 최솟값/최댓값을 미분을 통해서 구할 수 있다.\n\n여기서는 Convex라는 성질에 대해서 자세히 다루지 않는다. 시간이 있다면, 이에대한 개념도 반드시 숙지하기를 바란다.\n\n\u003e **최대/최소 구하기**\n\n모두가 알다시피 함수의 미분은 기울기를 의미한다. 만약 함수의 미분에 특정 값을 대입할 경우 이는 그 지점에서의 기울기를 의미한다.\n\n먼저, 함수의 미분에 대입한 값이 0인 경우에 해당 값(극값)이 가지는 성질을 기억해야 한다. 만약, 값이 0으로 하는 값을 기준으로 좌우 부호가 바뀐다면, 이는 정말 극대, 극소라는 의미를 가진다. 즉, 해당 구간에서 최소와 최대라는 의미를 가지는 것이다.\n\n이를 이해하기 위해서는 함수의 기울기의 부호가 바뀌었다는 의미를 살펴보아야 한다. 이는 함수의 값이 구간 내에서 가장 작은 값(극소) 또는 구간 내에서 가장 큰 값(극대)라는 것을 의미한다. 왜냐하면, 직관적으로 기울기가 0이 되기 전까지는 계속해서 증가/감소해왔다는 것을 알기 때문이다. 따라서, 우리는 기울기가 0인 지점을 모두 찾아 비교하면, 그 안에서 최대/최소를 찾을 수 있을 것이다.\n\n그런데 어떻게 하면, 기울기가 0인 지점이 극대인지 극소인지를 구분할 수 있을까? 이것은 바로 직전의 값을 미분 함수에 대입해보면 쉽게 알 수 있다. 하지만, 이것이 매번 그렇게 쉽게 판별되는 것은 아니다. 따라서, 우리는 이중 미분을 사용한다. 이중 미분 함수에 극값을 대입했을 때 양수라면 이는 극소를 의미하고, 음수인 경우는 극대를 의미한다. 이 또한, 직관적으로 기울기의 변화량이라는 이중 미분의 정의를 알면, 직관적으로 와닿을 수 있다.\n\n이렇게 해서 극대와 극소를 골라내고, 이중에서 가장 큰 값과 가장 작은 값을 찾아내면, 우리는 이것을 함수의 최적화를 수행했다고 한다.\n\n### Constraint Optimization\n\n여기서는 특별한 case를 위한 예시이다. 특정 조건이 주어졌을 때, 이를 만족하면서 특정 함수의 optimization을 수행하는 것이다.\n\n그러면 우리가 최적화하고자 하는 목적함수($\\mathcal{J}(\\bold{x})$)와 등식 제약 조건($h_{j}(\\bold{x})$), 부등식 제약 조건($g_{i}(\\bold{x})$)을 살펴보자.\n\n우리는 모든 최적화 문제를 다음과 같은 형태로 묘사할 수 있다.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 \\mathcal{J}(\\bold{x}) \u0026\\\\\n  \\text{subject to} \\quad \u0026 g_{i}(\\bold{x}) \\leq 0, \u0026 i = 1, ..., M \\\\\n                          \u0026 h_{j}(\\bold{x}) = 0, \u0026 j = 1, ..., L\n\\end{align*}\n$$\n\nmaximization인 경우는 음수를 취해서 결과를 구한 후 변환 시에 다시 음수를 취해주면 된다. 그리고 부등호가 반대인 제약 조건인 경우에도 양변에 음수를 취해서 간단하게 뒤집는 것이 가능하다.\n\n이러한 문제를 풀기 위해서는 우리는 식을 **Lagrangian** 형태로 변환해야 한다.\n\n\u003e **Lagrangian**\n\n이는 조건부식에 있는 조건에 변수($\\nu$, $\\lambda$)를 곱한 값의 합과 원래 목적 함수($\\mathcal{J}(\\bold{x})$)의 합이다.\n\n$$\n\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) = \\mathcal{J}(\\bold{x}) + \\sum_{i=1}^{M}{\\nu_{i}g_{i}(\\bold{x})} + \\sum_{j=1}^{K}{\\lambda_{j}h_{j}(\\bold{x})}\n$$\n\n여기서 **Lagrangian** 함수의 optimization이 곧 목적함수의 optimization이다. 증명은 수행하지 않는다. 이에 대한 추가적인 설명이 필요한 경우에는 직접 찾아보아야할 것이다.  \n여기서 만약 등식만 있는 식인 경우에 우리는 간단히 모든 변수에 대해서 편미분이 0이 되는 등식을 이용해서, 최적 $\\bold{x}$를 찾을 수 있다. 위에 식에서 부등식 조건($g_{i}(\\bold{x})$)이 사라진다면, 우리는 미분을 통해서 처리해야하는 값은 총 x의 크기(N)와 L이다. 이는 우리가 편미분해서 구할 수 있는 식의 갯수와 똑같다. 즉, 우리가 모르는 변수는 N+M개 우리가 가진 등식은 N+M개이므로 연립해서 각 값을 구할 수 있는 것이다.\n\n하지만, 부등식인 경우에는 추가적으로 고려해줘야할 것이 있다.\n\n\u003e **KKT Condition**\n\n이는 우리가 최적값($\\bold{x}_{*}$)를 찾았을 때, 다음과 같은 $\\boldsymbol{\\nu_{*}}$, $\\boldsymbol{\\lambda_{*}}$가 존재해야 한다는 정리이다.\n\n1. **Optimality**  \n   $\\nabla\\mathcal{L} = \\nabla\\mathcal{J}(\\bold{x}_{*}) + \\sum_{i=1}^{M}{\\nu_{*(i)}\\nabla g_{i}(\\bold{x}_{*})} + \\sum_{j=1}^{L}{\\lambda_{*(j)}\\nabla h_{j}(\\bold{x}_{*})} = 0$  \n   위에서 보았던 최적화를 수행하는 식이다.\n2. **Feasibility**  \n   $g_{i}(\\bold{x}_{*}) \\leq 0,  i = 1, ..., M$  \n   $h_{j}(\\bold{x}_{*}) = 0,  j = 1, ..., L$  \n   조건이 만족하는지를 확인하는 것이다.\n3. **Complementary slackness**  \n   $\\nu_{*(i)}g_{i}(\\bold{x}_{*}) = 0, i = 1, ..., M\\quad(\\nu_{*(i)} \\geq 0)$  \n   위의 식은 다소 헷갈릴 수 있는데 가장 알아듣기 쉬운 형태는 아래이다.  \n   $g_{i}(\\bold{x}_{*}) \\lt 0\\text{, then } \\nu_{*(i)} = 0$  \n   $g_{i}(\\bold{x}_{*}) = 0\\text{, then } \\nu_{*(i)} \u003e 0$\n\n위의 식을 만족하는 $\\bold{x}_{*}$, $\\boldsymbol{\\nu_{*}}$, $\\boldsymbol{\\lambda_{*}}$를 찾으면, 그것이 최적값에서의 $\\bold{x}_{*}$이다.\n\n\u003e **Lagrangian Dual Problem**\n\n여기서 한 발 더 나아가면, Lagrangian에 다시 한번 Lagrangian을 취할 수 있다.\n\n우리가 만약 Lagrangian의 하한을 $\\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda})$이라 하고, \n\n$$\n\\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) = \\inf_{\\bold{x} \\in \\mathcal{X}}\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda})\n$$\n\n$\\boldsymbol{f}^{*}$을 최적값이라고 한다면, 아래 식이 성립한다.\n\n$$\n\\boldsymbol{f}^{*} \\geq \\min_{\\bold{x}}\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) := \\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda})\n$$\n\n따라서, 우리는 $\\mathcal{G}$의 최댓값을 찾으면 해당 값이 최적해에 근사한다는 것을 알 수 있다.\n\n이는 우리가 풀고자 하는 문제의 형식을 다시 한 번 바꾸게 된다.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad \u0026 \\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) \u0026\\\\\n  \\text{subject to} \\quad \u0026 \\boldsymbol{\\nu}_{i} \\geq 0, \u0026 i = 1, ..., M\n\\end{align*}\n$$\n\n이 식을 KKT condition을 이용하여 푸는 것이 기존 식보다 쉽게 풀 수 있는 경우가 많다. 따라서, 이러한 형태로 문제를 풀이할 수도 있다.\n\n## Information Theory\n\n### Entropy\n\n수학에서 정보의 불확실성(uncertainty)을 표현하기 위해서 물리의 entropy 라는 개념을 도입한 것이다. 즉 정보가 가지는 \"surprise\" 정도가 크다면, entropy가 큰 정보를 의미하고, 일반적인 정보라면 이는 entropy가 작은 정보인 것이다.\n\n수학적으로 다시 정의하자면, 다음과 같다.  \nsample space $\\Omega$에서 정의된 random variable $X$와 확률 $p_{X}(x)$이 주어질 때, Entropy를 $H(x)$라 하자.\n\n$$\nH(X) = -\\sum_{x \\in \\Omega}p(x)\\log_{2}p(x)\n$$\n\n위 식에서 log의 밑이 2인 것을 알 수 있는데 computer science에서는 정보가 bit단위로 저장되기 때문에 기본적으로는 2를 사용한다. 하지만, 상황에 따라서는 다른 밑을 사용할 수도 있다.\n\n헷갈릴 수 있는데 표기법이 굉장히 다양하니 유의해서 보도록 하자.\n\n$$\nH(X) = H_{p}(X) = H(p) = H_{X}(p) = H(p_{X})\n$$\n\n\u003e **최댓값과 최솟값**\n\nEntropy는 정보의 불확실성을 나타낸다고 했다. 즉, 정보가 확실할 수록 Entrophy는 0으로 수렴하며, 확실히 아는 정보의 경우 Entropy가 최솟값인 0이 된다.  \n반대로 Entropy의 최댓값의 경우 $|\\Omega| = n$이라고 할 때, $\\log_{2}{n}$이다. 이는 uniform distribution(모든 Random Variable의 확률이 같은 분포)일 경우이다.\n\n$$\n0 \\leq H(x) \\leq log_{2}{|\\Omega|}\n$$\n\n\u003e **$\\bold{\\log_{2}({1 \\over p_{X}(x)})}$의 평균**\n\nEntropy를 random variable x의 확률의 역수의 log를 취한 값으로 해석할 수도 있다.\n\n$$\n\\begin{align*}\nE[\\log_{2}(({1 \\over p_{X}(x)})] \u0026= \\sum_{x \\in X}p_{X}(x)\\log_{2}({1 \\over p_{X}(x)}) \\\\\n\u0026= -\\sum_{x \\in X}p_{X}(x)\\log_{2}(p_{X}(x)) \\\\\n\u0026= H(p_{X})\n\\end{align*}\n$$\n\n여기서 우리는 $\\log_{2}({1 \\over p_{X}(x)})$을 **정보량**이라고 정의한다. 식에서도 알 수 있지만, 정보량과 해당 정보량을 얻을 확률은 반비례한다.\n\n\u003e **Joint, Conditional Entropy**\n\nRandom Variable이 두 개 이상일 때, 이를 적용할 수 있다. 유도 과정은 $H(X)$가 Expectation과 어떤 관계였는지를 떠올려 보면 알 수 있다.\n\n- **Joint Entropy** : $H(X, Y) = -\\sum_{x \\in \\Omega_{X}}\\sum_{y \\in \\Omega_{Y}}p(x, y)\\log_{2}p(x, y)$\n- **Conditional Entropy** : $H(Y|X) = -\\sum_{x \\in \\Omega_{X}}\\sum_{y \\in \\Omega_{Y}}p(x, y)\\log_{2}p(y|x)$\n\n\u003e **properties**\n\n1. **Chain Rule**  \n   $H(X, Y) = H(Y|X) + H(X)$  \n   $H(X, Y) = H(X|Y) + H(Y)$\n2. **Conditional Entropy's Maximum**  \n   $H(Y|X) \\leq H(Y)$  \n   독립일 때 같아지며 그 외에는 항상 Conditional의 Entropy가 더 낮다. 의미를 이해하면 쉽다. 한 마디로 X라는 정보가 Y라는 정보가 발생할 확률에 대한 티끌이라도의 힌트를 준다면, 해당 불확실성은 감소하게 되는 것이다.\n3. **Joint Entropy's Maximum**  \n   $H(X, Y) \\leq H(X) + H(Y)$  \n   동일하게 독립일 때 같아지며, 그 외에는 항상 Joint의 Entropy가 더 낮다. 이 또한, 두 사건이 티끌이라도 겹치는 Event가 있다면, 각 Entropy를 더하는 것보다 당연히 작을 수 밖에 없는 것이다.\n4. **Concave**  \n   Entropy의 그래프는 항상 concave하다. \n\n\u003e **Coding**\n\n정보 이론이 활발하게 사용되는 예시 중에 하나가 바로 데이터의 Encoding/Decoding을 수행하여 bit data로 mapping할 때이다. variable length encoding을 알고 있다면, 이에 대한 이해가 쉬울 것이다. 쉽게 설명하면, 데이터를 bit sequence로 mapping할 때 모든 데이터에게 동일한 bit sequence의 길이만큼을 할당하는 게 아니라 빈도가 높은 데이터부터 짧은 bit sequence 길이를 할당하는 방식이다. 이때 bit sequence의 길이를 Entropy를 이용해서 구할 수 있다. 이 길이는 항상 해당 데이터의 Entropy보다는 커야 한다. 따라서, 해당 Entropy보다 큰 가장 작은 자연수가 해당 데이터의 Bit Sequence 길이의 최적값이다.\n\n### KL divergence(Relative Entropy)\n\nKullback-Leibler Divergence의 약자로, 우리가 구하고자하는 실제 확률(p)과 추측 확률(q) 사이의 오차를 계산할 때 사용하는 지표이다. 따라서, 동일한 Sample Space와 Random Variable에 대한 서로 다른 확률 분포를 비교한다고 생각할 수 있다.\n\n$$\nD(p||q) = KL(p||q) = \\sum_{x \\in \\Omega}p(x)\\log_{2}(p(x)/q(x)) = E_{p}[\\log_{2}({p(x) \\over q(x)})]\n$$\n\n아쉽게도 KL divergence는 거리와 같은 연산을 적용할 수 없다. 즉, 둘 사이의 역연산은 같지 않을 뿐만 아니라($D(p||q) \\neq D(q||p)$), 서로 다른 Random Variable의 KL divergence의 합이 Random Variable의 합의 KL divergence와는 다르다. \n\n### Mutual Information\n\nKL divergence를 활용하여 서로 다른 Random Variable X,Y의 연관성을 유추하는 것도 가능하다.\n\n$$\nI(X, Y) = D(p(x,y) || p(x)p(y))\n$$\n\n$I$값이 의미하는 것은 Y를 아는 것이 X의 추측을 얼마나 쉽게하는지에 대한 지표로 작용한다.\n\n증명 과정은 생략하지만, 위의 식을 정리하면 결국은 아래와 같아지기 때문이다.\n\n$$\n\\begin{align*}\n  I(X, Y) \u0026= H(X) - H(X|Y) \\\\\n  \u0026= H(Y) - H(Y|X)\n\\end{align*}\n$$\n\n### Cross Entropy\n\n우리가 특정 corpus(dataset)를 통해서 확률을 추정했다고 해보자. 그렇다면, 우리는 이 가설을 증명하기 위해서 다른 data에 대해서 해당 추측이 적절한지를 확인하여 정당성을 증명할 수 있다. 그러기 위해서 우리가 만든 확률에서 정보량을 추출하고, 이를 우리가 알고 있는 data의 공간에 넣었을 때의 확률을 추정하기 위해서 Cross Entropy를 사용할 수 있다.\n\n$$\nH_{q}(p) = \\sum_{x \\in \\Omega}q(x)\\log_{2}{1 \\over p(x)}\n$$\n\n간단하게 요약하면, 위 식은 틀릴 수 있는 정보를 갖고 구한 최적의 Entropy로, 정보량에 추측 정보량을 넣고, 확률에는 실제 확률을 넣는 방식이다.\n\n또한, 이는 다음과 같이 변형되기도 한다.\n\n$$\nH_{q}(p) = H(q) + D(q || p)\n$$\n\n또한, 특정 조건이 주어졌을 때의 Conditional Cross Entropy는 다음과 같다.\n\n$$\nH_{q}(p) = - \\sum_{y \\in \\Omega_{Y}}\\sum_{x \\in \\Omega_{X}}q(y,x)\\log_{2}p(y|x)\n$$\n\n하지만, 실제 구현 level에서는 이러한 Cross Entropy를 정석으로 구하는 것은 비용이 크다. 따라서, 이와 근사하는 식을 사용한다.\n\n$$\n\\begin{align*}\n  H_{q}(p) \u0026= - \\sum_{y \\in \\Omega_{Y}}\\sum_{x \\in \\Omega_{X}}q(y,x)\\log_{2}p(y|x) \\\\\n  \u0026= {1\\over{|T_{Y}|}}\\sum_{i=1..|T_{Y}|}{\\log_{2}p(y_{i}|x_{i})}\n\\end{align*}\n$$\n\n위 식을 이용할 때에는 실제 데이터와 비교에 사용해서는 안된다. 대신 두 개의 서로 다른 p,q가 있을 때, s라는 실제 분포에 어떤 것이 더 적절한지를 판명할 때 사용할 수 있다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"ml-base-knowledge","date":"2022-10-14 19:28","title":"[ML] 0. Base Knowledge","category":"AI","tags":["ML","Probability","Calculus","InformationTheory"],"desc":"Machine Learning은 data들로 부터 특정 pattern을 나타내는 function을 만드는 것이라고 할 수 있다. 즉, pattern은 data에 대한 간단한 요약본이라고 볼 수 있다.확률/통계 이론 및 선형대수, 미적분, 정보 이론 관련 기본 내용을 해당 포스팅에 정리한다. 여기서 다루는 내용은 대게 많이 추상적인 내용이며, 키워드 중심의 내용이다. 만약, 추가적인 설명이 필요하다면 키워드를 기반으로 더 검색을 하는 것이 좋을 것이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"AI"}},"__N_SSG":true},"page":"/categories/[subject]","query":{"subject":"AI"},"buildId":"IY7V1zkqy9YrwU6Mfg7tn","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>