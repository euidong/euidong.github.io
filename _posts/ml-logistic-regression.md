---
slug: "ml-logistic-regression"
title: "[ML] 3. Logistic Regression"
date: "2022-10-18 09:58"
category: "AI"
tags: ["ML", "LogisticRegression", "Classification", "SigmoidFunction", "SoftmaxFunction", "NewtonMethod"]
thumbnailSrc: "/images/ml-thumbnail.jpg"
---

## Intro

ì´ì „ê¹Œì§€ ìš°ë¦¬ëŠ” input dataê°€ ë“¤ì–´ì™”ì„ ë•Œ, continuosí•œ outputì„ ì–»ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ” ëŒ€ê²Œ ì •í™•í•œ ìˆ˜ì¹˜ë³´ë‹¤ëŠ” íŠ¹ì • ë¶„ë¥˜ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ íš¨ê³¼ì ì¸ ê²½ìš°ê°€ ë§ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, spam í•„í„°ë§, object detection, ë“± ë“±. ë”°ë¼ì„œ, í•´ë‹¹ í¬ìŠ¤íŒ…ì—ì„œëŠ” classificationì„ ìœ„í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” logistic regressionì— ëŒ€í•´ì„œ ì‚´í´ë³¼ ê²ƒì´ë‹¤.

## Classification

**Classification**ì´ë€ ê²°êµ­ íŠ¹ì • inputì´ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ í•˜ë‚˜ì˜ Classë¼ëŠ” outputì„ ë‚´ë³´ë‚´ëŠ” ê²ƒì´ë‹¤. ì¦‰, outputì€ ì—°ì†ì ì´ì§€ ì•Šê³ , descretí•˜ë‹¤. ëŒ€ê²Œ Classificationì—ì„œëŠ” Classì˜ ê°¯ìˆ˜ë¥¼ Kë¼ê³  í‘œê¸°í•˜ê³ , $C_k$ëŠ” k ë²ˆì§¸ Classë¼ëŠ” ì˜ë¯¸ë¡œ ì‚¬ìš©ë˜ì–´ì§„ë‹¤.

ê·¸ë ‡ë‹¤ë©´, ì–´ë–»ê²Œ Classë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ê²ƒì¼ê¹Œ? ë§¤ìš° ë‹¨ìˆœí•˜ê²Œë„ ì´ëŠ” **Decision Boundary**ë¼ëŠ” ì„ ì„ ê·¸ì–´ì„œ í•´ê²° í•  ìˆ˜ ìˆë‹¤.

![decision-boundary](/images/decision-boundary.jpg)

ìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼ ìš°ë¦¬ëŠ” ì„ ì„ í•˜ë‚˜ ê·¸ì–´ì„œ $\red{\text{x}}$ì™€ $\blue{\text{o}}$ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” Class 1ì— í•´ë‹¹í•  ê²ƒì´ë¼ê³  ì˜ˆì¸¡í•˜ëŠ” êµ¬ê°„ $R_1$ì´ ë§Œë“¤ì–´ì§€ê³ , Class 2ë¼ê³  ì˜ˆì¸¡í•˜ëŠ” êµ¬ê°„ $R_2$ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.

ì¦‰, classificationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ í•´ì•¼í•  ì¼ì€ ê¸°ì¡´ì˜ Regression ê³¼ì •ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.

ê²°êµ­ ì°¾ê³ ì í•˜ëŠ” ê²ƒì´ ì„ ì´ë¼ë©´, ì´ê²ƒì„ Linear Regressionìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ Linear Regressionì„ ë°”ê¿”ì„œ ìƒê°í•  ìˆ˜ ìˆë‹¤.

- ì˜ˆì¸¡ê°’($\hat{y}$, $h(\bold{x})$)  
  $h(\bold{x}) = \text{sign}(\bold{w}^{\top}\bold{x}) = \begin{cases} +1 & \bold{w}^{\top}\bold{x} \geq 0 \\ -1 & \text{otherwise}\end{cases}$
- Least Squared Error(LS, MLE)  
  ì‹¤ì œë¡œ parameterë¥¼ êµ¬í•  ë•Œì—ëŠ” signì„ ì·¨í•˜ì§€ ì•ŠëŠ”ë°, signì„ ì·¨í•˜ê²Œ ë˜ë©´ ëª¨ë‘ LSëŠ” ê²°êµ­ ì˜¤ë‹µì˜ ê°¯ìˆ˜ ì •ë„ë¡œ ì·¨ê¸‰ëœë‹¤. ì¦‰, ì–¼ë§ˆë‚˜ ì˜ˆì¸¡ì´ ì˜ëª»ë˜ì—ˆëŠ”ì§€ë¥¼ ë°˜ì˜í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì´ëŠ” ê¸°ì¡´ Linear Regressionì˜ LSë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ ìˆ˜í–‰í•œë‹¤.  
  $\argmin_{w} {1\over2}\sum_{n=1}^{N}{(y_n - (\bold{w}^{\top}\bold{x}))^2}$

ì´ë ‡ê²Œ Linear Regressionì„ ì ìš©í•˜ë©´ ë¬¸ì œê°€ ì—†ì„ ê±° ê°™ë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë¬¸ì œê°€ ìˆë‹¤. ë°”ë¡œ, ë°ì´í„°ê°€ ë¶ˆê· í˜•í•  ë•Œì´ë‹¤. ë§Œì•½ ë°ì´í„°ê°€ decision boundaryë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ì¹­(symmetric)ì¸ í˜•íƒœë¡œ ì¡´ì¬í•œë‹¤ë©´, ë¬¸ì œê°€ ì—†ë‹¤. í•˜ì§€ë§Œ, ë¹„ëŒ€ì¹­(asymmetric)ì¸ ê²½ìš° ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤. ì™œëƒí•˜ë©´, linear regressionì€ ìµœì ì—ì„œ ë°ì´í„°ì˜ í‰ê· ì„ ë°˜ì˜í•˜ëŠ”ë° ë¶ˆê· í˜•í•œ ê²½ìš° ë°ì´í„°ì˜ í‰ê· ì´ Decision Boundaryê°€ ë˜ëŠ” ê²ƒì€ ë¬¸ì œê°€ ìˆë‹¤.

![linear-in-classification](/images/linear-in-classification.jpg)

## Logistic Regression

ìœ„ì—ì„œ ì œì‹œí•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ Classificationì—ì„œëŠ” Linear Regressionì´ ì•„ë‹Œ Logistic Regressionì„ í™œìš©í•œë‹¤. ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œ ê¸°ë°˜ì´ ë  ìš”ì†Œë“¤ì„ ë¨¼ì € ì‚´í´ë³´ì.

> **Discriminant Function**

íŒë³„í•¨ìˆ˜(Discriminant Function, Score Function) ë“±ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” í•´ë‹¹ í•¨ìˆ˜ëŠ” íŠ¹ì • dataê°€ íŠ¹ì • classì— ì†í•  ê°€ëŠ¥ì„±(likelihood, probability, score)ì„ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ì´ë‹¤. ì¦‰, inputìœ¼ë¡œ dataë¥¼ ë°›ê³ , outputìœ¼ë¡œ classì— ì†í•  í™•ë¥ ì„ ë‚´ë³´ë‚¸ë‹¤.

ì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ í•  ìˆ˜ ìˆë‹¤.

ë§Œì•½, $f_k(\bold{x}) \gt f_j(\bold{x})$ì´ë¼ë©´, $\bold{x}$ì˜ classëŠ” $C_k$ì´ë‹¤.

ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ Classê°€ ìˆëŠ” ê³µê°„ì—ì„œ dataë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.

$$
h(\bold{x}) = \argmax_{k}f_{k}(\bold{x})
$$

ê·¸ë ‡ë‹¤ë©´, Discriminant Functionìœ¼ë¡œ ì–´ë–¤ ê°’ì„ ì“°ë©´ ì¢‹ì„ê¹Œ? ì´ì— ëŒ€í•œ í•´ê²°ì±…ì„ Bayes Decision Ruleì—ì„œ ì œì‹œí•œë‹¤.

> **Bayes Decision Rule**

ë§Œì•½ ìš°ë¦¬ê°€ íŠ¹ì • dataê°€ íŠ¹ì • Classì— ì†í•  í™•ë¥ ì„ êµ¬í•œë‹¤ê³  í•˜ì. ìš°ë¦¬ëŠ” ë¨¼ì € Likelihoodë¥¼ ìƒê°í•  ìˆ˜ ìˆë‹¤. $P(x|C = k), P(x|C = j)$ë¥¼ êµ¬í•˜ì—¬ ê° Classì— ì†í•  í™•ë¥ ì„ ë¹„êµí•  ìˆ˜ ìˆì„ê¹Œ?  
ë¬¼ë¡  ë¹„êµëŠ” ê°€ëŠ¥í•˜ë‹¤ í•˜ì§€ë§Œ, ë°˜ìª½ì§œë¦¬ ë¹„êµë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë§Œì•½, class kì— ì†í•˜ëŠ” ë°ì´í„°ë³´ë‹¤ class jì— ì†í•˜ëŠ” ë°ì´í„°ê°€ í›¨ì”¬ ë§ë‹¤ê³  í•˜ì. ê·¸ëŸ¬ë©´, ì¼ë°˜ì ìœ¼ë¡œ class jê°€ ë°œìƒí•  í™•ë¥  ìì²´ê°€ ë†’ë‹¤. í•˜ì§€ë§Œ, likelihoodëŠ” ì´ëŸ¬í•œ ê²½í–¥ì„ ë°˜ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤. ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ì.

```plaintext
 ğŸ¤” ì–´ë–¤ ë™ë¬¼ì˜ í„¸ì— ì¡´ì¬í•˜ëŠ” ìƒ‰ì˜ ê°¯ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê³ ì–‘ì´ì¼ í™•ë¥ ê³¼ í˜¸ë‘ì´ì¼ í™•ë¥ ì´ë¼ê³  í•˜ì.

  ê·¸ë¦¬ê³ , input dataëŠ” í„¸ì— ì¡´ì¬í•˜ëŠ” ìƒ‰ì˜ ìˆ˜ë¼ê³  í•˜ì. (í˜¸ë‘ì´ëŠ” ëŒ€ê²Œ 3ê°€ì§€ ìƒ‰, ë°±í˜¸ = 2ê°€ì§€ ìƒ‰, ê³ ì–‘ì´ëŠ” ë§¤ìš° ë‹¤ì–‘)
  ê·¸ë ‡ë‹¤ë©´, P(í„¸ì˜ ìƒ‰ = 3|C = í˜¸ë‘ì´), P(í„¸ì˜ ìƒ‰ = 3|C = ê³ ì–‘ì´)ë¥¼ ë¹„êµí–ˆì„ ë•Œ, ìš°ë¦¬ëŠ” ë‹¹ì—°íˆ ì „ìê°€ í¬ë‹¤ê³  ìƒê°í•  ê²ƒì´ë‹¤.
  í•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ê³ ë ¤í•˜ì§€ ì•Šì€ ê²ƒì´ ìˆë‹¤. ë°”ë¡œ ì „ì²´ ê³ ì–‘ì´ì™€ í˜¸ë‘ì´ì˜ ë¹„ìœ¨ì´ë‹¤. 
  ìƒëŒ€ì ìœ¼ë¡œ ê³ ì–‘ì´ê°€ í˜¸ë‘ì´ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ë‹¤ëŠ” ê²ƒì„ ê³ ë ¤í–ˆì„ ë•Œ, ê³ ì–‘ì´ì˜ í™•ë¥ ì´ ë” ë†’ì„ ìˆ˜ë„ ìˆë‹¤. 

  ì¦‰, ì–´ë–¤ ë™ë¬¼ì˜ í„¸ì— ì¡´ì¬í•˜ëŠ” ìƒ‰ì˜ ê°¯ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê³ ì–‘ì´ì¼ í™•ë¥ ì€ 
  P(C=ê³ ì–‘ì´|í„¸ì˜ ìƒ‰=3) =  P(í„¸ì˜ ìƒ‰ = 3|C = ê³ ì–‘ì´)P(C=ê³ ì–‘ì´)ì´ë‹¤. (ë¶„ëª¨ëŠ” ìƒëµí•¨.)
```

ì¦‰, Bayes Ruleì— ê¸°ë°˜í•˜ì—¬ ìš°ë¦¬ê°€ ì›í•˜ëŠ” outputì€ Posteriorë¼ëŠ” ê²ƒì„ ëª…í™•íˆ ì•Œ ìˆ˜ ìˆë‹¤.

$$
\begin{align*}
p(C_{k}|\bold{x}) &= {{p(\bold{x}| C_{k}) p(C_{k})}\over{\sum_{j=1}^{K}{p(\bold{x}|C_{j})p(C_{j})}}} \\
&\propto p(\bold{x}| C_{k}) p(C_{k})
\end{align*}
$$

ìœ„ì˜ ê²½ìš° Classê°„ì˜ ìƒëŒ€ ë¹„êµì— ì‚¬ìš©í•˜ëŠ” ì§€í‘œë¡œ ì´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ë¶„ëª¨(Normalization Factor, í™•ë¥ ì˜ ì´í•©ì´ 1ì´ ë˜ë„ë¡ í•˜ëŠ” ì—­í• )ë¥¼ ì œì™¸í•˜ì—¬ë„ ìƒê´€ì—†ê¸°ì— ëŒ€ê²Œ ë³µì¡í•œ ë¶„ëª¨ ê³„ì‚°ì„ ì œì™¸í•˜ê³  í‘œí˜„í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.

ë˜í•œ, ì•ì„  ì˜ˆì‹œì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” insightëŠ” í¸í–¥ëœ ë°ì´í„°ì¼ìˆ˜ë¡ MLEë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ìœ„ì—ì„œ Linear Regressionì´ Classificationì— ë¶€ì í•¨í•œ ê²½ìš°ë„ ë°ì´í„°ì˜ í¸í–¥ì´ ìˆì„ ê²½ìš°ì´ë‹¤. ì´ ì—­ì‹œ Linear Regressionì´ ê²°êµ­ì€ MLEì— ê¸°ë°˜í•˜ê¸° ë•Œë¬¸ì¸ ê²ƒì´ë‹¤.

ìš°ë¦¬ëŠ” ê° Class ìì²´ì˜ í™•ë¥ (Prior)ê³¼ Likelihoodë¥¼ ì´ìš©í•  ìˆ˜ ìˆëŠ” Discriminant Functionì„ êµ¬í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.

> **Logistic Regression**

ì ì´ì œ ë“œë””ì–´ Logistric Regressionì„ ì‹œì‘í•´ë³´ì. ìš°ë¦¬ëŠ” Discriminant Functionì„ ë¨¼ì € ì§€ì •í•´ì•¼ í•œë‹¤. ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ì´ ìˆì§€ë§Œ, ê°€ì¥ ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€ **Softmax**ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤. **Sigmoid**ë¥¼ í™œìš©í•˜ì—¬ ì‹ì„ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
p(y_n = k | \bold{x}_n, \bold{w}) = {{\exp(\bold{w}_{k}^{\top}\bold{x}_n)}\over{\sum_{j=1}^{K}{\exp(\bold{w}_{j}^{\top}\bold{x}_n)}}} 
$$

ë§Œì•½, classê°€ 2ê°œì¸ Binary Classificationì¸ ê²½ìš°ì— **Softmax**ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤. íŠ¹íˆ ì´ë¥¼ **Sigmoid**(**Logit**)ë¼ê³  ì •ì˜í•œë‹¤.

$$
p(y_n = k | \bold{x}_n, \bold{w}) = {1\over{1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})}}
$$

ì´ë¥¼ ìœ ë„í•˜ëŠ” ê³¼ì •ì€ ìƒëµí•˜ì§€ë§Œ, ì—¬íƒ€ ë‹¤ë¥¸ ë¸”ë¡œê·¸ë¥¼ ë” ì°¸ê³ í•˜ë©´ ì¢‹ë‹¤.

ì´ë¥¼ Linear Regressionê³¼ ë¹„êµí•´ì„œ ì‚´í´ë³´ì.

![logistic-vs-linear](/images/logistic-vs-linear.jpg)

Linear Regressionì€ íŠ¹ì •ê°’ì„ í–¥í•´ ë‚˜ì•„ê°€ê³  ìˆë‹¤. í•´ë‹¹ ë°©ì‹ì„ ë³´ë©´ xê°€ ëŒ€ìƒì˜ íŠ¹ì„±ì„ ê°•í•˜ê²Œ ê°€ì§€ê³  ìˆë‹¤ë©´, ëª…í™•í•˜ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” **sigmoid**($\sigma$) í•¨ìˆ˜ê°€ [0, 1] ë²”ìœ„ ë‚´ì—ì„œ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— Regression ê³¼ì •ì—ì„œ ê·¹ë‹¨ ë°ì´í„°(outlier)ê°€ ê°€ì§€ëŠ” ì˜í–¥ë ¥ì´ Linear Regressionë³´ë‹¤ ê·¹ë‹¨ì ìœ¼ë¡œ ì ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

ì ì´ê²ƒì´ ê°€ì§€ëŠ” ì˜ë¯¸ë¥¼ ì´ì „ì— ì‚´í´ë³¸ **Bayes Decision Rule**ì— ê¸°ë°˜í•´ì„œ ìƒê°í•´ë³´ì. **sigmoid**($\sigma$)ëŠ” ê²°êµ­ ê·¹ë‹¨ì ì¸ ë°ì´í„°ì´ë“ , ì• ë§¤í•œ ë°ì´í„°ì´ë“  ê±°ì˜ ë¹„ìŠ·í•œ ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤. ê·¸ë ‡ë‹¤ëŠ” ê²ƒì€ ê¸°ì¡´ì—ëŠ” í‰ê· ì„ êµ¬í•˜ëŠ”ë°ì— input(x)ì˜ ê°’ì´ í° ì˜í–¥ì„ ë¯¸ì³¤ë‹¤ë©´, **sigmoid**($\sigma$)ì—ì„œëŠ” íŠ¹ì • classì— ì†í•˜ëŠ” xì˜ ê°¯ìˆ˜ê°€ ë§ì€ ì˜í–¥ì„ ì£¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ì„œ **sigmoid**($\sigma$)ê°€ ì™„ë²½í•˜ì§€ëŠ” ì•Šì§€ë§Œ, **Bayes Decision Rule**ì„ ë°˜ì˜í–ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ, MLEë¥¼ í†µí•´ì„œ Logistic Regressionì˜ parameterë¥¼ ì¶”ì •í•´ë³´ì. (MAPëŠ” ê¸°ì¡´ì— ì‚´í´ë³¸ Linear Regressionê³¼ ë™ì¼í•˜ê²Œ regularizerë¥¼ ë”í•´ì£¼ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì— ìƒëµí•œë‹¤.)

$$
\begin{align*}
\argmax_{w}\log{p(\mathcal{D}|\bold{w})} &= \argmax_{w}\sum_{n=1}^{N}{\log p(y_{n}|\bold{x}_{n}, \bold{w})} \\
&= \argmax_{w}\sum_{n=1}^{N}{\log ({1\over{1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})}}) } \\
&= \argmax_{w}\sum_{n=1}^{N}{-\log (1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})) } \\
&= \argmin_{w}\sum_{n=1}^{N}{\log (1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})) } \\
\end{align*}
$$

## Gradient Descent/Ascent

ìœ„ì˜ ë³µì¡í•œ ì‹ì„ ë´¤ìœ¼ë©´ ì•Œê² ì§€ë§Œ, ì•ˆíƒ€ê¹ê²Œë„ ì¼ë°˜ì‹ìœ¼ë¡œ $\bold{w}_{MLE}, \bold{w}_{MAP}$ ë“±ì„ êµ¬í•  ìˆ˜ëŠ” ì—†ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ê°€ ë¯¿ì„ ê²ƒì€ Gradientë¥¼ ì´ìš©í•œ ë°©ì‹ì´ë‹¤.

> **Gradient Descent**

ë¨¼ì €, ìœ„ì—ì„œ ë´¤ê² ì§€ë§Œ, LossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\mathcal{L} = \sum_{n=1}^{N}{\log(1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n}))}
$$

ì´ì œ ì´ë¥¼ ë¯¸ë¶„í•´ì„œ Gradientë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\nabla_{\bold{w}}\mathcal{L}(\bold{w}) = \sum_{n=1}^{N}{{{-y_{n}\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})}\over{1+\exp(-y_{n}\bold{w}^{\top}\bold{x}_{n})}}\bold{x}_{n}}
$$

ë”°ë¼ì„œ, Gradient Descent ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ëœë‹¤.

$$
\bold{w}_{t+1} = \bold{w}_{t} - \alpha\nabla_{\bold{w}}\mathcal{L}(\bold{w}_{t})
$$

> **Gradient Ascent**

ìœ„ì˜ ë°©ì‹ì´ ê°€ì¥ ì¼ë°˜ì ì´ì§€ë§Œ, ìš°ë¦¬ê°€ sigmoidì˜ classê°’ìœ¼ë¡œ $y \in \{-1, 1\}$ ëŒ€ì‹  $y \in \{0, 1\}$ì„ ì‚¬ìš©í–ˆì„ ê²½ìš° ë‹¤ë¥¸ ì‹ìœ¼ë¡œë„ ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë‹¤.

ì´ ê²½ìš°ì—ëŠ” Lossë¼ê¸° ë³´ê¸° ì–´ë µì§€ë§Œ, ë‹¤ë¥¸ í˜•íƒœì˜ optimization í˜•íƒœê°€ ë§Œë“¤ì–´ì§„ë‹¤. (ì—¬ê¸°ì„œ $\sigma$ëŠ” sigmoid í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.)

$$
\argmax_{\bold{w}} \sum_{n=1}^{N}y_{n}\log{\sigma(\bold{w}^{\top}\bold{x}_{n}) + (1-y_{n})\log{(1-\sigma(\bold{w}^{\top}\bold{x}_{n}))} }
$$

ì´ë¥¼ ë˜‘ê°™ì´ ë¯¸ë¶„í•˜ì—¬ ì‚¬ìš©í•˜ì§€ë§Œ, ë°˜ëŒ€ë¡œ ì´ ê²½ìš°ì—ëŠ” maximization ì´ê¸° ë•Œë¬¸ì— Gradient Ascentë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. 

ìš°ì„  ë¯¸ë¶„ ê²°ê³¼ ì–»ëŠ” GradientëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\nabla_{\bold{w}}\mathcal{L}(\bold{w}) = \sum_{n=1}^{N}{[y_{n} - \sigma(\bold{w}^{\top}\bold{x}_{n})]\bold{x}_{n}}
$$

êµ‰ì¥íˆ ê°„ë‹¨í•˜ê²Œ ì •ë¦¬ê°€ ë˜ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

$$
\bold{w}_{t+1} = \bold{w}_{t} - \alpha\nabla_{\bold{w}}\mathcal{L}(\bold{w}_{t})
$$

ë”°ë¼ì„œ, ì•„ë˜ì™€ ê°™ì´ Gradient Ascentë¥¼ í™œìš©í•˜ì—¬ ê³„ì‚°í•˜ëŠ” ê²ƒë„ ì¶©ë¶„íˆ ê°€ëŠ¥í•˜ë‹¤.

> **Newton Method**

ì´ëŸ¬í•œ í˜•íƒœë¡œ ë„˜ì–´ì˜¤ê²Œ ë˜ë©´, êµ‰ì¥íˆ ë§ì€ ì—°ì‚°ì´ ê° updateë§ˆë‹¤ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ ê³¼ì •ì„ ì¶•ì•½í•  ë°©ë²•ì„ ì°¾ê²Œ ëœë‹¤. ê·¸ ì•„ì´ë””ì–´ëŠ” ë°”ë¡œ gradientë¥¼ ì—…ë°ì´íŠ¸ í•  ë•Œ, linear í•˜ê²Œ updateí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ Quadraticí•˜ê²Œ updateí•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•œ ë°©ë²•ë¡ ì´ **Newton Method**ì´ë‹¤. ì´ ë°©ì‹ì„ Logistic Regressionì— ì ìš©í•˜ì˜€ì„ ë•Œ, ì´ë¥¼ IRLS(Iterative Re-weighted Least Squared) Algorithm ì´ë¼ê³  ë¶€ë¥¸ë‹¤.

![newton-method](/images/newton-method.jpg)

ìœ„ ê·¸ë˜í”„ì—ì„œ f(x)ê°€ Loss ë¼ê³  í•  ë•Œ, ìš°ë¦¬ëŠ” $x_k$ì—ì„œ ì§ì„ í˜•ì˜ gradientë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ quadratic í˜•íƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´ê°’ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

ì´ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒ 2ê°€ì§€ì— ëŒ€í•œ ì‚¬ì „ ì´í•´ê°€ í•„ìš”í•˜ë‹¤.

- Taylor Series  
  smoothí•œ í˜•íƒœë¥¼ ê°€ì§„ xì— ëŒ€í•œ í•¨ìˆ˜ë¥¼ xì— ëŒ€í•œ ê¸‰ìˆ˜ì˜ í˜•íƒœë¡œ ë³€í™˜í•œ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
  $T_{\infin}(x) = \sum_{k=0}^{\infin}{f^{(k)}(x_{0})\over{k\!}}(x-x_{0})^{k} $  
  ì¦‰, sine í•¨ìˆ˜ì™€ ê°™ì€ í˜•íƒœì˜ ê·¸ë˜í”„ë„ xì˜ ê¸‰ìˆ˜ í˜•íƒœë¡œ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. Newton Methodì—ì„œëŠ” ë¬´í•œëŒ€ê¹Œì§€ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , ëŒ€ê²Œ K=2ê¹Œì§€ë¥¼ ì“´ë‹¤.
- Hessian Matrix  
  íŠ¹ì • í•¨ìˆ˜ $f(\bold{x})$ë¥¼ ê° featureì— ëŒ€í•´ì„œ ì´ì¤‘ í¸ë¯¸ë¶„í•œ ê²°ê³¼ë¥¼ ì €ì¥í•œ í–‰ë ¬ì´ë‹¤. ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.  
  $
  H = \nabla^{2}f(x) =
  \left[
    \begin{array}{ccc}
      \dfrac{\partial^{2} f(\mathbf{x})}{\partial x_{1}^{2}} & \cdots & \dfrac{\partial^{2} f(\mathbf{x})}{\partial x_{1} \partial x_{D}} \\ 
      \vdots & \ddots & \vdots \\
      \dfrac{\partial^{2} f(\mathbf{x})}{\partial x_{D} \partial x_{1}} & \cdots & \dfrac{\partial^{2} f(\mathbf{x})}{\partial x_{n}^{2}}
    \end{array}
  \right]
  $

ì´ë¥¼ ì´ìš©í•´ì„œ, Newton Methodì˜ ê²°ê³¼ê°’ì„ ì •ë¦¬í•˜ë©´ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\bold{w}^{(k+1)} = \bold{w}^{(k)} - [\nabla^{2}\mathcal{J}(\bold{w}^{(k)})]^{-1}\nabla\mathcal{J}(\bold{w}^{(k)})
$$

ì ì´ì œ ì´ê²ƒì„ ì‹¤ì œë¡œ Logistic Regression ì‹ì— ëŒ€ì…í•´ë³´ì.

$$
\begin{align*}
  \nabla\mathcal{J}(w) &= - \sum_{n=1}^{N}(y_{n}-\hat{y}_{n})x_{n} \\
  \nabla^{2}\mathcal{J}(w) &= \sum_{n=1}^{N}\hat{y}_{n}(1-\hat{y}_{n})\bold{x}_{n}\bold{x}_{n}^{\top}
\end{align*}
$$

ì—¬ê¸°ì„œ, ì•„ë˜ì™€ ê°™ì´ ë³€ìˆ˜ë¥¼ ì •ì˜í•˜ë©´,

$$
S = 
  \begin{bmatrix}
    \hat{y}_{1}(1-\hat{y}_1)  & \cdots  & 0                         \\
    \vdots                    & \ddots  & \vdots                     \\
    0                         & \cdots  & \hat{y}_{N}(1-\hat{y}_N)  \\
  \end{bmatrix},

\bold{b} = 
  \begin{bmatrix}
    {{y_{1} - \hat{y}_{1}}\over{\hat{y}_{1}(1-\hat{y}_{1})}} \\
    \vdots \\
    {{y_{N} - \hat{y}_{N}}\over{\hat{y}_{N}(1-\hat{y}_{N})}}
  \end{bmatrix}
$$

ê²°ê³¼ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

$$
\begin{align*}
\bold{w}_{k+1} &= \bold{w}_{k} + (XS_{k}X^{\top})^{-1}XS_{k}\bold{b}_{k} \\
&= (XS_{k}X^{\top})^{-1}[(XS_{k}X^{\top})\bold{w}_{k} + XS_{k}\bold{b}_{k}] \\
&= (XS_{k}X^{\top})^{-1}XS_{k}[X^{\top}\bold{w}_{k} + \bold{b}_{k}]
\end{align*}
$$

ì´ëŠ” ê²°ì½” ê³„ì‚° ê³¼ì •ì´ ë‹¨ìˆœí•˜ë‹¤ê³ ëŠ” í•  ìˆ˜ ì—†ì§€ë§Œ, ë¹ ë¥´ê²Œ ìˆ˜ë ´í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê°€ì¹˜ìˆëŠ” ë°©ë²•ì´ë‹¤.


## Reference

- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)