{"pageProps":{"posts":[{"content":"\n## Intro\n\nMachine Learningì€ íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„°ë¡œ ë¶€í„° pattern ë˜ëŠ” ê°€ì • ë“±ì„ ìœ ë„í•´ë‚´ëŠ” ë°©ë²•ì´ë‹¤.\nì´ë¥¼ ìœ„í•œ ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì€ ì—¬ëŸ¬ ê°œì˜ í™•ë¥ ë¶„í¬ì™€ ì´ê²ƒì˜ parameterì˜ ì¡°í•©(probabilistic model)ë“¤ ì¤‘ì—ì„œ ì¸¡ì •ëœ ë°ì´í„°ë“¤ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” í•˜ë‚˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤.\nê·¸ ì¤‘ì—ì„œ, í™•ë¥  ë¶„í¬ë¥¼ ê²°ì •í•œ ìƒíƒœì—ì„œ parameterë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” í˜•íƒœì˜ ì ‘ê·¼ë²•ì„ ìš°ë¦¬ëŠ” Parametric Estimationì´ë¼ê³  í•œë‹¤. ê·¸ ì™¸ì—ë„ Nonparametric, Semi-parametric ë°©ì‹ë„ ì¡´ì¬í•˜ì§€ë§Œ ì´ëŠ” ì—¬ê¸°ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤.\n\n## Small Example\n\nê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ í†µí•´ì„œ Parametric Estimationì˜ íë¦„ì„ ìµí˜€ë³´ì.\n\ní•œ í•™ê¸‰ì—ì„œ í•™ìƒë“¤ì˜ í˜•ì œìë§¤ ìˆ˜ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•˜ê³  ì‹¶ë‹¤ê³  í•˜ì.  \nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ë¨¼ì € ì¡°ì‚¬(ê´€ì¸¡)ë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. ì´ë¥¼ í†µí•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ë¥¼ ì–»ê²Œ ë˜ì—ˆë‹¤ê³  í•˜ì.\n\n| x        | 1    | 2    | 3    | 4    | 5    | 6    | x$\\geq$7 |\n| :------- | :--- | :--- | :--- | :--- | :--- | :--- | :------- |\n| $p(X=x)$ | 17   | 59   | 15   | 6    | 2    | 0    | 1        |\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ì—¬ëŸ¬ ì‚¬ì „ ì§€ì‹ì„ í™œìš©í•˜ì—¬ í•´ë‹¹ ë°ì´í„°ë¥¼ ë³´ì•˜ì„ ë•Œ, í•´ë‹¹ ë¶„í¬ê°€ Poisson ë¶„í¬ì˜ í˜•íƒœë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.  \në”°ë¼ì„œ, ìš°ë¦¬ëŠ” í•´ë‹¹ ë¶„í¬ë¥¼ Poissonì´ë¼ê³  ê°€ì •í•œ ë‹¤ìŒì—ëŠ” ë‹¨ìˆœíˆ í•´ë‹¹ ë¶„í¬ì— ëŒ€ì…í•˜ë©°, ê°€ì¥ ì ì ˆí•œ parameterë§Œ ì°¾ìœ¼ë©´ ëœë‹¤.  \n\nì´ ê³¼ì •ê³¼ ë‹¨ìˆœíˆ ê° xì—ì„œì˜ í™•ë¥ ê°’ì„ êµ¬í•˜ëŠ” ë°©ì‹ì´ë‘ ë¬´ì—‡ì´ ë‹¤ë¥¸ì§€ë¥¼ ì•Œì•„ì•¼ì§€ í•´ë‹¹ ê³¼ì •ì˜ ì˜ì˜ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.\në¨¼ì €, ìš°ë¦¬ê°€ í•˜ê³ ì í•˜ëŠ” ì¼ì´ í˜•ì œìë§¤ì˜ í‰ê·  ìˆ˜ë¥¼ êµ¬í•œë‹¤ê³  í•˜ì. ì´ë•Œì˜ í‰ê·  ê°’ê³¼ Poisson ë¶„í¬ì—ì„œì˜ í™•ë¥ ê°’ì€ ë‹¤ë¥¼ ìˆ˜ ë°–ì— ì—†ë‹¤.\n\nì´ë ‡ê²Œ í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•˜ëŠ” ê²ƒì˜ ì˜ë¯¸ëŠ” ì´ê²ƒë§ê³ ë„ ë³´ì§€ ì•Šì€ ë°ì´í„°(unseen data)ë¥¼ ì²˜ë¦¬í•¨ì— ìˆë‹¤. ìš°ë¦¬ê°€ ë§Œì•½ ëª¨ë“  ê°€ëŠ¥í•œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ëª¨ë‘ ì•Œê³  ìˆê³ , ì´ë¥¼ ì €ì¥í•  ê³µê°„ì´ ì¶©ë¶„í•˜ë‹¤ë©´,\nì´ëŸ¬í•œ í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•  í•„ìš”ê°€ ì—†ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì¶”ì¸¡ì€ unseen dataì— ëŒ€í•´ì„œë„ ê·¸ëŸ´ì‚¬í•´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ê²°êµ­ í™•ë¥  ë¶„í¬ê°€ í•„ìš”í•˜ë‹¤.\n\nìœ„ì˜ ì˜ˆì‹œì—ì„œ ë§Œì•½, í˜•ì œìë§¤ê°€ 3ëª…ì¸ ê²½ìš°ì˜ ë°ì´í„°ê°€ ì—†ë‹¤ê³  í•˜ì. ì´ ê²½ìš°ì—ë„ í™•ë¥ ë¶„í¬ë¥¼ í†µí•œ ì¶”ì¸¡ì„ í•œë‹¤ë©´, ìš°ë¦¬ëŠ” ìœ ì˜ë¯¸í•œ ê°’ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n## Parametric Estimation\n\n> **ì •ì˜**\n\nsample space $\\Omega$ì—ì„œ í†µê³„ ì‹¤í—˜ì˜ ê´€ì¸¡ ê²°ê³¼ë¥¼ í†µí•´ì„œ ì–»ì€ sample $X_1$, $X_2$, ... , $X_n$ì´ ìˆë‹¤ê³  í•˜ì. ê° sampleì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ìš°ë¦¬ëŠ” $p_\\theta$ë¼ê³  í•œë‹¤.\nì—¬ê¸°ì„œ $\\theta$ëŠ” íŠ¹ì • í™•ë¥  ë¶„í¬ì—ì„œì˜ parameterë¥¼ ì˜ë¯¸í•œë‹¤. ë§Œì•½, bernoulli ë¼ë©´, ë‹¨ì¼ ì‹œí–‰ì— ëŒ€í•œ í™•ë¥ ì´ ë  ê²ƒì´ê³ , binomialì´ë¼ë©´, ë‹¨ì¼ ì‹œí–‰ì˜ í™•ë¥ ê³¼ íšŸìˆ˜ê°€ í•´ë‹¹ ê°’ì´ ë  ê²ƒì´ë‹¤.\n\n> **Risk**\n\nì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì°¾ê¸°ë¥¼ ì›í•˜ëŠ” ê²ƒì€ ì „ì²´ sample space $\\Omega$ë¥¼ ëª¨ë‘ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ” $\\theta_{*}$(ì‹¤ì œ true $\\theta$)ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.(ì´ë¯¸ í™•ë¥  ë¶„í¬ì˜ í˜•íƒœ(í•¨ìˆ˜, ex. Bernoulli, Binomial)ëŠ” ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤.)  \nê·¸ë ‡ë‹¤ë©´, ì‹¤ì œ $\\theta_*$ì™€ ì¶”ì¸¡ì„ í†µí•´ ë§Œë“  $\\hat{\\theta}$ ì‚¬ì´ì˜ ë¹„êµë¥¼ ìœ„í•œ ì§€í‘œë„ í•„ìš”í•  ê²ƒì´ë‹¤. ì´ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” **Risk**ë¼ëŠ” ê²ƒì„ ì‚¬ìš©í•œë‹¤.  \nê°„ë‹¨í•˜ê²Œë„ ì‹¤ì œ $\\theta_*$ì™€ $\\hat{\\theta}$ì˜ Mean Square Errorë¥¼ ê³„ì‚°í•œë‹¤.\n\n$$ \n\\begin{align*}\nRisk &= E[(\\hat{\\theta} - \\theta_*)^2] = E[\\hat{\\theta}^2 - 2\\hat{\\theta}\\theta_* + \\theta_*^2] \\\\\n&= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 \\\\\n&= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 + (E^2[\\hat{\\theta}] - E^2[\\hat{\\theta}]) \\\\\n&= (E[\\hat{\\theta}] - \\theta_*)^2 + E[\\hat{\\theta}^2] - E^2[\\hat{\\theta}] \\\\\n&= {Bias}^2 + Var[\\hat{\\theta}]\n\\end{align*}\n$$\n\ní•´ë‹¹ ì‹ì„ ë¶„ì„í•´ë³´ë©´, ì´ì™€ ê°™ì€ ì˜ë¯¸ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. ìš°ë¦¬ê°€ íŠ¹ì • í™•ë¥  ë¶„í¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë‹¨ í•˜ë‚˜ë¡œ ë‹¨ì •í•˜ê³  Riskë¥¼ ê³„ì‚°í•˜ëŠ” ê²½ìš°ëŠ” Variance ê°’ì€ 0ì´ë‹¤. ì¦‰, í•´ë‹¹ í™•ë¥  ë¶„í¬ê°€ ê°€ì§€ëŠ” RiskëŠ” ë‹¨ìˆœíˆ í•´ë‹¹ parameterì™€ ì‹¤ì œ parameterê°€ ì–¼ë§ˆë‚˜ ì°¾ì´ê°€ ë‚˜ëŠ”ê°€ë¥¼ ì˜ë¯¸í•œë‹¤.\n\ní•˜ì§€ë§Œ, parameterë¥¼ íŠ¹ì •í•˜ì§€ ì•Šê³ , ë²”ìœ„ë¡œ ì§€ì •í•œë‹¤ë©´, (ì˜ˆë¥¼ ë“¤ì–´, ì£¼ì‚¬ìœ„ë¥¼ ë˜ì ¸ 3ì´ ë‚˜ì˜¬ í™•ë¥ ì€ 1/6 ~ 1/3ì´ë‹¤.) í•´ë‹¹ í™•ë¥ ì˜ í‰ê· ê³¼ Varianceê°€ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë‹¤.  \në‹¤ì†Œ ì²˜ìŒì—ëŠ” í—·ê°ˆë¦´ ìˆ˜ ìˆì§€ë§Œ, í•´ë‹¹ ì‹ì—ì„œ í‰ê· ì´ ì˜ë¯¸ëŠ” ì˜ í™•ì¸í•˜ì. íŠ¹ì • í™•ë¥  ë¶„í¬ë¥¼ ê°€ì§€ë„ë¡ í•˜ëŠ” $\\theta$ê°€ $\\theta_*$ ì— ì–¼ë§ˆë‚˜ ê·¼ì ‘í•œì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì‹ì´ë¼ëŠ” ê²ƒì„ ë‹¤ì‹œ í•œ ë²ˆ ê¸°ì–µí•˜ì.\n\n> **Estimation**\n\nì´ì œë¶€í„°ëŠ” ì•ì—ì„œ ì‚´í´ë³´ì•˜ë˜, parameteric estimationì—ì„œ ì–´ë–»ê²Œ $\\hat{\\theta}$ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë‹¤ë£° ê²ƒì´ë‹¤. í™•ë¥ /í†µê³„ ì´ë¡ ì—ì„œëŠ” í¬ê²Œ 3ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê° ê°ì„ ì‚´í´ë³´ë„ë¡ í•˜ì.\n\n<mark>**1. MLE**</mark>\n\nMaximum Likelihood Estimationì˜ ì•½ìì´ë‹¤. ì—¬ê¸°ì„œ, LikelihoodëŠ” ê°€ëŠ¥ì„±ì´ë¼ëŠ” ëœ»ì„ ê°€ì§€ë©°, í™•ë¥ /í†µê³„ ì´ë¡ ì—ì„œ ì´ëŠ” í™•ë¥ ì„ í•´ë‹¹ ì‚¬ê±´ì´ ë°œìƒí•  ê°€ëŠ¥ì„±ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ì´ìš©í•´ì„œ ìš°ë¦¬ê°€ í’€ê³ ì í•˜ëŠ” ë¬¸ì œ, ìš°ë¦¬ê°€ ì¶”ì¸¡í•œ $\\theta$ê°€ ìš°ë¦¬ê°€ ê°€ì§„ Datasetë¥¼ ë§Œì¡±ì‹œí‚¬ ê°€ëŠ¥ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤. ì•„ë˜ ìˆ˜ì‹ì„ ë³´ì.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta;\\mathcal{D}) &= p(\\mathcal{D}|\\theta) = p(x_1, x_2, ..., x_n|\\theta) \\\\\n&= \\prod_{i=1}^{n}{p(x_i|\\theta)}\n\\end{align*}\n$$\n\n(ìœ„ ì‹ì„ ì´í•´í•˜ë ¤ë©´, ë¨¼ì € Datasetì˜ ê° dataë“¤ì€ ì„œë¡œ independentí•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¸°ì–µí•˜ì.)  \nê²°êµ­ $\\theta$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Datasetì¼ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ë‹¤ì‹œ ìƒê°í•˜ë©´, $\\theta$ê°€ ì–¼ë§ˆë‚˜ ë°ì´í„°ì…‹ì˜ í™•ë¥ ì„ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ê°€ì™€ ê°™ë‹¤.\n\nì´ê²ƒì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ë ¤ë©´ í•˜ë‚˜ì˜ ì˜ˆì‹œë¥¼ ë³´ë©´ ì¢‹ë‹¤.\n\n![MLE example](/images/MLE-example.png)\n\nì²« ë²ˆì§¸ ê·¸ë˜í”„ëŠ” ê°™ì€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ í•¨ìˆ˜ë¥¼ ì“°ë©´ì„œ, parameterë§Œ ë‹¤ë¥´ê²Œ í•œ ê²½ìš°ì´ê³ , ì•„ë˜ëŠ” ì‹¤ì œ ë°ì´í„°ì˜ ë¶„í¬ë¼ê³  í•˜ì.(ë¹¨ê°„ìƒ‰ ì„  í•˜ë‚˜ í•˜ë‚˜ê°€ ë°ì´í„°ë¥¼ ì˜ë¯¸)  \nì´ë•Œ, Likelihoodë¥¼ ê° ê° êµ¬í•˜ë©´ ê° xì—ì„œì˜ í™•ë¥ ë¶„í¬ì˜ í™•ë¥ ê°’ì„ ëª¨ë‘ ê³±í•˜ë©´ ëœë‹¤. ê·¸ ê²½ìš° ì–´ë–¤ ê²ƒì´ ì œì¼ í´ì§€ëŠ” ë¶„ëª…í•˜ë‹¤. ë°”ë¡œ íŒŒë€ìƒ‰ ë¶„í¬ì¼ ê²ƒì´ë‹¤.  \n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€? ë°”ë¡œ ê°€ì¥ ë†’ì€ ê°€ëŠ¥ì„±ì„ ê°€ì§€ê²Œ í•˜ëŠ” $\\theta$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ì‹ìœ¼ë¡œ í‘œì‹œí•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\n$$\n\\hat{\\theta}_{MLE} = \\argmax_{\\theta}\\mathcal{L}(\\theta;\\mathcal{D})\n$$\n\nì—¬ê¸°ì„œ í•˜ë‚˜ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆë‹¤. ë°”ë¡œ, ì»´í“¨í„°ë¡œ ì—°ì‚°í•˜ê²Œ ë˜ë©´ underflowê°€ ë°œìƒí•˜ëŠ” ê²ƒì´ë‹¤. íŠ¹ì • ì–¸ì–´ê°€ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ì†Œìˆ˜ì  ë²”ìœ„ë¥¼ ë²—ì–´ë‚œë‹¤ë©´, ì œëŒ€ë¡œ ëœ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ì—†ë‹¤. ì´ì™€ ê°™ì€ ë¬¸ì œë¥¼ **vanishing likelihood**ë¼ê³  í•œë‹¤.  \në”°ë¼ì„œ, ìš°ë¦¬ëŠ” logë¥¼ ì·¨í–ˆì„ ë•Œì™€ logë¥¼ ì·¨í•˜ì§€ ì•Šì•˜ì„ ë•Œì˜ ê²½í–¥ì„±ì´ ê°™ìŒì„ ë°”íƒ•ìœ¼ë¡œ likelihoodì— logë¥¼ ì·¨í•œ ê°’ì„ ì´ìš©í•˜ì—¬ MLEë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ì´ ë°©ì‹ì„ maxmum log likelihood estimation ì´ë¼ê³  ë¶€ë¥¸ë‹¤.\n\n$$\n\\mathcal{l}(\\theta;\\mathcal{D}) = \\sum_{i=1}^{n}{\\log{(p(x_i|\\theta))}}\n$$\n\nì´ ë°©ì‹ì„ ì´ìš©í•˜ê²Œ ë˜ë©´, ê³±ì…ˆì´ ëª¨ë‘ ë§ì…ˆìœ¼ë¡œ ë°”ë€Œê¸° ë•Œë¬¸ì— ê³„ì‚°ì—ì„œë„ ìš©ì´í•˜ë‹¤.\n\nì—¬ê¸°ê¹Œì§€ ì‚´í´ë³´ë©´, í•˜ë‚˜ì˜ ì˜ë¬¸ì´ ë“¤ ìˆ˜ë„ ìˆë‹¤. ë°”ë¡œ, $p(\\theta|\\mathcal{D})$ë„ ì¸¡ì • ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ ì•ŠëƒëŠ” ê²ƒì´ë‹¤. ì´ ì—­ì‹œë„ Datasetì´ ì£¼ì–´ì§ˆ ë•Œ, $\\theta$ì¼ í™•ë¥ ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.  \nì–´ì°Œë³´ë©´, ì‚¬ëŒì˜ ìƒê°ìœ¼ë¡œëŠ” ì´ê²Œ ë” ë‹¹ì—°í•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ë„ ìˆë‹¤. ì´ëŠ” ë°”ë¡œ ë‹¤ìŒ MAPì—ì„œ ë‹¤ë£° ê²ƒì´ë‹¤. ìš°ì„  MLEë¥¼ ë¨¼ì €í•œ ì´ìœ ëŠ” ì´ê²ƒì´ ë” êµ¬í•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì„ì„ ê¸°ì–µí•´ë‘ì. \n\n```plaintext\n ğŸ¤” ì¦ëª…\n\n (*í•´ë‹¹ ë‚´ìš©ì€ ì •ë³´ ì´ë¡ ì— ê¸°ë°˜í•œ MLEì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì´í•´ë¥¼ ìœ„í•œ ë‚´ìš©ì…ë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ìì„¸íˆ ì•Œ í•„ìš”ê¹Œì§€ëŠ” ì—†ìŠµë‹ˆë‹¤.)\n\n ë‘ í™•ë¥  ë¶„í¬ ê°„ information entrophyì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” KL divergenceì˜ ìµœì†Ÿê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ ëª©í‘œë¼ê³  ì •ì˜í•  ìˆ˜ ìˆë‹¤.  \n ë”°ë¼ì„œ, ìš°ë¦¬ê°€ ê²°êµ­ ì–»ê³ ì í•˜ëŠ” ê²ƒì€ í™•ë¥  ë¶„í¬ í•¨ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ,  \n nì´ ë¬´í•œëŒ€ë¡œ ê°ˆ ë•Œ, ê²½í—˜ì  í™•ë¥ (empirical probability)ì— ê°€ì¥ ê·¼ì‚¬í•˜ëŠ” parameterë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.  \n ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” KL divergenceì˜ ìµœì†Ÿê°’ì„ êµ¬í•˜ë©´ ëœë‹¤.\n```\n\n$$\n\\begin{align*}\n\\argmin_\\theta KL(\\tilde{p}||p_\\theta) &= \\argmin_\\theta \\int\\tilde{p}(x)\\log{\\tilde{p}(x)\\over{p_\\theta(x)}}dx \\\\ \n&=\\argmin_\\theta[-\\int\\tilde{p}(x)\\log{\\tilde{p}(x)dx} - \\int\\tilde{p}(x)\\log{p_\\theta(x)dx}] \\\\\n&= \\argmax_\\theta\\int{\\tilde{p}(x)\\log{p_\\theta(x)}dx} \\\\\n&= \\argmax_\\theta\\sum_{i=1}^{n}{\\log{p_\\theta(x_i)}} \\\\\n&= \\theta_{MLE}\n\\end{align*} \n$$\n\n<mark>**2. MAP**</mark>\n\nMaximum A Posterioriì˜ ì•½ìì´ë‹¤. PosterioriëŠ” ì‚¬í›„ í™•ë¥ ì´ë¼ê³ ë„ ë¶€ë¥´ë©°, datasetì´ ì£¼ì–´ì¡Œì„ ë•Œ, $\\theta$ì¼ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.  \nì´ë¥¼ ë°”ë¡œ êµ¬í•˜ëŠ” ê²ƒì€ ë‹¤ì†Œ ì–´ë µë‹¤. ì™œëƒí•˜ë©´, Datasetì´ ì¡°ê±´ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” í˜•íƒœì´ê¸° ë•Œë¬¸ì´ë‹¤. ($p(\\theta|\\mathcal{D})$)  \në”°ë¼ì„œ, ìš°ë¦¬ëŠ” Bayes' Theoremì— ë”°ë¼ì„œ ì´ì „ì— ë°°ìš´ Likelihoodì™€ parameterì˜ í™•ë¥ , ê·¸ë¦¬ê³  Datasetì˜ í™•ë¥ ì„ í™œìš©íˆì—¬ í’€ì–´ë‚¼ ê²ƒì´ë‹¤.\n\n$$\np(\\theta|\\mathcal{D}) = {p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}}\n$$\n\nì—¬ê¸°ì„œ ì£¼ì˜í•´ì„œ ë³¼ ê²ƒì€ ë°”ë¡œ $p(\\theta|\\mathcal{D})$ì™€ $p(\\theta)$ì˜ ê´€ê³„ì´ë‹¤. datasetì´ ì£¼ì–´ì§ˆ ë•Œì˜ parameterì˜ í™•ë¥ ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ ì›ë˜ parameterì˜ í™•ë¥ ì´ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.  \nì–´ì°Œë³´ë©´ êµ‰ì¥íˆ ëª¨ìˆœë˜ì–´ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ìš°ë¦¬ê°€ ì´ê²ƒì„ ì‚¬ì „ í™•ë¥ (priori)ë¡œ ë³¸ë‹¤ë©´ ë‹¤ë¥´ê²Œ ë³¼ ì—¬ì§€ê°€ ìˆë‹¤.  \nì˜ˆë¥¼ ë“¤ë©´, ìš°ë¦¬ê°€ ìˆ˜ìƒí•œ ì£¼ì‚¬ìœ„ë¡œ í•˜ëŠ” ê²Œì„ì— ì°¸ê°€í•œë‹¤ê³  í•˜ì. ì´ë•Œ, ìš°ë¦¬ëŠ” ìˆ˜ìƒí•œ ì£¼ì‚¬ìœ„ì˜ ì‹¤ì œ í™•ë¥ ì€ ì•Œ ìˆ˜ ì—†ì§€ë§Œ, ì£¼ì‚¬ìœ„ ìì²´ì˜ í™•ë¥ ì€ ëª¨ë‘ 1/6ì´ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤. ë”°ë¼ì„œ, $p(\\theta={1\\over6}) = \\alpha, p(\\theta\\neq{1\\over6}) = \\beta$ ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë§Œì•½ ì •ë§ ìˆ˜ìƒí•´ë³´ì¸ë‹¤ë©´, ìš°ë¦¬ëŠ” $\\alpha$ê°€ ì ì  ì‘ì•„ì§„ë‹¤ëŠ” ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê³ , í•˜ë‚˜ë„ ìˆ˜ìƒí•´ë³´ì´ì§€ ì•ŠëŠ” ì¼ë°˜ ì£¼ì‚¬ìœ„ë¼ë©´, $\\alpha=1, \\beta=0$ìœ¼ë¡œ í•  ìˆ˜ë„ ìˆë‹¤. ì´ ê²½ìš°ì—ëŠ” likelihood ê°’ì— ìƒê´€ì—†ì´ ë‹¤ë¥¸ ëª¨ë“  ê°’ì´ 0ì´ê¸° ë•Œë¬¸ì— ê²°êµ­ì€ $p(\\theta|\\mathcal{D}) = p(\\theta)$ ê°€ ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\nìµœì¢…ì ìœ¼ë¡œ, MAPë„ ê²°êµ­ì€ Datasetì„ ì–¼ë§ˆë‚˜ parameterê°€ ì˜ í‘œí˜„í•˜ëŠ”ê°€ì— ëŒ€í•œ ì§€í‘œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. \në”°ë¼ì„œ, ì´ë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” parameterëŠ” $\\theta_*$ì™€ êµ‰ì¥íˆ ê·¼ì ‘í•  ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{MAP} &= \\argmax_{\\theta}p(\\theta|\\mathcal{D}) \\\\\n&= \\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}} \\\\\n&=\\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)} \\\\\n&=\\argmax_\\theta{[\\red{\\log{p(\\mathcal{D}|\\theta)}} + \\blue{\\log{p(\\theta)}}]}\n\\end{align*}\n$$\n\nMLEì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì´ ë˜í•œ ì—°ì‚° ë° **vanishing**ì„ ë§‰ê¸° ìœ„í•´ì„œ logë¥¼ ì·¨í•œë‹¤. ì‚¬ì‹¤ìƒ likelihoodì™€ ì‚¬ì „ í™•ë¥ ì˜ í•©ì„ ìµœëŒ€ë¡œ í•˜ëŠ” $\\theta$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\n<mark>**3. Bayesian Inference**</mark>\n\nì´ì œ ë§ˆì§€ë§‰ ë°©ë²•ìœ¼ë¡œ ì œì‹œë˜ëŠ” Bayesian Inferenceì´ë‹¤. ì´ëŠ” ëŒ€ê²Œ Bayesian Estimationì´ë¼ê³  ë§ì´ ë¶ˆë¦¬ëŠ” ê²ƒ ê°™ë‹¤. ì´ì „ê¹Œì§€ MLE, MAPëŠ” ê²°êµ­ ì£¼ì–´ì§„ ì‹ì„ ìµœëŒ€ë¡œ í•˜ëŠ” í™•ì •ì  $\\theta$ í•˜ë‚˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆë‹¤.\n\nBayesian InferenceëŠ” Datasetì´ ì£¼ì–´ì¡Œì„ ë•Œ, $\\theta$ì˜ í‰ê· ê°’ì„ í™œìš©í•œë‹¤. ë” ìì„¸íˆ ë§í•˜ë©´, Posteriori(ì‚¬í›„ í™•ë¥ )ì˜ í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.  \nì´ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ë©´ ì´í•´í•˜ëŠ”ë° ë„ì›€ì´ ë  ê²ƒì´ë‹¤. í•œ ë²ˆ ì‚´í´ë³´ì.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{BE}&= E[\\theta|\\mathcal{D}] \\\\\n&= {\\int_{0}^{1}{{\\theta}p(\\theta|\\mathcal{D})}d\\theta} \\\\\n&= {\\int_{0}^{1}{\\theta}{{p(\\mathcal{D}|\\theta)p(\\theta)}\\over{p(\\mathcal{D})}}d\\theta} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)p(\\mathcal{D}|\\theta)}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\mathcal{D}|\\theta)p(\\theta)}d\\theta}} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\theta)\\prod_{i=1}^{n}p(x_i|\\theta)}d\\theta}} \\\\\n\\end{align*}\n$$\n\nì´ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì€ ì´ì „ê³¼ëŠ” ë‹¤ë¥´ê²Œ ìƒëŒ€ê°’ì´ ì•„ë‹Œ í‰ê· ì„ êµ¬í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— posteriori(ì‚¬í›„ í™•ë¥ ,$p(\\theta|\\mathcal{D})$)ë¥¼ êµ¬í•´ì•¼ í•œë‹¤.\n\ní•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ì¡ê¸°ìˆ ì´ í•˜ë‚˜ ì¡´ì¬í•œë‹¤. ë°”ë¡œ **Conjugate Prior**ì´ë‹¤.\n\në°”ë¡œ ë‘ í™•ë¥  ë¶„í¬ í•¨ìˆ˜(likelihood, prior)ì— ì˜í•œ posteriorì˜ í˜•íƒœê°€ ì •í•´ì§„ ê²½ìš°ê°€ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\n\n| Prior $p(\\theta \\mid \\alpha)$  | Likelihood $p(\\mathcal{D} \\mid \\theta)$                 | Posterior $p(\\theta \\mid \\mathcal{D}, \\alpha)$                                                                                                                                                                                                                      | Expectation of Posterior                                                                                                                                                         |\n| :----------------------------- | :------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Beta ($\\alpha, \\beta$)         | Benoulli ($\\sum _{i=1}^{n}x_{i}$)                       | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +n-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                                               | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + n}}$                                                                                                                     |\n| Beta ($\\alpha, \\beta$)         | Binomial ($\\sum _{i=1}^{n}N_{i}, \\sum _{i=1}^{n}x_{i}$) | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                            | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + \\sum _{i=1}^{n}N_{i}}}$                                                                                                  |\n| Gaussian ($\\mu_0, \\sigma_0^2$) | Gaussian ($\\mu, \\sigma^2$)                              | Gaussian (${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu _{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum _{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right),\\left({\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}\\right)^{-1}}$) | ${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu _{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum _{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right)}$ |\n\nì´ë¥¼ ì´ìš©í•˜ë©´, ìš°ë¦¬ëŠ” ê°„ë‹¨í•˜ê²Œ Posterioriì˜ í‰ê· ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n  ","slug":"ml-parametric-estimation","date":"2022-10-15 11:25","title":"[ML] 1. Parametric Estimation","category":"AI","tags":["ML","MLE","MAP","Bayesian"],"desc":"Machine Learningì€ íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„°ë¡œ ë¶€í„° pattern ë˜ëŠ” ê°€ì • ë“±ì„ ìœ ë„í•´ë‚´ëŠ” ë°©ë²•ì´ë‹¤.ì´ë¥¼ ìœ„í•œ ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì€ ì—¬ëŸ¬ ê°œì˜ í™•ë¥ ë¶„í¬ì™€ ì´ê²ƒì˜ parameterì˜ ì¡°í•©(probabilistic model)ë“¤ ì¤‘ì—ì„œ ì¸¡ì •ëœ ë°ì´í„°ë“¤ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” í•˜ë‚˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤.ê·¸ ì¤‘ì—ì„œ, í™•ë¥  ë¶„í¬ë¥¼ ê²°ì •í•œ ìƒíƒœì—ì„œ parameterë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” í˜•íƒœì˜ ì ‘ê·¼ë²•ì„ ìš°ë¦¬ëŠ” Parametric Estimationì´ë¼ê³  í•œë‹¤. ê·¸ ì™¸ì—ë„ Nonparametric, Semi-parametric ë°©ì‹ë„ ì¡´ì¬í•˜ì§€ë§Œ ì´ëŠ” ì—¬ê¸°ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"MLE"}},"__N_SSG":true}