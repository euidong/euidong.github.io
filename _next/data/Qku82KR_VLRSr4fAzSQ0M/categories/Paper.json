{"pageProps":{"posts":[{"content":"\n## Intro\n\n해당 논문은 Private 5G Network에서 Intent Based Network 관리를 Natural Language를 통해서 수행할 수 있는지를 보인 논문이다. 해당 논문에서는 Design과 실제 구현 사례를 3개의 usecase에 적용한 사례를 보인다. 해당 논문은 2023년에 publish 되었지만, LLM을 적용하지는 않았고, 이에 대한 적용은 Future Work에서도 고려하고 있다고 언급한다.\n\n## Background\n\n### Intent Based Network(IBN)\n\nNon-Public Network(NPN)은 관리자에 의해서 자유롭게 설정이 가능하다는 장점을 갖고 있다. 하지만, 5G를 다룰 수 있는 숙련된 기술자가 아닌 경우 이를 효율적으로 운용하는데에는 한계가 있다. 따라서, 이러한 문제를 해결하기 위해서 **Intent Based Network(IBN)**이 제안되었다. IBN을 활용하면, private 5G network를 네트워크에 대한 지식이 없이도 단순한 의도(Intent)를 작성하는 것만으로 문제를 해결할 수 있다. 즉, IBN의 목표는 Network가 하는 일을 최대한 추상화하여 private network 관리자의 운영을 단순화하는 것이다.\n\n```plaintext\n 🤔 Intent란?\n\n Intent란 사용자의 의도를 의미한다.\n 따라서, 사용자는 자신의 요구사항을 작성할 뿐,\n 이를 어떻게(How) 구현할지는 기술하지 않는다.\n```\n\n### 5G-CLARITY\n\n5G Private nework에 대한 하나의 architecture이다. 이는 이전 논문 \"5G-CLARITY: 5G-Advanced Private Networks Integrating 5GNR, WiFi, and LiFi\"에서 처음 제시된 구조로, 해당 구조에서는 5G New Radio(5GNR)을 포함한 WiFi, LiFi와 같은 Protocol을 지원한다. 이를 통해서 virtual, physical network function들을 모두 설정하고 관리할 수 있는 구조를 제시한다. 해당 architecture는 5G network를 위한 3가지의 구성 요소를 가진다.\n\n1. **Infrastructure Stratum(계층)**  \n   각종 가상화된 장치나 Router, Switch, 통신 단말 등을 실제로 접근할 수 있는 계층으로 이를 통해서 해당 장치를 모니티링하거나 설정 및 관리할 수 있다.\n2. **Virtualised Network and Application function Stratum(계층)**  \n   실제 Network function을 구현한 계층으로 Infrastructure의 Telemetry를 수집하는 역할이나 Virtual Machine(VM)에서 기능핧 가족 기능들을 포함하는 계층이다.\n3. **Management and Orchestration Stratum(계층)**  \n   각 장비와 VM을 관리하고, 이를 통해서 Service를 제공하는 계층이다.\n\n### 기존 연구의 한계\n\n기존에도 Intent를 반영하기 위한 여러 시도가 있었다. 하지만, 해당 논문에서는 현재까지 실제로 NLP를 활용하여 Intent를 추출한 사례는 존재하지 않는다고 말한다. 따라서, 해당 연구에서 핵심 골자는 NLP를 통해서 Intent를 추출할 수 있고, 이를 통해서 Network Management를 수행할 수 있음을 보이는 것이다.\n\n## Design\n\n해당 연구는 기존 Background에서 제시한 5G-CLARITY를 구조에서 나아가 하나의 계층(Intelligence Stratum)을 추가적으로 더하는 것을 제안한다. 해당 계층에서는 Machine Learning 모델을 포함한 ML Engine과 NLP 모델에 기반한 Intent Engeine을 포함하고 있는데, 이를 활용하여 기존 5G-CLARITY 구조에 적절한 Query를 전달하여 Network 장치의 상태를 조회하고, 관리를 할 수 있도록 하였다.\n해당 시스템에서는 우선 Natural Language로 입력이 들어오면 Intent Engine에서 이를 여러 개의 Intent로 분리한다. 여기서 중요한 것은 실제로 어떻게 Intent를 추출하냐인데, 해당 논문에서는 Intent를 정의하는 것부터 시작한다. 여기서는 Intent를 아래와 같은 형태로 정의한다.\n\n```json\n{\n  \"intent\": {\n    \"request\": \"Create a slice\",\n    \"parameters\": {\n      \"name\": \"nova\",\n      \"user-list\": [\n        {\"imsi\": \"001035432100005\"}\n      ],\n      \"location\": {\n        \"latitude\": 0.0,\n        \"longitude\": 0.0,\n      },\n      \"technology\": [\n        \"AMRISOFT_CELL\",\n        \"SUB6_ACCESS\",\n      ]\n    }\n  }\n}\n```\n\n`request`는 사용자의 요청을 natural language로 표현하고, `parameters`는 해당 요청에 대한 추가적인 정보를 포함한다. 이를 추출하는 과정에서 NLP를 수행했는데 이때는 굉장히 간단히 Wikipedia data를 활용해서 text distance를 측정하는 방식을 활용했다.\n\n## Usecases\n\n1. Network Slice Provisioning  \n   5G에서 서비스마다 다른 QoS를 제공하기 위해서 Network Slicing 기술을 제공하는데 이를 위한 기반을 Intent만 갖고도 적용이 가능하다. 만약, 특정 시스템을 위한 Slicing이 필요하다는 Intent가 들어온다면, 해당 시스템에서는 NLP를 통해서 Intent를 세분화하고, ML을 통해서 더 좋은 Network Configuration을 다시 변경할 수 있도록 구현하였다.\n2. NLoS Identification  \n   Line of Sight(LoS)는 통신 가용성을 평가하는 지표이다. 통신 장비마다 위치에 따라서 LoS가 다르기 때문에 이를 고려해서 통신 장비를 배치해야 한다. 이를 위해서는 NLoS(Non-Line of Sight)를 식별해야 한다. 이 또한, Intent를 통해서 구현할 수 있다. 단순히 각 Line의 상태를 받아서 처리하면 충분하다.\n3. Network Service Provisioning  \n   VNF와 같은 Network Service를 제공할 때에도 특정 요구사항에 맞게 Intent를 입력하면 이를 추출해서 수행할 수 있다.\n\n## Evaluation\n\nEvaluation은 아래와 같은 표가 제시된 것이 끝이다. 실제 예측이나 실행 능력에 대한 지표는 제시하지 않았다. 대신에 이는 충분히 잘 되었고, 이를 실행하는데 걸린 시간만 단순히 제시하였다.\n\n|                               | Network Slice Provisioning | NLoS Identification | Network Service Provisioning |\n| ----------------------------- | -------------------------- | ------------------- | ---------------------------- |\n| Average intent execution time | 3 minutes                  | 1 seconds           | 4 minutes                    |\n\n## Opinion\n\n전체적인 구현은 단순했다. 5G Network를 구성할 수 있는 system을 도입하고, 여기서 Intent를 해석하는 시스템을 도입했는데 이 또한 어려운 구현이 없이 Wikipedia data엤서 단순히 word distance만 활용하여 위에서 제시한 usecase를 모두 커버했다고 논문에서 주장하니 직접 확인해보지 않은 이상 뭐라고 말할 수 있는 것은 없는 거 같다. 대신에 전체 흐름 자체가 Natural Language를 입력 받아서 이를 어떻게 Encoding할 지 그리고, 이렇게 Encoding 된 데이터를 어떻게 시각화할지 그리고 이를 어떻게 실제 네트워크에 반영할지에 대한 힌트 정도는 얻을 수 있었다. 해당 시스템에서는 애초에 Natural Language로 다시 Encoding을 했었다. 이러한 구조도 이해하기 쉬운 구조로 만들기 좋은 거 같다. 그러나 현재 Network Configuration을 적용하는 과정에 대한 설명은 전혀없는데 이에 대한 내용도 추가적으로 찾아보아야할 것 같다.\n\n## Reference\n\n- J. Mcnamara et al., \"NLP Powered Intent Based Network Management for Private 5G Networks,\" in IEEE Access, vol. 11, pp. 36642-36657, 2023, doi: 10.1109/ACCESS.2023.3265894.\n- T. Cogalan et al., \"5G-CLARITY: 5G-Advanced Private Networks Integrating 5GNR, WiFi, and LiFi,\" in IEEE Communications Magazine, vol. 60, no. 2, pp. 73-79, February 2022, doi: 10.1109/MCOM.001.2100615.\n- Thumbnail: Photo by<a href=\"https://unsplash.com/@annikamaria?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Annika Gordon</a> on <a href=\"https://unsplash.com/ko/%EC%82%AC%EC%A7%84/cZISY8ai2iA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n","slug":"nlp-powered-intent-based-network-management","date":"2023-07-14 16:13","title":"NLP Powered Intent Based Network Management for Private 5G Networks","category":"Paper","tags":["IBN","IDN","NLP","5G"],"desc":"해당 논문은 Private 5G Network에서 Intent Based Network 관리를 Natural Language를 통해서 수행할 수 있는지를 보인 논문이다. 해당 논문에서는 Design과 실제 구현 사례를 3개의 usecase에 적용한 사례를 보인다. 해당 논문은 2023년에 publish 되었지만, LLM을 적용하지는 않았고, 이에 대한 적용은 Future Work에서도 고려하고 있다고 언급한다.","thumbnailSrc":"https://euidong.github.io/images/ibn-thumbnail.jpg"},{"content":"\n## Intro\n\nEthereum의 TPS를 Client 단에서 향상 시키기 위한 노력으로, LMPT는 Layered Merkle Partical Trie의 약자이다. 이는 기존 Ethereum에서 사용하던 MPT의 성능 향상을 위하여 제기된 아이디어로 Computer Architecture에서 흔하게 사용되는 cache를 접목한 방법이다. (해당 논문 ICBC 2022의 논문 중 LMPT를 기반으로 한 요약글이다.)\n\n## Terms\n\n- **ERC-20**  \n  Ethereum과 호환이 가능한 token에 대한 표준을 제시한 문서이다. 즉, 이 표준을 만족하는 token은 Ethereum을 통해서 교환이 가능하며 그 반대도 가능하다.\n- **Tether token**  \n  Tether token은 ERC-20에 기반한 대표적인 token으로, 미국 달러와 1:1로 대응하는 USDT로 유명하다. 실제 거래에서도 빈번히 사용되는 ERC-20 token이다.\n- **Trie**  \n  Trie는 sequence로 이루어진 데이터의 빠른 검색을 위해서 만들어진 tree의 일종이다. 데이터를 저장할 때 sequence 데이터의 검색을 최적화하는 것을 목표로 한다. 원리는 다음과 같다. sequence의 검색 시에 sequence의 앞에서부터 맞는 node를 root에서부터 검색하며 찾아나간다. 이 덕분에 검색 시에는 sequence의 길이만큼의 시간이면 충분히 데이터를 찾는 것이 가능하다. 하지만, 저장 sequence를 풀어서 저장하는 방식이기 때문에 경우의 수가 엄청 많아진다. 이는 sequence를 하나의 데이터로 보는 것보다 저장 공간을 많이 차지한다는 단점도 있다.\n- **Patricia Trie**  \n  Trie에서는 기본적으로 모든 sequence를 요소 하나를 node로 보았다면, Patricia Trie에서는 각 nnode가 두 개로 나뉘어진다. branch node, leaf node이다. leaf node는 각 sequence의 끝을 의미하며 각 sequence는 반드시 하나의 leaf node로 종결되어지고, branch node는 저장한 데이터 중에서 중복이 발생하는 경우 중간 지점으로 저장해두는 방식이다. 따라서, Trie에서는 각 노드가 sequence의 하나의 값을 의미했다면, Patricia Trie에서는 path가 sequence의 요소들을 의미한다. 이에 대한 이해를 위해서는 아래에 제시된 그림을 보는 것이 좋을 것이다.(좌. trie, 우. patricia trie)  \n  ![patricia-trie](/images/patricia-trie.png)\n- **Merkle Tree**  \n  Bitcoin에서 사용된 자료구조로 Blockchain의 모든 Block을 저장하는 것은 특정 node에게는 부담이 될 수 있기 때문에 이에 대한 인증을 쉽게 하기 위해서 요약본만을 저장하는 방식이다. 자세한 내용은 [🔗 bitcoin-4](/posts/bitcoin-4)의 2. Merkle Tree 부분에서 자세히 다루었다. 간단히 설명하자면, Block을 serialization하고 hash하여 결과값을 저장한 후 이를 leaf node로 하는 형태의 binary tree를 만드는 것이다. 그렇기에 우리는 Merkle Tree의 hash값 몇개만 갖고도 해당 transaction을 포함하는 block이 유효한지를 파악할 수 있다.  \n- **MPT**  \n  Merkle Patricia Trie의 줄임말로 기존 Bitcoin에서 사용하던 Merkle Tree와 Patricia Trie의 결합을 통해 만들어낸 자료구조이다. 이에 대한 설명은 아래에서 더 자세히 다룬다.\n- **Latency bound issue**  \n  Memory에서 너무 많은 데이터를 얻어오려고 할 때를 의미하는 Memory Bound 중의 하나로 data를 Memory 만으로는 가져올 수 없을 때, secondary storage에서 불러오는데 발생하는 latency를 의미한다.\n\n## Problem\n\nBitcoin에서 시작된 Blockchain에 대한 응용은 의료, 공급망 관리 등으로 확장되며 계속해서 발전되고 있다. 그럼에도 불구하고 아직까지 **transaction의 빠른 처리**는 challenge한 부분으로 남아있다. 이는 P2P 환경에서 안전한 transaction의 생성 및 조회 그리고 검증을 위해서 어쩔 수 없는 trade off로 받아들여졌다. 그 결과 7~30 tps(transaction per second)정도에 그치는 성능을 보여주고 있다. 주류인 중앙 처리 방식은 수 천개의 transaction을 처리하는 것과 비교했을 때에는 굉장히 낮은 수준이다.\n\n이를 해결하기 위해서 다년간 여러가지 접근 방식과 해결책이 제시되었다(AI-gorand, Conflux, Prism, OHIE, etc). 이를 통해서 수 천개의 transaction을 blockchain에서 처리하는 것이 가능하게 되었다. 하지만, 실제로 응용하는데에는 한계가 있었다. 그것은 state를 보관하는 ledger 단에서 발생하는 것이 아닌 실제로 transaction을 처리하는 client 단에서의 문제이다. 이는 바로 **blockchain state를 변경하는 transaction이 빈번하게 발생하는 경우 client 단에서 새로운 bottleneck이 발생한다는 점이다.** 실제로 가장 유명한 Ethereum Client인 GoEthereum과 OpenEtereum에서는 700 tps로 기존 제시된 수 천 transaction보다는 한참 못 미치는 성능을 보여준다.\n\n그 원인은 사실상 state machine이라고 할 수 있는 Ethereum과 이것의 검증을 위해 제안된 MPT의 구조적인 한계로 인해 발생한다(이는 Background에서 제대로 다룰 것이다). 이 구조적인 한계에 의해서 다음과 같은 현상들이 발생한다.\n\n1. key-value 짝으로 이루어지는 데이터의 read/write 연산이 증폭해서 발생한다.\n2. 특히 write operation은 모든 node에 대한 hash를 재계산하도록 한다.\n3. 이러한 동작이 완료되기 전까지 반드시 transaction을 처리하는 thread는 대기해야 한다.\n\n이를 해결하기 위해서, 해당 논문은 LMPT라는 새로운 자료구조를 제시한다. 이는 기존 Ethereum의 MPT를 기반으로 하는 시스템보다 6배 정도 상승된 tps 성능 지표를 보여주고 있다. 이것의 핵심 아이디어는 MPT를 계층화(layer)하는 것이다. 즉, 최근 update된 내용을 별도의 저장공간을 활용하여 저장해두고 이를 우선적으로 활용하기 때문에 더 빠른 처리 성능을 보여주는 것이다.\n\n## Background\n\n> <mark>**1. Ethereum**</mark>\n\nEthereum은 기존 Bitcoin Blockchain System과 확연히 다른점이 있다. 바로 State Machine이라는 점이다. 기존의 Bitcoin에서는 거래 내역을 모두 공개하고, 이를 통해서 우리는 최초 Block에서부터 이 거래 내역을 읽어들이면서 가진 자산을 확인할 수 있다. 즉, 거래 history를 종합해서 결과값을 얻는 것이다. Transaction의 수정과 삭제 없이 계속해서 추가만 이루어지는 형태라고 볼 수 있다. 하지만, Ethereum에서는 Transaction을 State Machine의 상태를 변화시키는 하나의 action으로 받아들인다. 따라서, Transaction에 의해서 우리는 상태가 변화하도록 하는 방식인 것이다. 따라서, 우리는 해당 State만 보고 자신의 자산을 파악할 수 있는 것이다.\n\n> <mark>**2. MPT**</mark>\n\n결국 Ethereum 시스템을 활용하기 위해서는 모든 것이 공개되는 Network 상에서 안전하게 State와 이를 변경하는 Transaction을 보관하는 것이 중요하다. 이러한 data를 무결하게 그러면서도 수정, 삭제, 검색 등이 용이할 수 있도록 하기 위해서 Ethereum에서는 MPT(Merkle Patricia Trie)를 활용한다. 이는 결국 위에서 설명한 Merkle Tree와 마찬가지로 하위 Node의 Hash값을 상위 Node에서 가지기 때문에 Root Hash만을 비교하여 검증을 할 수 있다는 점에서 강점을 가지고 있다.\n\nMPT는 3가지의 Node로 이루어진다.\n\n1. **Leaf Node**  \n   실제로 value를 저장하는 말단 node이다. 만약, path로 key가 모두 표시되지 않았다면, key-end에 남은 key를 모두 담는다.\n2. **Extension Node**  \n   Leaf Node 이외에 경로의 확장이 필요할 때 사용되어지는 Node로 Branch Node의 hash data를 하나로 합치는 등의 역할을 한다.\n3. **Branch Node**  \n   16개의 pointer를 포함하는 Node로 이를 통해서 Leaf, Extension, Branch Node를 가르키는 데 사용할 수 있다.\n\n따라서, 일반적인 구조는 아래와 같다.\n\n![mpt](/images/mpt.png)\n\n이 구조가 가지는 의의는 결국 우리는 하위 node들을 hash한 데이터를 상위 node에서 포함하고 있기 때문에 필요에 따라 trie에 일부분만을 저장해도 data의 검증은 충분히 가능하다는 점이다. 따라서, 모든 data를 가지는 full node와 달리 light client는 더 적은 데이터만 갖고도 검증이 가능한 것이다. 하지만, light client에서 authenticated read(full node의 도움이 필요한 read)를 수행하고자 하는 경우 full node에서는 read를 수행하기 위해서 path를 따라서 읽기를 반복해나가며, leaf node에 있는 최종 value를 얻어와야 한다.\n\n> <mark>**3. Further Observation**</mark>\n\n해당 논문에서는 OpenEthereum Client를 관측하고, 기존 논문들에서 여러 영감을 얻었다. 다음은 이 논문에서 insight를 얻는 데 중요한 역할을 한 관측 정보이다.\n\n1. Transaction이 Blockchain State에 빈번하게 접근할 수록 Transaction의 처리 성능은 낮아진다.\n2. 실제 Transaction의 실행 시간 중에서 가장 많은 시간을 차지하는 것은 Blockchain State에 접근하는 동작(SLOAD, SSTORE)이다.  \n   [🔗 기반 논문(Securing Smart Contract with Runtime Validation)](https://aoli.al/papers/solythesis-pldi20.pdf)\n3. 한 번의 Transaction은 여러 번의 IO을 유발한다.(IO amplication)  \n   MPT 구조에서 하나의 key 조회를 위해서 한 번에 데이터를 찾을 수 없기 때문에 결국 key를 통해서 Trie를 순회하여야 한다.  \n   이는 key에 대응되는 Node가 많을 수록 많은 IO를 요구한다.\n4. Transaction 실행 thread는 병렬적으로 실행되지 않고, 위에서 제시된 operation이 끝날 때까지 대기한다.  \n   즉, Transaction을 처리하는 Thread는 critical path(section)를 지키기 위해서 단 하나만 존재한다는 것이다.\n5. memory cache size를 늘리는 것은 성능향상에 큰 도움이 되지 않는다.  \n   | Memory Cache size(MB) | Hit Rate | TPS  |\n   | :-------------------- | :------- | :--- |\n   | 50                    | 0.635    | 1238 |\n   | 100                   | 0.758    | 1256 |\n   | 500                   | 0.862    | 1278 |\n   | 1000                  | 0.879    | 1292 |\n\n   위의 표를 보면 알 수 있지만, Cache Size를 늘렸을 때 Hit Rate는 늘릴 수 있지만 TPS의 성능 향상 폭은 5% 수준에 그친다. 이는 memory cache를 제대로 사용하지 못하고 있음을 의미한다.\n\n즉, 해당 논문에서는 하나의 Transaction에 의해서 IO가 빈번히 발생하는데 이를 병렬적으로 처리하는 것도 기존 MPT만으로는 한계가 있기 때문에 이를 해결할 수 있는 방법을 제시한다.\n\n## LMPT\n\nLayered Merkle Patricia Trie의 약자로 기존 Ethereum MPT의 한계를 극복하기 위해서 제안하는 자료구조이다. 이의 핵심적인 목표는 Authenticated Ethereum State를 더 효과적으로 저장하는 것이다. 여기서 사용하는 핵심 아이디어는 바로 기존 Computer Architecture에서 사용했던 Hierarchical Memory의 구조를 그대로 차용하는 것이다. 즉, cache로 사용할 수 있는 MPT를 더 구현해두는 것이다. 이는 결론상으로 MPT의 read시에 IO amplication을 효과적으로 줄일 수 있다.\n\n우선 구성 요소는 다음과 같다.\n\n1. **Delta MPT**  \n   Read access가 요청되면 가장 먼저 조회되는 MPT이다.\n2. **Intermediate MPT**  \n   Delta MPT 이후에 조회되는 MPT이다.\n3. **Snapshot MPT**  \n   원본이라고 할 수 있는 MPT이다. 전체 blockchain data를 저장하며, disk에 존재한다.\n4. **Flat KV Store**  \n   read시에 가장 마지막에 조회된다. 이 역시도 전체 blockchain data를 저장하지만, 차이점이라면 key를 path로 하여 조회하는 MPT와 다르게 key, value store형태이다.  \n   그렇기에 snapshot MPT와 동일하게 disk에 존재하지만, 더 빠르다게 조회가 가능하다는 장점이 있다.  \n   snapshot MPT를 조회하는 대신에 이를 통해서 조회를 한다면, key를 통해서 바로 조회할 수 있는 방식이기 때문에 read IO amplication을 효과적으로 줄일 수 있다.\n\n위의 까지는 read시에 최적화를 수행하였다면, write 시에는 이렇게 계층화를 해두었기 때문에 disk에 write하는 동안의 여유가 생길 수 있다. LMPT에서는 MPT의 적은 변화일 경우에는 delta MPT에 저장하고 있다가 periodic checkpoint를 두고, 해당 시점마다 delta MPT의 변경사항은 intermediate MPT, intermediate MPT의 변경 내용은 snapshot MPT에 합친다. 따라서, write 동작은 시간차를 두고 **batch 단위**로 **verification과는 독립적**으로 진행된다. 이는 결국 병렬적으로 IO 작업을 처리할 수 있는 여지를 만들어준다.\n\n> <mark>**[Design] Structure**</mark>\n\n어떻게 실제로 이를 구현했는지에 대한 outline을 제시하면 다음과 같다. (OpenEthereum은 Rust를 이용하기 때문에 LMPT도 Rust에 기반한 code이다. psuedo code이기 때문에 해당 언어를 몰라도 알아볼 수 있을 것이다.)\n\n```rust\nstruct Trie {\n  root: uint256,\n  kv:   Map\n}\nstruct LMPT {\n  delta, interm:  Trie, // In memory\n  snapshot:       Trie, // In Disk\n  flat:           Map   // In Disk\n}\n```\n\n위에서 제시한 전체 구성요소와 마찬가지로 delta, intermedidate, snapshot mpt를 정의하고 flat를 정의한 것을 볼 수 있다.\n\n> <mark>**[Design] Read/Write**</mark>\n\n실제로 Write와 Read는 아래와 같이 수행되어진다. 코드는 논문을 참조하였지만, 설명은 직접 작성하였다.\n\n```rust\nT := LMPT()\n\nfn write_LMPT(k, v) {\n  root := T.delta.put(T.delta.root, k, v) // put new k, v data and get recomputed root hash\n  T.delta.root := root                    // set new root\n}\n\nfn read_LMPT(k, auth_proof) -> <v, p> {\n  // get value and path from delta mpt with key\n  <v, p1> := T.delta.get(T.delta.root, k)\n  // check whether value is exist or not\n  // if exist, then return value and path\n  // if not exist, then we get a adjacent path from delta MPT and store it in p1.\n  if v is present\n    return <v, p1>\n  // get value and path from intermediate mpt with key\n  <v, p2> := T.interm.get(T.interm.root, k)\n  // check whether value is exist or not\n  // if exist, then return value and path(p1 is adjacent path in delta, p2 is real path in intermediate MPT)\n  // if not exist, then we get a adjacent path from intermediate MPT and store it in p2.\n  if v is present\n    return <v, p1 + p2>\n  // if client want authneticated read(when they can't have authenticity), \n  // then request to snapshot MPT and return result\n  // else then request to flat kv store and return result without path.\n  if auth_proof\n    <v, p3> := T.snapshot.get(T.snapshot.root, k)\n    return <v, p1 + p2 + p3>\n  else\n    v := T.flat.get(k)\n    return <v, dummy>\n}\n```\n\n간단하게 요약하자면, 결국 write의 경우에는 다음 절차가 끝인 것이며,\n\n1. key, value를 받아서 delta MPT에 저장한다.\n2. 변경된 root hash를 delta MPT에 적용한다.\n\nread의 경우에는 다음과 같은 절차가 끝인 것이다.\n\n1. delta MPT를 우선 조회한다.\n2. intermediate MPT를 다음으로 조회한다.\n3. 만약, 출처가 확실한 요청인 경우 auth_proof가 필요없으므로, flat KV store를 조회한다.\n4. auth_proof가 필요하다면, snapshot MPT를 조회한다.\n\n> <mark>**[Design] Merge**</mark>\n\nwrite 과정에서는 delta MPT에만 추가를 수행했었다. 아래에서는 실제로 변경사항을 snapshot MPT와 flat KV store에 적용한 것이다.\n\n```rust\n// merge intermediate MPT to snapshot MPT and flat KV store\nfn merge_compute(T) -> (root, flat) {\n  flat := T.flat            \n  root := T.snapshot.root   \n  for <k, v> in T.interm.kv(T.interm.root)\n    root := T.snapshot.append(root, k, v)\n    flat := flat.set(k, v)\n  return (root, flat) \n}\n\n// reconfigure flat, snapshot root, intermediate MPT, delta MPT\nfn merge_update(T, root, flat) {\n  T.flat := flat\n  T.snapshot.root := root\n  T.interm := T.delta\n  T.interm.root := T.delta.root\n  T.delta := Trie()\n  T.delta.root := None\n}\n```\n\n이 과정도 간단하게 요약하자면 다음과 같다.\n\n1. merge_compute, merge_update가 주기적으로 실행되도록 설정한다.\n2. merge_compute에서는 intermediate MPT의 data를 모두 snapshot MPT와 flat KV store에 추가한다.\n3. merge_update에서는 intermediate MPT는 이미 적용이 완료되었기 때문에 delta MPT를 intermediate MPT로 변경한 후, delta MPT를 비어 있는 MPT로 변경한다.\n4. 매 주기가 될 때마다 merge_compute와 merge_update가 순차적으로 실행된다.\n\n위 method가 중요한 점은 바로 transaction의 write/read와 독립적으로 동작할 수 있다는 점이다. 즉, disk에 data를 write 하기 위해서 실제 transaction 처리에 waiting이 필요없다는 것이다.\n\n> <mark>**[Design] Flush**</mark>\n\n위에서 제시한 Method를 다음과 같이 적용함으로서 다음과 같이 Ethereum state를 update하고, 무결하게 유지할 수 있다.\n\n```rust\nblock_cnt := 0\nmerge_interval := 100     // you can set this constant\n\nT := LMPT(genesis_state)  // setup initial state\n\nwhile Block is processing\n  for transaction in Block\n    T.update_trie(transaction)                  // apply transaction to LMPT\n  block_cnt += 1\n  if block_cnt % merge_interval == 0\n    Wait for last spawned thread to end         // wait until previous thread finished.\n    merge_update(T, root, flat)               \n    spawn_thread(root, flat=merge_compute(T))   // make new thread for merging\n```\n\n이 과정의 요약은 다음과 같다.\n\n1. merge_interval를 원하는 값으로 초기화한다. (이 값에 대해서 논문에서는 제시하지 않음)\n2. 초기 genesis_state를 통해 LMPT를 초기화한다.\n3. blockchain의 block을 처리하도록 한다. 새로운 block에 대한 처리도 이에 포함된다.\n4. 그 과정에서 특정 interval이 되면, 기존 merge_compute를 수행하는 thread가 있는지를 확인했다가 background로 merge_compute를 실행하는 thread를 생성한다.\n\n## Evaluation\n\n본 논문에서는 평가를 위해서 다음과 같은 실험을 수행한다.\n\n|       | data structure | workload              |\n| :---- | :------------- | :-------------------- |\n| Test1 | MPT vs LMPT    | simple payment        |\n| Test2 | MPT vs LMPT    | ETC-20 token (Tether) |\n\n여기서 실험 시에 고려한 사항은 다음과 같다.\n\n1. 실제와 같은 상황을 만들기 위해서 실제 Ethereum transaction 500,000개를 활용한 benchmark를 만들었다.\n2. 또한, 또 다른 insight를 얻기 위해서 각 address간의 고르면서도 random하게 transaction을 보내도록 하는 Random senders traces benchmark도 만들었다.\n3. 또한, 간단한 payment의 경우와 복잡한 smart contract에 의해 동작하는 ETC-20 token 중 가장 많이 사용되는 Tether token을 활용한 비교도 수행하였다.\n\n결과적으로 state로 보관해야할 account의 수가 많아질 수록 LMPT와 MPT 사이의 차이를 명확하게 볼 수 있는 형태를 보여준다. 이 차이는 더 복잡한 ETC-20 token에서 더 명확하게 들어나고 성능 차이는 적게는 1.2배에서 6배 이상까지도 벌어지는 것을 확인할 수 있다. 이를 통해서 결과적으로 cache를 활용하여 더 효율적인 Transaction 처리가 가능하다는 것이다.\n\n## Related Work\n\n해당 논문은 결국 Blockchain의 MPT 구현을 변형하여 효율적인 방법을 제시하였다. 이외에도 TPS를 향상시키기 위한 여러 방법이 제시되었다. 하지만, LMPT는 이러한 연구들보다 앞 선 결과를 보여준다. 그 이유를 들기 위해서 다음과 같은 사전 연구와 비교했을 때 어느 점이 좋은지를 밝힌다.\n\n1. **Distributed MPTs**  \n   LMPT와 가장 유사한 연구 사례들로 MPT의 구현을 변경하여 최적화를 하고자 한 사례이다. 하지만, 이들 중에서도 LMPT가 더 우수하다는 것을 다음을 통해서 알 수 있다.\n   - [🔗 mLSM](https://www.usenix.org/conference/hotstorage18/presentation/raju)  \n     MPT 자체를 여러 개의 MPT로 나누어 저장하는 방식을 택하였다. 이를 통해서 read, write 시에 IO amplication을 막을 수 **있었지만**, 오히려 write 시에는 여러 번의 중복 writing으로 인해서 더 많은 비용이 발생하였다.\n   - [🔗 RainBlock](https://www.usenix.org/conference/atc21/presentation/ponnapalli)  \n     MPT를 sharding하여 분배하여 저장하는 방식을 선택하였다. 이를 통해서 성능을 올리는 것이 가능했지만, 이는 전체적인 Ethereum 구현을 바꾸어야 한다. **하지만**, LMPT는 이러한 변경없이도 client단에서 쉽게 구현 및 변경이 가능하다.\n2. **Consensus protocols**  \n   consensus protocol을 변경하여 최적화를 하려는 시도 역시 많았다(HyperLedger Fabric, Prism, etc). **하지만**, 이를 통해서 storage bottleneck을 해결할 수는 없다. 따라서, 해당 연구와 consensus protocol 관련 연구는 상호 보완적인 관계로, 같이 사용하게 되면 더 좋은 성능을 보일 것이라고 기대하고 있다.\n3. **Sharding in Blockchains**  \n   sharding은 Ethereum v2의 architecture 중 하나로 제시된 내용 중 하나로 이를 이용하게 되면 transaction 실행을 분산하는 효과를 불러올 수 있다. 하지만, shard간의 일관성을 유지하기 위한 protocol과 shard간 cross-shard 통신으로 인한 overhead도 고려해야 한다. 이러한 내용을 제쳐두고도 결국은 sharding과 LMPT는 독립적으로 동작할 수 있기 때문에 consensus protocol과 같이 독립적으로 보아도 무관하다.\n\n## Opinion\n\nTPS를 향상 시키기 위한 방법은 Bitcoin이 처음 생기고 나서부터 계속해서 고려되고 있는 문제라고 생각한다. 해당 논문에서는 기존 논문이 였던 mLSM에서 영감을 얻어서 이를 더 발전시킨 방법을 찾았다고 생각한다. 단순히 여러 개의 MPT를 구현하는 것을 넘어서 실제 문제를 명확하게 정의해서 여러 개의 MPT로 어떻게 이를 해결할 수 있는지를 명확하게 제시한 것 같다. 여러 개의 MPT를 통해서 read 과정에서 locality를 확보했을 뿐만 아니라 merge 과정을 write 과정에서 분리함으로서 기존 transaction이 가지던 bottleneck을 해소한 것이다. 이 점이 명확하게 받아들여져서 해당 분야 전공이 아님에도 쉽게 이해할 수 있었다. 하지만, 아쉽게도 실제 구현 code를 찾을 수는 없었기에 flush단계에서 사용된 merge_interval의 정확한 값이 궁금했지만 찾지는 못했다. 또, 이 임계값이 상황에 따라서 성능에 큰 영향을 미치는 요소라고 파악을 하고 있었는데 이를 알 수 없어서 아쉬웠다.\n\n아마 여기서 후속 연구를 진행할 수 있다면, 해당 임계값을 ML, DL, RL을 이용해서 좀 더 효과적으로 적용할 수 있는 방법을 찾아보는 것도 좋은 연구 방향이 될 수 있을 것이다.\n이러한 주제를 처음 접했고, Ethereum의 구현에 대해서도 전혀 아는게 없었기 때문에 다양한 사전 조사를 요구했던 논문이였다. 하지만, Background에 대한 설명도 자세히 나와있고, Idea 자체가 명확했기에 훌륭하고 가독성이 높은 논문이 된 거 같다.\n\n## Reference\n\n- Jemin Andrew Choi, Sidi Mohamed Beillahi, Peilun Li, Andreas Veneris, Fan Long [`\"LMPTs: Eliminating Storage Bottlenecks for Processing Blockchain Transactions\"`](https://ieeexplore.ieee.org/document/9805484/), May 2022\n- Ao Li, Jemin Andrew Choi, Fan Long [`\"Securing Smart Contract with Runtime Validation\"`](https://aoli.al/papers/solythesis-pldi20.pdf), June 2020\n- Tumbnail : Photo by [Shubham Dhage](https://unsplash.com/@theshubhamdhage?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/blockchain?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Merkle Patricia Image : <https://ethereum.stackexchange.com/questions/268/ethereum-block-architecture>\n- Pandian Raju, Soujanya Ponnapalli, Evan Kaminsky, Gilad Oved, and Zachary Keener, University of Texas at Austin; Vijay Chidambaram, University of Texas at Austin and VMware Research; Ittai Abraham, VMware Research, [`mLSM: Making Authenticated Storage Faster in Ethereum`](https://www.usenix.org/conference/hotstorage18/presentation/raju), July 2018\n- Soujanya Ponnapalli, Aashaka Shah, and Souvik Banerjee, University of Texas at Austin; Dahlia Malkhi, Diem Association and Novi Financial; Amy Tai, VMware Research; Vijay Chidambaram, University of Texas at Austin and VMware Research; Michael Wei, VMware Research, [`RainBlock: Faster Transaction Processing in Public Blockchains`](https://www.usenix.org/conference/atc21/presentation/ponnapalli), July 2021\n","slug":"lmpt","date":"2022-10-28 17:17","title":"LMPT","category":"Paper","tags":["Blockchain","Ehtereum","MPT"],"desc":"Ethereum의 TPS를 Client 단에서 향상 시키기 위한 노력으로, LMPT는 Layered Merkle Partical Trie의 약자이다. 이는 기존 Ethereum에서 사용하던 MPT의 성능 향상을 위하여 제기된 아이디어로 Computer Architecture에서 흔하게 사용되는 cache를 접목한 방법이다. (해당 논문 ICBC 2022의 논문 중 LMPT를 기반으로 한 요약글이다.)","thumbnailSrc":"https://euidong.github.io/images/blockchain-thumbnail.jpg"},{"content":"\n## Intro\n\n<https://ieeexplore.ieee.org/document/6838228>에 기재된 `OpenNetMon: Network Monitoring in OpenFlow Software-Defined Networks`의 내용에 대한 리뷰이다. 이름에서 알 수 있듯이 OpenNetMon은 POX OpenFlow Controller를 이용하여 수많은 network component로 이루어진 fine-grained Network에서 Traffic Engineering을 위한 QoS Metric을 flow 단위로 제공하는 것을 목표로 한다. 수집하는 데이터는 **Throughput**, **Packet Loss**, **Delay**를 측정한다.\n\n## Monitoring\n\nMonitoring의 종류 그리고 OpenNetMon 이전의 Monitoring 방법론에 대한 소개를 먼저한다.\n\n> **Type of Monitoring**\n\n세 가지 기준에서 Monitoring 방법들을 구분한다. 그 중에서 첫 번째로 제시된 것이 **Active**, **Passive**이다.\n\n- **Active** : 추가적인 packet을 Network 상에 삽입하여, 해당 packet의 동작을 통해서 측정을 하는 방법으로 과도한 추가 packet은 Network에 영향을 주기 때문에 이에 유의해야 한다. 반대로 너무 적은 packet을 사용하는 경우에도 정확성과 실시간성이 떨어질 수 있다.\n- **Passiv** : 추가적인 packet의 삽입없이 기존 traffic에 대한 관측만으로 측정하는 방법으로 모든 네트워크에 적용할 수 잇는 범용적인 방법은 사실상 불가능하다고 할 수 있으며 각 Network를 위한 전용 Hardware 장비가 필요할 수도 있다. 또한, Synchronization을 위한 별도의 방법 또한 필요하다.(각 Network 장비에서 관측한 Traffic이 서로 대응되는지 확인할 수 있어야 하기 때문)\n\n두 번째는 어떤 Layer에서 Monitoring을 수행하는가이다. Application Layer에서 측정을 하게 된다면, 활용할 수 있는 데이터가 많아지고, 실제 user 입장에서의 최종 데이터를 얻는 것이 가능하다. 하지만, 실제 서비스 제공자 입장에서 모든 End Device에 대한 접근이 불가능한 경우가 많다. 반대로 Network Layer에서 측정을 수행하는 경우에는 사용할 수 있는 데이터가 각 Switch 와 같은 Network 장비에서 packet이 어느 port로 나갔는지와 같은 간단한 정보밖에 사용할 수 없다는 단점이 있었다.\n\n세 번째로 제시되는 것이 `OpenFlow`를 이용하였는가 아니면 이를 사용하지 않았는가이다. `OpenFlow`가 등장하면서 결국 Network Layer에서의 측정이 더 큰 의미를 가지게 된다. 왜냐하면, **Switch에 설정을 미리 주입하거나 새로운 traffic에 대한 정보를 실시간으로 추가할 수 있으며 Flow 단위로 통계치를 얻거나 임의의 packet을 추가하는 등의 작업을 수행할 수 있었기 때문에 Network Layer에서의 더 많은 Monitoring이 가능해졌다.** 이것이 해당 논문이 OpenFlow를 통해서 구현하고자 하는 바이다. Network Layer에서 복잡한 네트워크에서도 보다 정확하고, 실시간의 통계 데이터를 추출할 수 있다는 것이다.\n\n아래는 해당 논문에서 추가로 제시하는 OpenNetMon 이전에 존재했던 Monitoring 방법에 대한 정리이다.\n\n|                 | Active/Passive | Layer   | OpenFlow 여부 | 설명                                                                                                                                                                                                            |\n| :-------------- | :------------- | :------ | :------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| SNMP            | Active         | Network | X             | port 단위의 packet counter 기능 제공 <br/> switch의 통계정보 제공(CPU, RAM)                                                                                                                                     |\n| NetFlow / cSamp | Passive        | Network | X             | switch를 통과하는 n개의 packet마다 임의의 1개의 packet을 선별하여 이를 대표값으로 하여 Network 상태를 측정하지만, 정확도가 많이 떨어질 수 있다.                                                                 |\n| Skitter         | Active         | Network | X             | 전체적인 Network Delay를 측정하기 위해 대표지역에 측정기를 설치하여 실행한 project로, 이를 통해 대략적인 지연을 예측할 수는 있지만 정확하지는 않다.                                                             |\n| IPMON           | Passive        | Network | X             | TCP/IP packet header를 추출하여, timestamp와 함께 중앙 server로 전송하여 이를 분석하는 방식이다. <br /> Passive임에도 Synchronization을 맞추기 위해서 통신은 불가피하며 Network 크기가 커질 수록 비용이 커진다. |\n| OpenTM          | Passive        | Network | O             | 각 Flow의 통계 데이터를 일정 주기로 Query하여 수집한다. Flow의 시작과 끝 지점에서만 측정하는 Last-Switch 방식을 사용한다.                                                                                       |\n| OpenSAFE        | Passive        | Network | O             | SDN의 모든 새로운 Flow가 Controller로 이동하는 것을 이용하여 Monitoring 시스템으로 Controller가 이를 전달하도록 하는 방식이다.                                                                                  |\n| OpenSketch      | -              | -       | -             | Monitoring을 위해 OpenFlow 자체를 재정의하자는 것으로 새로운 Protocol을 정의해서 더 나은 Monitoring을 수행하자는 방법이다. <br /> 하지만, 이렇게 새로운 Protocol을 만드는 것은 Risk가 너무 크다.                |\n\n## OpenNetMon\n\n기존의 Monitoring 방식들은 OpenFlow의 Packet 삽입을 이용한 방식을 제대로 사용하지 않았을 뿐만 아니라 Flow 측정에 있어 최적화에 한계가 있었을 뿐만 아니라 **Throughput**, **Packet Loss**, **Delay**을 통합하고자 하지 않았다. 따라서, OpenNetMon은 이를 수행하는 것을 목표로 한다.\n\n각 측정 요소를 어떻게 수집할 것인지를 각 각 정리한다.\n\n> **1. Throughput**\n\nOpenFlow의 `FlowStatsReq`(Controller에서 각 Switch로 전송하는 Request)를 주기적으로 호출하여 각 Switch에서 전송한 Byte의 양과 각 Flow의 지속 시간을 얻는다. 주기적 호출 싱레는 경로 상의 처음과 끝을 호출하는 Last Switch 방식을 사용하며, 이는 Round Robin이 대규모 네트워크 환경에서 비효율적일 뿐만 아니라 Packet Loss 측정 시에도 Last Switch는 필수적으로 필요한 요소이기에 최적 요소만을 사용하는 것이다.\n\n또한, 이러한 호출 주기는 변동적으로 설정되었는데 이는 Link State Information에 기반한 Routing Discovery 시에 이를 교환 및 동기화하는 것은 Network Throughput에 많은 영향을 미치며 Throughput을 굉장히 큰 범위로 변동되게 한다. 이에 따라 아직 convergence(수렴)이 안된 시점에는 주기를 더 빈번하게 하고, 후에는 이를 되돌리도록 하는 설정이 필요하기에 이 또한 추가되었다.\n\n> **2. Packet Loss**\n\n시작지와 목적지의 Switch에서 해당 Flow에 대한 Packet Counting을 수행하여 계산한다.\n\n> **3. Delay**\n\nPacket을 하나 생성하여 출발 시간과 도착 시간을 통해 측정한다. 따라서, 다음과 같은 식으로 정리할 수 있다. 아래 RTT는 Controller와 각 Switch 간의 RTT를 의미한다.\n\n$$ t_{delay} = (t_{arrival} - t_{sent} - {{1}\\over{2}}({RTT}_{sentSwitch-controller} + {RTT}_{arrivalSwitch-controller})) $$\n\n## Evaluation\n\n실제 실험을 통해서 OpenNetMon에 대한 성능을 테스팅하였다. 이과정에서 총 4개의 OpenFlow를 지원하는 Switch를 일자로 연결하고 각 말단에 Server와 Client를 연결한 후에 각 Switch가 POX OpenFlow Controller에 직접적으로 연결되도록 구성하고, 각 말단과 Server/Client로의 연결은 1Gbps Ethernet로 연결하고, Switch간의 연결은 100Mbps에 1% loss, 1ms delay로 설정을 하였다. 실제 전송 데이터는 video stream traffic을 활용하여 복잡한 traffic 전송을 표현하고자 했다.\n\n실제 그래프와 Topology는 직접적으로 가져오지는 않았다. 아무래도 저작권에 대해서 제대로 알 수 없어서 가져오지 않았다. 하지만, 논문과 같이 비교하면서 보면 도움을 받을 수 있을 것이다.\n\n> **1. Throughput**\n\nApplication Level에서 구현한 `tcpstat`과 비교했을 때, 16KB/s(1.2%) 정도 밖에 차이가 나지 않았다. 하지만, 초기에 17.8%까지 차이가 발생하는데 이는 초기 준비 기간으로 만약 초기 준비 기간을 준다면 해결 가능하다. 또한, 한 번씩 spike가 발생하는데 이는 polling 과정에서 이전 요청보다 현재 요청이 먼저 도착할 때 발생한다. 이를 위해 synchronization을 맞춰주기 위한 로직을 추가한다면 해결이 가능하다. 여기서는 sleep과 mutual exclusion을 이용하기를 권장한다.\n\n> **2. Packet Loss**\n\n정확하지는 않지만 받아들일 수 있을 정도의 오차밖에 발생하지 않는다.\n\n> **3. Delay**\n\nControl Plane에 기반하여 전송하는 방법과 VLAN을 기반으로 Data plane으로 전송하는 법 그리고 실제로 user가 체감하는 delay를 비교했을 때, Control Plane을 그대로 이용하는 경우에는 생각보다 큰 오차가 발생한다. 이는 Controller에서 Software Scheduling 과정에서 발생하는 에러이기에, VLAN을 기반으로 Data Plane을 이용하는 경우에는 실제 데이터와 비교해도 오차가 거의 없는 것을 볼 수 있다.\n\n## Review\n\n해당 논문은 OpenFlow가 나오고 이를 이용해서 Monitoring을 수행하고자 했던 여러 ISP와 연구자들에게 도움을 주었다고 생각한다. 이를 통해서 OpenFlow를 이용한 여러 Monitoring 방식이 연구되었던 것 같다. 그래서 Network 논문임에도 261건 정도의 인용수를 획득한 것으로 보인다. 또한, OpenFlow를 이용한 Monitoring 방식을 사용한다는 것이 흥미로웠고, 기존의 Monitoring 방식을 OpenFlow를 통해서 적용해보는 것이 당시 연구에서 중요했던 포인트였던 것으로 보인다. 새로운 기술에 기존 기술의 아이디어를 접목하는 것도 고려해서 논문을 작성하는 것도 좋은 방법이라고 생각된다. 또한, 구현이 Github로 open되어있기 때문에 신뢰도도 높았던 것으로 보인다. 만약 해당 부분을 좀 더 확인하고 싶다면 인용 논문을 좀 더 탐색해보는 것도 좋은 방법이 될 것 같다.\n\n작성자가 Posting을 작성하는 날짜를 기준으로 261 번의 인용수를 자랑하는 논문을 리뷰한 것이다. 앞으로도 NOMS 논문 중에서 인용수가 많았던 논문들을 위주로 한 번 Review를 진행할 예정이다.\n\n## Reference\n\n- Niels L. M. van Adrichem, Christian Doerr, Fernando A.Kuipers, [`\"OpenNetMon: Network Monitoring in OpenFlow Software-Defined Networks\"`](https://ieeexplore.ieee.org/document/6838228), June 2014\n- Thumbnail : Photo by [Tobias Tullius](https://unsplash.com/@tobiastu?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@tobiastu?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"open-net-mon","date":"2022-06-27 14:02","title":"OpenNetMon","category":"Paper","tags":["SDN","Monitoring","OpenFlow"],"desc":"<https://ieeexplore.ieee.org/document/6838228에 기재된 OpenNetMon: Network Monitoring in OpenFlow Software-Defined Networks의 내용에 대한 리뷰이다. 이름에서 알 수 있듯이 OpenNetMon은 POX OpenFlow Controller를 이용하여 수많은 network component로 이루어진 fine-grained Network에서 Traffic Engineering을 위한 QoS Metric을 flow 단위로 제공하는 것을 목표로 한다. 수집하는 데이터는 Throughput, Packet Loss, Delay를 측정한다.","thumbnailSrc":"https://euidong.github.io/images/monitor.jpg"}],"params":{"subject":"Paper"}},"__N_SSG":true}