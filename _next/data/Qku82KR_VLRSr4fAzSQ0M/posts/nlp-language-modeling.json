{"pageProps":{"post":{"content":"\n## Intro\n\n이제 통계적인 관점에서 NL을 input으로 하는 문제를 해결할 방법을 찾을 것이다. 이를 Language Modeling이라고 하며, 이를 위해서 또는 이를 더 효과적으로 하기 위한 방법들을 소개할 것이다.\n\n## Noisy Channel\n\n일반적으로 우리는 원하는 결과가 있다. 그 결과를 얻기 위해서 우리는 말을 하거나 행동을 하거나 글을 쓴다. 그 과정은 우리가 갖고 싶은 A라는 것을 얻기 위해서 B라는 행동을 대신하는 것과 같다. 즉, 우리는 이를 A에 noise가 껴서 B라는 것이 생성되었다고 생각하는 것이다. 그리고 우리가 관측할 수 있는 것은 B밖에 없는 것이다.\n\n이는 우리가 사용하는 NL에서도 동일하다. 우리가 원하는 결과값 A를 얻기 위해서 우리는 B라는 문장, 음성을 제시한다. 그 결과가 원하는 결과로 될 수 있는 확률을 얻어서 최종 결과를 예측하는 것이 우리의 목표인 것이다.\n\n이 과정을 수식으로 표현하면 다음과 같아진다.\n\n$$\nP(A|B) = {P(B|A)P(A)\\over{P(B)}}\\quad(\\text{Bayes Rule})\n$$\n\n우리가 얻고 싶은 $P(A|B)$ 를 얻기 위해서, $P(A)$와 $P(B|A)$ 를 통해서 구할 수 있는 것이다. 이에 대한 더 자세한 내용은 ML을 이해하는 것이 좋을 것이다. 추천하는 Posting은 [🔗 [ML] Parametric Estimation](/posts/ml-parametric-estimation)이다.\n\n## Language Modeling\n\n결국 우리가 얻고 싶은 것은 특정 문장의 할당된 확률인 것이다. 따라서, Language Model은 input으로 word sequence과 들어왔을 때, 확률을 계산하는 것이다.\n그리고, 이 계산을 수행하기 위해서 필요한 parameter를 찾는 과정을 Language Modeling이라고 한다.\n\n## Input(N-gram)\n\n대게 이러한 모델은 문장 또는 word의 배열이 다음과 같이 주어질 때, $W = w_{1}\\ w_{2}\\ w_{3}\\ ...\\ w_{n}$ 아래와 같은 형태로 표현하는 것이 일반적이다.\n\n- **Single word probability**  \n  하나의 단어의 확률을 나타낼 때 단순하게 아래와 같이 표현한다.  \n  $P(w_{i})\\quad(w_{i} \\in W)$\n- **Sequence of Words probability**  \n  일반적으로 sentence의 확률을 나타낼 때, 여러 문장을 한꺼번에 가지는 확률이므로 아래와 같이 표현하는 것이 일반적이다.  \n  $P(W) = P(w_{1}, w_{2}, w_{3}, ..., w_{n})$\n- **single word probability with context**  \n  일반적으로 우리는 이전에 사용한 단어가 문맥이라고 이해할 수 있다. 따라서, 구체적인 단어들 이후에 특정 단어가 나오는 것은 문맥을 반영한 확률이라고 생각할 수 있다.  \n  $P(W) = P(w_{5}| w_{1}, w_{2}, w_{3}, w_{4})$\n\n위의 식을 보게 되면, 우리는 다시 한번 sentence의 확률을 다시 정리할 수 있다.\n\n$$\nP(W) = P(w_{1}) \\times P(w_{2}|w_{1}) \\times P(w_{3}|w_{1},w_{2}) \\times ... \\times P(w_{n}| w_{1},w_{2},..., w_{n-1})\n$$\n\n위의 식을 보게되면, W가 짧다고 하더라도 굉장히 많은 처리가 필요하고, 저장을 위해 많은 공간이 필요하다는 것을 알 수 있다. 따라서, 우리는 현재 단어를 기준으로 너무 오래된 단어에 대해서는 무시를 하도록 하는 방법을 취하는 것이다.(**Markov Chain**) 이를 \"n 번째까지 허락\"했을 때, 이를 n-gram 이라고 부른다.\n\n$$\np(W) = \\prod_{i=1}^{n}{p(w_{i}|w_{i-n+1},w_{i-n+2},...,w_{i-1})}\n$$\n\n그렇다면, n-gram에서 적절한 n이란 무엇일까? 일반적으로는 n이 크다는 것은 context를 많이 받아들일 수 있다는 의미로 받아들여질 수 있다. 따라서, n이 클수록 성능의 최적화 가능성이 더 높다. 하지만, Vocabulary의 사이즈가 커지는 경우를 예를 들어보자. 여기서는 $|V| = 60$k 라고 해보자.\n\n| n-gram          | p(w_{i})                         | # of parameters   |\n| :-------------- | :------------------------------- | :---------------- |\n| 0-gram(uniform) | ${1\\over\\vert V\\vert}$           | 1                 |\n| 1-gram(unigram) | $p(w_{i})$                       | $6\\times10^4$     |\n| 2-gram(bigram)  | $p(w_{i}\\vert w_{i-1})$          | $3.6\\times10^9$   |\n| 3-gram(trigram) | $p(w_{i}\\vert w_{i-2}, w_{i-1})$ | $2.16\\times10^14$ |\n\nn이 커질 수록 가능한 조합의 수는 굉장히 커지기 때문에 우리가 보지 못하는 경우의 수도 굉장히 증가하게 되어 data자체의 빈도가 적어지는 현상(sparse)이 발생한다. 따라서, 대게의 경우 최대 n의 크기는 3정도로 하는 것이 일반적이다.\n\n```plaintext\n 🤔 주의\n\n 실제 데이터를 가공할 때에는 bigram부터는 문장의 시작과 끝을 표시해주어야 한다. \n 그렇지 않으면, 첫번째 문자의 확률을 구할 때, 이전 단어의 영향을 받을 수 없다.\n 정해진 규칙은 없지만, 대게 <s></s>를 이용한다.\n ex.  bigram : <s> w1 w2 w3 w4 </s>\n     trigram : <s> <s> w1 w2 w3 w4 </s> </s>\n```\n\n```plaintext\n 🤔 Google N-gram\n\n 구글에서 2006년에 N-gram을 직접 구성한 것이 있다. \n 총 1,024,908,257,229개의 단어가 존재하고, 40회 이상 등장하는 5-gram이 1,176,470,663개 존재한다. \n 총 Vocabulary의 size는 200번 이하로 등장하는 것은 제외하면, 13,588,391개이다. \n```\n\n## Estimation\n\nML에서는 Estimation을 수행할 때, continuous하게 추정하였다. 즉, 보지 못한 데이터에 대한 처리를 수행하기 위해서 continuous한 분포의 parameter만 추정하면 되었다. 하지만, NLP에서는 다르다. NL를 continuous하게 표현할 마땅한 방법이 없다. 따라서, 우리는 결국 모든 확률을 discrete하게 구해야 한다. 따라서, 우리는 특정 단어의 확률을 구하는 방법은 단 하나가 된다.\n\n$$\n|T| = \\text{count of observed tokens}\n$$\n$$\nc(w_{i}) = \\text{count of observed } w_{i}\n$$\n$$\n\\begin{align*}\n&P(w_{i}) = {{c(w_{i})}\\over{|T|}} \\\\\n&P(w_{i}| w_{i-1}) = {{c(w_{i-1}, w_{i})}\\over{c(w_{i-1})}} \\\\\n&P(w_{i}| w_{i-2}, w_{i-1}) = {{c(w_{i-2}, w_{i-1}, w_{i})}\\over{c(w_{i-2}, w_{i-1})}} \\\\\n\\end{align*}\n$$\n\n이때, 반드시 sequence의 순서를 유의하도록 하자. 순서가 바뀌면 다른 종류이다.\n\n> **Small Example**\n\n데이터가 다음과 같이 주어진다고 하자.\n\n```plaintext\n He can buy the can of soda.\n```\n\n이때 각 n-gram을 이용한 model의 확률들을 살펴보자.\n\n| model    | probability                                                                                                                                                     |\n| :------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| uni-gram | $p(He)=p(buy)=p(the)=p(of)=p(soda)=p(.) = 0.125$<br /> $p(can)=0.25$                                                                                            |\n| bi-gram  | $p(He\\vert <s>)=p(can\\vert He)=p(the\\vert buy)=p(can\\vert the)=p(soda\\vert of)=p(.\\vert sode) =p(</s> \\vert .) = 1$<br /> $p(buy\\vert can)=p(of\\vert can)= 0.5$ |\n| tri-gram | $p(He\\vert <s>, <s>)=p(can\\vert <s>, He)=p(the\\vert He, buy)=...=p(</s>\\vert ., </s>) =1$                                                                       |\n\n## Evaluation\n\n평가할 때는 ML과 결국은 동일하다. 우리가 확률분포를 구할 때, 사용한 데이터 외에 데이터를 이용해서 잘 적용이 되었는지를 확인할 수 있다. 하지만, word의 갯수와 데이터의 수가 굉장히 많은 NL의 특성상 이 Evaluation 단계에만 굉장히 많은 시간을 소모할 수 있다. 따라서, 즉각적인 평가를 위해서 사용하는 척도가 있다.\n\n> **Perplexity**\n\ntrain set을 통해 학습을 하고, test set을 통해서 평가를 수행할 때, train set을 통해 구한 확률이 실제 test set에서 어느정도의 Entropy를 발생시키는지를 확인하는 것이다. 원래의 식은\n$PP = 2^{H}$이지만, 이를 변형하여 다음과 같이 나타낼 수 있다.\n\n$$\n\\begin{align*}\nPP(W) &= \\sqrt[N]{1 \\over P(w_1, w_2, ..., w_N)} \\\\\n&= \\sqrt[N]{\\prod_{i=1}^{N}{P(w_i|w_{i-n+1}, w_{i-n+2}, ..., w_{i-1})}}\n\\end{align*}\n$$\n\n이를 통해서, 실제로 해당 문제가 너무 어렵지는 않은지, 선택한 model이 잘못되지는 않았는지를 판단한다.\n\n| model    | probability                                                                                                                                                     | entropy |\n| :------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------ |\n| uni-gram | $p(He)=p(buy)=p(the)=p(of)=p(soda)=p(.) = 0.125$<br /> $p(can)=0.25$                                                                                            | 2.75    |\n| bi-gram  | $p(He\\vert <s>)=p(can\\vert He)=p(the\\vert buy)=p(can\\vert the)=p(soda\\vert of)=p(.\\vert sode) =p(</s> \\vert .) = 1$<br /> $p(buy\\vert can)=p(of\\vert can)= 0.5$ | 0.25    |\n| tri-gram | $p(He\\vert <s>, <s>)=p(can\\vert <s>, He)=p(the\\vert He, buy)=...=p(</s>\\vert ., </s>) =1$                                                                       | 0       |\n\n위의 예시를 가져와서 봐보자. 물론 동일한 dataset에서 perplexity를 측정하기는 했지만, n이 커질 수록 점점 entropy가 작아지는 것을 볼 수 있다. 그렇다면, 이런 data가 좋은 걸까? 이는 좋은 게 아니다. 왜냐하면, 해당 dataset에서만 잘 작동하도록 되어있기 때문이다. 일명 **overfitting**이다.\n\n## Smooting\n\n위에서 말한 overfitting을 어느정도 해소할 뿐만 아니라 정말 큰 문제가 될 수 있는 probability가 0이 되는 문제(우리가 trainset을 통해 학습한 확률 분포에서 testset에 들어오는 데이터에 해당하는 확률값이 없을 때, 즉 해당 확률이 0일때)를 해결하기 위해서는 smoothing이 필수적이다. probability가 0이 된다는 것은 후에 위에서 구한 확률로 Prediction을 할 때 모든 예측을 망치는 요인이 된다. 왜냐하면, 추정확률의 최적 Entropy를 의미하는 Cross Entropy를 $\\infin$로 만들기 때문이다. (최적 Entropy가 무한대라는 것은 추정이 불가능하다는 것이다.)\n\n따라서, 우리는 probability가 0이 되지 않게 하는 방법으로 기존의 확률의 일부를 나누어주도록 하는 방법을 제시한다. 이것이 smoothing이다.\n\n이때 반드시 유의할 점은 smoothing을 하건 안하건 각 확률의 총합은 1이 되도록 보장해야 한다는 것이다.\n\n$$\n\\sum_{w \\in \\Omega}p(w) = 1\n$$\n\n대략 6가지의 대표적인 smoothing 방식들을 소개하겠다.\n\n> <mark>**1. Add-1(Laplace)**</mark>\n\n가장 간단한 방법의 smoothing 방법이지만, 실용적인면은 다소 떨어지는 방법이다. 아이디어는 간단하다. prediction을 수행할 때, 현재 들어온 input까지 포함하여 만든 $|V|$를 분모에 더하고, 분자에 1을 더해주는 방법이다. 이 방법을 사용하면, 설사 count가 0이 더라도 확률이 0이 되지는 않는다.\n\n$$\np(w) = {X \\over Y} \\rArr p^{\\prime}(w) = {X + 1 \\over Y + |V|}\n$$\n\n여기서, $|V|$ 값이 정말 헷갈렸는데, 아무도 잘 설명을 안하는 것 같아서 짚고 넘어가면, 우리가 확률값을 얻기 위해서 사용했던 dataset과 현재 prediction을 하기 위해서 들어온 input 둘에서 발생한 모든 unique한 단어의 수를 의미한다. (# of vocabulary)\n\n그렇게 해야만 $\\sum_{w \\in \\Omega}p(w) = 1$을 만족하는 값이 나온다.\n\n따라서, 이를 각 각의 n-gram에 대입하면 다음과 같다.\n\n| n    | $p(w_{i})$                                                 | $p^{\\prime}(w_{i})$                                                            |\n| :--- | :--------------------------------------------------------- | :----------------------------------------------------------------------------- |\n| $1$  | $c(w_{i}) \\over \\vert T \\vert$                             | $c(w_{i}) + 1 \\over \\vert T \\vert + \\vert V \\vert$                             |\n| $2$  | ${{c(w_{i-1}, w_{i})}\\over{c(w_{i-1})}}$                   | ${{c(w_{i-1}, w_{i}) + 1}\\over{c(w_{i-1})} + \\vert V \\vert} $                  |\n| $3$  | ${{c(w_{i-2}, w_{i-1}, w_{i})}\\over{c(w_{i-2}, w_{i-1})}}$ | ${{c(w_{i-2}, w_{i-1}, w_{i}) + 1}\\over{c(w_{i-2}, w_{i-1})} + \\vert V \\vert}$ |\n\n> <mark>**2. Add-k**</mark>\n\n1이라는 숫자가 경우에 따라서는 굉장히 큰 영향을 줄 때가 있다. 특히 기존 데이터의 수가 적은 경우에 더욱 그렇다. 따라서, 이를 해결하기 위해서 1보다 작은 임의의 값(k)을 쓰는 방법이다.\n\n$$\np(w) = {X \\over Y} \\rArr p^{\\prime}(w) = {X + 1 \\over Y + k|V|}\n$$\n\n하지만, 위와 같은 방식은 결국 어떤 확률 값이든지 분자에 1을 더하기 때문에 불평등하게 값을 나눠준다고 할 수 있다. 왜냐하면, 애초에 count(분자)가 큰 데이터에게 1은 별로 영향을 안주겠지만, 분자가 처음부터 작았던 경우에는 이로 인해서 받는 영향이 굉장히 크기 때문이다. 따라서, 이러한 한계점을 극복할 수 있는 방법들이 아래와 같은 방법들이다.\n\n> <mark>**3.Good Turing**</mark>\n\n이를 이해하기 위해서는 우리는 새로운 feature의 데이터를 가져와야 한다. 바로 word의 frequency의 frequency이다.\n\n$$\nN_{k} = \\sum_{i=1}^{n}1[c(w_{i}) = k]\n$$\n\n아마 예시를 봐야 이해가 빠를테니 하나의 예시를 보자.\n\n```plaintext\n sam I am I am sam I do not eat\n```\n\n이 경우 우리는 다음과 같이 count를 구할 수 있다.\n\n$$\n\\begin{align*}\n  &c(I) &=\\ 3 \\\\\n  &c(sam) &=\\ 2\\\\\n  &c(am) &=\\ 2\\\\\n  &c(do) &=\\ 1\\\\\n  &c(not) &=\\ 1\\\\\n  &c(eat) &=\\ 1\\\\\n\\end{align*}\n\\quad\\rArr\\quad\n\\begin{align*}\n  & N_{1} = 3 \\\\\n  & N_{2} = 2 \\\\\n  & N_{3} = 1\n\\end{align*}\n$$\n\n여기서 Good Turing은 한 번도 안본 데이터에게는 한 번만 보는 경우의 수를 전체 경우의 수로 나눈값만큼의 확률을 나누어주고, 기존 데이터들에게는 laplace처럼 1을 더해주는 것이 아니라 비례하는 만큼을 곱해주어 적절한 확률을 가져갈 수 있게하였다.\n\n따라서, 식은 다음과 같다.\n$$\np(w_{i}) = {(c(w_{i})+1)N_{c(w_{i})+1}\\over{N_{c(w_{i})}}} \\times {1\\over |T|},\\quad (N_{0} = 1)\n$$\n\n대게 ${(c(w_{i})+1)N_{c(w_{i})+1}\\over{N_{c(w_{i})}}}$이 부분을 $c^{*}$라고도 부른다.\n\n> <mark>**4. Kneser-Ney**</mark>\n\n가장 널리 쓰이는 Smoothing 방식으로 기억해두는 것이 좋다. 이를 이해하기 위해서는 먼저, Absolute Discounting을 먼저 이해해야 한다.  \nGood-turing 방식을 사용했을 때 $c$와 $c^{*}$사이에 차이가 경험적으로 특정 상수만큼씩 차이가 난다는 것을 발견하여,\n\n$$\nc^{*} = c - d\n$$\n\nChurch과 Gale은 이를 Absolute Discounting 확률이라며 다음 식을 제시한다.\n\n$$\nP(w_{i}|w_{i-1}) = {c(w_{i-1}, w_{i}) -d \\over c(w_{i-1})} + \\lambda(w_{i-1})P(w)\n$$\n\n여기서 뒷에 부분 $\\lambda(w_{i-1})P(w)$은 discounting으로 발생한 오차를 매꾸기 위한 값이다.\n\n여기서 Kneser-Ney problem은 더 넓은 범위로 확장시킬 수 있는 범위로 확장시킨 것이다. 기존에는 bigram으로 제한되어 있던 Absolute Discounting의 식은 다음과 같이 변형된다.\n\n$$\nP_{KN}(w_{i}|w_{i-n+1}^{i-1}) = {\\max(c(w_{i-1}, w_{i}) -d, 0) \\over c(w_{i-n+1}^{i-1})} + \\lambda(w_{i-n+1}^{i-1})P_{KN}(w_{i}|w_{i-n+2}^{i-1})\n$$\n\n(위의 식에 대해서 정확하게 이해를 하지 않았지만, 그렇구나 하고 넘어가도 충분할 것 같다.)\n\n> <mark>**5. Backoff & Interpolation**</mark>\n\n상황에 따라서 unigram, bigram, trigram을 가중치만큼 더해서 사용하는 방식이다. 결국 n-gram에서 n이 작아질 수록 detail을 신경쓸 수 없지만, 신뢰도 자체는 늘어날 수 있다. 따라서, 이를 적절히 섞어쓰면 좋은 결과가 나온다는 이론이다. 하지만, 어떤 것을 더 중점으로 직접 정해주어야 한다.\n\n$$\np^{\\prime}(w_{i}|w_{i-2}, w_{i-1}) = \\lambda_{3}p(w_{i}|w_{i-2}, w_{i-1}) + \\lambda_{2}p(w_{i}|w_{i-1}) + \\lambda_{1}p(w_{i}) + {\\lambda_{0}\\over|V|}\n$$\n\n이를 정할 때는 대게 held-out data를 활용해서 구한다.(validation set이라고 부른다.) 즉, 전체 corpus를 (train, validation, test)로 적절히 나누어 쓰라는 것이다. 그래서 성능을 측정할 때는 testset을 쓰고, $\\lambda$를 추정할 때에는 validation(heldout)set을 사용하라는 것이다.\n\n## Word Class\n\nSmoothing 방식을 이용해서 unseen data를 처리해주었는데 좀 더 효과적으로 이를 처리하는 방법을 고려한 것이다. class단위로 word를 grouping하는 것이다. 그래서 존재하지 않는 단어였다고 하더라도 특정 group에 속한다면, 이를 활용해서 어느정도 확률을 부여할 수 있다는 것이다. 이 방식을 활용하면, 실제로 보지 않은 데이터에 대해서도 현실적인 추정이 가능하지만 detail에 대한 성능은 감소할 수 있다.\n\n$$\np(w_{i}|w_{i-1}) \\rArr p(w_{i}|c_{i-1}) ={ c(c_{i}, w_{i}) \\over c(c_{i-1}) }\n$$\n\n위의 식을 보면, 이전 단어의 context를 보는 것이 아니라 이제는 class를 보고 다음 단어를 probability를 계산하도록 바뀐 것이다. 그리고 이 확률은 class내부에서 해당 단어의 빈도를 이전 class의 빈도로 나누었다고 보면 되겠다.\n\n$$\np(w_{i}|c_{i-1}) = p(w_{i}|c_{i}) \\times p(c_{i}|h_{i})\n$$\n\n즉, class 단위로 단어를 묶고 class에서 단어의 발생 확률에 class에서의 n-gram을 곱한 값이 되는 것이다. 일반적인 Bayes Decison Rule에 기반하여 선택한다고 보면 되겠다.\n\n## Example. Spelling Correction\n\n여태까지 배운 내용을 활용하여 Spelling 오류를 정정해주는 application을 제작한다고 해보자. 먼저, Spelling Error의 종류부터 알아보도록 하자.\n\n- **Non word Error**  \n  잘못된 spelling에 의해서 전혀 뜻이 없는 단어가 만들어진 경우이다.  \n  해결을 위해서는 사전에서 유사한 단어를 찾아서 가장 가능성이 높은 것 또는 이전에 배웠던 shortest edit distance를 찾는 것이다.\n- **Real word Error**  \n  잘못된 spelling 또는 유사한 발음 때문에 뜻이 있는 단어가 만들어졌지만, 오류가 의심되는 경우이다.  \n  해결책은 비슷한 발음 또는 spelling의 모든 단어를 찾아서 해당 단어와 함께 language model에 넣어서 가장 높은 가능성을 가지는 값을 찾는 것이다.\n\n먼저, **Non Word Error** 같은 경우는 오타 데이터에 원래 쓰려고 했던 값을 labeling해서 모아두고 다음 값을 학습시키는 것이다.\n(*x=오타데이터, w=사전에있는단어)\n\n- $P(x|w)$ = x가 w일 가능성\n- $p(w)$ = w의 확률\n\n그리고 나서, 다음을 실행시켜서 가장 적절한 $\\hat{w}$를 찾으면 된다.\n$$\n\\begin{align*}\n\\hat{w} &= \\argmax_{w \\in V}P(w|x) \\\\\n&= \\argmax_{w \\in V}{P(x|w)P(w)}\n\\end{align*}\n$$\n\n**Real Word Error**의 경우에는 결국 이전 단어 sequence를 활용해야 한다. 전체 corpus를 학습해서 tri-gram을 추출해놓고, 번갈아가면서 후보 단어들을 집어넣어서 가장 높은 확률이 나오는 단어를 사용하는 것이다. 예를 들어, 후보 단어가 다음과 같이 정해졌다고 하자. ($\\bold{w}_{3} = {w_3^{(1)}, w_3^{(2)}, w_3^{(3)}, ...}$) 이때 우리가 원하는 w는 다음과 같이 구할 수 있다.\n\n$$\n\\hat{w}_{3} = \\argmax_{w_{3}^{(i)} \\in \\bold{w}_{3}} P(w_{3}^{(i)}| w_{1}, w_{2})\n$$\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-language-modeling","date":"2022-10-21 12:15","title":"[NLP] 3. Language Modeling","category":"AI","tags":["NLP","NoisyChannel","Ngram","LanguageModeling","Smoothing","WordClass"],"desc":"이제 통계적인 관점에서 NL을 input으로 하는 문제를 해결할 방법을 찾을 것이다. 이를 Language Modeling이라고 하며, 이를 위해서 또는 이를 더 효과적으로 하기 위한 방법들을 소개할 것이다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},"relatedPosts":[{"content":"\n## Intro\n\n해당 논문은 Private 5G Network에서 Intent Based Network 관리를 Natural Language를 통해서 수행할 수 있는지를 보인 논문이다. 해당 논문에서는 Design과 실제 구현 사례를 3개의 usecase에 적용한 사례를 보인다. 해당 논문은 2023년에 publish 되었지만, LLM을 적용하지는 않았고, 이에 대한 적용은 Future Work에서도 고려하고 있다고 언급한다.\n\n## Background\n\n### Intent Based Network(IBN)\n\nNon-Public Network(NPN)은 관리자에 의해서 자유롭게 설정이 가능하다는 장점을 갖고 있다. 하지만, 5G를 다룰 수 있는 숙련된 기술자가 아닌 경우 이를 효율적으로 운용하는데에는 한계가 있다. 따라서, 이러한 문제를 해결하기 위해서 **Intent Based Network(IBN)**이 제안되었다. IBN을 활용하면, private 5G network를 네트워크에 대한 지식이 없이도 단순한 의도(Intent)를 작성하는 것만으로 문제를 해결할 수 있다. 즉, IBN의 목표는 Network가 하는 일을 최대한 추상화하여 private network 관리자의 운영을 단순화하는 것이다.\n\n```plaintext\n 🤔 Intent란?\n\n Intent란 사용자의 의도를 의미한다.\n 따라서, 사용자는 자신의 요구사항을 작성할 뿐,\n 이를 어떻게(How) 구현할지는 기술하지 않는다.\n```\n\n### 5G-CLARITY\n\n5G Private nework에 대한 하나의 architecture이다. 이는 이전 논문 \"5G-CLARITY: 5G-Advanced Private Networks Integrating 5GNR, WiFi, and LiFi\"에서 처음 제시된 구조로, 해당 구조에서는 5G New Radio(5GNR)을 포함한 WiFi, LiFi와 같은 Protocol을 지원한다. 이를 통해서 virtual, physical network function들을 모두 설정하고 관리할 수 있는 구조를 제시한다. 해당 architecture는 5G network를 위한 3가지의 구성 요소를 가진다.\n\n1. **Infrastructure Stratum(계층)**  \n   각종 가상화된 장치나 Router, Switch, 통신 단말 등을 실제로 접근할 수 있는 계층으로 이를 통해서 해당 장치를 모니티링하거나 설정 및 관리할 수 있다.\n2. **Virtualised Network and Application function Stratum(계층)**  \n   실제 Network function을 구현한 계층으로 Infrastructure의 Telemetry를 수집하는 역할이나 Virtual Machine(VM)에서 기능핧 가족 기능들을 포함하는 계층이다.\n3. **Management and Orchestration Stratum(계층)**  \n   각 장비와 VM을 관리하고, 이를 통해서 Service를 제공하는 계층이다.\n\n### 기존 연구의 한계\n\n기존에도 Intent를 반영하기 위한 여러 시도가 있었다. 하지만, 해당 논문에서는 현재까지 실제로 NLP를 활용하여 Intent를 추출한 사례는 존재하지 않는다고 말한다. 따라서, 해당 연구에서 핵심 골자는 NLP를 통해서 Intent를 추출할 수 있고, 이를 통해서 Network Management를 수행할 수 있음을 보이는 것이다.\n\n## Design\n\n해당 연구는 기존 Background에서 제시한 5G-CLARITY를 구조에서 나아가 하나의 계층(Intelligence Stratum)을 추가적으로 더하는 것을 제안한다. 해당 계층에서는 Machine Learning 모델을 포함한 ML Engine과 NLP 모델에 기반한 Intent Engeine을 포함하고 있는데, 이를 활용하여 기존 5G-CLARITY 구조에 적절한 Query를 전달하여 Network 장치의 상태를 조회하고, 관리를 할 수 있도록 하였다.\n해당 시스템에서는 우선 Natural Language로 입력이 들어오면 Intent Engine에서 이를 여러 개의 Intent로 분리한다. 여기서 중요한 것은 실제로 어떻게 Intent를 추출하냐인데, 해당 논문에서는 Intent를 정의하는 것부터 시작한다. 여기서는 Intent를 아래와 같은 형태로 정의한다.\n\n```json\n{\n  \"intent\": {\n    \"request\": \"Create a slice\",\n    \"parameters\": {\n      \"name\": \"nova\",\n      \"user-list\": [\n        {\"imsi\": \"001035432100005\"}\n      ],\n      \"location\": {\n        \"latitude\": 0.0,\n        \"longitude\": 0.0,\n      },\n      \"technology\": [\n        \"AMRISOFT_CELL\",\n        \"SUB6_ACCESS\",\n      ]\n    }\n  }\n}\n```\n\n`request`는 사용자의 요청을 natural language로 표현하고, `parameters`는 해당 요청에 대한 추가적인 정보를 포함한다. 이를 추출하는 과정에서 NLP를 수행했는데 이때는 굉장히 간단히 Wikipedia data를 활용해서 text distance를 측정하는 방식을 활용했다.\n\n## Usecases\n\n1. Network Slice Provisioning  \n   5G에서 서비스마다 다른 QoS를 제공하기 위해서 Network Slicing 기술을 제공하는데 이를 위한 기반을 Intent만 갖고도 적용이 가능하다. 만약, 특정 시스템을 위한 Slicing이 필요하다는 Intent가 들어온다면, 해당 시스템에서는 NLP를 통해서 Intent를 세분화하고, ML을 통해서 더 좋은 Network Configuration을 다시 변경할 수 있도록 구현하였다.\n2. NLoS Identification  \n   Line of Sight(LoS)는 통신 가용성을 평가하는 지표이다. 통신 장비마다 위치에 따라서 LoS가 다르기 때문에 이를 고려해서 통신 장비를 배치해야 한다. 이를 위해서는 NLoS(Non-Line of Sight)를 식별해야 한다. 이 또한, Intent를 통해서 구현할 수 있다. 단순히 각 Line의 상태를 받아서 처리하면 충분하다.\n3. Network Service Provisioning  \n   VNF와 같은 Network Service를 제공할 때에도 특정 요구사항에 맞게 Intent를 입력하면 이를 추출해서 수행할 수 있다.\n\n## Evaluation\n\nEvaluation은 아래와 같은 표가 제시된 것이 끝이다. 실제 예측이나 실행 능력에 대한 지표는 제시하지 않았다. 대신에 이는 충분히 잘 되었고, 이를 실행하는데 걸린 시간만 단순히 제시하였다.\n\n|                               | Network Slice Provisioning | NLoS Identification | Network Service Provisioning |\n| ----------------------------- | -------------------------- | ------------------- | ---------------------------- |\n| Average intent execution time | 3 minutes                  | 1 seconds           | 4 minutes                    |\n\n## Opinion\n\n전체적인 구현은 단순했다. 5G Network를 구성할 수 있는 system을 도입하고, 여기서 Intent를 해석하는 시스템을 도입했는데 이 또한 어려운 구현이 없이 Wikipedia data엤서 단순히 word distance만 활용하여 위에서 제시한 usecase를 모두 커버했다고 논문에서 주장하니 직접 확인해보지 않은 이상 뭐라고 말할 수 있는 것은 없는 거 같다. 대신에 전체 흐름 자체가 Natural Language를 입력 받아서 이를 어떻게 Encoding할 지 그리고, 이렇게 Encoding 된 데이터를 어떻게 시각화할지 그리고 이를 어떻게 실제 네트워크에 반영할지에 대한 힌트 정도는 얻을 수 있었다. 해당 시스템에서는 애초에 Natural Language로 다시 Encoding을 했었다. 이러한 구조도 이해하기 쉬운 구조로 만들기 좋은 거 같다. 그러나 현재 Network Configuration을 적용하는 과정에 대한 설명은 전혀없는데 이에 대한 내용도 추가적으로 찾아보아야할 것 같다.\n\n## Reference\n\n- J. Mcnamara et al., \"NLP Powered Intent Based Network Management for Private 5G Networks,\" in IEEE Access, vol. 11, pp. 36642-36657, 2023, doi: 10.1109/ACCESS.2023.3265894.\n- T. Cogalan et al., \"5G-CLARITY: 5G-Advanced Private Networks Integrating 5GNR, WiFi, and LiFi,\" in IEEE Communications Magazine, vol. 60, no. 2, pp. 73-79, February 2022, doi: 10.1109/MCOM.001.2100615.\n- Thumbnail: Photo by<a href=\"https://unsplash.com/@annikamaria?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Annika Gordon</a> on <a href=\"https://unsplash.com/ko/%EC%82%AC%EC%A7%84/cZISY8ai2iA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n","slug":"nlp-powered-intent-based-network-management","date":"2023-07-14 16:13","title":"NLP Powered Intent Based Network Management for Private 5G Networks","category":"Paper","tags":["IBN","IDN","NLP","5G"],"desc":"해당 논문은 Private 5G Network에서 Intent Based Network 관리를 Natural Language를 통해서 수행할 수 있는지를 보인 논문이다. 해당 논문에서는 Design과 실제 구현 사례를 3개의 usecase에 적용한 사례를 보인다. 해당 논문은 2023년에 publish 되었지만, LLM을 적용하지는 않았고, 이에 대한 적용은 Future Work에서도 고려하고 있다고 언급한다.","thumbnailSrc":"https://euidong.github.io/images/ibn-thumbnail.jpg"},{"content":"\n## Intro\n\nNatural Language(자연어, 사람이 사용하는 통상 언어)를 input으로 활용하고자 하는 노력은 컴퓨터의 등장부터 시작하여 여러 번 시도되어 왔다. 지금까지도 완벽하게 이를 처리하는 것은 힘들다. 왜 Natural Language를 다루는 것은 어렵고, 이를 해결하기 위해서 NLP에서는 어떤 방식을 활용할지에 대한 개략적인 overview를 제시한다. 또한, Natural Language의 특성과 분석 단계를 이해하기 위해서 Linguistics(언어학)을 간략하게 정리한다.\n\n## NLP\n\nNatural Language Processing의 약자로 사람이 사용하는 언어를 input으로 하여 원하는 값을 추출해내는 것이 목표이다. 이를 위해서 우리는 사람의 언어를 이해하거나 다룰 수 있는 능력을 컴퓨터에게 부여해야 한다.\n\n먼저, 이러한 필요가 있는 대표적인 usecase를 살펴보면 다음과 같다.\n\n### Usecase\n\n- **Spam Detection**  \n  가장 간단한 예시로 mail에서 spam 여부를 확인하는 기능이다.\n- **POS tagging / NER**  \n  특정 단어 단위의 처리를 수행하게 되는데 단어의 품사와 대략적인 의미를 가진 category로 분류로 tagging하는 과정이다. 이를 기반으로 하여 다른 usecase에서 활용하는 경우가 많다. 품사와 category는 단어의 뜻을 추론하는데 큰 도움을 주며, 이것이 문장의 이해 등에 도움을 주기 때문이다.\n- **Sentiment Analysis**  \n  감정/여론 분석 등의 영역을 의미하며, 텍스트 또는 대화에서의 긍정/부정 여부를 판단하거나 평점 등을 추출하는 기능이다.\n- **Conference Resolution**  \n  \"he\", \"she\" 등 대명사, 생략 단어 등을 원래의 단어로 대체하거나 채우는 과정을 수행한다. 이 역시도 여러 영역에서 이를 기반으로 추가적인 작업을 할 수 있다.  \n- **Word Sense Disambiguation(WSD)**  \n  특정 단어가 주어졌을 때, 동의어, 동음이의어 등에서 가르키는 진짜 의미를 헷갈리지 않게 명확하게 다시 한 번 처리한다. 이 역시도 다른 NLP usecase에서 두루 사용된다.\n- **Parsing**  \n  문장에서 단어들을 의미를 가지는 단위(구, 절, 문장)로 다시 grouping한다. 이 과정을 잘 수행하기 위해서는 이전 단계에서 WSD와 Conference Resolution, POS tagging, NER이 이루어지면 좋다. 이 과정을 통해서 문장의 개략적인 의미를 파악할 수 있다.\n- **Machine Translation(MT)**  \n  특정 언어를 또 다른 Natural Language로 변경하는 기능이다.\n- **Information Extraction(IE)**  \n  특정 문장에서 사용자에게 의미있을만한 데이터를 추출하는 것이다.\n- **Q&A**  \n  특정 사용자가 질문을 하였을 때, 이 뜻을 이해하고, 이에 적절한 대답을 수행하는 방식이다.\n- **Paraphrase**  \n  문장의 뜻을 이해하고, 더 쉬운 형태의 표현으로 변환하는 기능이다.\n- **Summarization**  \n  여러 문장으로 이루어진 글의 의미를 이해하고, 적절한 내용으로 요약하는 기능이다.\n- **Dialog**  \n  Natural Language를 사용하는 사람과 1:1로 담화를 주고 받는 것이다. 의미를 이해할 뿐만 아니라 자신이 내보낼 output에 대해서도 적절하게 생성할 수 있는 능력이 필요하다.\n\n위와 같이 많은 usecase가 있는데 이를 구현하는 것은 지금까지도 굉장히 challenge한 부분이다. 그것은 Natural Language가 가지는 몇몇 특징 때문이다.\n\n### Why is NLP difficult?\n\n여기서는 Natural Language 중에서 영어를 기반으로 한 설명이지만, 한국어도 매우 유사하다.\n\n- **non-standard** : Natural Language를 사용하는 사람들이 표준을 항상 따르지는 않는다는 것이다. 우리는 약어를 사용하거나 문법에 맞지 않는 비문을 사용하여 의사소통을 하기 때문에 이것을  시스템이 이해하게 하는 것은 어렵다.\n- **segmentation** issues : 의미를 가지는 단어 단위로 묶는 것이 어렵다는 것이다. 우리는 문장의 띄어쓰기를 어디로 받아들이냐에 따라서 의미가 달라지는 것을 본 경우가 있을 것이다.\n- **idioms** : 관용구의 사용은 NLP에서 예외처리로 해주어야 하는 것이다. 단어 그대로의 의미와 다른 의미를 가지기 때문이다.\n- **neologisms** : 신조어는 계속해서 생겨나기 때문에 이를 계속해서 업데이트 해주는 것도 부담이 된다.\n- **world knowledge** : 사전 지식을 알고 있어야 이해할 수 있는 단어, 문장이 존재한다. 즉, 어떤 지식을 가지고 있느냐에 따라서 해석이 달라진다는 것이다.\n- **tricky entity names** : 고유 명사 중에서 특히 contents(노래, 그림, 소설) 등의 제목이 해석 시에 헷갈리게 한다. 예를 들면, \"Let it be\"라는 비틀즈의 노래는 문장 중간에 들어가면, 하나의 문장으로 받아들여지게 되는데 이를 잘 해결할 수 있도록 해야 한다.\n\n위의 내용을 요약하자면, 다양한 단어가 다양한 현상과 다양한 법칙(Grammer)의 영향을 받기에 어려우며, 단어가 가지는 모호성이 문제를 야기한다는 것이다.\n\n### Solutions\n\n이러한 문제를 해결하기 위해서 크게 두 가지 방식을 사용할 수 있다.\n\n- Rule based approach  \n  Gammer와 같은 법칙을 모두 적용해서 prgoramming을 하는 것이다. 하지만, 이 방식은 비문과 같은 문장을 제대로 처리할 수 없을 뿐만 아니라 정확한 형태의 문장이라도 여러 의미로 해석되는 문장에서 경향성과 문맥을 전혀 파악할 수 없다.\n- Statistic based approach  \n  그래서 최근에는 경향성과 문맥을 파악할 수 있도록 AI 기술, ML, Deep Learning을 이용하여 NLP를 수행하는 것이 하나의 trend로 자리 잡았다. 그렇다면, 어떻게 통계적인 접근법이 경향성과 문맥을 포함할 수 있을까? 이는 통계가 가지는 경향성이라는 특징과 conditional probability를 사용할 때의 문맥을 포함한 경향성을 파악할 수 있다는 점을 활용해서 가능하다.\n\n## Linguistics\n\n결국 앞으로 통계적인 방식을 활용하더라도 우리는 최소한의 언어학적인 기본이 필요하다. 왜냐하면, 통계에 사용할 데이터를 처리하기 위해서이다. 우리가 사용할 데이터는 text 또는 음성이다. 이를 적절하게 처리하여 통계에 사용할 유의미한 데이터로 변환하는 과정이 필요하다. 이를 위해서 언어학에 대한 이해가 필요한 것이다.\n\n일반적으로 언어를 분석할 때, 사용할 수 있는 도구는 **Grammar**이다. 이는 특정 language에서 허용되는 규칙의 집합을 정리한 것이다. 이것의 종류는 크게 두 가지로 나뉜다.\n\n- **Classic Grammar**  \n  사람이 실제로 언어를 사용함에 있어 발생하는 이상한 습관과 같은 언어 표현이다. 이러한 법칙들은 대게 예제들을 통해서 정의되는데 이런 것을 명확하게 구분할 수 있는 명백한 도구가 존재하지는 않는다. 예를 들면, 감탄사와 같은 것들이 여기에 포함되겠다. 이는 이러한 변칙적인 형태 때문에 programming적으로 표현하는 것이 불가능하다.\n- **Explicit Grammar**  \n  명백하게 정의되어 있는 언어 규칙을 의미한다. 이는 Programm으로 구현할 수 있으며, 여러 Grammar 정리 내용이 이미 정리되어 있다. (CFG, LFG, GPSG, HPSG, ....)  \n  이를 문법적으로 분석하기 위해서 우리는 6단계의 순차적인 처리가 필요하다.\n\n### 6 Layers in Language\n\n각 단계는 input과 output을 가진다. 단계적으로 진행되기 때문에 이전 단계의 output이 다음 단계의 input이 되며, 때때로 몇 단계는 생략될 수 있기에 유연하게 생각하도록 하자.\n\n각 단계에서 실제로 특정 문장이 처리되는 과정을 이해하기 위해서 \"Astronomers saw stars with telescope\"라는 문장이 음성 또는 text로 들어왔을 때를 가정하여 각 단계에는 무엇을 하고 이를 통해서 어떻게 이 문장을 바꿀 수 있는지를 확인해보겠다.\n\n> **1. Phonetics/Orthography(음성학/맞춤법)**\n\n먼저 Orthography는 맞춤법 검사를 의미하며, character sequence로 input이 들어오면, 이를 맞춤법에 맞는지를 확인하여 이것이 수정된 sequence로 반환한다.  \n예시 문장에 있는 \"telescope\"는 문법에 맞지 않으므로 \"telescopes\"로 바뀌어야 한다.\n\n| input                                 | output                                 |\n| :------------------------------------ | :------------------------------------- |\n| Astronomers saw stars with telescope. | Astronomers saw stars with telescopes. |\n\nPhonetics는 음성학을 의미하며, 혀와 음성의 영향을 주는 다양한 근육의 위치 형태, 모양, 빈도를 활용하여 자음과 모음을 분류하는 작업을 수행한다. Orthoography와는 달리 억양이라는 것을 추가적으로 활용할 수 있다.\n\n| input                                                       | output                                 |\n| :---------------------------------------------------------- | :------------------------------------- |\n| Astronomers saw stars with telescopes.를 의미하는 음성 신호 | əsˈtrɒnəməz sɔː stɑːz wɪð ˈtɛlɪskəʊps. |\n\n*<https://tophonetics.com/> 을 통해서 변환하여 얻을 수 있다.\n\n> **2. Phonology/Lexicalization(음운론/어휘화)**\n\nPhonology은 음운론으로 소리와 phonemes(음소)사이의 관계를 이용하여, 음소를 특정 word로 변환하고, Lexicalization에서는 해당 단어를 사전에서의 형태로 변환하는 과정을 수행한다.\n\n| input                                  | output                                 |\n| :------------------------------------- | :------------------------------------- |\n| əsˈtrɒnəməz sɔː stɑːz wɪð ˈtɛlɪskəʊps. | Astronomers saw stars with telescopes. |\n\n> **3. Morphology(어형론)**\n\nMorphology는 어형론으로 음소의 구성을 기본형(lemma)의 형태로 변환하며, 각 단어들을 형태학적인 의미를 갖는 카테고리(category, tag)로 분류한다.\n여기서 사용되는 lemma와 category가 무엇인지 좀 더 자세히 살펴보자.\n\n- **lemma**  \n  - 사전에 표기되는 단어의 기본형으로, 사전에서 word를 찾는 pointer가 된다.  \n  - 동음이의어의 경우 특정 뜻을 지칭하고 싶은 경우에는 numbering을 수행하기도 한다.\n  - 더 나아가서는 형태소(morpeheme)까지 구분하기도 한다. 이는 혼자서 쓰일 수 있는 자립 형태소(root)와 의존 형태소(stem)으로 나눌 수 있다.  \n    - 예를 들면, quotations -> quote[root] + -ation[stem] + -s[stem]\n    - 위와 같은 형태로 세분화할 수도 있지만, 대게는 lemma 단위에서 그친다.\n- **categorizing**  \n  - category는 정하기 나름이며, 이미 정해져있는 tagset들도(Brown, Penn, Multext) 많이 존재하고, 억양이나 실제 분류 등을 수행하는 것도 가능하다.\n  - **POS tagging**은 category를 분류하는 방법 중에서 가장 유명한데, 이는 여러 언어에서 거의 호환되기 때문에 이 방식을 활용하여 분석하는 것이 가장 안정적인 방법이라고 할 수 있다. 이는 별도의 Posting에서 더 자세히 다루도록 하겠다.\n\n또한, 단어의 형태는 언어마다 다양하기 때문에 어느정도 언어마다 다른 작업을 해주어야 한다. 크게 구분되는 형태로 언어를 3개의 종류로 나눌 수 있다.\n\n1. **Analytical Language(고립어)**  \n   하나의 단어가 대게 하나의 morpheme을 가진다. 따라서, 하나 이상의 category로 구분되어질 수 있다.  \n   ex. English, Chinese, Italian\n2. **Inflective Fusional Language(굴절어)**  \n   prefix/suffix/infix가 모두 morpheme에 영향을 미치며, morpheme의 정의 자체가 애매해지는 언어 형태  \n   ex. Czech, Russian, Polish, ...\n3. **Agglutinative Language(교착어)**  \n   하나의 단어에는 morpheme이 명확하게 구분되고, prefix/suffix/infix 또한 명확하게 구분 가능하다. 따라서, 각 morpheme에 명확한 category를 mapping하는 것이 가능하다.  \n   ex. Korean, ...\n\n| input                                  | output(based on Brown tagset)                                         |\n| :------------------------------------- | :-------------------------------------------------------------------- |\n| Astronomers saw stars with telescopes. | (astronomer/NNS) (see/VBD) (star/NNS) (with/IN) (telescope/NNS) (./.) |\n\n> **4. Syntax(통사론)**\n\nlemma나 morpheme을 구문의 요소인 S(Subject, 주어), V(Verb, 동사), O(Object, 목적어)와 같은 요소로 분류한다. 이 분류를 수행할 때에는 문장의 구성요소를 알아야 한다. 이를 bottom-up으로 살펴보자.\n\n- **Word(단어)**  \n  사전에 명시된 하나의 단위라고 볼 수 있다. 이는 관용어(dark horse)를 포함한다.\n- **Phrase(구)**  \n  둘 이상의 단어 또는 구의 결합으로 만들어진다. 대게 하나의 문법적인 의미로 변환되어진다.  \n  - 대표적인 예시\n    - Noun : a new book\n    - Adjective : brand new\n    - Adverbial : so much\n    - Prepositional : in a class\n    - Verb : catch a ball\n  - **Elipse(생략)**  \n    대게 단어 또는 구가 생략되는 경우가 많다. 특히 담화의 경우 더욱 그렇다.  \n    이를 추론을 통해서 추가할 수도 있다.\n- **Clause(절)**  \n  절은 주어와 서술어를 갖춘 하나의 문장과 유사하지만, 문장 요소로서 더 상위 문장에 속하는 경우이다.  \n  또한, 영어에서는 특히 접속사로 연결된 절이 아닌 경우에는 해당 절이 지칭하는 대상이 절 내부에서 생략된다. 이를 gap이라고 한다.\n- **Sentense**  \n  하나 이상의 절로 이루어지고, 영어에서는 시작 시에 대문자로 표기하며 종료 시에는 구분자로 .?!로 끝난다.\n\n결국 우리는 이러한 요소를 적절하게 표시해야 하는데, 이를 위해서 tree 구조를 사용하는 것이 일반적이다. 대표적으로 두 가지의 구조가 있다.\n\n1. **phrase structure(derivation tree)**\n   문장을 기점으로 절, 구, 단어로 top-down으로 내려가는 구조를 가진다.  \n   각 단위를 묶을 때에는 ()를 이용하고, 그 뒤애 해당하는 내용이 무슨 구, 절인지를 표기한다.\n2. **dependency structure**  \n   단어 간의 관계에 더 집중하여 나타낸다. 따라서, 사람이 보기에는 불명확해 보일 수 있지만 특정 usecase에서는 유용하다.\n\n| input                                                                 | output (phrase structure)                                                              |\n| :-------------------------------------------------------------------- | :------------------------------------------------------------------------------------- |\n| (astronomer/NNS) (see/VBD) (star/NNS) (with/IN) (telescope/NNS) (./.) | ((astronomer/NNS)NP ((see/VBD)V ((star/NNS)NP ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S |\n\n![nlp-phase-structure](/images/nlp-phase-structure.jpg)\n\n> **5. Semantics(Meaning, 의미론)**\n\n간단하게는 주어, 목적어와 같은 tag나 \"Agent\"나 \"Effect\"와 같은 tag를 적용하며, 전체적인 의미를 유추해낸다.\n\n| input                                                                                  | output                                                                                                |\n| :------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------- |\n| ((astronomer/NNS)NP ((see/VBD)V ((star/NNS)NP ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S | ((astronomer/NNS)NP/agent ((see/VBD)V ((star/NNS)NP/affected ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S |\n\n> **6. Discourse/Pragmatics(담화/화용론)**\n\n실제 대화 등과 같은 목표를 해결하기 위해서 앞서 보았던 문장 구조를 이용한다.\n\n만약, 해당 데이터를 통해서 하고자 하는 것이 이 이야기를 한 사람이 식당 내부에 있는지를 판단하고자 한다고 가정해보자.\n\n| input                                                                                                 | output |\n| :---------------------------------------------------------------------------------------------------- | :----- |\n| ((astronomer/NNS)NP/agent ((see/VBD)V ((star/NNS)NP/affected ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S | False  |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- text to phonetic converter, <https://tophonetics.com>\n","slug":"nlp-linguistics","date":"2022-10-19 09:03","title":"[NLP] 1. Linguistics","category":"AI","tags":["NLP","Languagistics"],"desc":"Natural Language(자연어, 사람이 사용하는 통상 언어)를 input으로 활용하고자 하는 노력은 컴퓨터의 등장부터 시작하여 여러 번 시도되어 왔다. 지금까지도 완벽하게 이를 처리하는 것은 힘들다. 왜 Natural Language를 다루는 것은 어렵고, 이를 해결하기 위해서 NLP에서는 어떤 방식을 활용할지에 대한 개략적인 overview를 제시한다. 또한, Natural Language의 특성과 분석 단계를 이해하기 위해서 Linguistics(언어학)을 간략하게 정리한다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nNLP를 수행할 때, 우리는 sequence of character를 처리하는 방식을 제대로 알아야 제대로 된 전처리와 후처리 등이 가능하다. 따라서 해당 chapter에서는 어떻게 원본 문자/단어/문장을 처리하는 방식을 다룰 것이다.\n\n## Regular Express\n\n아주 기본적인 문자열 처리 방법이다. 이를 알고 있어야 실질적인 처리가 가능하다. 해당 내용은 별도의 Posting으로 분리하여 다루었다. ([🔗 Regex](/posts/regex))를 살펴보도록 하자.\n\n## Text Normalization\n\n우리가 사용할 NL은 정제되어 있지 않아서 여러 전처리를 수행해야 한다. 그 중에서 대중적으로 좋다고 알려진 방법들을 살펴볼 것이다. 기본적으로는 아래 단계를 처리하는 것이 일반적이다.\n\n1. Word Tokenization  \n   말 그대로 NL 데이터가 입력되었을 때, 이를 단어 단위로 쪼개는 것이다.\n2. Word Reformating  \n   단어를 나누었다면, 각 단어의 형태를 처리하기 쉬운 형태로 Normalizing하는 것이다.  \n3. Sentence Segmentation  \n   문장 단위로 구분해는 과정이다.\n\n이제 각 단계를 세부적으로 다뤄보겠다.\n\n### 1. Word Tokenization\n\n우선 쉽게 생각할 수 있는 것은 단순히 띄어쓰기를 기준으로 구분하는 것이다. 그렇게 하면, 우리는 입력으로 주어진 Corpus에서 token을 추출할 수 있다.\n\n하지만, \"San Francisco\"와 같은 단어가 두 개의 token으로 나누는 것이 아니라 하나의 token으로 처리되기를 원할 수 있다. 뿐만 아니라 일부 언어들(특히 중국어와 일본어)의 경우 띄어쓰기 없이 작성하는 언어들의 경우 문제는 더 커질 수 있다. 이 경우에는 **Word Segmenting**이라는 알고리즘을 활용할 수도 있는데, 원리는 매우 간단하다. 언어의 모든 단어를 포함하는 사전을 기반으로 문장에서 사전에 일치하는 가장 긴 문자열을 찾을 수 있을 때까지 token을 연장해서 만드는 방식이다.\n\n그러나 이 방식도 결국은 특정 언어(중국어, 등)에서는 잘 작동하지만, 일부 언어(영어 등)에서는 잘 작동하지 않는 경우가 많다. 따라서, 최근에는 확률에 기반하여 같이 등장하는 횟수가 많을 경우 하나의 token으로 묶는 형태의 tokenization을 더 선호한다.\n\n이 과정에서 우리가 추가적으로 수행하는 것이 바로 word의 갯수를 추출하는 것이다. 대게 우리가 관심 있어하는 수는 총 3가지이다.\n\n1. **number of tokens**  \n   즉, 띄어쓰기로 나뉘어지는 token들의 총 갯수를 의미한다.\n2. **number of types(Vocabulary)**  \n   띄어쓰기로 나뉘어진 token들의 중복을 제거한 종류들의 갯수를 의미한다. 대게 이러한 type들의 모음을 Vocabulary라고 한다.\n3. **number of each type's tokens**  \n   각 종류의 token이 얼마나 많이 등장했는지를 의미한다.\n\n여기서 이러한 token이나 type이 서로 같나는 것을 어떻게 구분할 수 있을까? 이를 위해서 Word Reformating을 수행하여 좀 더 일반적인 형태로 변형하여 위의 수들을 파악하기도 한다.\n\n### 2. Word Normalization and Stemming(Word Reformating)\n\n대게의 언어는 word의 형태가 여러 개로 존재한다. 이 과정에서 우리가 고려해야 할 것이 정말 많다. 그 중에서 가장 기본적으로 수행되어야 할 내용은 다음과 같다. 해당 내용은 영어에 중심을 둔 설명이다.\n\n1. **Uppercase**  \n   영어에서 첫 글자는 대문자로 시작한다는 규칙이 있다. 또는 강조하고 싶은 단어를 대문자로 표현하기도 한다. 그 결과 token의 종류를 추출하는 과정에서 문제를 일으키기도 한다. 따라서, 이를 모두 lowercase로 바꿔버리는 것이다. 하지만, 모든 경우에 이를 적용할 수 잇는 것은 아니다. 가장 대표적인 예시로 US와 us의 의미가 다르다는 것이다. 또, 고유 명사인 General Motors와 같은 경우도 다르게 처리하는 것이 좋다. 따라서, 이를 고려해서 먼저 처리한 이후에 전체 데이터를 lowercase로 변환하는 방식을 수행한다.  \n2. **Lemmatization**  \n   Lemma(기본형, 사전형)로 단어를 변환하는 것이다. 가장 기본적인 것은 am, are, is와 같은 be동사를 모두 be로 변환하거나 car, cars, car's를 모두 기본형태인 car로 바꾸는 것이다. 대게의 경우에는 이 과정에서 의미를 일부 잃어버리기 때문에 lemma + tag로 기존 token을 복구할 수 있도록 하는 tag를 포함하는 것이 좋다.\n3. **Stemming**  \n   morpheme(형태소)은 중심 의미를 가지는 stem과 핵심 의미는 아니지만 stem에 추가 의미를 더해주는 affixes로 나누어 word를 나눌 수 있다. 따라서, 각 token을 가장 core의 의미를 가지는 stem으로 나타내는 방식이다. 대표적인 예시가 automate, automatic, automation을 automat으로 변환하는 것이다. 이는 lemmatization보다 넓은 범위의 word를 하나로 묶기 때문에 세부의미가 더 손실될 수 있다. 따라서, 기존 의미로 복구할 수 있는 tag를 포함하는 것이 좋다.\n\n### 3. Sentence Segmentation\n\n문장을 구분할 수 있는 도구로 우리는 \"?\", \"!\", \".\"을 활용한다. \"?\"와 \"!\" 같은 경우는 문장의 끝을 의미하는것이 대게 자명하다. 하지만, \".\"은 꽤나 애매할 수 있다. 소수점, Abbreviation(Mr., Dr., Inc.)와 같은 경우에 빈번하게 사용되기 때문이다. 따라서, 이를 판단하기 위해서 Decision Tree를 만들어서 이를 수행한다. 아래와 같이 사람이 직접 규칙을 정할 수도 있지만 현재는 대게 통계 기반으로 수행한다.\n\n![nlp-sentence-segmentation](/images/nlp-sentence-segmentation.jpg)\n\n## Collocation(연어) processing\n\nText Normalization을 통해서 우리는 sentence를 구분하고, word를 추출할 수 있었다. 하지만, 단순히 하나의 word를 기반으로 처리하는 것이 아니라 주변 단어를 활용하여 처리해야만 얻을 수 있는 정보들이 있다. 우리는 이를 Collocation(연어)를 활용하여 수행한다. 이는 특정 단어쌍이 높은 빈도로 같이 붙어 사용되는 현상을 말한다. **\"모든 단어는 이를 동반하는 주변 단어에 의해 특성 지어진다.\"** 따라서, 우리는 이 collocation을 co-ocurrence로 생각할 수 있다. 이를 통해서 우리는 다음과 같은 것들을 할 수 있다.\n\n1. **lexicography(사전 편찬)** : 같가니 유사한 뜻을 가지는 단어는 빈번하게 붙어서 사용되는데 이를 이용해서 하나의 단어의 뜻을 안다면, 이를 통해서 다른 단어의 뜻을 추론하며 확장해나갈 수 있다.\n2. **language modeling** : NL를 통해서 원하는 결과를 얻기 위해서 특정 parameter를 추정해내는 것을 language modeling이라고 하는데 이 과정에서 collocation을 활용하는 것이 단일 단어를 활용하는 것보다 context를 활용할 수 있다는 점에서 장점을 발휘할 수 있다.\n3. **NL generation** : 우리는 문맥상 매끄러운 문장을 원한다. 즉, \"감을 잡다\"를 \"감을 붙잡다\"라고 했을 때, 뜻을 이해할 수는 있지만 어색하다고 느낀다. 따라서, 이 관계를 활용해서 NL을 생성해야 하기 때문에 collocation을 고려해야 한다.\n\n그렇다면, 이러한 Collocation을 어떻게 찾을 수 있을까?\n\n1. **Frequency**  \n   가장 간단하게 단순히 동시 발생 빈도를 확인하는 것이다. 정확한 파악을 위해서는 빈번하게 등장하는 의미 없는 단어를 먼저 filtering할 필요가 있다. 대표적인 예시로 a, the, and 등이 있다.\n2. **Hypothesis Testing**  \n   가설 검증으로 우리가 가정한 collocation을 지정하고, 이 사건이 일어날 가능성을 굉장히 낮게 하는 가설을 반대로 가정한 후에 이것이 불가능하다는 것을 증명하는 Null Hypothesis를 이용한 증명으로 타당성을 확보하는 것이다. 따라서, 우리가 보이고자 하는 것은 word1, word2가 있을 때, 두 단어가 서로 의존적이라는 것을 증명하고 싶은 것이다. 따라서, Null Hypothesis로 두 단어는 독립이다라고 지정하면, 우리는 다음 식을 얻을 수 있다.  \n   $p(w_{1}, w_{2}) = p(w_{1})p(w_{2})$  \n   이를 바탕으로 t-검증을 다음과 같이 수행할 수 있다.  \n   $t = {{p(w_{1}, w_{2}) - p(w_{1})p(w_{2})} \\over \\sqrt{p(w_{1}, w_{2})\\over{N}}} $  \n   t값이 이제 커질 수록 우리는 해당 가설이 틀렸음을 증명하여 collocation임을 주장할 수 있다.\n\n## Minimum Edit Distance\n\n단어 또는 문장 간 유사도를 측정할 때, 사전을 기반으로 수행할 수도 있지만 참고할 corpus가 마땅하지 않거나 더 추가적인 수치가 필요하다면, Minimum Edit Distance로 유사도를 측정하기도 한다. 즉, 두 문자열이 같아지기 위해서 어느정도의 수정이 필요한지를 수치화한 것이다. 여기서 연산은 새로운 문자 추가, 삭제, 대체만 가능하다.\n\n```plaintext\nS - N O W Y  | - S N O W - Y\nS U N N - Y  | S U N - - N Y\ndistance : 3 | distance : 5\n```\n\n다음과 같이 표현이 가능하다.\n\n1. 문자열 x, y가 있을 때, $E(i, j)$는 x의 0\\~i까지를 포함하는 문자열과 y의 0~j까지를 포함하는 문자열의 distance라고 하자.\n2. 이렇게 되면, $E(i,j)$에서 우리는 끝문자의 규칙을 볼 수 있다.\n\n    오른쪽 끝 문자가 가질 수 있는 조합은 3가지 밖에 없다.\n\n    ```plaintext\n    x[i]        | -           | x[i]\n    -           | y[j]        | y[j]\n    distance: 1 | distance: 1 | distance: 0 or 1\n    ```\n\n3. 그렇다면 우리는 하나의 사실을 알게 된다.\n\n    $E(i,j)$는 다음 경우의 수 중 하나여야만 한다.\n\n    - $E(i-1, j) + 1$\n    - $E(i, j-1) + 1$\n    - $E(i-1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )$\n4. 따라서, 다음과 같은 식을 유도할 수 있다.\n\n    $E(i, j) =\\min( \\\\\n      \\quad E(i-1, j) + 1, \\\\\n      \\quad E(i, j-1) + 1,  \\\\\n      \\quad E(i - 1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )\\\\\n    )$\n\n만약, 각 연산의 비용이 다를 경우라면, 1 대신에 그 값을 넣어주면 충분히 풀 수 있으며, 추가적으로 최적의 이동형태를 알고 싶다면, back pointer 하나를 추가하는 것으로 충분하다.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-text-processing","date":"2022-10-19 21:59","title":"[NLP] 2. Text Processing","category":"AI","tags":["NLP","Regex","Tokenization","Collocation","MinimumEditDistance"],"desc":"NLP를 수행할 때, 우리는 sequence of character를 처리하는 방식을 제대로 알아야 제대로 된 전처리와 후처리 등이 가능하다. 따라서 해당 chapter에서는 어떻게 원본 문자/단어/문장을 처리하는 방식을 다룰 것이다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n이전 Posting에서는 sentence의 적절성을 확인한다든지 다음 단어를 유추한다든지 오타를 정정하는 등에 필요한 기본적인 Language Modeling 방식을 살펴보았다. 이번에는 실제로 가장 많이 사용되는 예제인 Classification을 Language Model을 이용하여 어떻게 구현하는지를 다룬다.\n\n## Classification\n\nClassification은 input이 들어왔을 때, 이를 알맞은 분류로 나누는 것이 목표이다. 단순히 Rule에 기반하여 이를 수행할 수도 있지만, Statistic한 Language Modeling을 이용하면, 더 정확도가 높은 분류를 수행할 수 있다. 결국 Statistic Prediction을 수행하기 위해서 우리는 3개(Estimation, Modeling, Evaluation)를 중점적으로 봐야 하는 것은 Classification도 동일하다. 따라서, 이에 대해서 살펴볼 것이고, 그 전에 먼저 Classification Model 의 종류를 살펴보도록 하겠다.\n\n## Generative Model vs Discriminative Model\n\nClassification에서 이용되는 Model을 크게 두가지로 나눌 수 있는데 이에 대해서 먼저 알아보도록 하자.\n\n1. **Generative Model(생성 Model)**\n   1. Naive Bayes\n   2. Hidden Markov Model(HMM)\n2. **Discriminative Model(판별 Model)**\n   1. Logistic Regression\n   2. K Nearest Neighbors\n   3. Support Vector Machine\n   4. Maximum Entropy Model(MaxEnt)\n   5. Neural Network(Deep Learning)\n\n두 Model의 가장 큰 차이점은 추론의 과정이다. 우리가 원하는 데이터 $P(\\text{class}=c | \\text{input} = \\text{data})$(특정 data가 주어졌을 때, 각 class의 속할 확률)를 얻는 과정이 서로 다르다.\n\n**첫 번째**로, $P(\\text{class}=c, \\text{input} = \\text{data})$일 확률을 구하여 **간접적**으로 구하는 방법이다.\n\n$$\n\\begin{align*}\nP(\\text{class}=c | \\text{input} = \\text{data}) &= {{P(\\text{class}=c, \\text{input} = \\text{data})}\\over{P(\\text{input} = \\text{data})}} \\\\\n&\\propto {P(\\text{class}=c, \\text{input} = \\text{data})}\n\\end{align*}\n$$\n\n이런 식으로 생성하여 추론하는 방식을 <mark>Generative Model</mark>이라고 한다. 이 방식은 결국 Conditional Probability를 추론하기 위해서 Joint Probability를 이용하는 방식이기 때문에 어느정도 한계가 존재한다는 점을 유의하자.\n\n**두 번째**로는, $P(\\text{class}=c | \\text{input} = \\text{data})$를 **직접적**으로 구하는 방법이 있다. 이를 위해서, 마친 Conditional Probability를 구한 것과 유사한 효과를 내는 **Discriminant Function(판별 함수)**이라는 특별한 함수를 input에 적용하는 방법이다. 이 함수 중에서 가장 대표적인 것이 Softmax function이다. 우리가 만약 input을 softmax function에 입력하게 되면, 이 값은 [0, 1] 사이의 값으로 표현된다. 이를 통해서 우리는 해당 input이 class인 경우 1에 가깝게, 그렇지 않은 경우 0에 가깝게 표현하여 여러 데이터에 적용하면, class의 inpuut에 따른 분포 양상을 확인할 수 있다. 그리고, 이 분포 양상을 확률로 즉각적으로 표현할 수 있기 때문에 softmax function을 취한 결과가 $P(\\text{class}=c | \\text{input} = \\text{data})$과 비례한다는 결론을 낼 수 있다. 자세한 설명이 필요하다면, [🔗 Logistic Regression](/posts/ml-logistic-regression#Logistic-Regression)을 참고하도록 하자. 이러한 방식을 우리는 <mark>Discriminative Model</mark>이라고 한다.\n\n위에서 제시한 방법들 중 대표적인 방법들은 별도의 Posting을 통해서 정리하였다. 해당 링크를 참조하여 확인해보도록 하자.\n\n- **Generative Model(생성 Model)**\n  - [🔗 Naive Bayes](/posts/nlp-naive-bayes)\n  - [🔗 Hidden Markov Model(HMM)](/posts/nlp-hmm)\n- **Discriminative Model(판별 Model)**\n  - [🔗 Maximum Entropy Model(MaxEnt)](/posts/nlp-maxent)\n  - [🔗 Logistic Regression](/posts/ml-logistic-regression)\n\n## Estimation\n\n어떤 Model을 선택했다고 하더라도 결국 우리가 Class를 결정하는 과정을 동일하다. 위의 과정을 통해서 어찌되었든 다음 값을 찾으면 된다.\n\n$$\n\\begin{align*}\nc^{\\prime} &= \\argmax_{c \\in C}{P(\\text{class}=c | \\text{input} = \\text{data})}\n\\end{align*}\n$$\n\n## Modeling\n\nModel을 만드는 과정, 즉 학습하는 과정은 결국 Model의 구현마다 천차 만별이다. Naive Bayes는 단순하게 data의 word와 count를 활용하고, HMM은 EM algorithm을 활용하며, Linear Regression은 Gradient Descent를 활용한다. 따라서, 여기서는 자세히 다루지 않고 위에서 제시한 링크를 따라가서 각 Model마다의 학습법을 확인해보도록 하자.\n\n## Evaluation\n\nClassification의 성능을 평가하는 것 역시 중요한 일이다. 가장 쉬운 Binary Classification부터 알아보자.\n\nbinary classificaiton의 결과는 아래와 같이 4개 중 하나로 결정된다.\n\n| prediction\\answer | True           | False          |\n| :---------------- | :------------- | :------------- |\n| Positive          | true positive  | false positive |\n| Negative          | false negative | true negative  |\n\n이를 쉽게 이해할려면, 병(코로나)의 양성/음성 판정이 row에 해당하고, 실제 병의 여부를 column으로 생각하면 쉽다. 또한, 각 cell의 값이 헷갈릴 수 있는데, 우리가 원하는 것이 예측의 정확도를 확인하는 것이기 때문에 예측 결과는 그대로 보여주면서, 이것이 틀렸는지 맞았는지를 앞에 true/false로 표현했다고 생각하면 쉽다.\n\nclassification의 성능을 측정하는 지표는 대표적으로 4 가지가 있다.\n\n1. **Accuracy(정확도)**  \n   가장 쉽게 그리고 일반적으로 생각하는 지표다. 위의 표에서는 전체 경우의 수를 더하여 옳게 예측한 것(true postive, true negative)의 합을 나누는 것이다.\n   $tp + fn \\over tp + fp + fn + tn$  \n   하지만, 이 방식은 한계가 있다. 바로, 데이터가 한쪽으로 치우쳐져있을 때이다. 예를 들어, 우리가 진짜를 진짜라고 맞출확률은 높지만, 가짜를 가짜라고 맞출 확률이 낮다고 할 때, 이를 제대로 반영하기가 어렵다. 그런데 데이터에서 진짜가 가짜보다 압도적으로 많을 경우 정확도는 좋은 지표로 쓰기 어렵다는 것이다.\n2. **Precision(정밀도, 정답률)**  \n   쉽게 정답 자체를 맞힐 확률입니다.  \n   $tp \\over tp + fn$\n3. **Recall(재현율)**  \n   예측이 맞을 확률을 의미합니다.  \n   $tp \\over tp + fp$\n4. **F1 Score**  \n   좀 더 세분화된 평가지표이다. 조화 평균에 기반하여 모델의 성능을 정확하게 평가할 때 사용한다.  \n   ${2\\over{{1\\over\\text{Precision}} + {1\\over\\text{Recall}}}} = 2 \\times {\\text{Precision} \\times \\text{Recall} \\over \\text{Precision} + \\text{Recall}}$\n\n여기까지 봤으면, 슬슬 multi class의 경우에는 어떻게 해야할지 궁금할 것이다. 대게 두 가지 방법을 통해서 수행할 수 있다.\n\n> **1. Micro Average**\n\n전체 class를 하나의 binary table로 합치는 것이다. 즉, 클래스가 A, B, C 3개가 있다면, 각 클래스 별로 예측 성공도를 binary로 표시하고, 이를 하나의 테이블로 합치는 것이다. 그 후에는 binary에서 계산하는 식을 그대로사용할 수 있다.  \n\n> **2. Macro Average**\n\nmulti class의 경우에도 별로 다를 것은 없다. 단지 Precision과 Recall 그리고 Accuracy가 어떻게 바뀌는지만 알면 쉽게 이해할 수 있을 것이다.  \n\n| prediction\\answer | c1            | c2            | c3            | c4            |\n| :---------------- | :------------ | :------------ | :------------ | :------------ |\n| c1                | true positive | x             | x             | x             |\n| c2                | x             | true positive | x             | x             |\n| c3                | x             | x             | true positive | x             |\n| c4                | x             | x             | x             | true positive |\n\n- Precision: $c_{ii} \\over \\sum_{j}c_{ij}$\n- Recall: $c_{ii} \\over \\sum_{j}c_{ji}$\n- Accuracy: $c_{ii} \\over \\sum_{i}\\sum_{j}c_{ij}$\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-classification","date":"2022-10-21 13:37","title":"[NLP] 4. Classification","category":"AI","tags":["NLP","Classification","Generative","Discriminative","ModelEvaluation"],"desc":"이전 Posting에서는 sentence의 적절성을 확인한다든지 다음 단어를 유추한다든지 오타를 정정하는 등에 필요한 기본적인 Language Modeling 방식을 살펴보았다. 이번에는 실제로 가장 많이 사용되는 예제인 Classification을 Language Model을 이용하여 어떻게 구현하는지를 다룬다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nNaive Bayes Model은 가장 쉽게 Classification을 수행할 수 있는 Model이지만, 성능이 다른 Model에 비해 뛰어나지는 않다. 그럼에도 Naive Bayes는 가장 기본이 되는 Model이기에 비교 대상으로 많이 사용되고, Classification의 insight를 키우는데 많은 도움을 줄 수 있다. 여기서는, 전반적인 개념과 이를 직접 Spam Filtering에서 어떻게 사용하는지 살펴본다.\n\n## Naive Bayes Model\n\n특정 class에서 해당 데이터가 얼마나 자주 발생되는지와 실제로 해당 class의 빈도를 활용하여, classification을 수행하는 것이다. 우선 이를 수식적으로 표현하기 위해서 다음 변수들을 먼저 정의해보자.\n\n- **documents($D$)**: 여러 개의 Document를 의미하며, 하나의 Document는 대게 여러 개의 words를 포함한다. 각 document는 $d_{i} \\in D$의 형태로 표현한다.\n- **classes($C$)**: class는 두 개 이상을 가진다. 각 클래스는 $c_{i} \\in C$의 형태로 표현된다.\n- **labeled dataset**: 이는 (document($d_{i}$), class($c_{i}$))가 하나씩 mapping된 형태로 존재한다. 우리가 가지는 dataset으로 학습, 평가 시에 사용한다. 대게 평가에 사용되는 데이터는 학습 시에 사용하는 것을 금지하기 때문에 별도로 분리하여 사용한다.\n- **word($w$)**: 하나의 word를 의미하며 NLP 학습 시에 사용하는 가장 작은 단위이다. 대게 document 하나에 있는 단어의 수는 N으로 표기하고, unique한 단어의 수는 V(size of vocabulary)로 표시한다.\n\n따라서, 우리가 찾고자 하는 가장 높은 확률을 가진 class는 다음을 통해서 구할 수 있다.\n\n$$\n\\begin{align*}\nc_{MAP} &= \\argmax_{c \\in C}{P(c|d)} \\\\\n&= \\argmax_{c \\in C}{p(d|c)p(c)\\over p(d)} \\\\\n&= \\argmax_{c \\in C}{p(d|c)p(c)} \\\\\n&= \\argmax_{c \\in C}{p(w_{1}, w_{2}, ... , w_{N} | c)p(c)} \\\\\n&= \\argmax_{c \\in C}{\\prod_{i=1}^{N}p(w_{i}|c)p(c)} \\\\\n&= \\argmax_{c \\in C}{\\log(\\prod_{i=1}^{N}p(w_{i}|c)p(c))} \\\\\n&= \\argmax_{c \\in C}{\\sum_{i=1}^{N}\\log p(w_{i}|c) + \\log{p(c)}} \\\\\n\\end{align*}\n$$\n\n여기서 우리가 language model을 무엇으로 정했는지가 중요하다. 위에서는 uni-gram이라고 가정해서 풀이했지만, bi-gram인 경우 document의 형태가 $d={(w_{1}, w_{2}), (w_{2}, w_{3}), ... , (w_{N-1}, w_{N})}$이다. 따라서, 전체적인 크기와 vocabulary자체도 바뀌게 된다.\n\n즉, 우리는 train set을 통해서 vocabulary를 완성한다. 그리고, 각 word의 count 및 필요에 따라 필요한 word sequence의 count를 수집하여 $p(w_i)$를 구한 후 위에 방법을 통해서 특정 class를 추측할 수 있는 것이다.\n\n이제 구체적인 Naive Bayes의 동작 절차는 Spam Filtering이라는 Case Study를 통해서 자세히 살펴보도록 하자.\n\n## Case Study. Spam Filtering\n\n초기 NLP가 가장 많이 사용되었던 예시 중에 하나이다. 여러 개의 메일에 spam인지 ham인지를 labeling한 데이터를 갖고 후에 input으로 mail 데이터가 들어왔을 때, 이를 filtering하는 것이다. 위에서 살펴보았던 확률을 그대로 적용하면 된다. 예측에 필요한 확률을 습득하고, 예측하는 방법과 이를 평가하는 방법의 순으로 설명하겠다.\n\n### 0. Preprocessing\n\n사실 mail data의 형태가 이상할 수도 있다. Subject부터 시작하여 날짜 데이터 그리고 특수 문자 등이 존재할 수 있는데, 이를 먼저 처리해서 후에 있을 Modeling 단계에서 잘 사용할 수 있도록 형태를 변형해주어야 한다.\n\n[🔗 이전 Posting(Text Processing)](/posts/nlp-text-processing)에서 배웠던 기술들을 활용하여 이를 해결할 수 있다.\n\n대표적으로 해줄 수 있는 작업들은 다음과 같다.\n\n1. 대소문자 통일\n2. alphabet이 하나라도 들어있지 않은 데이터는 삭제\n3. date, 참조 등을 의미하는 데이터 삭제\n\n### 1. Modeling\n\nParameter Estimation / Learning / Modeling 등으로 불리는 단계이다. 일단 우리는 train set으로부터 우리가 원하는 확률을 추출해야 한다. 그 전에 우리가 어떤 language model을 이용할지 선택해야 한다. 먼저 uni-gram인 경우에는 다음과 같은 방법으로 train set이 정의된다.\n$$\n\\text{TrainSet} = {(d_{1}, c_{1}),  (d_{2}, c_{2}), ..., (d_{N}, c_{N})}\n$$\n$$\nd_{i} = \\begin{cases}\n  {w_{1}, w_{2}, ... , w_{M_{i}}} \\quad&\\text{unigram} \\\\\n  {(<s>, w_{1}), (w_{1}, w_{2}), ... , (w_{M_{i}}, </s>)} \\qquad&\\text{bigram}\n\\end{cases}\n$$\n\n이제 우리가 원하는 parameter, 즉 확률은 다음과 같은 데이터이다.\n\n> **unigram**\n\n$$\n\\begin{align*}\np(w_{i}|c_{j}) &= {\\text{count}(w_{i}, c_{j}) \\over \\sum_{w \\in V} \\text{count}(w, c_{j})} \\\\\np(c_{j}) &= {\\sum_{i = 1}^{N}{1[c_{i} = c_{j}]} \\over N}\n\\end{align*}\n$$\n\n> **bigram**\n\n$$\n\\begin{align*}\np(w_{i}|w_{i-1},c_{j}) &= {\\text{count}((w_{i-1}, w_{i}), c_{j}) \\over \\sum_{(w^{(1)}, w^{(2)}) \\in V} \\text{count}((w^{(1)}, w^{(2)}), c_{j})} \\\\\np(c_{j}) &= {\\sum_{i = 1}^{N}{1[c_{i} = c_{j}]} \\over N}\n\\end{align*}\n$$\n\n여기서 우리는 반드시 Smoothing을 해주어야 한다. 왜냐하면, spam mail에서 안 본 단어가 나올 가능성이 너무나 높기 때문이다. 따라서, 실제 $p(w_{i}|c_{j})$는 아래와 같이 변경된다. (간단한 예시를 들기 위해서 Add-1 방식을 사용했다. - 해당 내용이 기억이 나지 않는다면, [🔗 이전 포스팅](/posts/nlp-language-modeling)을 다시 보고 오자.)\n\n$$\np(w_{i}|c_{j}) = {\\text{count}(w_{i}, c_{j}) + 1 \\over \\sum_{w \\in V} \\text{count}(w, c_{j}) + |V|}\n$$\n\n주의할 점은 다시 한 번 강조하지만, $V$는 후에 Estimation에서 input으로 사용하는 단일 document까지 포함한 Vocabulary이다.\n\n### 2. Estimation\n\n이제 우리가 얻은 parameter를 이용해서 실제 input data에 대한 estimation을 수행할 수 있다.\n\n이 경우 다음과 같은 과정을 수행할 수 있다.\n\n$$\n\\hat{c} = \\argmax_{c \\in C} p(c)\\prod_{w \\in d_{\\text{input}}}p(w|c)\n$$\n\n물론 어떤 n-gram을 쓰냐에 따라 $d_{\\text{input}}$도 형태가 달라질 것이다.\n\n### 3. Evaluation\n\n이제 평가를 수행할 것이다. 평가는 우리가 알아봤던 Accuracy와 F1 Score를 추출할 수 있다. Binary Classification이기 때문에 쉽게 구할 수 있을 것이다.\n\n| prediction\\answer | True                                                                       | False                                                                     |\n| :---------------- | :------------------------------------------------------------------------- | :------------------------------------------------------------------------ |\n| Positive          | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} = c, c = \\text{spam}]$    | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} \\neq c, c = \\text{ham}]$ |\n| Negative          | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} \\neq c, c = \\text{spam}]$ | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} = c, c = \\text{ham}]$    |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-naive-bayes","date":"2022-10-21 15:37","title":"[NLP] 5. Naive Bayes","category":"AI","tags":["NLP"],"desc":"Naive Bayes Model은 가장 쉽게 Classification을 수행할 수 있는 Model이지만, 성능이 다른 Model에 비해 뛰어나지는 않다. 그럼에도 Naive Bayes는 가장 기본이 되는 Model이기에 비교 대상으로 많이 사용되고, Classification의 insight를 키우는데 많은 도움을 줄 수 있다. 여기서는, 전반적인 개념과 이를 직접 Spam Filtering에서 어떻게 사용하는지 살펴본다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n이전까지 특정 word를 기반으로 하여 modeling을 수행하는 방법을 알아보았다. 하지만, 우리가 특정 word의 sequence를 통해서 각 word에 대한 classification을 한 번에 하고 싶은 경우는 어떻게 할까?(예를 들어, 각 단어의 품사를 지정하는 일) 일반적으로 각 단어가 특정 해당 class일 확률로 구하는 방법이 일반적일 것이다. 하지만, 문맥을 고려하여 확률을 구할 방법은 없을까? 그 방법은 바로 bigram을 이용하면 될 것이다. 그렇다면, 사실 우리가 사용하는 문맥이 단어 자체보다는 이전 class가 더 영향이 크다면, 이는 어떻게 해야할까? 이를 위한 해결책이 HMM이다. NLP 뿐만 아니라 여러 분야에서 넓게 사용되고 있지만, 여기서는 NLP 분야에서 어떻게 이를 사용하는지를 알아볼 것이다.\n\n## Markov Model\n\nHMM을 알아보기전에 Markov Model을 알아야 한다. 이는 특정 sequence의 확률을 추정하는 방법이다. 즉 우리에게 state sequence ($S= {s_{0}, s_{1}, ..., s_{N}}$)가 주어질 때, 각 state에서 다음 state로 전이(이동)할 확률을 이용해서 state sequence의 확률을 구하는 방법이다.\n\n![nlp-markov-model-1](/images/nlp-markov-model-1.jpg)\n\n위의 그림이 state 각 각에서 다음 state로 전이할 확률을 나타낸 것이라면, 우리는 아래 그림과 같은 그림으로 sequence의 확률을 추론할 수 있는 것이다.\n\n![nlp-markov-model-2](/images/nlp-markov-model-2.jpg)\n\n따라서, 위의 그림에서 우리가 만약 $(s_{0}, s_{1}, s_{0}, s_{2})$으로 이루어진 sequence의 확률을 얻기를 바란다면, 그 확률은 아래와 같아진다.\n$$\n\\begin{align*}\np(s_{0}, s_{1}, s_{0}, s_{2}) &= p(s_{0}| \\text{start}) \\times p(s_{1}|s_{0}) \\times p(s_{0}|s_{1}) \\times p(s_{2}|s_{1}) \\times p(end|s_{2}) \\\\\n&= \\pi_{0} \\times p_{01} \\times p_{10} \\times p_{12} \\times 1\n\\end{align*}\n$$\n\n이를 잘 살펴보니 bigram에서의 Likelihood를 구하는 공식과 똑같다. 즉, state 각 각을 word라고 본다면, Markov Model을 통해서 구할 수 있는 확률은 bigram의 Likelihood인 것이다.\n\n그리고 이를 일반화하면 다음과 같다.\n\n$$\np(seq) = \\prod_{i=1}^{N}p(seq_{i}|seq_{i-1})\n$$\n\n그런데, 여기서 n이 3 이상인 ngram을 적용하고 싶다면, 각 state를 n-1 gram으로 설정하면 된다.\n\n$$\n\\begin{align*}\nX_{i} &= (Q_{i-1}, Q_{i}) \\text{라면, }\\\\\nP(X_{i} | X_{i-1}) &= P(Q_{i-1}, Q_{i} | Q_{i-2}, Q_{i-1}) \\\\\n&= P(Q_{i} | Q_{i-2}, Q_{i-1})\n\\end{align*}\n$$\n\n따라서, trigram을 적용해보면 아래와 같다.\n\n$$\n\\begin{align*}\np((start, w_{0}), (w_{0}, w_{1}), (w_{1}, w_{0}), (w_{0}, w_{2})) &= p(w_{0}| \\text{start}, \\text{start}) \\times p(w_{1}|\\text{start}, w_{0}) \\times p(w_{0}|w_{0}, w_{1}) \\times p(w_{2}|w_{1}, w_{0}) \\times p(end|w_{0}, w_{2}) \\\\\n&= \\pi_{0} \\times p_{01} \\times p_{10} \\times p_{12} \\times 1\n\\end{align*}\n$$\n\n## Hidden Markov Model\n\nHidden Markov Model은 state를 하나 더 만든다는 것이 핵심이다. 그래서, 우리가 직접 관측하는 state(**observed state**)와 직접적으로 관측하지 않지만, 관측한 state들에 의존하는 state(**hidden state**) 총 두 개의 state를 사용한다. 일반적인 예시가 text가 입력되었을 때 우리는 각 단어를 observed state라고 한다면, 각 단어의 품사를 hidden state라고 정의할 수 있다.\n\n![nlp-markov-model-3](/images/nlp-markov-model-3.jpg)\n\n위의 예시는 우리가 관측하는 데이터($O$)가 3개의 state를 가지고, 이 사건에 의존적인 또 다른 사건($H$)이 3개의 state를 가지는 경우이다. 이를 이용해서 기존 Markov Model보다 복잡한 작업을 수행하는 것이 가능하다.\n\n### Estimation\n\n우리가 할 수 있는 작업은 크게 두 가지이다. 일반적인 Markov Model에서 할 수 있던 방식이 **Trellis** 방식이고, 또 다른 방식이 **Viterbi** 방식이다.\n\n1. $(o_{0}, o_{1}, o_{0}, o_{2})$의 확률이 궁금할 때(**Trellis**)\n2. $(o_{0}, o_{1}, o_{0}, o_{2})$가 주어질 때, 이것의 hidden state의 sequence 중 가장 유력한 sequence를 찾고자할 때(**Viterbi**)\n\n위의 경우를 각각 풀어보도록 하자.\n\n> <mark>**1. Trellis**</mark>\n\n우리가 직접 관측한 데이터의 sequence 자체의 확률이 궁금할 때이다. 따라서, 이에 대한 분석은 $(o_{0}, o_{1}, o_{0}, o_{2})$의 확률을 분석해보면서 설명하겠다.\n\n$$\n\\begin{align*}\np(o_{0}, o_{1}, o_{0}, o_{2}) &= p(o_{0}, o_{1}, o_{0}) \\times p(o_{2} | o_{0}, o_{1}, o_{0}) \\\\\n&= p(o_{0}, o_{1}, o_{0}) \\times \\{p(o_{2} | h_{0})p(h_{0} | o_{0}, o_{1}, o_{0}) + p(o_{2} | h_{1})p(h_{1} | o_{0}, o_{1}, o_{0}) + p(o_{2} | h_{2})p(h_{2} | o_{0}, o_{1}, o_{0})\\} \\\\\n&= p(o_{0}, o_{1}, o_{0}) \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n&= p(o_{0}, o_{1}) \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n&= p(o_{0}) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n&= \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) }\n\\end{align*}\n$$\n\n이를 그림으로 표현하면 다음과 같다.\n\n![nlp-hidden-markov-model-1](/images/nlp-hidden-markov-model-1.jpg)\n\n또한, 이 식을 다음과 같이 축소가 가능하다.\n\n$$\n\\begin{align*}\n  &\\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}\\alpha_{0 i} \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}{\\alpha_{1 i} } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}{\\alpha_{2 i} } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}{\\alpha_{3 i} }\n\\end{align*}\n$$\n\n우리는 이를 통해서, Markov Model의 특징을 하나 배울 수 있다. 그것은 바로 복잡한 sequence 전체의 확률에서 벗어나서 바로 직전의 확률값만 으로 다음 확률을 추론할 수 있다는 것이다. 이것이 Markov Chain이라는 이론이고, 이를 이용했기 때문에 Markov Model라고 부르는 것이기도 하다.\n\n따라서, $\\alpha$는 다음과 같이 정의할 수 있다.\n\n$$\n\\alpha(t, i) = \\sum_{k=1}^{N}{\\alpha(t-1, k)p(h_{i}|h_{k})p(o = s_{t}|h_{i})} \\quad (s_{t} = \\text{input으로 들어온 sequence의 t번째 값})\n$$\n\n또, 이를 반대로 할 경우에는 다음과 같은 식을 얻을 수 있다.\n\n![nlp-hidden-markov-model-2](/images/nlp-hidden-markov-model-2.jpg)\n\n$$\n\\begin{align*}\n  &\\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{\\beta_{3i}} \\\\\n  =& \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{\\beta_{2i}} \\\\\n  =& \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{\\beta_{1i}} \\\\\n  =& \\sum_{i=0}^{2}{\\beta_{0i}} \\\\\n\\end{align*}\n$$\n\n$$\n\\beta(t, i) = \\sum_{k=1}^{N}{\\beta(t+1, k)p(h_{k}|h_{i})p(o = s_{t}|h_{i})} \\quad (s_{t} = \\text{input으로 들어온 sequence의 t번째 값})\n$$\n\n위의 처럼 앞에서부터 풀이를 해나가면서, $\\alpha$의 합으로 끝이 나도록 푸는 방법을 forwarding 방식이라하고, 반대로 뒤에서부터 풀이하면서 $\\beta$의 합으로 푸는 방법을 backwarding 방식이라고 한다. 사실 이 경우는 HMM이 굳이 아니더라도, MM으로 구할 수 있으니 굳이 필요는 없다. 하지만, 이것은 후에 modeling 단계에서 사용하기 때문에 알아두어야 한다.\n\n> <mark>**2. Viterbi**</mark>\n\n이는 observed state의 sequence에 의해서 파생되는 가장 적절한 hidden sequence를 구하는 것이 목표이다. 이를 통해서 할 수 있는 대표적인 것이 sequence classification이다.\n\n그렇다면 가장 유력한 hidden state의 sequence를 $\\hat{s}^{(H)}$라고 하자. 이는 다음과 같다.\n\n$$\n\\begin{align*}\n\\hat{s}^{(H)} &= \\argmax_{s^{(H)} \\in S^{(H)}}P(s^{(H)}|s^{(O)}) \\\\\n&= \\argmax_{s^{(H)} \\in S^{(H)}}P(s^{(O)}|s^{(H)})P(s^{(H)}) \\\\\n&= \\argmax_{{h_{1}, h_{2}, ..., h_{N}} \\in S^{(H)}}\\underbrace{P(o_{1}, o_{2}, ... , o_{N}|h_{1}, h_{2}, ... , h_{N})}_{\\text{Markov Model}}\\underbrace{P(h_{1}, h_{2}, ... , h_{N})}_{\\text{Markov Model}} \\\\\n&= \\argmax_{{h_{1}, h_{2}, ..., h_{N}} \\in S^{(H)}}\\prod_{i=1}^{N}p(o_{i}|h_{i})p(h_{i}|h_{i-1})\n\\end{align*}\n$$\n\n![nlp-hidden-markov-model-3](/images/nlp-hidden-markov-model-3.jpg)\n\n즉, 각 layer에서 단 하나의 가장 큰 output만 살아남을 수 있게 되는 것이다. 이 과정이 사실상 HMM의 본질적인 목표이다. sequence를 입력해서 sequence 형태의 classification 결과를 얻는 것이다.\n\n### Modeling\n\n여태까지 HMM을 활용하여 sequential class를 어떻게 estimation 하는지 알아보았다. 그렇다면, 이제는 이를 위해서 사용되는 확률값을 구해야한다. 필요한 확률값은 다음과 같다.\n\n- $p(h_{i}|h_{i-1})$ : Hidden State에서 Hidden State로 넘어가기 위한 확률이다.\n- $p(o_{i}|h_{i})$ : 방출 확률로 특정 Hidden State에서 다음 State의 Observed State로 넘어가는 방법이다.\n- $\\pi_{i}$\n\nTrelli 방식에서 만들었던, $\\alpha$와 $\\beta$의 의미를 이해해야 한다. 각 각은 해당 과정까지 오면서 누적해온 확률이라고 할 수 있다. 그리고, 우리가 원하는 것은 입력으로 주어진 데이터를 잘 반영할 수 있는 확률 값을 찾는 것이다. 그렇다면, 우리가 생각할 수 있는 방법은 평균을 활용하는 것이다. 이를 구하는 과정을 먼저 살펴보자.\n\n$$\n\\begin{align*}\n  c(i, j, k) &= h_{i}\\text{에서 } h_{j}\\text{로 넘어가고, } o_{k}\\text{가 관측될 확률의 합} \\\\\n  &= \\sum_{t=2}^{T} \\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j) \\\\\n  \\\\\n  c(i,j) &= h_{i}\\text{에서 } h_{j}\\text{로 넘어갈 확률의 합} \\\\\n  &= \\sum_{k=1}^{K}\\sum_{t=2}^{T}{\\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j)} \\\\\n  \\\\\n  c(i) &= h_{i}\\text{에서 상태를 변경하는 확률의 합} \\\\\n  &= \\sum_{j=1}^{N}\\sum_{k=1}^{K}\\sum_{t=2}^{T}{\\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j)} \\\\\n\\end{align*}\n$$\n\n위의 값을 통해서 우리는 우리가 가지고 있던 확률을 업데이트할 수 있다.\n\n$$\n\\begin{align*}\np(h_{j}|h_{i}) &= {c(i,j)\\over c(i)} \\\\\np(o_{k}|h_{i}) &= {c(i,j,k)\\over c(i,j)}\n\\end{align*}\n$$\n\n즉, 우리는 다음 과정을 수행하여 Modeling을 수행할 수 있는 것이다.\n\n1. 초기값 ($p(h_{i}|h_{i-1})$, $p(o_{i}|h_{i})$, $\\pi_{i}$)을 초기화 한다.  \n2. Trelli를 통해서 $\\alpha$, $\\beta$를 계산한다.\n3. $p(h_{i}|h_{i-1})$, $p(o_{i}|h_{i})$를 업데이트 한다.  \n   ($pi_{i}$같은 경우는 발생 빈도로 업데이트 한다.)\n4. 임계치에 도달할 때까지 2,3번을 반복한다.\n\n이 과정을 대게 10번 정도만 하면 수렴하게 되고, 이를 확률로 사용하는 것이다.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-hmm","date":"2022-10-21 21:55","title":"[NLP] 6. Hidden Markov Model","category":"AI","tags":["NLP","MarkovModel","HMM","HiddenMarkovModel"],"desc":"이전까지 특정 word를 기반으로 하여 modeling을 수행하는 방법을 알아보았다. 하지만, 우리가 특정 word의 sequence를 통해서 각 word에 대한 classification을 한 번에 하고 싶은 경우는 어떻게 할까?(예를 들어, 각 단어의 품사를 지정하는 일) 일반적으로 각 단어가 특정 해당 class일 확률로 구하는 방법이 일반적일 것이다. 하지만, 문맥을 고려하여 확률을 구할 방법은 없을까? 그 방법은 바로 bigram을 이용하면 될 것이다. 그렇다면, 사실 우리가 사용하는 문맥이 단어 자체보다는 이전 class가 더 영향이 크다면, 이는 어떻게 해야할까? 이를 위한 해결책이 HMM이다. NLP 뿐만 아니라 여러 분야에서 넓게 사용되고 있지만, 여기서는 NLP 분야에서 어떻게 이를 사용하는지를 알아볼 것이다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n해당 Posting에서는 Maximum Entropy를 이용하여 최적의 parameter를 찾아나가는 Machine Learning 접근법에 기반한 NLP 방식을 제안한다. 이를 위해서 NL를 수학적인 형태로 변형하기 위한 기술 중 하나인 word2vec에 대한 설명도 같이 진행한다.\n\n## MaxEnt Model\n\nMaximum Entropy Model(MEM)의 약자로, 이것의 의미는 주어진 dataset을 표현할 수 있는 가장 적절한 분포는 Prior Knowledge를 만족하는 분포들 중에서 가장 높은 Entropy를 가지는 분포라는 것이다.  \n\n이는 다음과 같은 관측에 의해서 정의된 것이다.\n\n1. 다양한 물리현상들은 시간이 지남에 따라 Entropy를 최대화하려는 방향으로 이동하려는 경향이 있다.\n2. 더 적은 수의 논리로 설명이 가능한 경우, 많은 수의 논리를 세우지 말라 (오컴의 면도날)\n\n다소 억지같아 보이는 논리일지라도 후에 가서 살펴보면, Machine Learning의 Logistic Regression에 연결되는 것을 볼 수 있다. 우선은 이 정도 논리로 사용하겠다는 정도로 이해해보자.\n\n따라서, 우리가 풀어야할 식은 다음과 같다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & H(p) = - \\sum_{i=1}^{N}p_i\\log p_i &\\\\\n  \\text{subject to} \\quad & p_i \\geq 0, & i = 1, ..., N \\\\\n                          & \\sum_{i=1}^{N}p_i = 1 &\\\\\n                          & \\text{Other Prior Knowledge}\n\\end{align*}\n$$\n\n이를 이용해서 문제를 세 개 정도 풀어보면 감이 잡을 수 있는데 한 번 따라와보도록 하자.\n\n### Example\n\n> <mark>**1. 주사위 던지기**</mark>\n\n1부터 6까지의 눈이 있는 주사위가 있다고 할 때, 주사위의 각 눈이 나올 확률을 알고 싶다고 하자. 이때 우리는 간단하게 $1\\over6$이라고 말할 것이다. 이것도 Maximum Entropy에 기반한 추론 방법 중에 하나라고 할 수 있다. 다음 식을 보자.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & H(p) = - \\sum_{i=1}^{N}p_i\\log p_i &\\\\\n  \\text{subject to} \\quad & p_i \\geq 0, & i = 1, ..., N \\\\\n                          & \\sum_{i=1}^{N}p_i = 1 &\n\\end{align*}\n$$\n\n정말 아무런 정보가 없을 때에는 위의 식을 Lagrangian을 쓰지 않고도 uniform distribution이라는 것을 알 수 있다. 이는 [🔗 [ML] Base Information Theory](/posts/ml-base-knowledge#Information-Theory)에서 살펴보았었다.\n\n그렇다면, 좀 더 복잡한 경우를 고려해보자. 아래는 Duke University ECE587 수업 PPT의 예제이다.\n\n> <mark>**2. 평균이 주어졌을 때의 추론**</mark>\n\n우리가 만약 평균 데이터를 알고 있다면, 이를 Maximum Entropy로 어떻게 추정할 수 있는지를 살펴볼 것이다. 아래는 어느 fastfood점의 메뉴라고 하자.\n\n| Item    | Price | Calories |\n| :------ | :---- | :------- |\n| Burger  | $1    | 1000     |\n| Chicken | $2    | 600      |\n| Fish    | $3    | 400      |\n| Tofu    | $8    | 200      |\n\n그리고 특정 학생이 이 가게에서 하루에 하나씩 먹는다고 할 때, 평균 소비 가격이 $2.5라고 하자. 그렇다면, 이 학생이 가장 많이 먹는 메뉴는 무엇일지를 추론해보는 것이다.  \n즉, 이를 식으로 정리하면 다음과 같다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & H(p) = - \\sum_{i=1}^{N}p_i\\log p_i &\\\\\n  \\text{subject to} \\quad & p_i \\geq 0, & i = 1, ..., N \\\\\n                          & \\sum_{i=1}^{N}p_i = 1 &\\\\\n                          & E[\\text{price}] = 2.5 &\n\\end{align*}\n$$\n\n이를 Lagrangian 방식을 이용해서 표현하면 다음과 같이 나타낼 수 있다.\n\n$$\n\\mathcal{L} = - \\sum_{i}^{N}p_{i}\\log{p_{i}} + \\lambda_{0}(\\sum_{i=1}^{N}p_{i} - 1) + \\lambda_{1}(\\sum_{i=1}^{N}\\text{price}_{i}\\times{p_{i}} -2.5)\n$$\n\n위 식을 각 각의 $p_{i}$에 대해서 미분하면 다음과 같다.\n\n$$\n{\\partial \\mathcal{L}\\over\\partial p_{i}} = -\\log{p_{i}} -1 + \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i}\n$$\n\n따라서, $p_{i}$는 다음과 같다.\n\n$$\n\\begin{align*}\n0 &= -\\log{p_{i}} -1 + \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} \\\\\n\\log{p_{i}} &= \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} - 1 \\\\\np_{i} &= e^{\\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} - 1}\n\\end{align*}\n$$\n\n여기서 나오는 모든 식과 제한 조건을 정리하면 다음과 같다.\n\n- $p(Burger) = e^{\\lambda_{0} + \\lambda_{1} - 1}$, $p(Chicken) = e^{\\lambda_{0} + 2\\lambda_{1} - 1}$, $p(Fish) = e^{\\lambda_{0} + 3\\lambda_{1} - 1}$, $p(Tofu) = e^{\\lambda_{0} + 8\\lambda_{1} - 1}$\n- $p(Burger) + p(Chicken) + p(Fish) + p(Tofu) = 1$\n- $p(Burger) + 2p(Chicken) + 3p(Fish) + 8p(Tofu) = 2.5$\n\n위의 식을 연립해서 풀면, $\\lambda_{0} = 1.2371$, $\\lambda_{1}=0.2586$이고, 전체 확률은 다음과 같다.\n\n| Item    | p      |\n| :------ | :----- |\n| Burger  | 0.3546 |\n| Chicken | 0.2964 |\n| Fish    | 0.2478 |\n| Tofu    | 0.1011 |\n\n> <mark>**3. 주사위의 눈의 합**</mark>\n\n1번에서 보았던 주사위를 n개 던져서 나온 눈의 합을 알 때, 주사위의 비율을 추정한다고 해보자.\n\n이때 우리는 다음과 같은 변수를 정의할 수 있다.\n\n- 주사위의 갯수 : $n$\n- i개의 눈을 가진 주사위의 갯수 : $n_{i}$\n- 전체 눈의 수의 합 : $n\\alpha$\n- 추가되는 Prior Knowledge : $\\sum_{i=1}^{6}{i n_{i}} = n\\alpha$\n\n이를 Maximum Entropy를 이용해서 풀게 되면 다음과 같은 결론을 얻을 수 있다.\n\n$$\np_{i} = {e^{\\lambda_{i}}\\over{\\sum_{i=1}^{6}{e^{\\lambda_{i}}}}}\n$$\n\n## Generalization\n\nMaximum Entropy를 위의 식을 통해서 구하는 것도 문제는 없지만 우리는 좀 더 일반화된 식을 원한다. 따라서, 이를 표현하기 위해서 다음과 같은 상황을 고려해보는 것이다. 우리가 마지막 보았던 예시가 사실은 우리가 하고자 하는 과정을 대표하는 하나의 예시이다. 우리가 가진 사전 지식은 이전에 관측한 데이터와 이것의 class이다. 따라서, 우리는 관측 결과의 가짓수(class)가 $K$개이고, 데이터의 input과 결과를 $(X, Y)$ 쌍이 라고 할 때, 특정 input data($X_{i}$)가 class k일 확률은 다음과 같이 표현할 수 있다.\n\n$$\np(Y_{i} = k) = {e^{w^{\\top}_{k}X_{i}}\\over{\\sum_{k^{\\prime}=1}^{K}{e^{w_{k^{\\prime}}^{\\top}X_{i}}}}}\n$$\n\n여기서 가장 중요한 Point가 발견된다. 바로 이 식이 **softmax** 함수라는 것이다. <mark>즉, Maximum Entropy를 통한 classification의 의미는 사실상 multinomial logistic regression의 다른 이름일 뿐이다.</mark> (logistic regression에 대한 내용은 [🔗 [ML] 3. Logistic Regression](/posts/ml-logistic-regression)에서 다루었다.)\n\n따라서, 여기서는 별도로 Modeling, Estimation, Smoothing 절차를 다루지 않는다. machine learning의 방법과 동일하기 때문이다.\n\n## Features\n\nNL의 가장 큰 특징은 data가 sparse하다는 것이다. domain마다 사용되는 언어와 빈도가 너무나 천차만별이기 때문에 sparse 현상이 필연적으로 발생한다. 이를 극복하기 위해서 대게의 data는 domain 별로 따로따로 수집하는 것이 일반적이다. 또한, data에서 올바른 feature를 추출하는 것이 굉장히 중요하다.  \n이를 위해 NL에서 전통적으로 쓰던 방식은 대소문자 여부, 억양 표기, 품사, 문장구조, 뜻 등을 단어에 미리 적용하기도 하여 이를 이용하는 방법도 있다. 그런데 이러한 품사, 뜻 등을 찾아내는 과정도 Statistical Inference가 필요하다. 따라서, 앞으로 chapter에서는 품사와 문장구조 뜻을 정의하기 위한 기술들과 이를 어떻게 찾을 수 있는지를 알아볼 것이다.\n\n또한, Word자체를 Vector로 치환하여 사용하는 Word2Vec방식에 대해서도 살펴보도록 하겠다.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Maximum Entropy 자료 참고, <https://www2.isye.gatech.edu/~yxie77/ece587/Lecture11.pdf>\n","slug":"nlp-maxent","date":"2022-11-07 10:02","title":"[NLP] 7. MaxEnt","category":"AI","tags":["NLP","MaximumEntropyModel","softmax"],"desc":"해당 Posting에서는 Maximum Entropy를 이용하여 최적의 parameter를 찾아나가는 Machine Learning 접근법에 기반한 NLP 방식을 제안한다. 이를 위해서 NL를 수학적인 형태로 변형하기 위한 기술 중 하나인 word2vec에 대한 설명도 같이 진행한다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n우리가 NL을 제대로 분석하기 위해서 각 단어가 가진 의미를 알아야하며, 이를 넘어서 문장이 가지는 의미를 파악해야 한다. 결론적으로 이 과정이 고도화된 NLP를 위한 핵심 단계이다. 이를 위해서는 Raw한 형태로 주어진 text를 처리해서 더 나은 형태의 구조를 만들 필요가 있다. 따지고 보면 하나의 전처리 과정이라고 볼 수 있다. 그치만 이전 text processing chapter과 다른 점은 문장 구분과 같은 간단한 과정이 아닌 Linguistic 단계에 따른 처리 과정을 수행한다고 볼 수 있다. 또한, 각 단계 역시 NLP 중에 하나라고 할 수 있으므로 이 또한 ML과 DL을 통해서 고도화하는 것도 가능하다. Morphology 단계부터 시작하여 Syntax, Semantic까지 어떻게 다루게 되는지를 살펴보도록 하겠다.\n\n## POS tagging\n\nMorphology 단계에서 가장 기본이되는 요소이기 때문에 이를 먼저 살펴보도록 하겠다. Part of Speech라는 단어의 뜻 자체가 \"품사\"이다. 이는 단어의 문법적인 기능이나 형태 등을 표현하기 위해서 제시되었다. 이를 구분하려는 시도는 디오니소스 이전부터 있었지만 근본적인 형태를 제시한 것은 디오니소스가 첫 번째이다. 그는 기원전 100년에 지금과 굉장히 유사한 형태의 8개의 품사를 제시하였다. 지금도 8개지만, 감탄사와 형용사 등이 추가되고 몇몇 요소가 빠졌다. 이를 NLP 과정에서 input으로 활용하게 되면 언어의 모호성을 해결하는데 도움을 줄 수 있다. 품사를 통해서 단어가 가지는 뜻의 범위가 더 줄어들 수 있기 때문이다. 따라서, 이를 각 단어마다 표시하는 절차를 preprocessing으로 진행하는 경우도 많다.\n\n우선 POS의 일반적인 종류는 다음과 같다.\n\n- Noun(명사)\n- Verb(동사)\n- Adjective(형용사)\n- Adverb(부사)\n- Preposition(전치사)\n- Conjunction(접속사)\n- Pronoun(대명사)\n- Interjection(감탄사)\n\n하지만, Computer Science에서는 이를 좀 더 명확하게 표현하기 위해서 더 많은 분류(tag)를 사용하는 것이 일반적이다. 대표적인 예시가 [🔗 Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)이다. 여기서는 36개의 종류를 활용하여 표기한다. 이외에도 Brown Corpus 등 다양한 tagging 방법이 있다. 또한, 언어에 따라서는 별도의 품사를 정의하는 경우도 많기 때문에 언어마다 적절한 방식을 사용해주는 것이 좋다.\n\ntag를 정할 때 일반적인 규칙은 우리가 중/고등학교 시간에 배웠을 문법 요소를 적용한 것이 많다는 점을 기억하면 된다. NNS 같은 경우는 복수명사 뒤에 붙은 s를 포함하는 tag를 의미하고, VBD는 동사 과거형을 의미한다. 이와 같은 형태로 품사를 좀 더 세분화한 것 외에는 차이가 없다.\n\n### How can I get?\n\n그렇다면, 어떻게 하면 POS tagging된 데이터를 얻을 수 있을지가 궁금할 것이다. 신기하게도 가장 쉬운 추론을 하더라도 90%의 정확도를 가질 수 있다. 다음과 같은 방법이다.\n\n1. 단어가 가지는 품사 중 가장 빈도가 높은 것을 표기한다.\n2. 못 본 단어인 경우 Noun(명사)로 표기한다.\n\n이것이 가능한 이유는 사실상 대부분의 word는 모호하지 않다는 점이다. 대부분의 word는 품사 앞에서는 그렇게 변화무쌍하지 않다. **하지만,** 특정 word는 사람 조차도 헷갈리는 경우가 있다. 대게 통계적으로 11%정도는 사람 조차도 헷갈릴 수 있는 형태의 품사가 주어진다고 한다. 그래서, 이를 해결하기 위해서 Statistic Inference를 활용하는 경우가 있고, 우리가 앞 서 배웠던 HMM을 활용하면 97%, MaxEnt를 활용하면 99% 정확도를 가지는 tagger를 만들 수 있다. 물론 더 복잡한 Deep Learning을 활용한다면 더 높은 성능도 가능은 할 것이다.\n\n## Morphology\n\nMorphology 단계에서 POS tagging이 중요하긴 하지만 더 나아갈 필요가 있다. 결국 우리가 원하는 것은 단어의 의미를 더 완벽하게 찾는 것이다. 따라서, 대게의 경우 POS tagging을 포함하는 Morphology tagging을 수행한다. 특정 단어를 사전형 기본형(lemma) 또는 더 나아가 가장 뿌리가 되는 요소 root와 stem으로 나누고 여기에 품사를 덧붙이는 형태이다. 우리가 얻은 품사(tag)와 lemma만 갖고도 우리는 원래 단어를 만드는 것이 가능하고, 뜻의 범위를 더 한정할 수 있다. 더 나아가 root와 stem으로 나누게 되면 보지 못한 데이터에 대해서도 더 면밀한 의미 파악이 가능해진다. 이를 구현할 때에는 대게 4가지 방법 중에 하나를 수행하는 것이 일반적이다.\n\n1. Word form list  \n   간단하게 생각하면, word list에서 단어를 조회하는 방식이다. 대게 key, value보다는 Trie 형태로 담는 것을 선호한다. Trie는 각 node가 sequence 데이터의 요소 하나하나가 되는 tree를 의미하며, sequence 데이터의 조회를 위해 사용된다.\n2. Direct coding  \n   root와 stem을 찾는 과정은 사실 영어에서는 간단하다. 앞 뒤에서 부터 진행하면서 대표적인 stem을 제거해 나가면, root만 남기 때문이다. 하지만, 일부 일본어와 같은 경우에는 이것이 불가능한 경우도 있다. 이 경우에는 다른 방식을 적용해야 한다.\n3. Finite state machinery  \n   각 단어의 형태를 FSM으로 정의하여 변할 수 있는 형태와 이에 따른 품사 등을 미리 표현하여 정의하는 방법이다.\n4. CFG, DATR, Unification  \n   언어학에 기반한 분석법이다.\n\n사실 이러한 방법을 직접 구현하는 것은 한계가 있을 수 있다. 따라서, 이미 구현되어 있는 POS tagger를 사용하는 것이 현명할 수 있다. 일반적으로 가장 많이 사용되는 POS tagger는 다음과 같은 것들이 있다.\n\n| Library | Language | ProgrammingLanguage |\n| :------ | :------- | :------------------ |\n| NLTK    | English  | Python              |\n| spaCy   | English  | Python              |\n| KoNLPy  | 한글     | Python              |\n\n## Syntactic Analysis\n\nMorphology 단계에서는 각 word의 뜻을 다루었다면, 이 단계는 word의 결합으로 이루어지는 문장 구조를 분석하는 단계이다. 문장 구조를 분석(구문 분석)하는 방법은 크게 두 가지로 나뉘어진다.\n\n1. <mark>**Phrase Structure**</mark>  \n   문장을 Phrase(구) 단위로 나누어 구조화 시키는 방법이다. 단어 각 각의 품사에서 부터 시작하여 이들을 묶어서 하나의 문장 요소(대게 phrase)를 만들어 하나의 문장을 만드는 구조를 가진다.\n2. <mark>**Dependency Structure**</mark>  \n   문장에서 각 단어가 가지는 의존 관계를 나타낸 구조이다.\n\n각 구조는 둘다 Tree 형태로 이루어지며, 분석하는 방법도 서로 매우 다르다. 각 방법은 밑에서부터 자세히 다루도록 하겠다.\n\n### Phrase Structure\n\n문장을 이루는 요소들과 요소들의 구조화 규칙을 정의해야 우리는 이를 분석할 수 있을 것이다. 따라서, 이를 정의한 것을 Grammar라고 한다. 그리고 이를 위해서 대표적으로 사용되는 것이 **CFG**이다. **CFG**는 Context Free Grammar의 약자로, 모든 잘 구조화된 문장들을 정의할 수 있는 규칙들을 의미한다. 각 각의 Rule은 왼쪽에는 문법적 type이 주어지고, 오른쪽에는 이를 이루는 요소들이 정의되어진다. 각 요소는 하위 문법적 type 또는 이전에 제시한 POS가 될 수 있다.\n\n가장 기본적으로 사용되어지는 문법적 type들은 다음과 같다. 이외에도 기술하지 않은 POS도 사용이 가능하다.\n\n| Symbol  | Mean               | Korean   |\n| :------ | :----------------- | :------- |\n| NP      | Noun Phrase        | 명사 구  |\n| VP      | Verb Phrase        | 동사 구  |\n| S       | Sentence           | 문장     |\n| DET(DT) | Determiner         | 관사     |\n| N       | NOUN               | 명사     |\n| V       | Verb               | 동사     |\n| PREP    | Preposition        | 전치사   |\n| PP      | Preposition Phrase | 전치사구 |\n\n이에 따라 대표적인 Rule은 다음과 같다.\n\n- S -> NP VP\n- NP -> (DT) N\n- NP -> N\n- VP -> V (NP)\n\n위에 제시된 Rule은 가장 기본적인 규칙으로 여기서 더 확장된 규칙을 만들어서 Parsing을 수행할 수 있다. 하지만, 이렇게 규칙을 만들어서 수행을 하게 되면 문제가 발생할 수 있다. 바로 여러 개의 Parsing Result가 만들어졌을 때 이 중에서 어떤 것이 가장 적절한지를 알 수 없다는 것이다. 즉, 너무 구체적인 Rule을 만들기에는 Parsing이 하나도 되지 않는 문장이 만들어질 가능성이 높고, 그렇다고 너무 적은 Rule을 적용하게 되면 Parsing이 너무 많이 만들어지게 된다.\n\n따라서, 결론적으로 말하자면 위와 같은 형태의 CFG로는 phrase structure를 구조화하는데 한계가 있다는 결론을 내리게 된다. 결국 아래와 같은 두 개의 문제점에 직면하게 되고 이를 해결하기 위한 방법이 각 각 제시된다.\n\n1. Repeated work  \n   문장 구조가 동일한 경우 결국 동일한 작업을 반복하게 된다. 이를 해결하기 위해서 Treebank라는 구조를 도입하고, 이것의 일부를 Dynamic Programming의 Memoization처럼 저장해두었다가 쓰는 방식을 적용한다. 즉, 기존에는 Rule만을 저장하고, 때에 따라 이를 적용하였다면, 이제는 모든 단어의 품사와 구조를 기록해두는 것이다. 이를 통해서 이미 나왔던 작업의 경우 빠른 처리가 가능해진다.\n2. Choosing the correct parse  \n   위에서 말했던 것처럼 우리는 결국 <mark>가장 적절할 거 같은 parsing result를 선택해야 한다.</mark> Rule에 기반한 방식으로는 한계가 있지만 우리가 Statistic한 방식을 활용한다면 이를 극복할 수 있다. 따라서, 우리는 CFG에서 나아가 PCFG(Probabilistic CFG)를 적용하여 이를 처리할 수 있다. 이러한 Statistical한 결과를 얻기 위해서도 Treebank 구조가 필요하다.\n\n![nlp-cfg-treebank](/images/nlp-cfg-treebank.jpg)\n\n이제부터는 실제로 PCFG를 어떻게 수행할 수 있는지를 자세히 다뤄보도록 하겠다.\n\n#### PCFG\n\n앞 서 얘기한 것처럼 Probabilistic CFG로, 각 Rule마다 Probability를 적용하는 것이다. 여기서 유의할 것은 다음 내용이다.\n\n1. 기존 정의한 문법적 type을 만드는 Rule에 각 각의 확률을 정의한다.\n2. 이때 각 문법적 type을 만들 수 있는 Rule의 확률의 합은 반드시 1이다.\n3. 또한, 각 단어가 특정 POS일 확률도 같이 구해야 한다.  \n   ex. N -> fish (0.5), V -> fish (0.1)  \n   (실제로 이렇게 크게 나오지 않는다. N 또는 V 일 때, Fish일 확률이므로 굉장히 작은 값이 나오는 것이 일반적이다.)\n\n따라서, 어떤 treebank가 더 적절한 지는 각 각의 treebank의 모든 Rule의 확률의 곱을 구해서 비교하면 된다. 굉장히 쉽게 이 과정이 가능한 것이다. 아래는 간단한 예시이다.\n\n![nlp-pcfg](/images/nlp-pcfg.jpg)\n\n이렇게 주어졌을 때, $p(t_1)$과 $p(t_2)$는 아래와 같이 구할 수 있다.\n\n$$\n\\begin{align*}\np(t_{1}) &= 1.0 \\\\\n&\\times 0.3 \\times 0.6 \\\\\n&\\times 0.4 \\\\\n&\\times 0.5 \\times 0.1 \\times 0.4 \\times 1.0 \\times 0.4  \\\\\n&= 0.000576 \\\\\n\\\\\np(t_{2}) &= 1.0 \\\\\n&\\times 0.3 \\times 0.4 \\\\\n&\\times 1.0 \\\\\n&\\times 0.4 \\\\\n&\\times 0.5 \\times 0.6 \\times 1.0 \\times 1.0 \\times 0.4 \\\\\n&= 0.00576 \\\\\n\\\\\n\\therefore p(t_{1}) &\\lt p(t_{2})\n\\end{align*}\n$$\n\n따라서, $t_{2}$ 형태가 더 적절하다고 판별할 수 있는 것이다.\n\n```plaintext\n 🤔 Chomsky Normal Form\n\n 기존 CFG의 형태의 모호함을 제거하고, 좀 더 명확한 형태로 정의하는 것을 의미한다.\n 대표적으로 모호한 내용이 Sentence안에 Sentence를 포함하는 경우(n-ary)라든지,\n 명령문과 같은 문장을 위한 주어 삭제(unary/empty) 등이 존재한다.\n 이를 해결하기 위한 recursive 형태나 empty 형태 등을 제거하는 것을 의미한다.\n\n 결론상 PCFG에서는 확률 표기시에 모호한 표기를 제거할 수 있다는 장점이 있다.\n```\n\n#### CKY Parsing\n\n앞 서 우리가 treebank 중에서 더 큰 확률곱 값을 가지는 것이 최적값이라는 것을 알 수 있었다. 하지만, 사실 이 과정이 그렇게 쉽지는 않다. 왜냐하면, 우리가 가지는 Parsing Result는 굉장히 많을 수도 있기 때문이다. 그렇다면, 이를 연산하는 비용이 굉장히 비싸진다. 이를 효과적으로 연산하기 위한 알고리즘으로 제시된 것이 CKY Parsing이다.\n\nPseudo code는 다음과 같다.\n\n```javascript\nfunction CKY(words, grammar) returns [scores, backpointers]\n  // score[i][j] = \n  // 모든 Symbol(문법적 type, ex. S, NP, VP)에 대하여 \n  // i부터 j까지 word를 사용했을 때의 최댓값을 저장\n  score = new double[#(words) + 1][#(words)+1][#(Symbol)]\n  // back[i][j] = \n  // 모든 Symbol(문법적 type, ex. S, NP, VP)에 대하여 \n  // i부터 j까지 word를 사용했을 때의 최댓값을 만드는 요소의 위치를 저장\n  // (=back pointer)\n  back = new Pair[#(words)+1][#(words)+1][#(Symbol)]\n  for (i=0; i < #(words); i++)\n    // 초기화 단계로 각 단어가 Symbol일 확률을 입력\n    for (A in Symbol)\n      if A -> words[i] in grammar\n        score[i][i+1][A] = P(A -> words[i])\n    // unary 즉 생략되어서 표현되는 경우를 위해서 확률 재계산\n    // ex. Stop!! (S->VP,VP->V)\n    boolean added = true\n    while (added)\n      added = false\n      for A, B in Symbol\n        if score[i][i+1][B] > 0 && A -> B in grammar\n          prob = p(A -> B) * score[i][i+1][B]\n          if prob > score[i][i+1][A]\n            score[i][i+1][A] = prob\n            back[i][i+1][A] = B\n            added = true\n  for (span = 2 to #(words))\n    for (begin = 0 to #(words) - span)\n      // 일반적인 두 항의 합으로 이루어지는 경우를 계산\n      end = begin + span\n      for (split = begin + 1 to end-1)\n        for (A, B, C in Symbol)\n          prob = score[begin][split][B] * score[split][end][C]*P(A -> B C)\n          if prob > score[begin][end][A]\n            score[begin][end][A] = prob\n            back[begin][end][A] = new Triple(split, B, C)\n      // unary인 경우를 고려해서 재계산\n      boolean added = true\n      while (added)\n        added = false\n        for (A, B in Symbol)\n          prob = P(A -> B) * score[begin][end][B]\n          if prob > score[begin][end][A]\n            score[begin][end][A] = prob\n            back[begin][end][A] = B\n            added = true\n  return score, back\n```\n\n전체적인 동작과정은 그림을 통해서 이해할 수 있다. 먼저초기 score 할당부터 첫 단계에 데이터를 저장하기까지는 아래 그림으로 이해할 수 있다.\n각 그림을 다음을 의미한다.\n\n1. score를 위한 공간 할당\n2. score에 가장 기본이 되는 Symbol -> word 확률 입력\n3. unari case를 확인해서 확률 입력\n\n![nlp-cky-1](/images/nlp-cky-1.png)\n\n그 다음 단계로는 단계적으로 관계를 적립한다.\n\n1. 같은 형광펜으로 칠해진 데이터간 관계가 최댓값을 가진다.\n2. unari case도 확인한 결과 S->VP가 초기화 된다.\n\n![nlp-cky-2](/images/nlp-cky-2.png)\n\n마지막에서 다시 관계를 정리할 때, 유의할 점이 있다. 바로 score\\[0\\]\\[3\\]와 score\\[1\\]\\[4\\]도 중요하지만 score\\[0\\]\\[2\\]와 score\\[2\\]\\[4\\]에 의한 관계도 반드시 유의해서 보아야 한다.\n\n![nlp-cky-3](/images/nlp-cky-3.png)\n\n#### modeling\n\n원래라면, modeling 단계도 다루어야하지만, 해당 단계에서는 넘어가도록 한다. 이를 수행하기 위해서는 간단하게는 단순히 빈도를 확인하는 것부터 EM algorithm을 활용하여 업데이트 하는 방식이 있다. 하지만 여기서는 자세히 다루지 않겠다.\n\n### Dependency Structure\n\n문장에서 각 단어의 의존 관계를 나타내는 Dependency Structure는 중심 의미를 가지는 word로 부터 이에 의존하는 word들의 관계로 확장되며 표기된다. 따라서, 문장에서는 대게 동사가 중심이 되고, 그리고 그 다음으로는 전치사, 명사 등이 뒤를 잇게 된다. 이를 파악하게 되면, 단어가 연관성과 전체적인 구조의 안정성 등을 파악하는데 도움을 줄 수 있다.\n\n이 형태를 얻기 위해서 할 수 있는 대표적인 방법은 다음과 같은 방법이 있다.\n\n1. Dynamic Programming  \n   아주 쉽게 생각할 수 있는 방법으로 **PCFG를 활용**하는 것이다. 일반적으로 PCFG를 활용하여 tree 형태를 구축하면 이를 이용하여 Dependency Structure를 쉽게 구할 수 있다. 단순히 tree의 아래서 부터 의존 관계를 가진 단어를 고르면서 root까지 올라오면 이것으로 충분하다. 하지만, 이 과정은 시간적 비용이 많이 든다는 단점이 있다.\n2. Graph Algorithm  \n   **가장 정확도가 높은 방식**으로 Sentence에 대한 Minimum Spanning Tree를 구성하고, 이를 활용하여 ML classifier를 제작하여 구현할 수 있다. 가장 높은 정확도를 원한다면 해당 방식을 활용하는 경우가 많다.\n3. Constraint Satisfaction  \n   모든 경우의 수를 만들고 거기서 제한사항을 만족하지 않는 구조를 제거하는 방식이다. 이 또한 많이 사용되지는 않는다.\n4. Deterministic Parsing  \n   Greedy algorithm에 기반하여 구현된 방식으로 매우 높지는 않지만 적절한 정확도에 **빠른 속도**를 가지기 때문에 많이 사용되어진다.\n\n#### Malt Parser\n\n여기서는 Deterministic Parsing 중에서 가장 쉬운 방법 중에 하나인 Malt Parser를 좀 더 다뤄보도록 하겠다.\n\n이는 3개의 자료 구조와 4개의 action을 통해서 정의되는 알고리즘이다.  \n먼저 자료구조는 다음과 같다.\n\n1. stack($\\sigma$)  \n   dependency tree의 상위 요소를 저장해두는 공간으로, 처음에는 ROOT라는 요소를 갖고 시작한다.\n2. buffer($\\beta$)  \n   input sequence를 저장하는 공간으로, 처음에는 input sequence를 전체를 저장하고 있다.\n3. arcs($A$)  \n   최종으로 만들고자 하는 dependency tree를 의미한다. 처음에는 비어 있는 상태로 시작한다.\n\naction은 다음과 같다.\n\n1. Reduce  \n   stack($\\sigma$)에서 word를 pop한다.\n2. Shift  \n   buffer($\\beta$)에서 stack($\\sigma$)으로 word를 push한다. 이때 문장의 앞의 단어부터 차례대로 전달한다.\n3. Left-Arc  \n   stack($\\sigma$)의 현재 word가 buffer($\\beta$)의 다음 word에 의존하는 경우, 이 관계를 연결하여 arcs($A$)에 저장한다.  \n   결론상 stack($\\sigma$)에서는 pop이 되고, buffer($\\beta$)는 그대로 유지되며, arcs($A$)에는 depdendency가 하나 추가된다.\n4. Right-Arc  \n   buffer($\\beta$)에서 다음 word를 stack($\\sigma$)에 push하고, 기존 stack($\\sigma$)의 이전 word에 의존하는 관계를 arcs($A$)에 추가한다.\n5. Finish  \n   buffer($\\beta$)에 더 이상 word가 없다면, 모든 연산을 마무리할 수 있다.\n\n이 또한 예시를 통해서 알아보는 것이 명확하다.\n\n우리가 `Happy children like to play with their friends.`를 분석하고 싶다고 하자. 그렇다면, 절차는 다음과 같이 진행된다.\n\n| Index | Action    | Stack($\\sigma$)                   | Buffer($\\beta$)        | Arcs($A$)                                      |\n| :---- | :-------- | :-------------------------------- | :--------------------- | :--------------------------------------------- |\n| 0     |           | [ROOT]                            | [Happy, children, ...] | $\\empty$                                       |\n| 1     | Shift     | [ROOT, Happy]                     | [children, like, ...]  | $\\empty$                                       |\n| 2     | LA(amod)  | [ROOT]                            | [children, like, ...]  | {amod(children, happy) = $A_{1}$}              |\n| 3     | Shift     | [ROOT, children]                  | [like, to, ...]        | $A_{1}$                                        |\n| 4     | LA(nsubj) | [ROOT]                            | [like, to, ...]        | $A_{1} \\cup ${nsubj(like, children)} = $A_{2}$ |\n| 5     | RA(root)  | [ROOT, like]                      | [to, play, ...]        | $A_{2} \\cup ${root(ROOT, like)} = $A_{3}$      |\n| 6     | Shift     | [ROOT, like, to]                  | [play, with, ...]      | $A_{3}$                                        |\n| 7     | LA(aux)   | [ROOT, like]                      | [play, with, ...]      | $A_{3} \\cup ${aux(play, to)} = $A_{4}$         |\n| 8     | RA(xcomp) | [ROOT, like, play]                | [with, their,...]      | $A_{4} \\cup ${xcomp(like, play)} = $A_{5}$     |\n| 9     | RA(prep)  | [ROOT, like, play, with]          | [their, friends, .]    | $A_{5} \\cup ${prep(play, with)} = $A_{6}$      |\n| 10    | Shift     | [ROOT, like, play, with, their]   | [friends, .]           | $A_{6}$                                        |\n| 11    | LA(poss)  | [ROOT, like, play, with]          | [friends, .]           | $A_{6} \\cup ${poss(friends, their)} = $A_{7}$  |\n| 12    | RA(pobj)  | [ROOT, like, play, with, friends] | [.]                    | $A_{7} \\cup ${pobj(with, friends)} = $A_{8}$   |\n| 13    | Reduce    | [ROOT, like, play, with]          | [.]                    | $A_{8}$                                        |\n| 14    | Reduce    | [ROOT, like, play]                | [.]                    | $A_{8}$                                        |\n| 15    | Reduce    | [ROOT, like]                      | [.]                    | $A_{8}$                                        |\n| 16    | RA(punc)  | [ROOT, like, .]                   | []                     | $A_{8} \\cup${punc(like, .)} = $A_{9}$          |\n| 17    | Finish    | [ROOT, like, .]                   | []                     | $A_{9}$                                        |\n\n자 이런 예시를 보았다면, 당연히 궁금해할 것은 어떻게 Action을 고를 것인가이다. 이는 Discriminative classifier 즉, Maxent나 여타 Machine Learning 방법을 동원하여 결정한다. PCFG를 활용하는 방식보다는 성능이 약간 낮을지라도 이를 활용하면 매우 빠르게 parsing이 가능하다는 장점이 있다.\n\n```plaintext\n 🤔 Projectivity\n \n 사실 여태까지 우리는 연속되어 있는 word간의 의존성을 파악하는 과정을 살펴보았다.(특히 PCFG)\n 하지만, 그렇지 않은 경우도 분명히 존재한다. 대표적인 예시가 아래이다.\n Who did Bill buy the coffee from yesterday?\n 여기서 from은 Who와 관계가 있지만, 우리가 여태까지 살펴본 PCFG와 Malt Parser로 \n 이 관계를 밝히는데에는 한계가 있다.\n 따라서, 이를 해결하기 위해서 후처리나 추가적인 action을 Malt Parser에 더하거나 \n 아니면 아예 다른 방식을 앙상블하여 해결하기도 한다.\n```\n\n## Semantics\n\n자세히 여기서 다루지 않지만, 구문 분석을 통해 얻은 Tree를 통해서 어떻게 의미를 추출할 수 있는지를 알아보겠다. 먼저, 우리는 전체 요소를 다시 한 번 두 가지로 나눈다.\n\n1. Entities  \n   특정 의미를 가지는 하나의 주체이다. 주로 NP가 모두 여기에 속한다.\n2. Functions  \n   Entity 또는 다른 Function에게 동작, 특성, 등을 적용한다. 형용사, 동사 등이 여기에 속한다.\n\n따라서, 우리가 `Every nation wants George to love Laura.`라는 문장을 갖고 있다면, 우리는 아래와 같이 Tree를 그릴 수 있고, 이를 이용해서 의미 분석이 가능하다.\n\n![nlp-semantic](/images/nlp-semantic.jpg)\n\n위 Tree를 아래에서부터 연결해서 나가면 다음과 같이 구조화되는 것을 알 수 있다.\n\n| Index | Expression                                                |\n| :---- | :-------------------------------------------------------- |\n| 1     | love(x, Laura)                                            |\n| 2     | love(x, Laura)                                            |\n| 3     | love(George, Laura)                                       |\n| 4     | want(x, love(George, Laura))                              |\n| 5     | present(want(x, love(George, Laura)))                     |\n| 6     | Every(nation)                                             |\n| 7     | present(want(Every(nation), love(George, Laura)))         |\n| 8     | assert(present(want(Every(nation), love(George, Laura)))) |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Penn Treebank POS tagging, <https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html>\n- spaCy, <https://spacy.io/>\n- NLTK, <https://www.nltk.org/>\n- KoNLPy, <https://konlpy.org/ko/latest/index.html>\n- NLP CFG, <https://tildesites.bowdoin.edu/~allen/nlp/nlp1.html>\n","slug":"nlp-language-parsing","date":"2022-11-07 15:05","title":"[NLP] 8. Language Parsing","category":"AI","tags":["NLP","POS","PCFG","Morphology","Syntax","Semantics"],"desc":"우리가 NL을 제대로 분석하기 위해서 각 단어가 가진 의미를 알아야하며, 이를 넘어서 문장이 가지는 의미를 파악해야 한다. 결론적으로 이 과정이 고도화된 NLP를 위한 핵심 단계이다. 이를 위해서는 Raw한 형태로 주어진 text를 처리해서 더 나은 형태의 구조를 만들 필요가 있다. 따지고 보면 하나의 전처리 과정이라고 볼 수 있다. 그치만 이전 text processing chapter과 다른 점은 문장 구분과 같은 간단한 과정이 아닌 Linguistic 단계에 따른 처리 과정을 수행한다고 볼 수 있다. 또한, 각 단계 역시 NLP 중에 하나라고 할 수 있으므로 이 또한 ML과 DL을 통해서 고도화하는 것도 가능하다. Morphology 단계부터 시작하여 Syntax, Semantic까지 어떻게 다루게 되는지를 살펴보도록 하겠다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"}]},"__N_SSG":true}