{"pageProps":{"posts":[{"content":"\n## Intro\n\nì´ì „ê¹Œì§€ì˜ Postingì—ì„œëŠ” Supervised Learning ì¦‰, ì´ë¯¸ Labelingì´ ì™„ë£Œëœ ë°ì´í„°ì— ì˜í•œ Learningì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ì—ˆë‹¤. ì§€ê¸ˆë¶€í„°ëŠ” Unsupervised Learningì— ëŒ€í•´ì„œ ì¡°ê¸ˆ ë” ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤. ëŒ€í‘œì ì¸ Unsupervised Learningì€ Clustering, Feature Selection(or Dimensionality Reduction), Generative Model ë“±ì´ ì¡´ì¬í•œë‹¤. ì´ë“¤ì— ëŒ€í•´ì„œ ì°¨ê·¼ì°¨ê·¼ ì‚´í´ë³´ë„ë¡ í•˜ê³ , í•´ë‹¹ Postingì—ì„œëŠ” ê°€ì¥ ëŒ€í‘œì ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” Clusteringì„ ë¨¼ì € ì‚´í´ë³´ë©´ì„œ Unsupervised Learningì— ëŒ€í•œ ê³„ëµì ì¸ ì´í•´ë¥¼ í•´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## Clustering\n\nClusteringì€ unlabeled dataë¥¼ dataê°„ ìœ ì‚¬ì„± ë˜ëŠ” ê±°ë¦¬ ì§€í‘œ ë“±ì„ í™œìš©í•˜ì—¬ ë¯¸ë¦¬ ì§€ì •í•œ ìˆ˜ ë§Œí¼ì˜ partitioningí•˜ëŠ” ì‘ì—…ì„ ì˜ë¯¸í•œë‹¤. ì¦‰, ìš°ë¦¬ê°€ í•™ìŠµì„ ì§„í–‰í•¨ì— ìˆì–´ dataëŠ” labelì´ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” dataê°„ì˜ ê´€ê³„ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì„œ ì´ë¥¼ ë¶„ë¥˜í•´ë‚´ëŠ” ê²ƒì´ ëª©í‘œì¸ ê²ƒì´ë‹¤.\n\nì´ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë°©ë²•ì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.\n\n1. **Non-Parametric Approach**  \n   ì´ë¦„ ê·¸ëŒ€ë¡œ í™•ë¥ ì  ë¶„í¬ë¥¼ ê°€ì •í•œ í›„, Parameterë¥¼ ì°¾ì•„ê°€ëŠ” ë°©ì‹ì´ ì•„ë‹Œ ì§ê´€ì ì¸ ë°©ë²•(Huristic Approach)ì„ í™œìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê·¸ë ‡ê¸°ì— í™•ë¥ ì ì¸ í•´ì„ì´ ë’·ë°›ì¹¨ë˜ê¸° ë³´ë‹¤ëŠ” Algorithmì„ í†µí•´ì„œ ì´ë¥¼ ì„¤ëª…í•œë‹¤. ëŒ€í‘œì ì¸ ë°©ë²•ì´ K-Means Clusteringì´ë‹¤.\n2. **Parametric Approach**  \n   í™•ë¥ ì  ë¶„í¬ë¥¼ ê°€ì •í•œ í›„, Parameterë¥¼ ì°¾ì•„ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ, ëŒ€í‘œì ì¸ ë°©ë²•ì´ Gaussian ë¶„í¬ë¥¼ ê°€ì •í•˜ê³  ì°¾ì•„ë‚˜ê°€ëŠ” Gaussian Mixture Model(GMM, or MoG, Mixture of Gaussian)ì´ ìˆë‹¤.\n\në”°ë¼ì„œ, Clusteringì„ ëŒ€í‘œí•˜ëŠ” K-means Clusteringê³¼ GMMì„ ê° ê° ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n### K-Means Clustering\n\nK-Means Clusteringì€ Kê°œì˜ í‰ê· ê°’ì„ í†µí•œ Clusteringìœ¼ë¡œ í•´ì„í•˜ë©´ ì˜ë¯¸ íŒŒì•…ì´ ì‰½ë‹¤. ì¦‰, Kê°œì˜ Partitionì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ Kê°œì˜ í‰ê· ê°’ì„ ì°¾ì•„ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë” ê°€ê¹Œìš´ í‰ê· ê°’ì— ì†í•˜ëŠ” Partitionì— dataë¥¼ ë¶„ë°°í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ êµ¬í•´ì•¼í•  ê°’ì€ ê° dataê°€ ì–´ëŠ Partitionì— ì†í•˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´($\\bold{r}\\leftarrow\\text{one hot vector}$)ì™€ ê° Partitionì˜ í‰ê· ê°’($\\mu$)ì´ë‹¤. ì¦‰, K-means Clusteringì—ì„œëŠ” ê¸°ì¡´ dataë“¤ì„ í†µí•´ì„œ Kê°œì˜ í‰ê· ê°’(K-means)ì„ ì°¾ì•„ì„œ(**Learning**) ì´í›„ì— ì¶”ê°€ë¡œ ë“¤ì–´ì˜¬ dataì— ëŒ€í•´ì„œë„ ë˜‘ê°’ì€ K-meansë¥¼ í†µí•´ì„œ Partitionì„ ì°¾ì„ ìˆ˜ ìˆë‹¤(**Inference**).ë˜ëŠ” ëª¨ë“  dataë¥¼ ì €ì¥í•´ë‘ì—ˆë‹¤ê°€ K-meansë¥¼ ë‹¤ì‹œ ê³„ì‚°í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤(Online K-means).\n\nê·¸ë ‡ë‹¤ë©´, $\\boldsymbol{\\mu}(=\\{\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}\\})$ì™€ $\\bold{R}$(ëª¨ë“  dataì˜ $\\bold{r}$ë¡œ ì´ë£¨ì–´ì§„ Matrix)ì„ ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? ì´ì— ëŒ€í•œ í•´ë‹µì€ ë‹¤ìŒê³¼ ê°™ì€ Cost Functionì„ ì œì‹œí•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}||x_{i} - \\mu_{k}||^{2}\n$$\n\ní˜„ì¬ dataì˜ pointë¡œ ë¶€í„° ê°€ì¥ ê°€ê¹Œìš´ í‰ê· ì„ ì„ íƒí•˜ëŠ” ê²½ìš°ë¥¼ ìµœëŒ€í™”í•´ì•¼ í•´ë‹¹ ê°’ì´ ê°€ì¥ ì‘ì•„ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. <mark>ì¦‰, ì—¬ê¸°ì„œëŠ” í‰ê· ê³¼ì˜ ê±°ë¦¬ë¥¼ ìœ ì‚¬ì„±ì˜ ì§€í‘œë¡œ ì‚¬ìš©í•œ ê²ƒì´ë‹¤.</mark> ì—¬ê¸°ì„œëŠ” Euclidean distance(L2-norm)ë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, Manhatan distance(L1-norm)ì„ í™œìš©í•  ìˆ˜ë„ ìˆê³  ì•„ì˜ˆ ë‹¤ë¥¸ ì§€í‘œë¥¼ í™œìš©í•  ìˆ˜ë„ ìˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€ Cost Functionì´ ì•„ë˜ì™€ ê°™ì€ formì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\times(\\text{Similarity measure})\n$$\n\nê·¸ë ‡ë‹¤ë©´, ì‹¤ì œë¡œ ìœ„ì—ì„œ ì œì‹œí•œ Cost Functionì„ í™œìš©í•˜ì—¬ ì–´ë–»ê²Œ $\\boldsymbol{\\mu}$ì™€ $\\bold{R}$ì„ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? minimizeí•˜ê³ ìí•˜ëŠ” ìš”ì†Œê°€ ë‘ ê°œì´ê¸° ë•Œë¬¸ì— ë¯¸ë¶„ì„ í•˜ê¸°ë„ ë‹¤ì†Œ ë‚œí•´í•˜ë‹¤. ë”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” EM Algorithmì´ë¼ëŠ” ë°©ì‹ì„ ì œì‹œí•œë‹¤. ì´ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒ Postingì— ëŒ€í•´ì„œ ìì„¸íˆ ë‹¤ë£¨ê² ì§€ë§Œ, ê°„ë‹¨íˆ ì„¤ëª…í•˜ìë©´ í•˜ë‚˜ì˜ Variableì„ Randomí•˜ê²Œ ì§€ì •í•˜ê³ , ë‹¤ë¥¸ Variableì˜ ìµœì ê°’ì„ êµ¬í•œ í›„ ì´ë¥¼ ë‹¤ì‹œ ëŒ€ì…í•˜ê³  ë°˜ëŒ€ Variableì„ ìµœì ê°’ìœ¼ë¡œ êµ¬í•˜ê¸°ë¥¼ ë°˜ë³µí•˜ë©´ì„œ ë” ì´ìƒ Variableì´ ìœ ì˜ë¯¸í•˜ê²Œ ë³€ê²½ë˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ë°˜ë³µí•´ì„œ êµ¬í•œ ê°’ì´ ìµœì ê°’ê³¼ ê·¼ì‚¬í•œë‹¤ëŠ” ì ì„ í™œìš©í•œ Algorithmì´ë‹¤. ì§€ê¸ˆì€ ë‹¤ì†Œ ì—‰ëš±í•  ìˆ˜ ìˆì§€ë§Œ, ì§€ê¸ˆì€ í•´ë‹¹ ë°©ë²•ì„ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ë‹¤. ì¦ëª…ì´ ê¶ê¸ˆí•˜ë‹¤ë©´, í•´ë‹¹ Posting([ğŸ”— [ML] 10. EM Algorithm](/posts/ml-em-algorithm))ì„ ì°¸ê³ í•˜ì.\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ ìˆ˜í–‰í•  ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. $\\boldsymbol{\\mu}$ë¥¼ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•œë‹¤.\n2. Assignment step: $\\boldsymbol{\\mu}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $\\bold{R}$ì„ êµ¬í•œë‹¤.  \n   $$\n   R_{ik} = \\begin{cases}\n    1 & \\text{if}\\quad k = \\argmin_{k}||x_{i} - \\mu_{k}||^{2} \\\\\n    0 & \\text{otherwise}\n   \\end{cases}\n   $$\n3. Update step: $\\bold{R}$ì´ ì£¼ì–´ì¡Œì„ ë•Œ, $\\boldsymbol{\\mu}$ë¥¼ êµ¬í•œë‹¤.  \n   ìš°ë¦¬ê°€ ë¶„ë¥˜í•œ $R$ì„ í™œìš©í•˜ì—¬ ê° kì— ì†í•˜ëŠ” dataì˜ í‰ê· ì„ í†µí•´ì„œ $\\boldsymbol{\\mu}$ë¥¼ êµ¬í•œë‹¤.\n   $$\n   \\mu_{k} = \\frac{\\sum_{i=1}^{N}R_{ik}x_{i}}{\\sum_{i=1}^{N}R_{ik}}\n   $$\n4. íŠ¹ì •ê°’ìœ¼ë¡œ $\\boldsymbol{\\mu}$ê°€ ìˆ˜ë ´í•  ë•Œê¹Œì§€ 2ë²ˆ, 3ë²ˆ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.\n\nì•„ë˜ëŠ” ì´ ê³¼ì •ì„ ê·¸ë¦¼ì„ í†µí•´ì„œ í‘œí˜„í•œ ê²ƒì´ë‹¤.\n\n![ml-clustering-1](/images/ml-clustering-1.jpg)\n\nK-means ë°©ì‹ì€ ìœ„ì™€ ê°™ì€ Iteration ì ˆì°¨ë¥¼ ë§ì´ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ë„ ëª‡ë²ˆì˜ ìˆ˜í–‰ë§Œìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤ëŠ” ê²ƒì„ ê´€ì¸¡í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, Assignment ì‹œì—ëŠ” $O(KND)$ì˜ ì‹œê°„ì´ ì†Œëª¨ë˜ê³ , Update ì‹œì—ëŠ” $O(N)$ ë§Œí¼ì˜ ì‹œê°„ì´ ì†Œëª¨ë˜ê¸° ë•Œë¬¸ì— ë¬´ê²ì§€ ì•Šê³ , êµ‰ì¥íˆ ê°„ë‹¨í•˜ë‹¤ëŠ” ì¥ì ì„ ê°–ê³  ìˆë‹¤. í•˜ì§€ë§Œ, ì´ ë°©ë²•ì€ Global Optimalì„ ì°¾ì„ ê²ƒì´ë¼ëŠ” í™•ì‹ ì„ ì¤„ ìˆ˜ ì—†ë‹¤. ê·¸ë ‡ê¸°ì— ì´ˆê¸°ê°’ì„ ì–´ë–»ê²Œ ì¡ëŠëƒì— ë”°ë¼ì„œ ê²°ê³¼ê°€ í¬ê²Œ ë³€í•  ìˆ˜ë„ ìˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼, outlier dataì— ëŒ€í•´ì„œë„ êµ‰ì¥íˆ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ ì‚¬ì§„ì—ì„œ ì™¼ìª½ë³´ë‹¤ ì˜¤ë¥¸ìª½ì´ ë” ì„±ê³µì ì¸ Clusteringì´ë¼ê³  ë§í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n![ml-clustering-2](/images/ml-clustering-2.jpg)\n\nì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ë“¤ì´ ì œì‹œë˜ì—ˆë‹¤.\n\n1. **K-means++**: ì´ˆê¸°ê°’ì„ ì˜ ì„¤ì •í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ, ì´ˆê¸°ê°’ì„ ì˜ ì„¤ì •í•˜ë©´ ìˆ˜ë ´í•˜ëŠ” ì†ë„ê°€ ë¹¨ë¼ì§€ê³ , Global Optimalì— ìˆ˜ë ´í•  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§„ë‹¤.\n2. **K-mendoids**: K-meansì—ì„œëŠ” ì¤‘ì‹¬ì ì„ dataì˜ í‰ê· ìœ¼ë¡œ ì„¤ì •í–ˆì§€ë§Œ, K-mendoidsì—ì„œëŠ” ì¤‘ì‹¬ì ì„ dataì˜ ì¤‘ê°„ê°’ìœ¼ë¡œ ì„¤ì •í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´ outlierì— ë¯¼ê°í•˜ì§€ ì•Šê²Œ ëœë‹¤.\n\n> <mark>**Soft K-means**</mark>\n\në§ˆì§€ë§‰ìœ¼ë¡œ K-means Clusteringì—ì„œ í™•ë¥ ì ì¸ ì ‘ê·¼ì„ ì‹œë„í•œ ë°©ë²• ë˜í•œ ì†Œê°œí•˜ê² ë‹¤. ì• ì„œ ë³¸ (Hard)K-meansì—ì„œëŠ” $\\bold{R}_{ik}$ë¥¼ 0 ë˜ëŠ” 1ë¡œ ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ, ì´ë¥¼ í™•ë¥ ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì— ëŒ€í•´ì„œ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰, ë‹¤ìŒê³¼ ê°™ì´ soft-max functionì„ í™œìš©í•œë‹¤ë©´ í‘œí˜„ì´ ê°€ëŠ¥í•  ê²ƒì´ë‹¤.\n\n$$\n\\bold{R}_{ik} = \\frac{\\exp(-\\beta||x_{i}-\\mu_{k}||^{2})}{\\sum_{l \\in {1, 2, \\cdots, K}} \\exp(-\\beta||x_{i}-\\mu_{l}||^{2})}\n$$\n\nì´ë ‡ê²Œ í™•ë¥ ì ìœ¼ë¡œ í‘œí˜„í•˜ê²Œ ë˜ë©´, ìš°ë¦¬ëŠ” ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ íŠ¹ì • Clusterë¡œ í•´ë‹¹ í™•ë¥ ì´ í¸í–¥ë˜ì–´ ìˆì„ ìˆ˜ë¡ ë” ì¢‹ì€ ë¶„ë¥˜ì¼ ê²ƒì´ë¼ëŠ” ì‚¬ì „ ì§€ì‹(Prior)ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Cost Functionì„ ë³€ê²½í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}||x_{i} - \\mu_{k}||^{2} - \\frac{1}{\\beta}\\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\log\\bold{R}_{ik}\n$$\n\në’· ë¶€ë¶„ì— ìƒˆë¡œ ì¶”ê°€ëœ $-\\frac{1}{\\beta}\\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\log\\bold{R}_{ik}$ëŠ” $R_{ik}$ê°€ í™•ë¥ ì´ ë˜ì—ˆê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ìƒ Entropyë¥¼ ì˜ë¯¸í•œë‹¤. EntropyëŠ” ê· í˜•ì¡íŒ ë¶„í¬ì¼ ìˆ˜ë¡ ì»¤ì§€ê³ , skewëœ ê²½ìš°ì—ëŠ” ì‘ì•„ì§€ê¸° ë•Œë¬¸ì— ì ì ˆí•œ ì§€í‘œë¼ê³  í•  ìˆ˜ ìˆë‹¤. $\\beta$ëŠ” ì´ëŸ¬í•œ priorë¥¼ ì–¼ë§ˆë‚˜ ì‚¬ìš©í• ì§€ì— ëŒ€í•œ hyperparameterì´ë‹¤. $\\beta$ê°€ í´ ìˆ˜ë¡ ì‚¬ì‹¤ìƒ Hard K-meansì™€ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ê²Œ ë˜ê³ , $\\beta$ê°€ ì‘ì„ ìˆ˜ë¡ Entropyë¥¼ ë” ì¤‘ìš”ì‹œí•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê²Œ ëœë‹¤.\n\n### Gaussian Mixture Model\n\nGaussian Mixture Model, ì¼ëª… GMMì€ Finite Mixture Modelì˜ ì¼ì¢…ì´ë‹¤. Finite Mixture Modelì€ ìš°ë¦¬ê°€ ì¶”ì •í•˜ê³ ì í•˜ëŠ” í™•ë¥  ë¶„í¬ê°€ ë‹¤ì–‘í•œ í™•ë¥  ë¶„í¬ ëª‡ ê°œì˜ ì¡°í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë¶„í¬ë¼ê³  ê°€ì •í•˜ê³ , í•´ë‹¹ í™•ë¥  ë¶„í¬ì˜ Parameterë¥¼ í•™ìŠµ(Learning) ë‹¨ê³„ì—ì„œ ì°¾ì•„ë‚´ê³ , ì´ë¥¼ ì´ìš©í•´ì„œ ìƒˆë¡œìš´ dataì— ëŒ€í•´ì„œ ì¶”ì •(Inference)í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n\n$$\np(x) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x, z=k) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x|z=k)p(z=k) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p_{k}(x)p(z=k)\n$$\n\nì—¬ê¸°ì„œ $z$ëŠ” ê´€ì¸¡í•  ìˆ˜ ì—†ëŠ” latent(hidden) variableë¡œ dataê°€ ëª‡ ë²ˆì§¸ í™•ë¥  ë¶„í¬ì— ì†í•  ê²ƒì¸ì§€ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ, $p(z=k)$ kë²ˆì§¸ ë¶„í¬ì— ì†í•  í™•ë¥ ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ëŒ€ê²Œ ì´ê²ƒì´ ì–´ëŠì •ë„ë¡œ í™•ë¥  ë¶„í¬ë¥¼ ì„ëŠ”ì§€ë¥¼ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì— mixing parameterë¼ê³ ë„ ë¶€ë¥¸ë‹¤.\n\nì´ì— ë”°ë¼ GMMì€ ê° $p_{k}(x)$ê°€ Gaussian Distributionì´ë¼ê³  ê°€ì •í•˜ëŠ” Finite Mixture Modelì¸ ê²ƒì´ë‹¤.\n\n![ml-gmm-graphical-form](/images/ml-gmm-graphical-form.jpg)\n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Graphical Model í˜•íƒœë¡œ Finite Mixture Modelì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ $\\pi,\\, \\mu,\\, \\Sigma$ëŠ” Parameterë¥¼ ì˜ë¯¸í•œë‹¤.\n\n- $\\pi_{k} = p(z = k)$\n- $\\mu_{k} = E[x|z=k]$ ì¦‰, Gaussianì˜ ê¸°ëŒ“ê°’ì„ ì˜ë¯¸í•œë‹¤.\n- $\\Sigma_{k} = Cov[x|z=k]$ ì¦‰, Gaussianì˜ ë¶„ì‚°ì„ ì˜ë¯¸í•œë‹¤.\n\nì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” ìœ„ì—ì„œ ì œì‹œí•œ í™•ë¥ ì„ ë‹¤ìŒê³¼ ê°™ì´ ì¬ì •ì˜í•  ìˆ˜ ìˆë‹¤. (Joint Probabilityë¥¼ Bayesian Networkë¡œ í‘¼ ì‹ì´ë‹¤. ëª¨ë¥´ê² ë‹¤ë©´, [ğŸ”— [ML] 8. Graphical Model](/posts/ml-graphical-model#Graphical-Model)ì—ì„œ Bayesian Networkë¥¼ ë‹¤ì‹œ ì‚´í´ë³´ê³  ì˜¤ì.)\n\n$$\n\\begin{align*}\np(x) &= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x, z=k) \\\\\n&= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(z=k| \\pi_{k})p(x|z=k, \\mu_{k}, \\Sigma_{k}) \\\\\n&= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\pi_{k}\\mathcal{N}(x|\\mu_{k}, \\Sigma_{k})\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì‹¤ì œë¡œ ì¶”ì¸¡(Inference)ì„ í•  ë•Œì—ëŠ” $p(z|x)$ê°€ í•„ìš”í•˜ë‹¤. ì´ëŠ” ìš°ë¦¬ê°€ posteriorë¥¼ í™œìš©í•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\hat{k} &=\\argmax_{k}p(z=k|x) \\\\\n&= \\argmax_{k}\\frac{p(x|z=k)p(z=k)}{p(x)} \\\\\n&= \\argmax_{k}p(x|z=k)p(z=k)\\\\\n&= \\argmax_{k}\\pi_{k}\\mathcal{N}(x|\\mu_{k}, \\Sigma_{k})\n\\end{align*}\n$$\n\ní•™ìŠµ(Learning)ì„ í•  ë•Œì—ëŠ” ê²°êµ­ $\\pi,\\, \\mu,\\, \\Sigma$ ì´ ì„¸ ê°œì˜ parameter ê°’ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì´ê²ƒì€ ìš°ë¦¬ê°€ Parametric Estimationì—ì„œ ì¤„ê¸°ì°¨ê²Œ í–ˆë˜ MLEë¥¼ ì´ìš©í•˜ë©´ ëœë‹¤. ì´ë¥¼ ìœ„í•œ LikelihoodëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\pi,\\, \\mu,\\, \\Sigma) &= \\log{p(\\mathcal{D} | \\pi,\\, \\mu,\\, \\Sigma)} \\\\\n&= \\log{\\prod_{i=1}^{N}{p(x_{i} | \\pi,\\, \\mu,\\, \\Sigma)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{p(x_{i}| \\pi,\\, \\mu,\\, \\Sigma)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{\\sum_{k=1}^{K}{\\pi_{k}\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma_{k})}}}\n\\end{align*}\n$$\n\ní•˜ì§€ë§Œ, ì´ê²ƒì„ ë‹¨ìˆœí•œ Optimization Techniqueìœ¼ë¡œëŠ” í’€ ìˆ˜ ì—†ë‹¤. ì™œëƒí•˜ë©´, ë‹¨ìˆœí•œ ë¯¸ë¶„ìœ¼ë¡œ ê° parameterë¥¼ êµ¬í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, EM Algorithmì„ ì´ìš©í•´ì„œ í’€ì–´ì•¼ í•œë‹¤. (ì´ê²ƒì€ [ğŸ”— [ML] 10. EM Algorithm](/posts/ml-em-algorithm)ì—ì„œ ë‹¤ë£¬ë‹¤.)\n\në”°ë¼ì„œ, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì„ì˜ì˜ $\\pi,\\, \\mu,\\, \\Sigma$ë¥¼ ê°€ì •í•œ ìƒíƒœì—ì„œ dataì— ì•Œë§ëŠ” ìµœì ì˜ Cluster setì„ êµ¬í•˜ê³ , dataì— clusterê°€ labelëœ ìƒíƒœì—ì„œ ìµœì ì˜ $\\pi,\\, \\mu,\\, \\Sigma$ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ëŠ” ê²ƒì´ë‹¤.\n\n![ml-gmm-1](/images/ml-gmm-1.jpg)\n\nê·¸ë ‡ë‹¤ë©´, ì´ë¥¼ ì‹¤ì œë¡œ ì–´ë–»ê²Œ í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤. í•˜ì§€ë§Œ, ê·¸ëƒ¥ ëª¨ë“  Gaussian í˜•íƒœë¥¼ ìœ„í•œ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ë‹¤ì†Œ ì‹ì´ ë³µì¡í•´ì§€ê¸° ë•Œë¬¸ì— isotropic Gaussian(ëª¨ë“  ë°©í–¥ì—ì„œ ë¶„ì‚°ì´ ë™ì¼í•œ Gaussian)ì„ ê°€ì •ìœ¼ë¡œ í•˜ê² ë‹¤.\n\në˜í•œ, ë‹¤ìŒê³¼ ê°™ì€ ìš”ì†Œë¥¼ ì¶”ê°€ë¡œ ì •ì˜í•˜ì.\n\n1. $z_{i} \\in \\{1, 2, \\cdots, K\\}$ : $i$ë²ˆì§¸ dataê°€ ì†í•˜ëŠ” clusterì˜ index  \n   $z_{ik} = \\begin{cases} 1 & \\text{if } z_{i} = k \\\\ 0 & \\text{otherwise} \\end{cases}$\n2. $\\theta_{k} = (\\pi_{k},\\, \\mu_{k},\\, \\Sigma_{k})$ : $k$ë²ˆì§¸ clusterë¥¼ ìœ„í•œ parameterì˜ ì§‘í•©  \n   $\\theta = (\\pi,\\, \\mu,\\, \\Sigma)$ : parameterì˜ ì§‘í•©  \n\nì• ì„œ ë§í•œ ë°”ì™€ ê°™ì´ ì´ì œ ìš°ë¦¬ëŠ” $\\theta$ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ $z_{i}$ì— í•´ë‹¹í•˜ëŠ” ì •ë³´ë„ ì•Œê³  ìˆë‹¤. ë”°ë¼ì„œ, Likelihood ì‹ë„ ë³€í˜•ë˜ì–´ì•¼ í•œë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) &= \\log{p(\\mathcal{D} | \\theta)} \\\\\n&= \\sum_{i=1}^{N}{\\log{p(x_{i}, z_{i}| \\theta)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{p(z_{i} | \\theta) \\times p(x_{i}| z_{i}, \\theta)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{(\\prod_{k=1}^{K}{\\pi_{k}^{z_{ik}}} \\times \\prod_{k=1}^{K}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)^{z_{ik}}})}} \\\\\n&= \\sum_{i=1}^{N}{\\sum_{k=1}^{K}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})^{z_{ik}}}} \\\\\n&= \\sum_{i=1}^{N}{\\sum_{k=1}^{K}z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}} \\\\\n\\end{align*}\n$$\n\n<!-- TODO: ì—¬ê¸° ë¶€ë¶„ EM algorithm ë³µìŠµí•œ ì´í›„ì— ë‹¤ì‹œ ì ê²€(p.28~31) -->\n\nì´ì— ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ ìš°ë¦¬ëŠ” ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.\n\n- **E-step**  \n  $$\n  r_{ik} = p(z_{i} = k | x_{i}, \\theta) = \\frac{p(z_{i} = k, x_{i} | \\theta)}{p(x_{i} | \\theta)} = \\frac{p(z_{i} = k, x_{i} | \\theta)}{\\sum_{l=1}^{K}{p(z_{i} =l, x_{i} | \\theta)}} = \\frac{\\pi_{k}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)}}{\\sum_{l=1}^{K}{\\pi_{l}{\\mathcal{N}(x_{i}|z_{i} = l, \\mu_{l}, \\Sigma_{l} I)}}}\n  $$\n- **M-step**  \n  $$\n  \\begin{align*}\n  \\mu_{k} &= \\frac{\\sum_{i=1}^{N}{r_{ik}x_{i}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\Sigma_{k} &= \\frac{1}{D}\\frac{\\sum_{i=1}^{N}{r_{ik}||x_{i} - \\mu_{k}||^{2}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\pi_{k} &= \\frac{1}{N}\\sum_{i=1}^{N}{r_{ik}} \\\\\n  \\end{align*}\n  $$\n\nì´ë ‡ê²Œ ì‹ì„ ì •ë¦¬í•˜ê³  ë‚˜ë©´, í•˜ë‚˜ì˜ insightë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ê·¸ê²ƒì€ ë°”ë¡œ K-means Clusteringì€ ì‚¬ì‹¤ GMMì˜ í•˜ë‚˜ì˜ special caseë¼ëŠ” ê²ƒì´ë‹¤. ë§Œì•½, ìš°ë¦¬ê°€ $\\pi_{k},\\, \\Sigma_{k}$ë¥¼ ëª¨ë‘ ê°™ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ë©´, $\\pi_{k} = \\frac{1}{K}$ì´ê³  $\\Sigma_{k} = \\Sigma$ê°€ ëœë‹¤ê³  í•˜ì. ì´ë•Œ EM algorithmì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n- **E-step**  \n  $$\n  r_{ik} = \\begin{cases} 1 & k = \\argmax_{l\\in \\{1, 2, \\cdots, K\\}} p(x_{i}|z_{i} = l, \\mu_{l}, \\Sigma) \\\\ 0 & \\text{otherwise} \\end{cases}\n  $$  \n  ì´ëŠ” ì‚¬ì‹¤ìƒ K-means Clusteringì—ì„œ ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬ë¥¼ í†µí•´ì„œ êµ¬í–ˆë˜ ê²ƒê³¼ ë§¤ìš° ìœ ì‚¬í•œ ì‹ì´ë‹¤.\n- **M-step**  \n  $$\n  \\begin{align*}\n  \\mu_{k} &= \\frac{\\sum_{i=1}^{N}{r_{ik}x_{i}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\pi_{k} &= \\frac{1}{N}\\sum_{i=1}^{N}{r_{ik}}\n  \\end{align*}\n  $$  \n  $\\pi_{k}$ê°€ ì¶”ê°€ë˜ê¸°ëŠ” í–ˆì§€ë§Œ, $\\mu_{k}$ë¥¼ êµ¬í•˜ëŠ” ì‹ì€ ì™„ì „ ë™ì¼í•˜ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-clustering","date":"2022-11-23 09:19","title":"[ML] 9. Clustering","category":"AI","tags":["ML","UnsupervisedLearning","Clustering","K-means","GMM"],"desc":"ì´ì „ê¹Œì§€ì˜ Postingì—ì„œëŠ” Supervised Learning ì¦‰, ì´ë¯¸ Labelingì´ ì™„ë£Œëœ ë°ì´í„°ì— ì˜í•œ Learningì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ì—ˆë‹¤. ì§€ê¸ˆë¶€í„°ëŠ” Unsupervised Learningì— ëŒ€í•´ì„œ ì¡°ê¸ˆ ë” ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤. ëŒ€í‘œì ì¸ Unsupervised Learningì€ Clustering, Feature Selection(or Dimensionality Reduction), Generative Model ë“±ì´ ì¡´ì¬í•œë‹¤. ì´ë“¤ì— ëŒ€í•´ì„œ ì°¨ê·¼ì°¨ê·¼ ì‚´í´ë³´ë„ë¡ í•˜ê³ , í•´ë‹¹ Postingì—ì„œëŠ” ê°€ì¥ ëŒ€í‘œì ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” Clusteringì„ ë¨¼ì € ì‚´í´ë³´ë©´ì„œ Unsupervised Learningì— ëŒ€í•œ ê³„ëµì ì¸ ì´í•´ë¥¼ í•´ë³´ë„ë¡ í•˜ê² ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"GMM"}},"__N_SSG":true}