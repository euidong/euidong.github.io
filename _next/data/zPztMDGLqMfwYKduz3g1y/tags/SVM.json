{"pageProps":{"posts":[{"content":"\n## Intro\n\nìš°ë¦¬ëŠ” Classificationì„ í•˜ê¸° ìœ„í•´ì„œ Logistic Regressionì„ ìˆ˜í–‰í•˜ì˜€ë‹¤. ê·¸ ê²°ê³¼ ê²°êµ­ Classificationë„ ê²°êµ­ì€ ì„ ì„ ê¸‹ëŠ” ê²ƒì´ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë¦¬ê²Œ ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ê·¸ì¹˜ì§€ ì•Šê³  í•˜ë‚˜ ë” ê³ ë¯¼í•´ ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ ìˆë‹¤. ë°”ë¡œ ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ì„œ ì™„ë²½í•˜ê²Œ êµ¬ë¶„í•˜ëŠ” decision boundaryê°€ ì—¬ëŸ¬ ê°œ ìˆì„ ë•Œ, ì–´ë–¤ ê²ƒì´ ê°€ì¥ ì¢‹ì€ ê²ƒì¼ê¹Œ? ì´ê²ƒì— ëŒ€í•œ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ” ê²ƒì´ SVMì´ë‹¤. í•´ë‹¹ Postingì—ì„œëŠ” ì´ì— ëŒ€í•´ì„œ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## (Hard Margin) SVM\n\nSoft Vector Machineì˜ ì•½ìë¡œ, ìœ„ì—ì„œ ì œì‹œí•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ Marginì´ë¼ëŠ” ê²ƒì„ ë„ì…í•˜ì˜€ë‹¤.\n\n> **Margin**\n\n**Margin**ì´ë€ decison boundaryì™€ ê°€ì¥ ê°€ê¹Œìš´ ê° classì˜ ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ 2ë¡œ ë‚˜ëˆˆ ê°’ì´ë‹¤.\n\n![svm-1](/images/svm-1.jpg)\n\nìœ„ì˜ ê·¸ë¦¼ì€ ë˜‘ê°™ì€ ë°ì´í„° ë¶„í¬ì—ì„œ ëŒ€í‘œì ì¸ decision boundary ë‘ ê°œë¥¼ ì œì‹œí•œ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” êµ‰ì¥íˆ ë§ì€ decision boundaryë¥¼ ê·¸ë¦´ ìˆ˜ ìˆë‹¤. ê·¸ ì¤‘ì—ì„œë„ íŒŒë€ìƒ‰ ì‹¤ì„ ì´ ì§ê´€ì ìœ¼ë¡œ ê°€ì¥ ì ì ˆí•œ decision boundaryê°€ ë  ê²ƒì´ë¼ê³  ì§ì‘í•  ìˆ˜ ìˆë‹¤. ê·¸ ì´ìœ ëŠ” í•„ì—°ì ìœ¼ë¡œ dataëŠ” noiseì— ì˜í•œ ì˜¤ì°¨ê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ”ë° ì‹¤ì œ ë°ì´í„°ì˜ ì˜¤ì°¨ì˜ í—ˆìš© ë²”ìœ„ë¥¼ ìš°ë¦¬ëŠ” **margin**(=capability of unexpected noise)ë§Œí¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ë¡œ ì´ë¥¼ í•´ì„í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì´ marginì„ í¬ê²Œ í•˜ë©´ í•  ìˆ˜ë¡ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§€ëŠ” ì„ ì„ ê·¸ì„ ìˆ˜ ìˆì„ ê²ƒì´ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.\n\nì´ê²ƒì´ SVMì˜ í•µì‹¬ ì•„ì´ë””ì–´ì´ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, marginì„ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•´ë³´ì. ìš°ë¦¬ê°€ decision boundaryë¥¼ $f(\\bold{x}) := \\bold{w}^{\\top}\\bold{x} + b = 0$ì´ë¼ê³  í•œë‹¤ë©´, ì ($\\bold{x}_{i}$)ê³¼ vector ì§ì„  vector ì‚¬ì´ì˜ ê±°ë¦¬ ê³µì‹ì„ í†µí•´ì„œ ${{|f(\\bold{x}_{i})|}\\over{||\\bold{w}||}}$ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\në”°ë¼ì„œ marginì€ ìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\min_{i}{{|f(\\bold{x}_{i})|}\\over{||\\bold{w}||}}\n$$\n\n```plaintext\n ğŸ¤” Canonical(ë²•ì¹™ê¹Œì§€ëŠ” ì•„ë‹ˆì§€ë§Œ ì‚¬ì‹¤ìƒ í‘œì¤€í™”ëœ) SVM\n\n SVMì—ì„œëŠ” f(x) = 0ì¸ ë“±ì‹ í˜•íƒœë¥¼ ê°™ëŠ”ë‹¤. ì¦‰ f(x)ì— ì–´ë–¤ ê°’ì„ ê³±í•´ë„ ë˜‘ê°™ë‹¤ëŠ” ê²ƒì´ë‹¤.\n ê·¸ëŸ°ë° marginì˜ í¬ê¸°ë¥¼ êµ¬í•  ë•Œì—ëŠ”, wì™€ bì— ì–´ë–¤ ê°’ì´ ê³±í•´ì§„ë‹¤ë©´ ì´ ê°’ì´ êµ‰ì¥íˆ ë‹¬ë¼ì§€ê²Œ ëœë‹¤.\n ë”°ë¼ì„œ, ì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” marginì—ì„œì˜ |f(x)| = 1ì´ ë  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•œë‹¤. \n ì´ë ‡ê²Œ í•˜ë©´ ê³„ì‚°ì´ êµ‰ì¥íˆ ì‰¬ì›Œì§„ë‹¤.\n```\n\n![svm-2](/images/svm-2.jpg)\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ í˜•íƒœë¡œ $\\bold{x}^{-}$ì™€ $\\bold{x}^{+}$ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\nì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ marginì„ ì •ì˜í•´ë³´ì.\n\n$$\n\\begin{align*}\n\\rho &= {1\\over2}\\{ {{|f(\\bold{x}^{+})|}\\over{||\\bold{w}||}} - {{|f(\\bold{x}^{-})|}\\over{||\\bold{w}||}}  \\} \\\\\n&= {1\\over2}{1\\over{||\\bold{w}||}}\\{\\bold{w}^{\\top}\\bold{x}^{+} - \\bold{w}^{\\top}\\bold{x}^{-}\\} \\\\\n&= {1\\over{||\\bold{w}||}}\n\\end{align*}\n$$\n\n> **Optimization**\n\nê·¸ë ‡ë‹¤ë©´, ì´ì œ ìš°ë¦¬ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ì¤€ë¹„ê°€ ëœ ê²ƒì´ë‹¤. ìš°ë¦¬ê°€ í•˜ê³ ì í•˜ëŠ” ê²ƒì€ marginì„ ìµœëŒ€í™”í•˜ë©´ì„œë„, ëª¨ë“  dataë¥¼ ì˜¤ë¥˜ì—†ì´ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ Constraint Optimization í˜•íƒœë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & {1\\over{||\\bold{w}||}} &\\\\\n  \\text{subject to} \\quad & y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\geq 1, & i = 1, ..., N\n\\end{align*}\n$$\n\nConditional Optimizationì€ ì´ì „ Posting([[ML] 0. Base Knowledge](/posts/ml-base-knowledge))ì—ì„œ ë‹¤ë£¬ë°” ìˆë‹¤. í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ ë¯¸ìˆ™í•˜ë‹¤ë©´ í•œ ë²ˆ ì‚´í´ë³´ê³  ì˜¤ë„ë¡ í•˜ì.\n\nìœ„ ë‚´ìš©ì„ ìˆ™ì§€í•˜ì˜€ë‹¤ë©´, ìœ„ì˜ í¼ì´ ë‹¤ì†Œ ë°”ë€Œì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ê²ƒì´ë‹¤. í•´ë‹¹ í˜•íƒœë¥¼ ë°”ê¾¸ë©´ì„œ, minimize í˜•íƒœë¥¼ ë¯¸ë¶„ì´ ê°„í¸í•  ìˆ˜ ìˆë„ë¡ ë°”ê¾¸ë„ë¡ í•˜ê² ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}|| &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\leq 0, & i = 1, ..., N\n\\end{align*}\n$$\n\nìš°ì„  lagrangianì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}\\alpha_{i}(1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b))\n$$\n\nì´ê²ƒì— KKT Conditionì„ ì ìš©í•˜ì—¬ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë“±ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\nì´ë¥¼ $\\mathcal{L}$ì— ëŒ€ì…í•˜ì—¬ ì‹ì„ ì •ë¦¬í•˜ë©´, ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\mathcal{L} = -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i}\n$$\n\nì´ì œ ì´ê²ƒì„ ì´ìš©í•´ì„œ Dual Problemì„ ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & \\alpha_{i} \\geq 0, & i = 1, ..., N \n\\end{align*}\n$$\n\nì´ ì‹ì—ì„œ ëˆˆì—¬ê²¨ ë³¼ì ì€ ë°”ë¡œ constraint ë¶€ë¶„ì´ë‹¤. ì´ ê³¼ì •ì„ í†µí•´ì„œ ê²°ë¡ ì ìœ¼ë¡œ constraint ë¶€ë¶„ì´ ë¶€ë“±ì‹ì—ì„œ ë“±ì‹ì´ ë˜ì—ˆë‹¤. ì´ëŠ” ì—°ì‚° ê³¼ì •ì„ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ í•œë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼ $\\bold{x}_{i}^{\\top}\\bold{x}_{j}$ëŠ” í•œ ë²ˆ ê³„ì‚°í•˜ë©´, ì „ì²´ ê³¼ì •ì—ì„œ ê³„ì†í•´ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì»´í“¨íŒ… ì‹œì—ëŠ” êµ‰ì¥í•œ ì´ì ì„ ë°œíœ˜í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì‹¤ì œë¡œ ê°’ì„ êµ¬í•  ë•Œì—ëŠ” ì´ê²ƒì„ ì´ìš©í•˜ì—¬ ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¼ë°˜ì ì´ë‹¤.\n\n## (Soft Margin) SVM\n\nSVMì˜ ëª¨ë“  ì ˆì°¨ë¥¼ ì‚´í´ë³¸ ê²ƒ ê°™ì§€ë§Œ, ìš°ë¦¬ê°€ ê°„ê³¼í•œ ì‚¬ì‹¤ì´ í•˜ë‚˜ ìˆë‹¤. ë°”ë¡œ ê·¸ê²ƒì€ ìš°ë¦¬ëŠ” dataê°€ í•˜ë‚˜ì˜ ì„ ì„ í†µí•´ì„œ ì™„ë²½í•˜ê²Œ ë‚˜ë‰˜ì–´ì§„ë‹¤ê³  ê°€ì •í–ˆë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ëŠ” ê·¸ë ‡ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ í¬ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì–´ëŠ ì •ë„ì˜ ì˜¤ì°¨ë¥¼ í—ˆìš©í•  ìˆ˜ ìˆë„ë¡ í•´ì•¼ í•œë‹¤. ì´ë¥¼ slack($\\zeta$)ì´ë¼ê³  í•œë‹¤.\n\n![svm-2](/images/svm-2.jpg)\n\nì´ë¥¼ ì ìš©í•˜ë©´, ìš°ë¦¬ì˜ ëª©ì í•¨ìˆ˜ì™€ ì œì•½ ì¡°ê±´ì„ ë³€ê²½í•´ì•¼ í•œë‹¤. ì´ë¥¼ ë³€ê²½í•˜ëŠ” ë°©ë²•ì€ ë‘ ê°€ì§€ê°€ ì¡´ì¬í•˜ëŠ”ë° ê° ê° slack variableì˜ L2-normì„ ëª©ì í•¨ìˆ˜ì— ë”í•˜ëŠ” ë°©ì‹ê³¼ L1-normì„ ë”í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n\n> **L2-norm Optimization**\n\në¨¼ì € L2-normì„ ë”í•˜ëŠ” ë°©ì‹ì„ ì•Œì•„ë³´ì\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}|| + C\\sum_{i=1}^{N}\\zeta_{i}^{2} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, & i = 1, ..., N\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ $C$ëŠ” margin ìµœëŒ€í™”ì™€ slackness ì •ë„ì˜ ìƒëŒ€ê°’ì„ ì˜ë¯¸í•œë‹¤. ë§Œì•½, slacknessë³´ë‹¤ marginì˜ ìµœëŒ€í™”ê°€ ì¤‘ìš”í•˜ë‹¤ë©´, Cê°’ì€ ì»¤ì§€ê³  ë°˜ëŒ€ë¼ë©´ ì´ ê°’ì€ ì‘ì•„ì§„ë‹¤.\n\nìš°ì„  lagrangianì„ ë¨¼ì € êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}|| + {C\\over2}\\sum_{i=1}^{N}\\zeta_{i}^{2} + \\sum_{i=1}^{N}\\alpha_{i}(1 - \\zeta_{i} y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b))\n$$\n\nKKT conditionì„ ì´ìš©í•˜ì—¬ ì£¼ìš” ê°’ë“¤ì„ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë“±ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n$$\n\\boldsymbol{\\zeta} = {\\alpha\\over{C}}\n$$\n\në§ˆì§€ë§‰ìœ¼ë¡œ ì´ë¥¼ Dual Problemìœ¼ë¡œ ì¬ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\bold{x}_{i}^{\\top}\\bold{x}_{j} + {1\\over{C}}\\delta_{ij}) + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & \\alpha_{i} \\geq 0, & i = 1, ..., N \n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ $\\delta_{ij}$ëŠ” ë‹¨ìœ„í–‰ë ¬ì´ë‹¤. ê¸°ì¡´ hard margin svmê³¼ ë¹„êµí–ˆì„ ë•Œ, ${1\\over{C}}\\delta_{ij}$ ì™¸ì—ëŠ” ë°”ë€Œì§€ ì•ŠëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n> **L1-norm Optimization**\n\nê·¸ ë‹¤ìŒì€ L1-normì´ë‹¤.\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}|| + C\\sum_{i=1}^{N}\\zeta_{i} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, & \\\\\n  & \\zeta_{i} \\geq 0 & i = 1, ..., N\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œëŠ” slack variableì´ ë°˜ë“œì‹œ 0ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ë‹¤ëŠ” ê²ƒì„ ì£¼ì˜í•˜ì.\n\nlagrangianì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}|| + C\\sum_{i=1}^{N}\\zeta_{i} + \\sum_{i=1}^{N}\\alpha_{i}(1 - \\zeta_{i} y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b)) -  \\sum_{i=1}^{N}\\beta_{i}\\zeta_{i}\n$$\n\nKKT conditionì„ ì´ìš©í•˜ì—¬ ì£¼ìš” ê°’ë“¤ì„ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë“±ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n$$\n\\sum_{i=1}^{N}\\beta_{i} = C\n$$\n\në§ˆì§€ë§‰ìœ¼ë¡œ ì´ë¥¼ Dual Problemìœ¼ë¡œ ì¬ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & 0 \\leq \\alpha_{i} \\leq C, & i = 1, ..., N \n\\end{align*}\n$$\n\nê²°êµ­ ê¸°ì¡´ Hard marginê³¼ ë¹„êµí–ˆì„ ëŒ€ëŠ” ë§ˆì§€ë§‰ constraintì— $\\alpha_{i} \\leq C$ê°€ ì¶”ê°€ëœ ê²ƒ ë°–ì— ì—†ë‹¤.\n\n---\n\në§ˆì§€ë§‰ìœ¼ë¡œ ì—¬ê¸°ì„œ í•˜ë‚˜ì˜ insightë¥¼ ë” ì–»ì„ ìˆ˜ ìˆë‹¤.  \nL1-normì˜ optimizationìœ¼ë¡œ ëŒì•„ê°€ë³´ì.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}|| + C\\sum_{i=1}^{N}\\zeta_{i} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, & \\\\\n  & \\zeta_{i} \\geq 0 & i = 1, ..., N\n\\end{align*}\n$$\n\nëª©ì  í•¨ìˆ˜ì˜ slack variableì— constraintì˜ ê°’ì„ ëŒ€ì…í•˜ì—¬, ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë‹¤.\n\n$$\n\\min {C^{\\prime}\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}max\\{ 0, 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\}\n$$\n\nì´ í˜•íƒœëŠ” logistric regressionì— regularizationì„ ìˆ˜í–‰í•œ ê²ƒê³¼ ë™ì¼í•œ í˜•íƒœë¥¼ ê°€ì§€ê²Œ ëœë‹¤. ì¦‰, ì´ì „ logistic regressionì—ì„œ regularizationì„ ë‹¤ë£¨ì§€ ì•Šì•˜ëŠ”ë°, ê²°êµ­ì€ soft margin svmì˜ L1-norm ëª©ì í•¨ìˆ˜ê°€ logistic regression ì¤‘ì—ì„œë„ hinge functionì´ë¼ëŠ” ê²ƒì„ ì´ìš©í–ˆì„ ë•Œì˜ regularizationì´ ë˜ëŠ” ê²ƒì´ë‹¤.\n\n## Generalization\n\nì—¬íƒœê¹Œì§€ ì‚´í´ë³¸ Regressionì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” Generalí•œ Classification ë°©ì‹ì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤. ìš°ì„  ì•„ë˜ ì‹ì„ ì‚´í´ë³´ì.\n\n- Linear Regression(Quadratic Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}{1\\over2}(1 - y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})) )^{2}$\n- Logit Regresion(Log Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}\\log( 1 + \\exp[-y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})]) )$\n- Binary SVM(Hinge Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}max\\{ 0, 1 - y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})) \\}$\n\nì—¬íƒœê¹Œì§€ ë‚˜ì˜¨ ì‹ë“¤ì„ ì‚´í´ë³´ë©´ ìœ„ì™€ ê°™ë‹¤. ìš°ë¦¬ëŠ” ì—¬ê¸°ì„œ ì•„ë˜ì™€ ê°™ì€ ì¼ë°˜ì ì¸ í˜•íƒœì˜ Classificationì„ ì œì‹œí•  ìˆ˜ ìˆë‹¤. \n\n- General Classification  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}|| + \\sum_{i=1}^{N}\\varepsilon\\log( 1 + \\exp[-y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})]) )$\n\nì—¬ê¸°ì„œ $\\varepsilon$ì´ 1ì´ë©´ ë°”ë¡œ logistic regressionì´ ë˜ê³ , $\\varepsilon$ì´ 0ì— ìˆ˜ë ´í•  ìˆ˜ë¡ SVMì´ ëœë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ ì´ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.\n\n![compare-regressions](/images/compare-regressions.jpg)\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)","slug":"ml-svm","date":"2022-10-18 17:29","title":"[ML] 4. SVM","category":"AI","tags":["ML","SVM","GeneralClassifier"],"desc":"ìš°ë¦¬ëŠ” Classificationì„ í•˜ê¸° ìœ„í•´ì„œ Logistic Regressionì„ ìˆ˜í–‰í•˜ì˜€ë‹¤. ê·¸ ê²°ê³¼ ê²°êµ­ Classificationë„ ê²°êµ­ì€ ì„ ì„ ê¸‹ëŠ” ê²ƒì´ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë¦¬ê²Œ ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ê·¸ì¹˜ì§€ ì•Šê³  í•˜ë‚˜ ë” ê³ ë¯¼í•´ ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ ìˆë‹¤. ë°”ë¡œ ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ì„œ ì™„ë²½í•˜ê²Œ êµ¬ë¶„í•˜ëŠ” decision boundaryê°€ ì—¬ëŸ¬ ê°œ ìˆì„ ë•Œ, ì–´ë–¤ ê²ƒì´ ê°€ì¥ ì¢‹ì€ ê²ƒì¼ê¹Œ? ì´ê²ƒì— ëŒ€í•œ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ” ê²ƒì´ SVMì´ë‹¤. í•´ë‹¹ Postingì—ì„œëŠ” ì´ì— ëŒ€í•´ì„œ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"SVM"}},"__N_SSG":true}