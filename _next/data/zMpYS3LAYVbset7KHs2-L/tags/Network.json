{"pageProps":{"posts":[{"content":"\n## Reference\n\n- [🔗 Docker Deep Dive](https://www.oreilly.com/library/view/docker-deep-dive/9781800565135/), Nigel Poulton\n- Tumbnail : Photo by [Michael](https://unsplash.com/@michael75?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/cargo-ships?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n\n## Intro\n\nDocker Swarm을 docker stack을 이용하여 실행시키게 된다면, 무엇이 생성되는가? 우리는 서비스가 생성되기도 전에 network가 생성되는 것을 볼 수가 있다. container와 container간 그리고, host를 통해 외부 internet환경에 container를 연결 시키는 모든 과정을 알아보자.\n\nDocker를 사용하다보면, host와 통신을 위해 외부로 port를 열어주는 것과 container 간의 통신을 헷갈려 하는 사람들이 생각보다 많은 것 같다. 심지어는 container간 통신을 위해서 localhost로 정보를 주고받을려고 하는 몹쓸 시도를 하는 관경도 몇몇 봐왔다.\n\n따라서, 우리는 한 번 Docker의 network에 대해서 한 번 공부해보는 것이 좋을 것이다.\n\n해당 차시에서는 우선 전체적인 docker network를 설명하는 기본적인 키워드를 알아볼 것이고,\n\n2 차시에서는 주로 사용되는 docker network driver를 알아볼 것이고,\n\n3 차시에서는 libnetwork의 핵심 기능 중 service discovery, load balancing에 대해서 알아보겠다.\n\n## Docker Networking Base\n\n우리가 기억해야 할 것은 CNM, libnetwork, Driver 이렇게 3가지다. 각 각이 무엇인지는 차례차례 알아보자.\n\n### Container Network Model (CNM)\n\ncontainer간의 network를 구현하기 위한 design을 제시한 내용입니다. 따라서, idea일 뿐입니다. 자세한 내용은 하위 링크를 통해서 확인 가능합니다.\n\n[🔗 Github - moby/libnetwork](https://github.com/moby/libnetwork/blob/master/docs/design.md)\n\n하지만, 이를 좀 더 요약해봅시다. 일단 핵심 요소 3가지를 먼저 이해해봅시다.\n\n- **Sandbox** : 고립된 하나의 Network 공간을 의미합니다. 해당 공간에는 ehternet interface나 port 그리고 routing table같은 구현이 포함됩니다.\n- **Endpoints** : Virtual Network를 서로 연결하는 interface의 역할입니다. (veth라고도 불립니다.) CNM에서는 Sandbox 내부에서 이와 Network를 연결하는 역할을 합니다.\n- **Networks** : Virtual Switch로 여기면 됩니다. 이를 통해서 여러 개의 endpoints를 연결할 수 있습니다.\n\n자 이제 이렇게 3개의 네트워크를 정리하면, 이제 Container 내부에 Sandbox가 존재하고, 그 Sandbox 내부의 endpoints를 연결하는 Network를 통해서 결론적으로 Container 간의 연결을 수행하게 됩니다.\n\n![cnm](/images/cnm.jpeg)\n\n### libnetwork\n\n위에서 이야기한 것처럼 CNM은 단순히 idea일 뿐입니다. 이를 구현허여 표준화된 것이 바로 libnetwork라고 생각하면 됩니다. 이는 Go를 이용하여 작성된 open source로 위에서 제시한 링크를 통해서 해당 open source에 접근할 수 있습니다. 위에서 언급한 CNM을 구현하였고, 추가적으로 service discovery, ingress-based container load balancing, network control plane 및 management plane 기능을 구현하였다. 현재에는 docker에서 network 구현에 사용된다.\n\n\\* control & management plane : 직접적으로 network의 흐름을 제어하는 단계로, routing과 같은 제어를 수행한다.\n\n### Drivers\n\n즉, libnetwork가 전체적인 network의 control plane과 management plane 기능을 구현하였다면, driver는 data plane을 구현한다. 즉, 직접적으로 데이터를 전달하는 역할을 수행한다. 이러한 기능들은 docker에서 여러 개의 driver라는 submodule을 통해서 구현하였다. docker pub를 통해서 default보다 나아간 driver 역시 설치가 가능하다. 하지만 기본적으로, host, bridge, overlay, ipvlan, macvlan 등을 포함하고 있다.\n\n여기까지가 docker network에 대한 overview이다. 다음 차시에 계속...\n","slug":"docker-network-1","date":"2021-07-10 21:21","title":"[Docker] Network(1)","category":"Tech","tags":["Docker","Container","Network"],"thumbnailSrc":"https://euidong.github.io/images/docker-picture.jpg"},{"content":"\n## Reference\n\n- [🔗 Docker Deep Dive](https://www.oreilly.com/library/view/docker-deep-dive/9781800565135/), Nigel Poulton\n\n## Intro\n\n저번 글에 이어서 이번에는 docker network의 driver들에 대한 자세한 내용을 다루겠다.\n\n- bridge networks\n- overlay networks\n- host networking\n- IPVlan networks\n- MacVlan networks\n\n### Bridge Network\n\ncontainer간의 통신을 위해서 필요한 것이 bridge 네트워크이다. 하지만, 여기서 주의해야할 것은 오직, single host에서만 동작한다는 점이다. 즉, 다른 docker host에 존재하는 container와는 연결이 불가능하다.\n\n그렇다면, bridge가 무엇인가? 이는 두 개의 network 장치를 연결하는 L2 switch를 말한다. 즉, container를 연결하는 도구라고 보면 되겠다. 이를 통해서 연결된 container는 해당 container의 모든 port에 접근이 가능해진다.\n\n![docker-bridge-network](/images/docker-bridge-network.png)\n\n위는 `$ docker network ls`를 입력하면 기본적으로 볼 수 있는 내용이다. 위에 세개는 처음부터 끝까지 docker에 존재하는 default network입니다. host는 직접적으로 host에 연결하는 경우의 network이고,(후에 설명합니다.) none은 아무 네트워크에도 연결되지 않아 외부로 어떤 traffic도 보내지 않을 container들이 속하게 된다. 여기서 bridge는 default bridge라고 불리며, network를 설정하지 않고, container를 생성하게 되면 기본적으로 해당 bridge로 연결되게 된다. 이를 통해서 container 간의 연결도 구현하는 것이 가능하다.\n\n하지만, 일반적으로 단일 기기에서 container 간의 연결을 수행할 때에는 bridge를 직접 생성하여 연결하는 것이 일반적이다. (그 이유는 도메인 네임 설정을 자동으로 해준다는 점에서 이점이 있기 때문 -> [🔗 참고](https://docs.docker.com/network/bridge/#differences-between-user-defined-bridges-and-the-default-bridge))\n\n아래는 이를 이용한 간단한 예시이다.\n\n```bash\n# bridge 생성\n$ docker network create -d bridge eui_bridge\n\n# container 생성\n$ docker container run -d --name c1 \\\n  -network eui_bridge \\\n  alpine sleep 1d\n  \n# container2 생성\n$ docker container run -it --name c2 \\\n  -network eui_bridge \\\n  alpine sh\n   \n# ping을 통해 c1과 연결 여부 확인\n$ ping c1\n```\n\n위의 과정을 처음부터 설명하자면,\n\n1. eui\\_bridge라는 network를 bridge로 생성한다.\n2. container에 eui\\_bridge를 연결하고, alpine 이미지를 기반으로 생성한다. 이때 시작 시에 sleep을 하루 동안 시행한다.(sleep 하는 이유는 꺼지지 않게 하기 위함)\n3. 마찬가지로 eui\\_bridge에 연결하고, alpine 이미지로 container를 생성한 후에 shell을 실행시킨다.\n4. c2에서 실행된 shell에서 c1으로 ping을 전송한다. (이때 같은 network bridge끼리는 container name으로 domain이 생성된다.)\n\n참고로 여기서 기억해야할 것이 있다면, bridge는 container간의 연결을 위한 것이고, container의 특정 port를 host와 mapping하고자 할 때에는 `--publish` 를 활용해야 한다.\n\n```bash\n$ docker run -p 5000:80 nginx\n```\n\n이를 통해서 host의 5000번과 container의 80번 port를 연결할 수 있다.\n\n### Overlay Network\n\n위에서 설명한 것이 단일 호스트 내부에서 container 간의 연결이었다면, 여러 host가 존재하는 cluster 환경에서 docker의 container간 통신을 위한 driver가 overlay이다. 현재에는 docker swarm을 통해서 application을 여러 host에서 제공하는 경우에 사용하게 된다.\n\n먼저 원리를 알아보자면, VXLAN을 활용한다는 것이다. 이는 L3 network 상위에서 다른 두 기기 간에 L2 통신을 지원하는 것인데, 이를 통해서 우리는 다른 node간에 존재하는 container 끼리도 통신할 수 있도록 할 수 있다. docker swarm에 의해서 관리되어 L3로 연결된 두 node의 위에서는 VXLAN Tunnel EndPoint(VTEP)이 각 각 존재한다. 이들을 통해서, tunnel이 형성되고 통신이 가능해지는데, 기존에 container에 존재하고 있던 CNM에서 정의한 Sandbox 속에 virtual switch가 생성되고 이와 VTEP이 연결되어 다른 기기에 있는 container간에도 통신이 가능해지는 것이다.\n\n예시를 든다면, docker stack을 통해서 시스템을 구성해본 적이 있다면, container를 생성하는 과정에서 network가 먼저 생성되는 것을 확인할 수 있을 것이다. 이때 생성되는 것이 overlay 네트워크로 이를 통해서 여러 container가 replica가 어느 node에 생길지 확정할 수 없음에도 통신을 자유롭게 하는 것을 볼 수 있다.\n\n### Host Networking\n\n해당 방식은 docker를 한 번이라도 써본 사람이라면 다음 명령어는 익숙할 것이다.\n\n```bash\n$ docker run -p 80:80 nginx\n```\n\nnginx image를 기반으로 container를 실행시키고, container 내부의 80번과 host의 80번 port를 mapping하겠다는 것이다. 이를 통해서 container는 host의 network에 관여하는 것이 가능하다.\n\n하지만, host networking을 이용하게 되면 container 내부에 network stack이 생성되지 않고, 해당 container의 모든 network 설정이 해당 host의 설정에 그대로 mapping되는 것이다. 이를 이용하면 성능상의 이점은 있겠지만, 상당히 설정이 난잡해질 수 있다.\n\n### IPVlan Network\n\nMAC address와 IP adress를 부여하여, 실제 네트워크에 container를 직접 연결하는 방식이다.\n\n장점은 별도의 port forwarding이나 bridge를 사용하지 않으므로 당연히 빠르지만, NIC를 이용하기에 promiscuous mode를 open해야 한다는 단점이 있다. 이는 switch가 데이터를 전송할 대상을 찾지 않고, 연결된 모든 대상에게 보내는 모드로, sniffing에 취약하고 이 때문에 public cloud system에서는 이를 막아 놓기에 사용할 수 없다.\n\n![docker-ip-vlan](/images/docker-ip-vlan.png)\n여기까지 말했을 때, 이해했다면, 이미 설정하는 것을 알아보러 떠나면 될 것이고, 이해하지 못했다면, 아마 쓸 일이 없을 것이니 넘어가시면 될 것이다.\n\n자세한 사항은 공식 페이지를 참고하자.\n\n[🔗 IPvlan networks](https://docs.docker.com/network/ipvlan/)\n\n### MacVlan Network\n\nipvlan과 동일하지만 차이점은 MAC 주소를 할당한다는 점이다. 그 외에는 다를 것이 없다.\n\n[🔗 macvlan networks](https://docs.docker.com/network/macvlan/)\n","slug":"docker-network-2","date":"2021-07-11 00:04","title":"[Docker] Network(2)","category":"Tech","tags":["Docker","Container","Network"],"thumbnailSrc":"https://euidong.github.io/images/docker-picture.jpg"},{"content":"\n## Reference\n\n- [🔗 Docker Deep Dive](https://www.oreilly.com/library/view/docker-deep-dive/9781800565135/), Nigel Poulton\n\n## Intro\n\n여태까지 docker의 driver를 통한 networking 기술을 알아보았고, 이제 libnetwork로 1/3에서 제시했던 기본 routing과 같은 기능 외에 구현되어 있는 기능들에 대해서 알아봅니다.\n\n- service discovery\n- load balancing\n\n### Service discovery\n\n모든 container들과 swarm의 서비스들이 이름을 통해서 각 각을 찾을 수 있도록 하는 것이다. Docker는 자체적으로 내부의 DNS 서버를 이용하여 이를 수행한다. 과정을 요약하자면 다음과 같다.\n\n1. container가 이름을 통해서 특정 container를 찾아야 함을 인식한다.\n2. 먼저 Local 내부에서 이에 대한 정보를 갖고 있는지를 탐색한다. -> 있다면, 종료\n3. Docker DNS server에 이를 요청하는 query를 전송한다.\n4. Docker DNS server는 모든 container의 name과 network alias(별칭)를 알기 때문에 이를 찾을 수 있다.\n5. 이때, DNS server는 먼저 동일한 network에 해당 container가 존재하는지를 확인한다. -> 없다면, 외부 DNS server로\n6. 존재한다면, 이를 요청을 보낸 resolver에게 전달하고, 이게 다시 container로 전달된다.\n\n### Load balancing\n\ndocker swarm은 기본적인 load balancer를 지원하여, 아래 그림과 같이 구현되어진다.\n\n```bash\n$ docker service create \\\n  --name my-web \\\n  --publish published=8080,target=80 \\\n  --replicas 2 \\\n  nginx\n```\n\n![docker-ingress-network](/images/docker-ingress-network.png)\n\n즉, 어디로 요청을 보낸다고, 할지라도 load balancer는 어디에 해당 서비스가 존재하는지를 파악하고, 이를 전달하는 것이 가능해진다. 따라서, 어느 노드로 요청을 보내더라도 정상적으로 요청이 전달될 수 있는 것이다. 이를 Ingress load balancing이라고 부른다.\n\n만약, 특정 node로 전달된 요청은 해당 node에 있는 container로 전달되기를 바란다면, host모드를 이용하여 진행할 수도 있다.\n\n여기까지가 network에 대한 전반적이 내용입니다.\n","slug":"docker-network-3","date":"2021-07-11 00:40","title":"[Docker] Network(3)","category":"Tech","tags":["Docker","Container","Network"],"thumbnailSrc":"https://euidong.github.io/images/docker-picture.jpg"}],"params":{"subject":"Network"}},"__N_SSG":true}