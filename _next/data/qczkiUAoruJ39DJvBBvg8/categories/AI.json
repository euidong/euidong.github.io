{"pageProps":{"posts":[{"content":"\n## Intro\n\nClustering과 같은 Unsupervised Learning으로 Feature Selection 또는 Feature Extraction 등 여러가지 이름으로 불리는 Dimensionality Reduction 기법에 대해서 다룰 것이다. 특정 data에서 유의미한 정보를 얻기 위해서 우리는 data 전체를 볼 필요가 없다. 따라서, feature들을 최소한으로 줄이면서 유의미한 정보를 얻을 수만 있다면 굉장히 효율적인 inferencing과 learning을 할 수 있다.\n\n## Dimensionality Reduction\n\n가장 AI가 활발하게 연구되고 있는 분야는 Vison과 Natural Language 분야일 것이다. 그리고 이들 분야의 공통점은 data의 크기가 굉장히 크다는 것이다. 예를 들어, 숫자를 표현하는 하나의 이미지가 $1920\\times1080$ 정도의 화질이고, 검은 색 또는 흰색으로 구분하는 bitmap 형식이라고 하자. 이 때 우리가 각 image에 존재하는 숫자를 classification하고 싶은 경우, data에 사용되는 feature가 $1920\\times1080=2,073,600$이다. 상당히 큰 숫자이고, 화질이 커질 수록 그리고 색이 생길 수록 이 값은 커질 것이다. Natural Language 분야에서도 마찬가지이다. 영어 단어의 종류만 따져도 170,000개가 넘는다. 즉, 이들 각 각을 단순 one-hot encoding으로 표현한다면, data에 사용되는 feature의 수는 170,000개가 넘는다. 이렇게 feature의 수가 많아지면, 우리는 data를 효율적으로 학습시키기가 매우 어려워진다. 이를 해결하기 위해서는 feature의 수를 줄이는 것이 반드시 필요하다.\n\n이를 위해서, 우리는 하나의 insight가 필요하다. data에서 필요한 부분이 눈으로 보이는 것이 아닐 수도 있다는 점이다. 예를 들어 우리가 $10\\times10$ 크기의 bitmap에 숫자가 쓰여있다고 하자.\n\n![ml-observed-dim](/images/ml-observed-dim.png)\n\n이때 우리가 갖는 경우의 수는 $2^{100}$개일 것이다. 여기서 숫자 형태로 존재하지 않는 data가 사실은 더 많음에도 불구하고 우리는 이 경우의 수도 고려하는 dimension에서 inferencing과 learning을 수행하는 것이다. 이는 매우 비효율적이라고 할 수 있다. 따라서, 우리는 실제로 관측한 observed dimensionality보다는 더 훌륭한 true dimensionality를 찾아내는 것이 필요하다. 이를 위해서, 다양한 Dimensionality Reduction 관련 방법들이 존재한다. 그 중에서 PCA가 가장 일반적으로 많이 사용되고 있는 방법이다.\n\n## Principal Component Analysis\n\nPrincipal Component Analysis(PCA)는 핵심이 되는 principal component(basis, 기저, 차원 축)를 재정의하여 차원을 줄이는 방법이다. 그렇다면, 어떤 basis가 좋은 basis인지를 알 수 있을까? 아마도 좋은 basis는 이전 basis에서 가지고 있던 정보들을 최대한 보존할 수 있다면 좋다고 할 수 있을 것이다. 그렇다면, 여기서 어떻게 information을 많이 보관할 수 있을지에 대한 통찰이 필요하다.\n\n![ml-pca-1](/images/ml-pca-1.png)\n\n위의 그림을 봤을 때, 왼쪽과 오른쪽 중 어떤 basis가 더 좋은 basis일지를 보자. 차원을 옮긴다는 것은 기존 basis에서의 data를 새로운 basis로 projection하는 것이다. 다시 말해, 해당 축에 직각으로 data를 내려보내는 것이다. 그렇게 했을 때, 왼쪽 그림이 오른쪽 그림보다 더 넓게 data가 퍼지는 것을 알 수 있다. 즉, 차원 이동 시에 충돌로 인해 사라지는 data의 수가 더 적다는 것을 의미한다. 이에 따라 더 많은 정보를 포함하는 것은 data를 최대한 넓게 퍼트릴 수 있는 basis를 가지는 것이라고 할 수 있다. <mark>**따라서, PCA는 기존 basis보다 적은 수의 새로운 basis로 옮기는 과정에서 기존 dimension에서의 정보를 최대한 포함하기 위해서 variance가 최대가 되는 basis를 찾는 것이 목표이다.**</mark> 그럼 이를 수학적으로 표현해볼 것이다. 우선은 이해를 위해서 새로운 basis의 수를 1이라고 가정하고 우리는 sample mean과 variance를 이용해서 이를 추론할 것이다. (해당 basis가 $u_{1}$이다.)\n\n$$\n\\begin{align*}\nVar[u_{1}^{\\top}x] &= E[(u_{1}^{\\top}x - E[u_{1}^{\\top}x])^{2}] \\\\\n&= E[(u_{1}^{\\top}x - u_{1}^{\\top}\\bar{x})^{2}] \\quad (\\bar{x} = E[x] = \\frac{1}{N}\\sum_{n\\in[N]}x_{n}) \\\\\n&=\\frac{1}{N}\\sum_{n\\in[N]}(u_{1}^{\\top}x_{n} - u_{1}^{\\top}\\bar{x})^{2} \\\\\n&=\\frac{1}{N}\\sum_{n\\in[N]}(u_{1}^{\\top}x_{n} - u_{1}^{\\top}\\bar{x})^{\\top}(u_{1}^{\\top}x_{n} - u_{1}^{\\top}\\bar{x}) \\\\\n&=\\frac{1}{N}\\sum_{n\\in[N]}(u_{1}^{\\top}(x_{n}-\\bar{x})(x_{n} -\\bar{x})^{\\top}u_{1}) \\\\\n&=u_{1}^{\\top} \\times \\{\\frac{1}{N}\\sum_{n\\in[N]}(x_{n}-\\bar{x})(x_{n} -\\bar{x})^{\\top}\\} \\times u_{1} \\\\\n&=u_{1}^{\\top} S u_{1} \\quad (S = \\frac{1}{N}\\sum_{n\\in[N]}(x_{n}-\\bar{x})(x_{n} -\\bar{x})^{\\top})\n\\end{align*}\n$$\n\n결국 우리가 얻고자 하는 다른 basis에서의 variance를 구하기 위해서 우리는 기존 차원에서의 모든 data들의 covariance($S_{ij} = Cov[x_{i}, x_{j}]$)를 구해야한다.\n\n자 이제 이를 maximization하는 답을 찾아볼 것이다. 그러기 위해서, 우선 basis의 크기 역시 variance의 영향을 미치기 때문에 이를 unit vector로 제한해야 한다. 이를 종합하면 결론적으로 dimension을 1로 바꾸는 PCA는 다음과 같은 형태로 표현할 수 있다.\n\n$$\n\\begin{align*}\n\\text{maximize }&\\quad u_{1}^{\\top} S u_{1} \\\\\n\\text{subject to }&\\quad u_{1}^{\\top}u_{1} = 1\n\\end{align*}\n$$\n\n위의 maximization problem을 해결해보자. 그러면 다음과 같은 lagrange function을 얻을 수 있다.\n\n$$\n\\mathcal{L} = u_{1}^{\\top} S u_{1} + \\lambda(1-u_{1}^{\\top}u_{1})\n$$\n\n이를 미분하면 다음과 같다.\n\n$$\n\\begin{align*}\n\\frac{\\partial\\mathcal{L}}{\\partial u_{1}} &= 2S u_{1} - 2\\lambda u_{1} = 0 \\\\\nS u_{1} &= \\lambda u_{1}\n\\end{align*}\n$$\n\n즉, 우리가 찾고자 하는 basis는 S matrix의 eigenvector 중 하나의 eigenvector인 것이다. 간단하게 eigenvector와 eigenvalue가 뭔지를 설명하자면, 위처럼 동일한 vector에 대해서, matrix와 vector의 곱이 scalar와 vector과 동일하게 하는 scalar(eigenvalue)와 vector(eigenvector)를 의미한다. (후에 시간이 되면 해당 개념을 다루겠지만, 여기서는 eigenvalue와 eigenvector에 대한 개념은 생략한다.)\n\n그리고 우리가 구한 식을 maximization 식에 한 번 대입하면 다음과 같은 결과를 얻을 수 있다.\n\n$$\n\\begin{align*}\n\\text{maximize }& \\quad u_{1}^{\\top} S u_{1} \\\\\n\\text{maximize }& \\quad u_{1}^{\\top} \\lambda u_{1} \\\\\n\\text{maximize }& \\quad \\lambda u_{1}^{\\top} u_{1} \\\\\n\\text{maximize }& \\quad \\lambda\n\\end{align*}\n$$\n\n따라서, eigenvalue 중 가장 큰 값을 가질 때의 eigenvector가 우리가 구하고자 하는 basis가 된다는 것을 알 수 있다. 또한, 결론적으로 $u_{1}^{\\top} S u_{1} = \\lambda$이기 때문에 $\\lambda$가 variance가 된다는 것도 포인트 중 하나이다.\n\n또한 최종적으로 이를 확장해서 이제 M개의 basis를 사용하는 PCA를 다음과 같이 표현할 수 있다.\n\n$$\n\\begin{align*}\n\\text{maximize }\\quad& \\sum_{i \\in [M]} u_{i}^{\\top} S u_{i} \\\\\n\\text{subject to }\\quad& u_{i}^{\\top}u_{j} = \\delta_{ij} \\quad \\forall i,j \\in [M] \\\\\n&(\\delta_{ij} = \\begin{cases} 1 & i=j \\\\ 0 & i \\neq j \\end{cases})\n\\end{align*}\n$$\n\n따라서, PCA는 data의 covariance matrix에 대한 eigenvalue decomposition을 통해 얻은 eigenvalue 중에 큰 값을 가지는 것을 총 M개 뽑고, 이에 상응하는 eigenvector들을 찾는 것이다. 그리고, 이에 따라서 우리가 가지는 M개의 basis의 eigenvalue($\\lambda$)의 합은 우리가 옮긴 차원에서 갖고 있는 총 variance를 의미한다.\n\n### Other Approaches\n\n위에서 우리는 PCA를 variance를 최대로 하는 basis를 찾는 것으로 정의했다. 하지만, 관점을 바꿔서 문제를 projection error, 기존 data와 projected data 사이의 거리를 최소화할 수 있는 basis를 찾는 것으로 정의할 수도 있다. 이 또한 생각하기에 합리적이라고 생각할 수 있는데 이를 실제로 수학적으로 나타내면 어떻게 되는지 알아보겠다.\n\n먼저, 우리가 총 D개의 unit vector가 있고, 이 중에 임의의 M개의 vector를 basis로 하는 차원으로 reduction을 한다고 해보자. 그렇다면, 우리는 다음과 같이 N개의 data들 중 하나인 $x_{n}$를 표현할 수 있을 것이다.\n\n$$\n\\begin{align*}\n  x_{n} &= \\sum_{i=1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\} \\\\\n  &= \\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\}\n\\end{align*}\n$$\n\n그리고 우리는 옮겨진 data($\\tilde{x}_{n}$)를 다음과 같이 표현할 수 있다.\n\n$$\n\\begin{align*}\n  \\tilde{x}_{n} &= \\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(\\bar{x}^{\\top}u_{i})u_{i}\\}\n\\end{align*}\n$$\n\n여기서 의문이 들 수 있다. $\\sum_{i=M+1}^{D}$ 부분이 와닿지 않을 것이다. 왜냐하면, 우리는 projected data가 실제로는 $\\sum_{i=1}^{M}$ 부분만을 가지기 때문이다. 뒷 부분이 추가된 이유는 해당 M차원 data를 D차원의 공간에 표현할 때, 어느 부분을 중점으로 할지를 정한 것이다. 따라서, 앞 서 보았던 전체 data의 평균만큼을 남은 모든 방향에 더해준 것이다. 이 값은 모든 projected data에 동일하게 더해지는 상수값이라고 봐도 무방하다. 이래도 이해가 조금 어렵다면 아래 그림을 통해 이해할 수 있다.\n\n![ml-pca-3](/images/ml-pca-3.png)\n\n위와 같이 3개의 vector는 1차원에서는 동일한 vector이다. 하지만, projected data와 원래 data간의 적절한 거리를 구하기 위해서는 중앙에 있는 형태로 basis로 옮겨야 한다. 이를 위해서 필요한 것이 뒤의 요소이다. 따라서, 우리는 아래식과 같이 두 projected data와 원래 data간의 거리를 구할 수 있다.\n\n$$\n\\begin{align*}\n  x_{n} - \\tilde{x}_{n} &= (\\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\}) - (\\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(\\bar{x}^{\\top}u_{i})u_{i}\\}) \\\\\n  &= \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\} - \\sum_{i = M+1}^{D}\\{(\\bar{x}^{\\top}u_{i})u_{i}\\} \\\\\n  &= \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})u_{i}\\}\n\\end{align*}\n$$\n\n따라서, 우리가 구하고자하는 최종 목적함수를 mean squared error라고 한다면 다음과 같이 정의할 수 있다.\n\n$$\n\\begin{align*}\n\\mathcal{J} &= \\frac{1}{N}\\sum_{n=1}^{N}||x_{n} - \\tilde{x}_{n}||^2 \\\\\n&= \\frac{1}{N}\\sum_{n=1}^{N}(\\sum_{i = M+1}^{D}x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})^{\\top}(\\sum_{i = M+1}^{D}x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i}) \\\\\n&= \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{i = M+1}^{D}(x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})^{2}u_{i}^{\\top}u_{i} \\\\\n&= \\sum_{i = M+1}^{D}\\frac{1}{N}\\sum_{n=1}^{N}(x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})^{2}\\\\\n&= \\sum_{i=M+1}^{D} u_{i}^{\\top} S u_{i}\\quad (\\because \\text{앞 서 살펴본 첫 번째 관점과 동일})\n\\end{align*}\n$$\n\n따라서, 우리는 다음과 같은 결론에 도달할 수 있다.\n\n$$\n\\begin{align*}\n\\text{minimize }\\quad& \\sum_{i=M+1}^{D} u_{i}^{\\top} S u_{i} \\\\\n\\text{subject to }\\quad& u_{i}^{\\top}u_{j} = \\delta_{ij} \\quad \\forall i,j \\in [M] \\\\\n&(\\delta_{ij} = \\begin{cases} 1 & i=j \\\\ 0 & i \\neq j \\end{cases})\n\\end{align*}\n$$\n\n기존 식과 다른 점이라면, maximization이 minimization으로 바뀌었고, 범위가 $[1, M]$에서 $[M+1, D]$로 바뀌었다는 점이다. 그리고, 우리는 다음과 같은 통찰을 얻을 수 있다. eigenvalue들 중에서 가장 큰 값부터 M번째로 큰 값을 구하는 것과 M+1부터 시작해서 가장 작은 eigenvalue를 찾는 과정은 동일하므로 두 식은 사실상 동일하다.\n\n![ml-pca-4](/images/ml-pca-4.png)\n\n### Very High Dimensional Data\n\n일반적으로 우리가 PCA를 구하기 위해서 eigenvector를 구하는 비용은 O($D^3$)이다. 하지만, 차원이 data의 수보다 큰 경우에 약간의 꼼수를 쓸 수 있다. 간단하게 S를 (M x M) matrix에서 (N x N) matrix로 변환하여 사용하는 것이다. 이를 이용하면, O($N^3$)로 시간 복잡도를 바꾸는 것이 가능하다. 식은 아래를 참고 하자. 아래에서 사용하는 $X$는 각 행이 $(x_{n} - \\bar{x})^{\\top}$인 matrix이다.\n\n$$\n\\begin{align*}\n  Su_{i} &= \\lambda_{i}u_{i} &\\\\\n  \\frac{1}{N}X^{\\top}Xu_{i} &= \\lambda_{i}u_{i} &\\leftarrow X^{\\top}X \\in R^{D\\times D} \\\\\n  X \\times \\frac{1}{N}X^{\\top}Xu_{i} &= X \\times \\lambda_{i}u_{i}& \\\\\n  \\frac{1}{N}XX^{\\top}(Xu_{i}) &= \\lambda_{i}(Xu_{i})& \\\\\n  \\frac{1}{N}XX^{\\top}(v_{i}) &= \\lambda_{i}(v_{i}) &\\leftarrow XX^{\\top} \\in R^{N\\times N}\\\\\n\\end{align*}\n$$\n\n## Probabilistic PCA\n\nPCA를 통해서 data를 다른 차원으로 옮기는 과정을 수행했다. 사실 이것으로 끝나기는 조금 아쉽다. 왜냐하면, 우리는 현재 가지고 있는 data에 대응하는 차원으로의 이동을 수행한 것이다. 즉, 우리가 실제로 inferencing 단계에서 unseen data를 보게 되었을 때 제대로 동작할지는 미지수라고 할 수 있다. 이를 극복하기 위해서, continueous한 형태를 만들어야 한다. 이를 위해서, PCA를 확률적으로 해석하는 것이 필요하다. 하지만, 여기서는 자세히 다루지 않는다. 후에 더 자세히 다루도록 하겠다.\n\n## Kernel PCA\n\n여태까지 앞에서 살펴봤던 PCA는 새로운 basis가 기존 Dimenalality에서 linear하게 만들었다. 하지만, 우리가 다루는 data가 사실은 그렇지 않은 형태일 가능성이 높다. 우리가 관측한 특징들에 의한 좌표 공간에서 선형 형태로 data가 존재하는게 아니라 더 복잡한 곡선 형태를 가질 수 있다. 가장 일반적인 data가 원형으로 이루어진 data 분포이다. 원형으로 반지름이 0.5이하인 data와 반지름이 1인 data를 구분하고 싶다고 하자. 일반적인 x, y 공간에서 linear basis를 이용해서 이를 적절하게 나누려면, dimensionality reduction에서는 정보를 모두 거의 균일하게 잃을 수 밖에 없다.\n\n![ml-kernel-pca-1](/images/ml-kernel-pca-1.png)\n\n하지만, 우리는 일반적으로 중심과 떨어진 정도($x^{2} + y^{2}$)와 같은 기존 feature를 non-linear하게 조합하여 활용하면 더 효과적인 구분이 가능할 것이라는 것을 알 수 있다. (아래 그림은 다른 방식을 사용한 것이지만, 원의 중심과 비슷하다.)\n\n![ml-kernel-pca-2](/images/ml-kernel-pca-2.png)\n\n따라서, 우리는 기존 차원에서 선형으로 basis를 변경하는 것이 아니라 비선형으로 basis를 찾고 싶은 것이다. 기존 차원과 비교했을 때 비선형의 basis를 통해서 만들어지는 차원을 manifold라고 하며, 아래 왼쪽 위와 같은 manifold를 발견만 한다면, Dimensionality reduction을 기존 linear 방식과 비교했을 때 더 효과적으로 수행할 수 있다.\n\n![ml-kernel-pca-3](/images/ml-kernel-pca-3.png)\n\n자 그렇다면 이 또한 어떻게 수학적으로 표현할 수 있을까? 이전에 썼던 Maximum Variance 방식을 이용할 것이다. 우선 우리가 data($x$)를 non-linear 공간으로 차원 이동시킨 값을 $\\phi(x)$라고 정의하고, $\\sum_{i=1}^{N}\\phi(x_{i})=0$이 되는 상황이라고 가정을 해보자.(그렇지 않으면 굉장히 수식이 복잡해지기 때문에 우선 이렇게 가정을 할 것이다.)\n\n$$\n\\begin{align*}\nVar[u_{1}^{\\top}\\phi(x)] &= E[(u_{1}^{\\top}\\phi(x) - E[u_{1}^{\\top}\\phi(x)])^{2}] \\\\\n&= E[(u_{1}^{\\top}\\phi(x) - u_{1}^{\\top}\\bar{\\phi}(x))^{2}]\\quad (\\bar{\\phi}(x) = E[\\phi(x)] = \\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})) \\\\\n&= E[(u_{1}^{\\top}\\phi(x))^2] \\\\\n&= \\frac{1}{N}\\sum_{n \\in [N]}(u_{1}^{\\top}\\phi(x_{n}))^{2} \\\\\n&= u_{1}^{\\top}\\frac{1}{N}\\{\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top}\\}u_{1} \\\\\n&= u_{1}^{\\top}\\bar{S}u_{1} \\quad (\\bar{S} = \\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})\n\\end{align*}\n$$\n\n따라서, 우리는 결론상 이전과 마찬가지로 다음과 같은 식을 얻을 수 있다.\n\n$$\n\\begin{align*}\n\\text{maximize}\\quad& \\sum_{i \\in [M]}u_{i}^{\\top}\\bar{S}u_{i} \\\\\n\\text{subject to}\\quad& u_{i}^{\\top}u_{j} = \\delta_{ij}\\quad \\forall i,j \\in [M] \\\\\n\\end{align*}\n$$\n\n이를 푸는 과정은 앞에서 살펴봤으니 정리를 하자면, 결국 다음과 같은 eigenvalue, eigenvector를 찾는 것이다.\n\n$$\n\\bar{S}u_{i} = \\lambda_{i}u_{i}\n$$\n\n하지만, 이를 푸는 것은 굉장히 곤혹스럽다. 왜냐하면, 우리는 모든 data에 대해서 차원 변환 함수인 $\\phi$를 적용해주어야 하기 때문에 연산의 복잡도는 급격하게 증가하게 된다. 따라서, 우리는 이를 차원 변환 함수 $\\phi$의 연산 과정을 효과적으로 줄이는 방법을 제시한다. 이것이 kernel 함수이다. 이는 앞 서 살펴봤었던, [🔗 5. Multiclass Classification](/posts/ml-multiclass-classification-in-svm#Kernel-Method)의 Kernel Method와 동일하다. 간단하게 설명하자면, $\\phi(x_{i})^{\\top}\\phi(x_{j})$의 결과와 동일한 $\\kappa(x_{i}, x_{j})$의 연산을 활용해서 총 2번의 차원 변환과 곱셈 연산을 두 개의 변수를 받는 하나의 함수로 대체한다는 idea이다. 이를 통해서, 연산 비용을 획기적으로 줄일 수 있다. 따라서, 우리는 기존 식에서 $\\phi$를 없애고, kernel 만으로 이루어진 형태로 바꿀 것이다.\n\n이를 위해서, 우리는 먼저 $u_{i}$를 다시 표현해보자.\n\n$$\n\\begin{align*}\n\\bar{S}u_{i} &= (\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})u_{i} \\\\\n&= \\frac{1}{N}\\sum_{n \\in [N]}\\{\\phi(x_{n})\\times (\\phi(x_{n})^{\\top}u_{i})\\} \\\\\n&= \\frac{1}{N}\\sum_{n \\in [N]} <\\phi(x_{n}), u_{i}>\\phi(x_{n})\\quad(<\\phi(x_{n}), u_{i}> = \\phi(x_{n})^{\\top}u_{i})  \\\\\n\\\\\n\\lambda_{i}u_{i} &= \\bar{S}u_{i} \\\\\n\\lambda_{i}u_{i} &= \\frac{1}{N}\\sum_{n \\in [N]} <\\phi(x_{n}), u_{i}>\\phi(x_{n}) \\\\\nu_{i} &= \\sum_{n \\in [N]} \\frac{<\\phi(x_{n}), u_{i}>}{N\\lambda_{i}}\\phi(x_{n}) \\\\\n\\therefore u_{i} &= \\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})\n\\end{align*}\n$$\n\n해당 과정을 통해서, 우리가 얻고자 하는 basis는 임의의 상수 $\\alpha_{in}$에 의해서 정의된다는 것을 알 수 있다. 따라서, 이제부터 문제는 $\\alpha_{in}$을 모든 N과 M에 대해서 찾기만 하면 되는 것이다. 이제 기존 식을 다시 정리해보도록 하자.\n\n$$\n\\begin{align*}\n\\bar{S}u_{i} &= \\lambda_{i}u_{i} \\\\\n(\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})(\\sum_{m \\in [N]} \\alpha_{in}\\phi(x_{m})) &= \\lambda_{i}(\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})) \\\\\n\\phi(x_{\\ell})^{\\top}\\times (\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})(\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{m})) &= \\phi(x_{\\ell})^{\\top}\\times \\lambda_{i}(\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})) \\\\\n\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{\\ell})^{\\top}\\phi(x_{n})\\phi(x_{n})^{\\top}\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{m}) &= \\lambda_{i}\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{\\ell})^{\\top}\\phi(x_{n}) \\\\\n\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{\\ell})^{\\top}\\phi(x_{n})\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{n})^{\\top}\\phi(x_{m}) &= \\lambda_{i}\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{\\ell})^{\\top}\\phi(x_{n}) \\\\\n\\frac{1}{N}\\sum_{n \\in [N]}\\kappa(x_{\\ell}, x_{n})\\sum_{m \\in [N]} \\alpha_{im} \\kappa(x_{n}, x_{m}) &= \\lambda_{i}\\sum_{n \\in [N]} \\alpha_{in}\\kappa(x_{\\ell}, x_{n}) \\\\\n\\sum_{n \\in [N]}\\kappa(x_{\\ell}, x_{n})\\begin{bmatrix} \\kappa(x_{n}, x_{1}) \\\\ \\kappa(x_{n}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{n}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} &= N\\lambda_{i} \\begin{bmatrix} \\kappa(x_{\\ell}, x_{1}) \\\\ \\kappa(x_{\\ell}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{\\ell}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \\\\\n\\begin{bmatrix} \\kappa(x_{\\ell}, x_{1}) \\\\ \\kappa(x_{\\ell}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{\\ell}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix} \\kappa(x_{1}, x_{1}) & \\kappa(x_{1}, x_{2}) & \\cdots & \\kappa(x_{1}, x_{N}) \\\\\\kappa(x_{2}, x_{1}) & \\kappa(x_{2}, x_{2}) & \\cdots & \\kappa(x_{2}, x_{N}) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\kappa(x_{N}, x_{1}) & \\kappa(x_{N}, x_{2}) & \\cdots & \\kappa(x_{N}, x_{N}) \\end{bmatrix}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} &= N\\lambda_{i} \\begin{bmatrix} \\kappa(x_{\\ell}, x_{1}) \\\\ \\kappa(x_{\\ell}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{\\ell}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \\\\\n\\begin{bmatrix} \\kappa(x_{1}, x_{1}) & \\kappa(x_{1}, x_{2}) & \\cdots & \\kappa(x_{1}, x_{N}) \\\\\\kappa(x_{2}, x_{1}) & \\kappa(x_{2}, x_{2}) & \\cdots & \\kappa(x_{2}, x_{N}) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\kappa(x_{N}, x_{1}) & \\kappa(x_{N}, x_{2}) & \\cdots & \\kappa(x_{N}, x_{N}) \\end{bmatrix}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} &= N\\lambda_{i} \\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \\\\\nK\\alpha_{i} &= N\\lambda_{i} \\alpha_{i} \\\\\n\\end{align*}\n$$\n\n결국 또 다른 eigenvalue problem을 만들 수 있게 된다. 하지만, 여기서 $\\alpha_{i}$의 크기는 1이 아니다. 따라서, $\\alpha_{i}$를 정규화해주어야 한다.\n\n$$\n\\begin{align*}\nu_{i}^{\\top}u_{i} &= \\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{m}) \\\\\n&= \\sum_{n \\in [N]}\\alpha_{in} \\sum_{m \\in [N]}\\alpha_{im}\\phi(x_{n})\\phi(x_{m}) \\\\\n&= \\sum_{n \\in [N]}\\alpha_{in} \\sum_{m \\in [N]}\\alpha_{im}\\kappa(x_{n}, x_{m})\\\\\n&= \\sum_{n \\in [N]}\\alpha_{in} \\begin{bmatrix} \\kappa(x_{n}, x_{1}) \\\\ \\kappa(x_{n}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{n}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix}^{\\top} \\begin{bmatrix} \\kappa(x_{1}, x_{1}) & \\kappa(x_{1}, x_{2}) & \\cdots & \\kappa(x_{1}, x_{N}) \\\\\\kappa(x_{2}, x_{1}) & \\kappa(x_{2}, x_{2}) & \\cdots & \\kappa(x_{2}, x_{N}) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\kappa(x_{N}, x_{1}) & \\kappa(x_{N}, x_{2}) & \\cdots & \\kappa(x_{N}, x_{N}) \\end{bmatrix}  \\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix}\\\\\n&= \\alpha_{i}^{\\top}K\\alpha_{i} \\\\\n&= \\alpha_{i}^{\\top}N\\lambda_{i} \\alpha_{i}\\quad(\\because K\\alpha_{i} = N\\lambda_{i} \\alpha_{i}) \\\\\n&= N\\lambda_{i}\\alpha_{i}^{\\top}\\alpha_{i} = 1 \\\\\n\n\\therefore ||\\alpha_{i}||^{2} &= \\frac{1}{N\\lambda_{i}}\n\\end{align*}\n$$\n\n따라서, 결론적으로 우리는 nonlinear PCA를 수행하기 위해서 아래의 조건을 만족하는 $\\lambda_{i}$를 큰값부터 시작하여 M번째까지에서의 $\\alpha_{i}$를 구하는 것이다.\n\n$$\n\\begin{align*}\nK\\alpha_{i} &= N\\lambda_{i} \\alpha_{i} \\\\\n||\\alpha_{i}||^{2} &= \\frac{1}{N\\lambda_{i}}\n\\end{align*}\n$$\n\n그리고, 마지막으로 data를 우리가 구한 basis로 projection한 결과는 다음과 같이 구할 수 있다.\n\n$$\n\\begin{align*}\n<u_{i}, \\phi(x)> &=\n  \n\\end{align*}\n$$\n\n\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Kernel PCA, <https://sebastianraschka.com/Articles/2014_kernel_pca.html>\n","slug":"ml-dimensionality-reduction","date":"2022-12-04 14:19","title":"[ML] 11. Dimensionality Reduction","category":"AI","tags":["ML"],"desc":"Clustering과 같은 Unsupervised Learning으로 Feature Selection 또는 Feature Extraction 등 여러가지 이름으로 불리는 Dimensionality Reduction 기법에 대해서 다룰 것이다. 특정 data에서 유의미한 정보를 얻기 위해서 우리는 data 전체를 볼 필요가 없다. 따라서, feature들을 최소한으로 줄이면서 유의미한 정보를 얻을 수만 있다면 굉장히 효율적인 inferencing과 learning을 할 수 있다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n주어지는 data에 항상 feature, label이 정확하게 매칭되지는 않는다. 이럴 경우 우리는 각 data에 대한 label과 주어진 data와 label을 잘 설명할 수 있는 probability distribution을 모두 구해야 한다. 여기서 label과 probability distribution을 동시에 구하기 위해서는 어떻게 해야할지에 대한 방법 중에서 대표적인 EM Algorithm에 대해서 살펴보도록 하겠다.\n\n## Problem\n\n여태까지 우리가 살펴봤던 supervised learning에서는 학습(learning) 시에는 feature와 label이 모두 동시에 주어지고, 예측/추론(inference)을 수행할 때에는 feature만 존재하는 data가 주어졌다. 따라서, 학습 시에 feature 정보들을 특정 pattern에 녹여냈을 때, label값을 얻는지를 확인할 수 있었다. 하지만, label이 주어지지 않은 data를 학습시킬 때에는 어떻게 해야할까? 우리는 label이 있어야 해당 data가 가진 실제 결과값을 알고 probability distribution을 얼마나 수정할지를 알 수 있었다. 하지만, 이 값을 모르니 probability distribution을 만들 수 없다. 정확한 probability distribution이 있다면, 반대로 label을 생성하는 것도 가능할 것이다. 하지만, 우리는 아무것도 알 수 없다.\n\n이렇게 답답한 상황에서 우리는 다음과 같은 아이디어를 발상해낼 수 있다. 만약, 대략적인 label을 안다면, 이것을 이용해서 최적의 확률 분포를 찾고, 이 확률 분포에 맞는 label을 다시 생성하고 이를 기반으로 다시 확률 분포를 찾는다면 어떨까? 이렇게 반복하면 꽤나 그럴싸한 분포를 만들 수 있지 않을까? 이 과정을 예를 들자면, 다음과 같다.\n\n각 기 다른 나라(label)의 동전 3종류(500원, 100cent, 100엔)를 구분하고 싶다고 하자. 이때, 알 수 있는 정보는 무게(feature) 밖에 없다고 가정하겠다. 이때 우리는 어떻게 구분할 수 있을까? 우리가 길 거리에서 무작위로 동전을 수집했다고 하자. 각 동전은 흠집도 있을 것이고 공장마다 조금씩 무게가 차이있을 수 있다. 그 결과 다음과 같은 분포가 나왔다고 하자.\n\n![ml-em-algorithm-1](/images/ml-em-algorithm-1.jpg)\n\n그래서 우리는 확률 분포가 아마 Gaussian distribution이라고 생각할 것이다. 따라서, 임의의 Gaussian Distribution을 따르는 세 개의 분포를 아래와 같이 가정해보는 것이다.\n\n![ml-em-algorithm-2](/images/ml-em-algorithm-2.jpg)\n\n그렇다면, 우리는 이 분포에 따라 가장 적절한 label을 생성할 수 있다. 아래와 같이 생성할 수 있다.\n\n![ml-em-algorithm-3](/images/ml-em-algorithm-3.jpg)\n\n그러면 결과적으로 우리는 다음과 같은 label된 data를 갖게 되는 것이다.\n\n![ml-em-algorithm-4](/images/ml-em-algorithm-4.jpg)\n\n이렇게 labeling data를 이용해서 우리는 더 효과적인 확률 분포 변수를 찾아보면 아래와 같이 이전과는 사뭇 다른 분포를 가진다는 것을 알 수 있다.\n\n![ml-em-algorithm-5](/images/ml-em-algorithm-5.jpg)\n\n결과적으로 해당 분포가 이전에 임의로 추정했던 분포보다 더 적절하다는 것을 알 수 있다. 이 과정을 계속해서 반복하면 어떻게 될까?\n\n![ml-em-algorithm-6](/images/ml-em-algorithm-6.jpg)\n\n반복을 통해서 우리는 그럴싸한 확률분포를 습득했다. 대략 머릿속으로는 그럴 수 있을 것 같다는 생각이 들 것이다. 그렇다면, 이것이 어떻게 가능하며 수학적으로 표현이 가능할까? 이를 이제부터 자세히 알아보도록 하겠다.\n\n## Base Knowledge\n\n본론으로 들어가기에 앞 서 우리는 두 가지 정의를 알아야 EM Algorithm을 증명하고 설명할 수 있다.\n\n1. Jensen’s Inequality\n2. Gibb's Inequality\n\n이 두 가지를 모두 안다면 바로 다음으로 넘어가는 것이 좋다. 하지만, 알지 못한다면 이 정의에 대해서 먼저 알아보고 가도록 하자.\n\n### Jensen’s Inequality\n\n일반적으로 우리는 다음 성질을 만족하는 집합을 Convex set이라고 한다.\n\n$$\n\\lambda x + (1-\\lambda)y \\in C,\\quad \\forall x, y \\in C \\text{ and } \\forall\\lambda\\in[0,1]\n$$\n\n즉, 집합에서 random으로 고른 두 수 사이의 수도 집합에 포함되는 집합이라는 것이다. convex set이라고 불리는 이유는 결국 이러한 집합을 2, 3 차원상에 그려보면 볼록하게 튀어나오는 형태라는 것을 알 수 있기 때문이다.\n\n또한, 아래와 같은 조건을 만족하는 함수(f)를 Convex function이라고 한다.\n\n$$\nf(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y),\\quad \\forall x,y \\in C(\\text{Convex set}) \\text{ and } \\forall\\lambda \\in [0,1]\n$$\n\n아래로 볼록한 함수에서는 위와 같은 과정이 너무나 당연하게도 성립한다. 사잇값의 함수값보다 함수값의 사잇값이 더 크기 때문이다.\n\n![ml-convex-function](/images/ml-convex-function.jpg)\n\n반대로 concave(위로 볼록) 함수인 경우에는 반대로 다음과 같이 정의된다.\n\n$$\nf(\\lambda x + (1-\\lambda)y) \\geq \\lambda f(x) + (1-\\lambda)f(y),\\quad \\forall x,y \\in C(\\text{Convex set}) \\text{ and } \\forall\\lambda \\in [0,1]\n$$\n\n여기서 Jensen's Inequality는 다음과 같은 수식이 convex에서 성립한다는 것이다.\n\n$$\nE[f(X)] \\geq f(E[X])\n$$\n\nconvex function에서는 어찌보면 당연해보인다. 그렇지만 이는 EM Algorithm에서 토대로 사용되는 아이디어이기 때문에 반드시 기억하자. 반대로 Concave function인 경우에는 다음과 같다.\n\n$$\nE[f(X)] \\leq f(E[X])\n$$\n\n### Gibb's Inequality\n\nKL divergence 식에 Jensen's Inequality를 적용하여 KL divergence가 항상 0보다 크거나 같고, KL divergence가 0이 되기 위해서는 두 확률분포가 같아야 한다는 것을 증명한 것이다.\n\n이에 대한 증명을 간단하게 하면 다음과 같다.\n\n$$\n\\begin{align*}\nKL(p||q) &= \\sum_{i}{p_{i}\\log\\frac{p_{i}}{q_{i}}} \\\\\n&= -\\sum_{i}p_{i}\\log{\\frac{q_{i}}{p_{i}}} \\\\\n&= E_{p}[-\\log{\\frac{q_{i}}{p_{i}}}] \\geq -\\log{E_{p}[\\frac{q_{i}}{p_{i}}]}\\, (\\because \\text{Jensen's Inequality}) \\\\\n&= -\\log{\\sum_{i}p_{i}\\frac{q_{i}}{p_{i}}} = -\\log{1} = 0\\\\\n\\therefore KL(p||q) &\\geq 0\n\\end{align*}\n$$\n\n## EM Algorithm\n\n우리는 parametric estimation 방법을 사용하기 위해서 다음과 같은 Likelihood를 계산했다.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) &= \\log p(D| \\theta) \\\\\n\\mathcal{L}(\\theta) &= \\log \\prod_{x\\in D} p(x| \\theta) \\\\\n&= \\sum_{x\\in D} \\log{p(x|\\theta)}\n\\end{align*}\n$$\n\n그렇지만 우리가 이 값을 구하는 것이 어렵다는 것을 위에서 제시했다. 따라서, 이를 다음과 같이 바꿔서 풀어보자는 것이다.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) &= \\sum_{x\\in D} \\log{p(x|\\theta)} \\\\\n&= \\sum_{x\\in D} \\log{\\int_{z \\in K}p(x, z|\\theta)} dz\n\\end{align*}\n$$\n\n이렇게 바꾸게 된다고 무슨 이득이 있을까? 단순히 식이 더 복잡해보인다. 하지만, 이 식을 다음과 같이 바꿀 수 있다.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) &= \\sum_{x\\in D} \\log{p(x|\\theta)} \\\\\n&= \\sum_{x\\in D} \\log{\\int_{z \\in K} p(x, z|\\theta)} dz \\\\\n&= \\sum_{x\\in D} \\log{\\int_{z \\in K} p(x, z|\\theta) \\frac{p(z|x, \\theta^{\\prime})}{p(z| x, \\theta^{\\prime})} dz} \\\\\n&= \\sum_{x\\in D} \\log{\\int_{z \\in K} p(z|x, \\theta^{\\prime}) \\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})} dz} \\\\\n&= \\sum_{x\\in D} \\log{E_{z|x, \\theta^{\\prime}}{[\\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})}]}} \\geq \\sum_{x\\in D} E_{z|x, \\theta^{\\prime}}{[\\log{\\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})}}]}\\,(\\because \\text{Jensen's Inequality}) = \\mathcal{F}(p_{z|x, \\theta^{\\prime}}, \\theta)\n\\end{align*}\n$$\n\n이를 통해서 우리는 아래와 같은 과정을 수행해볼 수 있다.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) - \\mathcal{F}(p_{z|x, \\theta^{\\prime}}, \\theta) &= \\sum_{x\\in D} \\log{p(x|\\theta)} - \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{\\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})}}}\\} \\\\\n&= \\sum_{x\\in D} \\log{p(x|\\theta)} - \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{\\frac{p(z|x, \\theta)p(x|\\theta)}{p(z| x, \\theta^{\\prime})}}}\\} \\\\\n&= \\sum_{x\\in D} \\log{p(x|\\theta)} - \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{p(x|\\theta)}} + \\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{\\frac{p(z|x, \\theta)}{p(z| x, \\theta^{\\prime})}}}\\} \\\\\n&= \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{\\frac{p(z|x, \\theta^{\\prime})}{p(z| x, \\theta)}}}\\} \\\\\n&= \\sum_{x\\in D}{KL(p_{z|x, \\theta^{\\prime}}, p_{z|x, \\theta})}\n\\end{align*}\n$$\n\n우리가 원하는 것은 결국 해당 값의 minization이다. 따라서, 우리는 모든 $x$에 대해서 KL-divergence의 최솟값을 구해야 한다(모든 사건은 독립이기 때문이다). KL-divergence는 0과 같거나 큰 수이고, KL-divergence는 $p_{z|x, \\theta^{\\prime}} = p_{z|x, \\theta}$일 때, 0이므로 이를 만족할 수 있는 값을 찾는 것이 중요하다.\n\n따라서, 우리는 다음과 같은 insight를 얻을 수 있다.\n\n1. $\\mathcal{F}(p_{z|x, \\theta^{(t-1)}}, \\theta^{(t)}) \\leq \\mathcal{L}(\\theta^{(t)})$ 아래 식에 의해서 이를 증명할 수 있다.  \n   $\\mathcal{F}(p_{z|x, \\theta}, \\theta^{(t)}) \\leq \\mathcal{L}(\\theta^{(t)}) (\\because \\text{Jensen's Inequality})$\n2. $\\mathcal{L}(\\theta^{(t)}) = \\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta^{(t)})$ 바로 위에서 살펴보았 듯이 $p_{z|x, \\theta^{\\prime}} = p_{z|x, \\theta}$일 때, 등식이 성립한다. (Gibb's Inequality)\n3. $\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta^{(t)}) \\leq \\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta^{(t+1)})$  \n   여기서 $\\theta^{(t+1)}$은 다음과 같이 구할 수 있다.  \n   $\\theta^{(t+1)} = \\argmax_{\\theta}{\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta)}$\n4. $\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta^{(t+1)}) \\leq \\mathcal{L}(\\theta^{(t+1)})$  \n   이는 1번과 동일한 식이다.\n5. $\\mathcal{L}(\\theta^{(t)}) \\leq \\mathcal{L}(\\theta^{(t+1)})$\n\n결론적으로, 5번에 의해서 우리는 매단계가 이전보다 같거나 크다는 것을 알 수 있다. 또한, 각 단계를 차례대로 설명한다면 다음과 같다.\n\n1. 이전 확률과 현재 parameter의 추정치를 이용해서 구한 $\\mathcal{F}(p_{z|x, \\theta^{(t-1)}}, \\theta^{(t)})$는 $\\theta^{(t)}) \\leq \\mathcal{L}(\\theta^{(t)})$보다 작다는 것을 Jensen's Inequality에 의해서 알 수 있다.\n2. 우리는 이제 $\\mathcal{F}(p_{z|x, \\theta}, \\theta^{(t)})$를 최대화하기 위해서 $p_{z|x, \\theta}$를 $p_{z|x, \\theta^{(t)}}$로 업데이트 한다. 그렇다면, 이 결과는 앞 서 보았듯이 이 값은 $\\mathcal{L}(\\theta^{(t)})$와 동일한 결과를 갖는다.\n3. 여기서 우리가 얻은 $ \\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta)$를 최대화할 수 있는 $\\theta^{(t+1)}$를 구한다면, 이는 당연하게도 $\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta^{(t)})$ 보다 크다.\n4. 이렇게 얻은 새로운 parameter에 의한 결과는 역시 당연하게도 1번과 같은 결론에 도달하게 된다.\n5. 결국 우리는 1~4번 까지의 과정을 거치면서 $\\mathcal{L}(\\theta)$를 계속해서 증가시킬 수 있다.\n\n결국 우리가 해야할 것은 다음값을 매차시마다 구하는 것이다.\n\n$$\n\\theta^{(t+1)} = \\argmax_{\\theta}{\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta)}\n$$\n\n이를 위해서 먼저 우리는 $\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta)$의 식을 좀 더 정리해볼 것이다.\n\n$$\n\\begin{align*}\n\\mathcal{F}(p_{z|x, \\theta^{\\prime}}, \\theta) &= \\sum_{x\\in D} E_{z|x, \\theta^{\\prime}}{[\\log{\\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})}}]} \\\\\n&= \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{\\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})}}}\\}\\\\\n&= \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{p(x, z|\\theta)}} + \\int_{z|x, \\theta^{\\prime}}{p(z|x, \\theta^{\\prime})}{\\log{\\frac{1}{p(z|x, \\theta^{\\prime})}}}\\} \\\\\n&= \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{p(x, z|\\theta)}} + H(p_{z|x,\\theta^{\\prime}})\\}\n\\end{align*}\n$$\n\n위 식에서 우리는 $\\argmax_{\\theta}{\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta)}$를 구하기 위한 과정에서 $H(p_{z|x,\\theta^{\\prime}})$는 필요없다는 것을 알 수 있다.\n\n$$\n\\begin{align*}\n\\mathcal{Q}(\\theta; \\theta^{\\prime}) &= \\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta) - H(p_{z|x,\\theta^{\\prime}}) \\\\\n&= \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{p(x, z|\\theta)}}\\} \\\\\n&= \\sum_{x\\in D}\\{ E_{z|x, \\theta^{\\prime}}[\\log{p(x, z|\\theta)}] \\}\n\\end{align*}\n$$\n\n그 결과 우리는 다음과 같은 결론을 내릴 수 있다.\n\n$$\n\\begin{align*}\n\\theta^{(t+1)} &= \\argmax_{\\theta} \\mathcal{Q}(\\theta; \\theta^{(t)}) \\\\\n&= \\argmax_{\\theta} \\sum_{x\\in D}\\{ E_{z|x, \\theta^{(t)}}[\\log{p(x, z|\\theta)}] \\}\n\\end{align*}\n$$\n\n따라서, 우리는 이를 효과적으로 구하기 위해서 EM Algorithm을 다음과 같이 정의하고, 단계에 따라 수행한다.\n\n1. Expectation Step  \n   앞에서 제시한 $\\mathcal{Q}(\\theta; \\theta^{(t)})$의 식을 구하는 단계이다. 즉, 변수를 $\\theta$ 외에는 모두 없애는 단계이다. Expectation 단계라고 부르는 이유는 $\\mathcal{Q}(\\theta; \\theta^{(t)})$가 $\\sum_{x\\in D}\\{ E_{z|x, \\theta^{\\prime}}[\\log{p(x, z|\\theta)}] \\}$와 같이 Expectation의 합의 형태로 표현되기 때문이다.  \n   이를 좀 더 쉽게 표현하자면 다음과 같이 말할 수도 있다. 이전 parameter $\\theta^{(t)}$가 주어졌을 때, 각 데이터에 대한 latent variable $z$의 확률을 구하는 것이다. 즉, $p(z|x, \\theta^{(t)})$를 구하는 것이다.\n2. Maximization Step  \n   이제 앞 서 구한 $\\mathcal{Q}(\\theta; \\theta^{(t)})$를 $\\theta$에 대해 최대화하여, $\\theta^{(t+1)}$를 구하는 단계이다.\n\n이것이 EM Algorithm의 본질이다.\n\n그래서 앞 선 Clustering에서 살펴보았던 것처럼 EM Algorithm을 다음과 같이 정의할 수도 있는 것이다.\n\n1. 초기 parameter $\\theta^{(0)}$를 설정한다.  \n2. 이를 기반으로 data가 해당 분포에서 $z$일 확률을 구한다.\n3. 구한 확률을 바탕으로 해당 확률과 data를 잘 표현할 수 있는 새로운 parameter $\\theta^{(t+1)}$를 구한다.\n4. 2, 3번 과정을 parameter가 일정 수준에 수렴할 때까지 반복한다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-em-algorithm","date":"2022-11-24 20:15","title":"[ML] 10. EM Algorithm","category":"AI","tags":["ML","EM-Algorithm","JensensInequality","GibbsInequality"],"desc":"주어지는 data에 항상 feature, label이 정확하게 매칭되지는 않는다. 이럴 경우 우리는 각 data에 대한 label과 주어진 data와 label을 잘 설명할 수 있는 probability distribution을 모두 구해야 한다. 여기서 label과 probability distribution을 동시에 구하기 위해서는 어떻게 해야할지에 대한 방법 중에서 대표적인 EM Algorithm에 대해서 살펴보도록 하겠다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n이전까지의 Posting에서는 Supervised Learning 즉, 이미 Labeling이 완료된 데이터에 의한 Learning을 중점적으로 다루었다. 지금부터는 Unsupervised Learning에 대해서 조금 더 살펴보도록 하겠다. 대표적인 Unsupervised Learning은 Clustering, Feature Selection(or Dimensionality Reduction), Generative Model 등이 존재한다. 이들에 대해서 차근차근 살펴보도록 하고, 해당 Posting에서는 가장 대표적이라고 할 수 있는 Clustering을 먼저 살펴보면서 Unsupervised Learning에 대한 계략적인 이해를 해보도록 하겠다.\n\n## Clustering\n\nClustering은 unlabeled data를 data간 유사성 또는 거리 지표 등을 활용하여 미리 지정한 수 만큼의 partitioning하는 작업을 의미한다. 즉, 우리가 학습을 진행함에 있어 data는 label이 존재하지 않기 때문에 우리는 data간의 관계에서 정보를 추출해서 이를 분류해내는 것이 목표인 것이다.\n\n이를 수행하기 위한 방법은 크게 두 가지로 나눌 수 있다.\n\n1. **Non-Parametric Approach**  \n   이름 그대로 확률적 분포를 가정한 후, Parameter를 찾아가는 방식이 아닌 직관적인 방법(Huristic Approach)을 활용하는 방법이다. 그렇기에 확률적인 해석이 뒷받침되기 보다는 Algorithm을 통해서 이를 설명한다. 대표적인 방법이 K-Means Clustering이다.\n2. **Parametric Approach**  \n   확률적 분포를 가정한 후, Parameter를 찾아가는 방식으로, 대표적인 방법이 Gaussian 분포를 가정하고 찾아나가는 Gaussian Mixture Model(GMM, or MoG, Mixture of Gaussian)이 있다.\n\n따라서, Clustering을 대표하는 K-means Clustering과 GMM을 각 각 살펴보도록 하겠다.\n\n### K-Means Clustering\n\nK-Means Clustering은 K개의 평균값을 통한 Clustering으로 해석하면 의미 파악이 쉽다. 즉, K개의 Partition을 만들기 위해서 K개의 평균값을 찾아 이를 기반으로 더 가까운 평균값에 속하는 Partition에 data를 분배하는 방식이다.\n\n그렇다면, 우리가 구해야할 값은 각 data가 어느 Partition에 속하는지에 대한 정보($\\bold{r}\\leftarrow\\text{one hot vector}$)와 각 Partition의 평균값($\\mu$)이다. 즉, K-means Clustering에서는 기존 data들을 통해서 K개의 평균값(K-means)을 찾아서(**Learning**) 이후에 추가로 들어올 data에 대해서도 똑값은 K-means를 통해서 Partition을 찾을 수 있다(**Inference**).또는 모든 data를 저장해두었다가 K-means를 다시 계산하는 방법도 있다(Online K-means).\n\n그렇다면, $\\boldsymbol{\\mu}(=\\{\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}\\})$와 $\\bold{R}$(모든 data의 $\\bold{r}$로 이루어진 Matrix)을 어떻게 구할 수 있을까? 이에 대한 해답은 다음과 같은 Cost Function을 제시하는 것으로 해결할 수 있다.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}||x_{i} - \\mu_{k}||^{2}\n$$\n\n현재 data의 point로 부터 가장 가까운 평균을 선택하는 경우를 최대화해야 해당 값이 가장 작아질 수 있다는 것을 알 수 있다. <mark>즉, 여기서는 평균과의 거리를 유사성의 지표로 사용한 것이다.</mark> 여기서는 Euclidean distance(L2-norm)를 사용했지만, Manhatan distance(L1-norm)을 활용할 수도 있고 아예 다른 지표를 활용할 수도 있다. 중요한 것은 Cost Function이 아래와 같은 form을 가진다는 것이다.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\times(\\text{Similarity measure})\n$$\n\n그렇다면, 실제로 위에서 제시한 Cost Function을 활용하여 어떻게 $\\boldsymbol{\\mu}$와 $\\bold{R}$을 구할 수 있을까? minimize하고자하는 요소가 두 개이기 때문에 미분을 하기도 다소 난해하다. 따라서, 여기서는 EM Algorithm이라는 방식을 제시한다. 이에 대해서는 다음 Posting에 대해서 자세히 다루겠지만, 간단히 설명하자면 하나의 Variable을 Random하게 지정하고, 다른 Variable의 최적값을 구한 후 이를 다시 대입하고 반대 Variable을 최적값으로 구하기를 반복하면서 더 이상 Variable이 유의미하게 변경되지 않을 때까지 반복해서 구한 값이 최적값과 근사한다는 점을 활용한 Algorithm이다. 지금은 다소 엉뚱할 수 있지만, 지금은 해당 방법을 사용하도록 하겠다. 증명이 궁금하다면, 해당 Posting([🔗 [ML] 10. EM Algorithm](/posts/ml-em-algorithm))을 참고하자.\n\n따라서, 우리가 수행할 과정은 다음과 같다.\n\n1. $\\boldsymbol{\\mu}$를 랜덤하게 초기화한다.\n2. Assignment step: $\\boldsymbol{\\mu}$가 주어졌을 때, $\\bold{R}$을 구한다.  \n   $$\n   R_{ik} = \\begin{cases}\n    1 & \\text{if}\\quad k = \\argmin_{k}||x_{i} - \\mu_{k}||^{2} \\\\\n    0 & \\text{otherwise}\n   \\end{cases}\n   $$\n3. Update step: $\\bold{R}$이 주어졌을 때, $\\boldsymbol{\\mu}$를 구한다.  \n   우리가 분류한 $R$을 활용하여 각 k에 속하는 data의 평균을 통해서 $\\boldsymbol{\\mu}$를 구한다.\n   $$\n   \\mu_{k} = \\frac{\\sum_{i=1}^{N}R_{ik}x_{i}}{\\sum_{i=1}^{N}R_{ik}}\n   $$\n4. 특정값으로 $\\boldsymbol{\\mu}$가 수렴할 때까지 2번, 3번 과정을 반복한다.\n\n아래는 이 과정을 그림을 통해서 표현한 것이다.\n\n![ml-clustering-1](/images/ml-clustering-1.jpg)\n\nK-means 방식은 위와 같은 Iteration 절차를 많이 수행하지 않아도 몇번의 수행만으로 수렴한다는 것을 관측할 수 있다. 또한, Assignment 시에는 $O(KND)$의 시간이 소모되고, Update 시에는 $O(N)$ 만큼의 시간이 소모되기 때문에 무겁지 않고, 굉장히 간단하다는 장점을 갖고 있다. 하지만, 이 방법은 Global Optimal을 찾을 것이라는 확신을 줄 수 없다. 그렇기에 초기값을 어떻게 잡느냐에 따라서 결과가 크게 변할 수도 있다. 뿐만 아니라, outlier data에 대해서도 굉장히 민감하게 반응한다는 단점이 있다. 예를 들어, 아래 사진에서 왼쪽보다 오른쪽이 더 성공적인 Clustering이라고 말할 수 있을 것이다.\n\n![ml-clustering-2](/images/ml-clustering-2.jpg)\n\n이를 해결하기 위한 방법으로 다음과 같은 방법들이 제시되었다.\n\n1. **K-means++**: 초기값을 잘 설정하기 위한 방법으로, 초기값을 잘 설정하면 수렴하는 속도가 빨라지고, Global Optimal에 수렴할 가능성이 높아진다.\n2. **K-mendoids**: K-means에서는 중심점을 data의 평균으로 설정했지만, K-mendoids에서는 중심점을 data의 중간값으로 설정한다. 이렇게 하면 outlier에 민감하지 않게 된다.\n\n> <mark>**Soft K-means**</mark>\n\n마지막으로 K-means Clustering에서 확률적인 접근을 시도한 방법 또한 소개하겠다. 앞 서 본 (Hard)K-means에서는 $\\bold{R}_{ik}$를 0 또는 1로 보았다. 하지만, 이를 확률적으로 표현하는 것에 대해서 생각해 볼 수 있다. 즉, 다음과 같이 soft-max function을 활용한다면 표현이 가능할 것이다.\n\n$$\n\\bold{R}_{ik} = \\frac{\\exp(-\\beta||x_{i}-\\mu_{k}||^{2})}{\\sum_{l \\in {1, 2, \\cdots, K}} \\exp(-\\beta||x_{i}-\\mu_{l}||^{2})}\n$$\n\n이렇게 확률적으로 표현하게 되면, 우리는 추가적인 정보를 활용할 수 있다. 대표적으로 특정 Cluster로 해당 확률이 편향되어 있을 수록 더 좋은 분류일 것이라는 사전 지식(Prior)을 활용할 수 있다. 따라서, 우리는 다음과 같이 Cost Function을 변경할 수 있다.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}||x_{i} - \\mu_{k}||^{2} - \\frac{1}{\\beta}\\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\log\\bold{R}_{ik}\n$$\n\n뒷 부분에 새로 추가된 $-\\frac{1}{\\beta}\\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\log\\bold{R}_{ik}$는 $R_{ik}$가 확률이 되었기 때문에 사실상 Entropy를 의미한다. Entropy는 균형잡힌 분포일 수록 커지고, skew된 경우에는 작아지기 때문에 적절한 지표라고 할 수 있다. $\\beta$는 이러한 prior를 얼마나 사용할지에 대한 hyperparameter이다. $\\beta$가 클 수록 사실상 Hard K-means와 동일한 결과를 얻게 되고, $\\beta$가 작을 수록 Entropy를 더 중요시하는 결과를 얻게 된다.\n\n### Gaussian Mixture Model\n\nGaussian Mixture Model, 일명 GMM은 Finite Mixture Model의 일종이다. Finite Mixture Model은 우리가 추정하고자 하는 확률 분포가 다양한 확률 분포 몇 개의 조합으로 이루어진 분포라고 가정하고, 해당 확률 분포의 Parameter를 학습(Learning) 단계에서 찾아내고, 이를 이용해서 새로운 data에 대해서 추정(Inference)하는 방식이다.\n\n$$\np(x) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x, z=k) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x|z=k)p(z=k) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p_{k}(x)p(z=k)\n$$\n\n여기서 $z$는 관측할 수 없는 latent(hidden) variable로 data가 몇 번째 확률 분포에 속할 것인지를 의미한다. 따라서, $p(z=k)$ k번째 분포에 속할 확률이라고 볼 수 있다. 대게 이것이 어느정도로 확률 분포를 섞는지를 의미하기 때문에 mixing parameter라고도 부른다.\n\n이에 따라 GMM은 각 $p_{k}(x)$가 Gaussian Distribution이라고 가정하는 Finite Mixture Model인 것이다.\n\n![ml-gmm-graphical-form](/images/ml-gmm-graphical-form.jpg)\n\n그렇다면, 우리는 다음과 같이 Graphical Model 형태로 Finite Mixture Model을 생성할 수 있다. 여기서 $\\pi,\\, \\mu,\\, \\Sigma$는 Parameter를 의미한다.\n\n- $\\pi_{k} = p(z = k)$\n- $\\mu_{k} = E[x|z=k]$ 즉, Gaussian의 기댓값을 의미한다.\n- $\\Sigma_{k} = Cov[x|z=k]$ 즉, Gaussian의 분산을 의미한다.\n\n이를 통해서 우리는 위에서 제시한 확률을 다음과 같이 재정의할 수 있다. (Joint Probability를 Bayesian Network로 푼 식이다. 모르겠다면, [🔗 [ML] 8. Graphical Model](/posts/ml-graphical-model#Graphical-Model)에서 Bayesian Network를 다시 살펴보고 오자.)\n\n$$\n\\begin{align*}\np(x) &= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x, z=k) \\\\\n&= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(z=k| \\pi_{k})p(x|z=k, \\mu_{k}, \\Sigma_{k}) \\\\\n&= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\pi_{k}\\mathcal{N}(x|\\mu_{k}, \\Sigma_{k})\n\\end{align*}\n$$\n\n여기서 우리가 실제로 추측(Inference)을 할 때에는 $p(z|x)$가 필요하다. 이는 우리가 posterior를 활용해서 구할 수 있다.\n\n$$\n\\begin{align*}\n\\hat{k} &=\\argmax_{k}p(z=k|x) \\\\\n&= \\argmax_{k}\\frac{p(x|z=k)p(z=k)}{p(x)} \\\\\n&= \\argmax_{k}p(x|z=k)p(z=k)\\\\\n&= \\argmax_{k}\\pi_{k}\\mathcal{N}(x|\\mu_{k}, \\Sigma_{k})\n\\end{align*}\n$$\n\n학습(Learning)을 할 때에는 결국 $\\pi,\\, \\mu,\\, \\Sigma$ 이 세 개의 parameter 값을 찾는 것이 중요하다. 이것은 우리가 Parametric Estimation에서 줄기차게 했던 MLE를 이용하면 된다. 이를 위한 Likelihood는 다음과 같다.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\pi,\\, \\mu,\\, \\Sigma) &= \\log{p(\\mathcal{D} | \\pi,\\, \\mu,\\, \\Sigma)} \\\\\n&= \\log{\\prod_{i=1}^{N}{p(x_{i} | \\pi,\\, \\mu,\\, \\Sigma)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{p(x_{i}| \\pi,\\, \\mu,\\, \\Sigma)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{\\sum_{k=1}^{K}{\\pi_{k}\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma_{k})}}}\n\\end{align*}\n$$\n\n하지만, 이것을 단순한 Optimization Technique으로는 풀 수 없다. 왜냐하면, 단순한 미분으로 각 parameter를 구할 수 없기 때문이다. 따라서, EM Algorithm을 이용해서 풀어야 한다. (이것은 [🔗 [ML] 10. EM Algorithm](/posts/ml-em-algorithm)에서 다룬다.)\n\n따라서, 아래 그림과 같이 임의의 $\\pi,\\, \\mu,\\, \\Sigma$를 가정한 상태에서 data에 알맞는 최적의 Cluster set을 구하고, data에 cluster가 label된 상태에서 최적의 $\\pi,\\, \\mu,\\, \\Sigma$를 구하는 과정을 반복하는 것이다.\n\n![ml-gmm-1](/images/ml-gmm-1.jpg)\n\n그렇다면, 이를 실제로 어떻게 하는지를 알아보도록 하겠다. 하지만, 그냥 모든 Gaussian 형태를 위한 방법을 사용하면 다소 식이 복잡해지기 때문에 isotropic Gaussian(모든 방향에서 분산이 동일한 Gaussian)을 가정으로 하겠다.\n\n또한, 다음과 같은 요소를 추가로 정의하자.\n\n1. $z_{i} \\in \\{1, 2, \\cdots, K\\}$ : $i$번째 data가 속하는 cluster의 index  \n   $z_{ik} = \\begin{cases} 1 & \\text{if } z_{i} = k \\\\ 0 & \\text{otherwise} \\end{cases}$\n2. $\\theta_{k} = (\\pi_{k},\\, \\mu_{k},\\, \\Sigma_{k})$ : $k$번째 cluster를 위한 parameter의 집합  \n   $\\theta = (\\pi,\\, \\mu,\\, \\Sigma)$ : parameter의 집합  \n\n앞 서 말한 바와 같이 이제 우리는 $\\theta$를 구하는 과정에서 $z_{i}$에 해당하는 정보도 알고 있다. 따라서, Likelihood 식도 변형되어야 한다.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) &= \\log{p(\\mathcal{D} | \\theta)} \\\\\n&\\geq \\log{p(X, Z | \\theta)} \\\\\n&= \\sum_{i=1}^{N}{\\log{p(x_{i}, z_{i}| \\theta)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{p(z_{i} | \\theta) \\times p(x_{i}| z_{i}, \\theta)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{(\\prod_{k=1}^{K}{\\pi_{k}^{z_{ik}}} \\times \\prod_{k=1}^{K}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)^{z_{ik}}})}} \\\\\n&= \\sum_{i=1}^{N}{\\sum_{k=1}^{K}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})^{z_{ik}}}} \\\\\n&= \\sum_{i=1}^{N}{\\sum_{k=1}^{K}z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}} \\\\\n\\end{align*}\n$$\n\n이에 따라서 우리는 EM Algorithm의 $\\mathcal{Q}$를 다음과 같이 구할 수 있다.\n\n$$\n\\begin{align*}\n\\mathcal{Q}(\\theta; \\theta^{\\prime}) &= \\sum_{i=1}^{N}E_{z_{i}|x_{i}, \\theta^{\\prime}}[\\log p(x_{i}, z_{i} | \\theta)] \\\\\n&= \\sum_{i=1}^{N}E_{z_{i}|x_{i}, \\theta^{\\prime}}[\\sum_{k=1}^{K}z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}] (\\because \\text{위의 식에서 3번째 줄을 참고})\\\\\n&= \\sum_{i=1}^{N}\\sum_{k=1}^{K}E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}] \\\\\n&= \\sum_{i=1}^{N}\\sum_{k=1}^{K}E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}] \\\\\n&= \\sum_{i=1}^{N}\\sum_{k=1}^{K}E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}]\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})} \\\\\n\\end{align*}\n$$\n\n따라서, 우리는 각 step을 다음과 같이 정의할 수 있다.\n\n- **E-step**  \n  $\\mathcal{Q}$에서 parameter($\\pi,\\, \\mu,\\, \\Sigma$)를 제외하고, 아직 미지수로 남아있는 값은 $E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}]$이다. 즉, 이 값만 구하면 $\\mathcal{Q}$를 구했다고 할 수 있다.  \n  $$\n  \\begin{align*}\n  E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}] &= \\sum_{k=1}^{K}{z_{ik}p(z_{i} = k | x_{i}, \\theta^{\\prime})} \\\\\n  &= p(z_{i} = k^{*} | x_{i}, \\theta^{\\prime}) = r_{ik^{*}}\n  \\end{align*}\n  $$  \n  결국 우리가 해당 단계에서 구할 것은 관측 가능한 data와 이전 parameter가 주어졌을 때, 속하게 되는 cluster에서의 확률을 구하는 것이다. 이것을 모든 data에 대해서 구하면, $\\mathcal{Q}$에서 parameter를 제외한 모든 부분을 구할 수 있다. 따라서, 식을 좀 더 정리하면 다음과 같은 결론을 얻을 수 있다.\n  $$\n  \\begin{align*}\n  E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}] = r_{ik^{*}} &= \\frac{p(x_{i}, z_{i}=k^{*} | \\theta^{\\prime})}{p(x_{i}|\\theta^{\\prime})} \\\\\n  &= \\frac{\\pi_{k^{*}}^{\\prime}{\\mathcal{N}(x_{i}|\\mu_{k^{*}}^{\\prime}, \\Sigma_{k^{*}}^{\\prime} I)}}{\\sum_{l=1}^{K}{\\pi_{l}{\\mathcal{N}(x_{i}|z_{i} = l, \\mu_{l}, \\Sigma_{l} I)}}}\n  \\end{align*}\n  $$  \n  \n- **M-step**  \n  결론적으로 우리는 다음과 같은 $\\mathcal{Q}$와 constraint를 얻었다.  \n  $$\n  \\begin{align*}\n  \\text{maximize}&\\quad \\mathcal{Q}(\\theta; \\theta^{\\prime}) = \\sum_{i=1}^{N}\\sum_{k=1}^{K}r_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})} \\\\\n  \\text{subject to}&\\quad \\sum_{k=1}^{K}{\\pi_{k}} = 1\n  \\end{align*}\n  $$  \n  이제 우리는 이를 Optimization 방식을 활용하여 풀기만 하면 끝이다. ([🔗 참고(Base Knowledge)](/posts/ml-base-knowledge))  \n  $$\n  \\begin{align*}\n  \\mu_{k} &= \\frac{\\sum_{i=1}^{N}{r_{ik}x_{i}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\Sigma_{k} &= \\frac{\\sum_{i=1}^{N}{r_{ik}||x_{i} - \\mu_{k}||^{2}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\pi_{k} &= \\frac{1}{N}\\sum_{i=1}^{N}{r_{ik}}\n  \\end{align*}\n  $$\n\n---\n\n마지막으로 짚고 넘어갈 것은, 바로 K-means Clustering은 사실 GMM의 하나의 special case라는 것이다. 만약, 우리가 $\\pi_{k},\\, \\Sigma_{k}$를 모두 같은 값으로 설정하면, $\\pi_{k} = \\frac{1}{K}$이고 $\\Sigma_{k} = \\Sigma$가 된다고 하자. 이때 EM algorithm을 살펴보면 다음과 같다.\n\n- **E-step**  \n  $$\n  r_{ik} = \\begin{cases} 1 & k = \\argmax_{l\\in \\{1, 2, \\cdots, K\\}} p(x_{i}|z_{i} = l, \\mu_{l}, \\Sigma) \\\\ 0 & \\text{otherwise} \\end{cases}\n  $$  \n  이는 사실상 K-means Clustering에서 중심과의 거리를 통해서 구했던 것과 매우 유사한 식이다.\n- **M-step**  \n  $$\n  \\begin{align*}\n  \\mu_{k} &= \\frac{\\sum_{i=1}^{N}{r_{ik}x_{i}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\pi_{k} &= \\frac{1}{N}\\sum_{i=1}^{N}{r_{ik}}\n  \\end{align*}\n  $$  \n  $\\pi_{k}$가 추가되기는 했지만, $\\mu_{k}$를 구하는 식은 완전 동일하다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-clustering","date":"2022-11-23 09:19","title":"[ML] 9. Clustering","category":"AI","tags":["ML","UnsupervisedLearning","Clustering","K-means","GMM"],"desc":"이전까지의 Posting에서는 Supervised Learning 즉, 이미 Labeling이 완료된 데이터에 의한 Learning을 중점적으로 다루었다. 지금부터는 Unsupervised Learning에 대해서 조금 더 살펴보도록 하겠다. 대표적인 Unsupervised Learning은 Clustering, Feature Selection(or Dimensionality Reduction), Generative Model 등이 존재한다. 이들에 대해서 차근차근 살펴보도록 하고, 해당 Posting에서는 가장 대표적이라고 할 수 있는 Clustering을 먼저 살펴보면서 Unsupervised Learning에 대한 계략적인 이해를 해보도록 하겠다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nMachine Learning은 주어진 data를 가장 잘 설명할 수 있는 pattern(Model)을 찾는 것이 목표라고 하였다. 그렇다면, \"data가 가지는 여러가지 정보(feature)들 중에서 어떤 feature를 중점적으로 보고 이용할 수 있을까?\" 그리고, \"만약 여러 feature들이 서로 연관이 있다면 이를 연산의 최적화를 위해 이용할 수 있지 않을까?\" 라는 접근이 가능하다. 여기서 Graphical Model은 이러한 관계를 시각적으로 표현할 수 있으며, 이를 통해서 연산 최적화에 대한 insight를 얻을 수 있다.\n\n## Relation\n\n각 feature들 즉, Random Variable들 간의 관계는 크게 세 가지 종류가 있다.\n\n1. **Correlation(상관관계)**  \n   쉽게 생각하면 두 Random Variable이 있을 때, 서로가 값 추정에 영향을 준다는 것이다. 즉, 특정 Random Variable의 값이 관측되었을 때, Random Variable이 가지는 값의 범위가 제한되고, 확률이 변화한다.  \n   즉, $X$와$Y$가 서로 Correlation이 존재한다면, $P(X) \\neq P(X|Y)$  \n   그렇기에 두 Random Variable이 서로 독립(independence)이라면, Correlation이 존재하지 않는 것이다.  \n2. **Causality(인과관계)**  \n   쉽게 Correlation과 헷갈릴 수 있지만, Causality는 원인과 결과가 나타나는 관계를 의미한다. 쉬운 예시로 X라는 사건과 Y라는 사건이 빈번하게 같이 발생한다고, 쉽게 X라는 사건이 Y의 원인이라고 말할 수는 없는 것과 같은 원리이다. 또한, 중요한 특징 중에 하나는 방향이 분명하다는 것이다. 원인과 결과는 대게 분리되기 때문에 원인이 되는 사건과 결과가 되는 사건이 분명이 구분된다. 결론적으로, Causality를 가지는 두 사건은 서로 Correlation이 있는 것은 자명하지만, Correlation이 존재한다고 Causality를 단정할 수 있는 것은 아니다. 즉, Correlation이 Causality를 포함하는 개념이다. 그렇기에 서로 독립이라면, Causality도 존재하지 않는 것이다.\n3. **Independence(독립)**  \n   위에 제시된 두 가지는 dependence 관계를 나타낸다. 이는 두 Random Variable의 값이 서로의 값에 영향을 전혀 주지 않음을 의미한다.  \n   즉, $X$와 $Y$가 서로 독립하다면, $P(X) = P(X|Y), P(Y) = P(Y|X)$이다.  \n   (결과적으로 Independence가 아니라면 최소한의 Correlation이 존재한다.)\n\n이러한 관계를 어떻게 활용할 수 있을지를 고민해보자. 우리가 집중적으로 살펴볼 것은 **Independence**이다. 만약, 우리가 구하고자 하는 결과값($Y$)가 존재할 때, 특정 feature($X_{1}$)가 서로 독립한다고 하자. $P(Y|X_{1})=P(Y)$에 의해서 $X_{1}$는 전혀 쓸모가 없는 정보임을 알 수가 있다. 이렇게 명확한 independence를 안다면 해당 feature를 Learning 및 Estimation에서 제거하는 것은 쉬울 것이다. 하지만, 우리는 이러한 관계를 명확하게 밝히기 어려울 때가 많다. 그렇다면 결국 우리가 Machine Learning을 통해서 구하고자 하는 식인 아래 식을 어떻게 하면 좀 더 최적화할 수 있을까?\n\n$$\nP(Y|X_{1}, X_{2}, \\cdots, X_{N}) = \\frac{P(Y, X_{1}, X_{2}, \\cdots, X_{N})}{P(X_{1}, X_{2}, \\cdots, X_{N})}\n$$\n\n여기서의 핵심은 바로 **Joint Probability**에 있다. 우리는 결국 좋든 싫든 **Joint Probability**를 구해야 한다.\n\n$$\n\\begin{align*}\nP(X_{1}, X_{2}, \\cdots, X_{N}) &= P(X_{1} | X_{2}, X_{3}, \\cdots, X_{N}) \\times P(X_{2}, X_{3}, \\cdots, X_{N})\\\\\n&= P(X_{1} | X_{2}, X_{3}, \\cdots, X_{N}) \\times P(X_{2} |, X_{3}, X_{4}, \\cdots, X_{N}) \\times P(X_{3}, X_{4}, \\cdots, X_{N}) \\\\\n&= \\prod_{i=1}^{N} P(X_{i} | X_{i+1}, X_{i+2}, \\cdots, X_{N})\n\\end{align*}\n$$\n\n위에 제시한 **Probability Chain Rule**에 의해서 우리는 Joint Probability는 각각의 Random Variable 의 Conditional Probability라고 할 수 있다. 그렇다면, 우리는 Random Variable이 N개 있고, 각 Random Variable의 dimension이 L이라고 할 때, 다음과 같아짐을 알 수 있다.\n\n$$\nL^{N} \\times L^{N-1} \\times \\cdots \\times L^{1} = O(L^{N})\n$$\n\n이러한 연산을 어떻게 하면 좀 더 최적화할 수 있을까? Hint는 Conditional Probability 각 각의 변수의 양을 줄이는 것이다. 우리가 어떤 관계가 있을 때, 이 Random Variable의 갯수를 줄일 수 있을까? 바로 변수 간 Conditional Independence가 이에 대한 해답을 제시한다.\n\n### Conditional Independence\n\nConditional Independence는 Conditional Probability처럼 특정 정보(다른 Random Variable의 값)가 주어졌을 때, 두 Random Variable이 서로 독립이라는 것이다.\n\n쉽게 예를 들어 설명한다면, \"과음\"과 \"빨간 얼굴\" 사이의 관계라고 할 수 있다. 일반적으로 우리는 \"빨간 얼굴\"인 사람이 \"과음\"을 했을 것이라고 판단할 것이다. 즉, \"빨간 얼굴\"과 \"과음\" 사이에는 관계가 존재한다(dependency). 하지만, \"혈중 알코올 농도\"라는 정보가 주어진다면 어떨까? \"혈중 알코올 농도\"가 주어진다면, 사실 \"빨간 얼굴\"은 더 이상 \"과음\" 여부를 판단하는 기준에 영향을 1도 주지 않을 것이다. 이때에는 \"과음\"과 \"빨간 얼굴\"은 independence하다. 우리는 이런 경우를 다음과 같이 표현할 수 있다.\n\n$$\n\\text{과음} \\not\\!\\perp\\!\\!\\!\\perp \\text{빨간 얼굴}\n$$\n$$\n\\text{과음} \\perp\\!\\!\\!\\!\\perp \\text{빨간 얼굴} |\\ \\text{혈중 알코올 농도}\n$$\n\n즉, 확률에 적용하면 다음과 같다.\n\n$$\nP(\\text{과음} | \\text{빨간 얼굴, 혈중 알코올 농도}) = P(\\text{과음} | \\text{혈중 알코올 농도})\n$$\n\n여기서 우리가 하고 싶었던 것이 나왔다. 바로 \"빨간 얼굴\"이라는 Random Variable이 없어졌다. 즉, \"과음\"과 \"빨간 얼굴\" 사이의 관계 같은 것을 찾을 수 있다면, 우리는 계산 과정을 단순화할 수 있다.\n\n즉, 이것이 우리가 **Graph**를 통해서 찾고자 하는 것이다.\n\n## Graphical Model\n\n**Graphical Model**은 **Graph**를 이용해서 Random Variable들의 관계를 표현하고, 이를 통해서 **Joint Probability**를 계산하는 방법이다. **Graph**를 그리는 방법은 기본적으로 Random Variable 하나 하나가 Graph의 Node가 되고, 각 Node간의 관계가 Edge가 된다. 그런데, 이 관계가 Correlation이냐, Causality냐에 따라서 두 가지 종류로 나뉘게 된다. <mark>**Correlation**은 일반적으로 관계의 방향이 없기에 **Undirected Graph**</mark>로 표현하고, <mark>**Causality**는 관계의 방향이 있기에 **Directed Graph**</mark>로 표현한다. 이는 아래에서 더 자세히 다루도록 하겠다.\n\n### Markov Random Field(Undirected Graphical Model, Correlation)\n\n**Markov Random Field**(MRF)라고 불리며, **Correlation**를 표현한 Graph이다. 각 Node는 Random Variable을 의미하며, Edge는 Correlation를 의미한다. 즉, 두 Node가 Edge로 연결되어 있다면, 두 Random Variable은 Independence하지 않다는 것이다.\n\n![ml-undirected-graph-1](/images/ml-undirected-graph-1.jpg)\n\n여기서 중요한 것은 Random Variable을 대표하는 Node와 Correlation을 대표하는 Edge이기 때문에, Graph $G=(V, E)$에서 Random Variable의 집합 $X = \\{X_{1}, X_{2}, \\cdots, X_{|V|}\\}$이고, $\\{1,2, \\cdots, |V|\\}$가 주어질 때 반드시 아래에 제시된 **Markov Property들**을 만족해야 한다.\n\n1. <mark>**Pairwise Markov Property**</mark>  \n   인접하지 않은 Node 두 개는 다른 모든 Node가 주어질 때 conditionally independent하다.  \n   (아래에서 \\는 포함하지 않는다는 의미이다.)\n   $$\n   X_{i} \\perp\\!\\!\\!\\!\\perp X_{j} | X_{S\\backslash\\{i, j\\}}\n   $$\n2. <mark>**Local Markov Property**</mark>  \n   한 Node에 인접한 모든 Node(Neighbors)가 주어질 때, 해당 Node는 다른 모든 Node와 conditionally independent하다.  \n   (아래에서 $\\mathcal{N}_{i}$는 Node i와 인접한 모든 Node를 의미한다.)\n   $$\n   X_{i} \\perp\\!\\!\\!\\!\\perp X_{S\\backslash \\mathcal{N}_{i}} | X_{\\mathcal{N}_{i}}\n   $$\n3. <mark>**Global Markov Property**</mark>  \n   만약, Node들의 Subset으로 이루어진 $A, B$가 특정 subset $C$가 주어질 때, 서로 conditionally independent하다면, $A, B$에 속하는 어떤 subset이라도 서로 independent하다.  \n   (subset간의 conditionally independent를 확인하기 위해서는 특정 Subset들간에 이어지는 모든 경로를 차단할 수 있는 subset이 있는지를 확인한다.)  \n   $$\n   \\begin{align*}\n   X_{A} &\\perp\\!\\!\\!\\!\\perp X_{B} | X_{C} \\\\\n   X_{\\text{subset of }A} &\\perp\\!\\!\\!\\!\\perp X_{\\text{subset of }B} | X_{C} \\\\\n   \\end{align*}\n   $$  \n   ![ml-global-markov-property](/images/ml-global-markov-property.jpg)\n\n따라서, 우리는 이전 그림에서 Conditional Independence를 활용할 수 있다. $X_{1}, X_{4}$의 경우 다른 모든 Random Variable과 correlation이 존재하지만, $X_{2}, X_{3}$의 경우 $X_{1}, X_{4}$만 알면 된다. 즉, $X_{2} \\perp\\!\\!\\!\\!\\perp X_{3} | X_{1}, X_{4}$이다. 따라서, 우리는 이 관계를 확률 식에서 녹여낼 수 있다.\n\n$$\n\\begin{align*}\nP(X_{1}, X_{2}, X_{3}, X_{4}) &= P(X_{2}|X_{1},\\cancel{X_{3}},X_{4})P(X_{1}, X_{3}, X_{4}) (\\because X_{2} \\perp\\!\\!\\!\\!\\perp X_{3} | X_{1}, X_{4}) \\\\\n&= P(X_{2}|X_{1},X_{4})P(X_{1}, X_{3}, X_{4})\n\\end{align*}\n$$\n\n또한, 우리는 Graph를 통해서 Joint Probability를 다음과 같이 정의할 수 있다.\n\n$$\nP(\\cap_{i=1}^{N}X_{i}) = \\frac{1}{Z} \\prod_{C \\in \\mathcal{C}} \\psi_{C}(\\cap_{X_{j}\\in C}X_{j})\n$$\n\n식이 다소 난해하다. 하나 하나 해석을 해보도록 하자. 먼저, $P(\\cap_{i=1}^{N}X_{i})$이다. 이는 Joint Probability를 표현하는 방법 중의 하나로 단순히 이를 정리하면, $P(\\cap_{i=1}^{N}X_{i})=P(X_{1} \\cap X_{2} \\cap \\cdots \\cap X_{N})=P(X_{1}, X_{2}, \\cdots, X_{N})$이다. 다음은 $C$와 $\\mathcal{C}$이다. 둘 다 아마 집합일 것이라는 것은 $\\in$ 기호 덕분에 알 수 있을 것이다. 그렇다면, 어떤 데이터를 담고 있는 집합일까? 이는 Random Variable들로 이루어진 부분 집합이다. 이를 <mark>**Clique($C$)**</mark>라고 한다. Clique는 Graph에서 Node들의 부분 집합으로, Graph에서 **Fully Connected Node**의 집합을 의미한다. 이것이 가지는 의미는 사실상 하나의 Node로 합칠 수 있다는 것이다.(이를 Graph 상에서의 인수분해(**factorization**)라고도 한다.) Clique에 속하는 Node끼리는 서로 완벽하게 연결되어 있기 때문에 이 중에 하나의 Node라도 다른 Node와 연결을 가진다면, 이에 속하는 모든 Node가 이 관계로 연결된다는 것이다. 추가적으로 Clique들 중에서 다른 Clique에 속하지 않는 Clique들을 <mark>**Maximal Clique($\\mathcal{C}$)**</mark>라고 한다. 아래 그림에서는 Maximal Clique를 빨간색으로 표기한 것이다.\n\n![ml-max-clique](/images/ml-max-clique.jpg)\n\n마지막으로 $\\psi$이다. 이는 <mark>**Clique Potential Function**</mark>로, 각 Clique의 Node(Random Variable)를 parameter로 사용하는 함수로 확률과 비슷한 성질을 가지지만 확률처럼 합이 1이 아닐 수도 있고, 값 자체가 음수일 수도 있다. 즉, 이를 구할 때에는 각 Random Variable의 경우의 수와 해당 경우의 상대적 확률로 이루어진 table을 작성하고, 이를 표현할 수 있는 함수를 찾아낸 것이 $\\psi$이다. 대게의 경우 $\\psi$는 해당 Parameter로 이루어진 Condition Probability 또는 Joint Probability가 되는 경우가 많다. 하지만, 그렇지 않은 경우에도 $\\psi$로 표현이 가능하다.(이에 대한 엄밀한 증명은 여기서 다루지 않을 뿐만 아니라 중요하지 않다.) 여기서 <mark>$Z$</mark>의 의미를 마지막으로 짚어보자면, 단순한 normalization이다. $\\psi$가 운좋게도 Joint Probability, Conditional Probability로 쉽게 구해진다면 $Z=1$이다. 하지만, 그렇지 않을 경우에는 이들의 합이 1이 아니기 때문에 Normalization이 필요한 것이다.\n\n$$\n\\begin{align*}\nZ &= \\sum_{X_{1}}\\sum_{X_{2}} \\cdots \\sum_{X_{N}}{\\prod_{C \\in \\mathcal{C}} \\psi_{C}(\\cap_{X_{j}\\in C}X_{j})} \\\\\n&= \\sum_{X_{1}, X_{2}, \\cdots, X_{N}}{\\prod_{C \\in \\mathcal{C}} \\psi_{C}(\\cap_{X_{j}\\in C}X_{j})}\n\\end{align*}\n$$\n\n결론적으로 의미를 따지자면, 위에서 구한 **Maximal Clique**에 특정 함수를 취한 $\\psi$가 인수분해(**factorization**)에서 하나의 인자(**factor**)가 되는 것이다. 따라서, 이를 **factor function**이라고도 부른다.\n\n자, 마지막으로 우리가 4개의 Random Variable 4개($A, B, C, D$)가 있을 때, Graph로 그릴 수 있는 형태를 네 개 정도 가정하여 예시들을 살펴볼 것이다.\n\n![ml-undirected-graph-2](/images/ml-undirected-graph-2.jpg)\n\n1. $A, B, C, D$가 선형으로 이루어진다.  \n   여기서는 **Maximal Clique**가 3개이다($\\{\\{A, B\\}, \\{ B, C\\}, \\{ C, D\\}\\}$). 따라서, 이를 통해서 Joint Probability를 추정하면 다음과 같다.  \n   $$\n   P(A, B, C, D) = \\frac{1}{Z} \\times \\psi_{1}(A, B) \\times \\psi_{2}(B, C) \\times \\psi_{3}(C, D)\n   $$  \n   여기서 직접적으로 한 번 $P(A, B, C, D)$를 추정해보자.  \n   $$\n   \\begin{align*}\n   P(A, B, C, D) &= P(A| B, C, D) \\times P(B | C, D) \\times P(C, D) \\\\\n   &= P(A|B) \\times P(B|C) \\times P(C, D)\n   \\end{align*}\n   $$  \n   즉, 이렇게 일렬로 된 Graph에서는 마지막 $\\psi$를 제외하고는 모두 Conditional Probability이고, 마지막 $\\psi$는 Joint Probability이다. 그리고, $Z$는 1이라는 것을 알 수 있다.\n2. $A, B, C, D$가 모두 완벽하게 연결되어 있다.  \n   이 경우에는  **Maximal Clique**가 1개이다($\\{\\{A, B, C, D\\}\\}$). 따라서, Joint Probability를 다음과 같이 추정할 수 있다.  \n   $$\n   P(A, B, C, D) = \\frac{1}{Z} \\times \\psi(A, B, C, D)\n   $$  \n   결론적으로 Clique가 하나기 때문에 줄일 수 있는 변수가 없다. 즉, $\\psi$가 Joint Probability이고, $Z$는 1이다.\n3. **Maximal Clique**가 2개이다($\\{\\{A, B, D\\}\\, \\{A, C, D\\}\\}$). 따라서, 다음과 같이 정리된다.  \n   $$\n   \\begin{align*}\n   P(A, B, C, D) &= P(B|A, C, D) \\times P(A, C, D) \\\\\n   &= P(B|A,D) \\times P(A, C, D) \\\\\n   &= \\frac{1}{Z} \\times \\psi_{1}(A, B, D) \\times \\psi_{2}(A, C, D) \\\\\n   \\end{align*}\n   $$\n4. **Maximal Clique**가 4개이다($\\{\\{A, B\\}, \\{ A, C\\}, \\{ B, D\\}, \\{ C, D\\}\\ \\}$).  \n   $$\n   \\begin{align*}\n   P(A, B, C, D) &= P(A|B, C, D) \\times P(B|C, D) \\times P(C, D) \\\\\n   &= P(A|B, C) \\times P(B|D) \\times P(C, D) \\\\\n   &= \\frac{P(A, B, C)}{P(B, C)} \\times P(B|D) \\times P(C, D) \\\\\n   &= \\frac{P(B, C| A)}{P(A)P(B, C)} \\times P(B|D) \\times P(C, D) \\\\\n   &= \\frac{P(B|A)P(C|A)}{P(A)P(B, C)} \\times P(B|D) \\times P(C, D) \\\\\n   &= \\frac{1}{P(B,C)} \\times P(A, B) \\times P(C|A) \\times P(B|D) \\times P(C, D) \\\\\n   &\\neq \\frac{1}{Z} \\times \\psi_{1}(A, B) \\times \\psi_{2}(A, C) \\times \\psi_{3}(B, D) \\times \\psi_{4}(C, D) \\\\\n   \\end{align*}\n   $$  \n   이것이 바로 $\\psi$를 확률 함수라고 부르지 않는 이유이다.  \n   우리가 $\\psi$를 확률 함수 형태로 표현하기 위해서는 **Chordal graph**(4개 이상의 Node로 이루어진 Cycle에서는 중간에 반드시 Cycle을 이루는 Edge가 아닌 Edge가 존재하는 Graph) 형태를 가져야 한다는 것이다. $\\psi$가 확률 함수로 표현되지 않는다고 우리가 하고자 하는 일에 영향을 주지는 않으니 그런가보다 하고 넘어가도 무방하다.\n\n여기서 우리는 **factorization**이라는 개념을 익혔고, 이것이 가능하기 위해서는 Chordal graph가 주어진 상황에서 Markov Property를 만족해야 함을 확인했다. 그리고, 우리는 이러한 **factorization** 형태를 좀 더 명확하게 나타내기 위해서 다음과 같은 형태로 표현하고, 이를 <mark>**factor graph**</mark>라고 정의한다. 따라서, 각 **factor**(인수)는 **Maximal Clique** 단위로 생성된다.\n\n![ml-factor-graph-1](/images/ml-factor-graph-1.jpg)\n\n### Bayesian Network(Directed Graphical Model, Causality)\n\n**Bayesian Network**라고 불리며, **Causality**를 표현한 Graph이다. 각 Node는 Random Variable을 의미하며, Edge는 Causality(원인($C$) -> 결과($R$))를 의미한다. 그렇기에 굉장히 명확하게 표현이 될 수 있다. 왜냐하면, $P(R, C) = P(C|R)P(R)$임을 명백하게 드러낸다. 그렇기에 우리는 해당 Graph가 주어지는 순간 Joint Probability를 다음과 같이 유추할 수 있는 것이다.\n\n![ml-bayesian-network](/images/ml-bayesian-network.jpg)\n\n즉, 이것을 식으로 나타내면 다음과 같다.\n\n$$\nP(\\cap_{i=1}^{N}X_{i}) = \\prod_{i \\in \\{1, 2, \\cdots, N\\}} P(X_{i}| \\cap_{j \\in \\text{Parents}(X_{i})} X_{j})\n$$\n\n이러한 점 때문에 Bayesian Network에서는 Cycle이 존재할 수 없다. 왜냐하면, Cycle이 존재한다는 것은 각 Random Varaible이 서로 원인과 결과가 되는 것이 때문에 사실상 하나의 사건이라는 의미를 내포하는 것이다. 그렇기에 이는 사실상 존재할 수 없다.\n\n여기서도 마찬가지로 Conditional Independence를 찾을 수 있다. 뿐만 아니라 Marginal Independence에 대한 힌트도 얻을 수 있다. 이때 우리는 <mark>**D-Seperation**</mark>이라는 방법을 활용한다. 이를 위해서는 자신과 주변 2개의 Node가 이룰 수 있는 관계 3가지를 정의해야 한다.\n\n![ml-bayesian-network-2](/images/ml-bayesian-network-2.jpg)\n\n1. **head-to-tail**  \n   이 경우에는 $X \\rightarrow Y \\rightarrow Z$의 관계를 의미한다. 따라서, $X$와 $Z$는 $Y$가 주어질 때, 서로 Independent하다.  \n   $$\n   \\begin{align*}\n   P(X, Z | Y) &= \\frac{P(X,Y,Z)}{P(Y)}\\\\\n   &= \\frac{P(X)P(Y|X)P(Z|Y)}{P(Y)}\\\\\n   &= \\frac{P(X, Y)}{P(Y)} \\times P(Z|Y) \\\\\n   &= P(X | Y)P(Z | Y)\n   \\end{align*}\n   $$\n2. **tail-to-tail**  \n   이 경우에는 $X \\leftarrow Y \\rightarrow Z$의 관계를 의미한다. 따라서, $X$와 $Z$는 $Y$가 주어질 때, 서로 Independent하다.  \n   $$\n   \\begin{align*}\n   P(X, Z | Y) &= \\frac{P(X, Y, Z)}{P(Y)} \\\\\n   &= \\frac{P(X|Y)P(Y)P(Z|Y)}{P(Y)} \\\\\n   &= P(X | Y)P(Z | Y)\n   \\end{align*}\n   $$\n3. **head-to-head**  \n   이 경우에는 $X \\rightarrow Y \\leftarrow Z$의 관계를 의미한다. 따라서, $X$와 $Z$는 서로 Independent하다.  \n   즉, Conditional Independence가 아니라 Marginal Independence이다.  \n   $$\n   \\begin{align*}\n   P(X, Z) &= \\sum_{Y} P(X, Y, Z) \\\\\n   &= \\sum_{Y} P(X)P(Y|X,Z)P(Z) \\\\\n   &= P(X)P(Z)\\sum_{Y} P(Y|X,Z) \\\\\n   &= P(X)P(Z)\n   \\end{align*}\n   $$\n\n이 관계에서 중요한 것은 $X,Z$간 edge가 존재해서는 안된다는 점이다. 위의 관계를 활용하면, 인접한 관계에서의 Conditional Independence는 판별이 가능하다.하지만, <mark>**D-Seperation**</mark>을 통해서 이를 더 넓은 범위로 확장할 수 있다. 세 Node의 집합 $A, B, C$가 주어질 때, $A \\perp\\!\\!\\!\\!\\perp B | C$이기 위해서는 다음 조건을 만족해야 한다.\n\n1. A에서 B로 가는 경로가 하나 이상 존재한다.(여기서 경로는 방향을 신경쓰지 않고 연결 여부에 따라 결정한다.)\n2. 모든 경로에 대해서, C에 속하는 Node가 적어도 하나 head-to-tail 또는 tail-to-tail 관계를 중계할 수 있어야 한다.\n3. 모든 경로에 대해서, C에 속하는 Node는 head-to-head 관계를 중계하면 안되며, head-to-head 관계를 중계하는 Node의 자손이여도 안된다.\n\n즉, $A, B, C$가 이러한 조건을 모두 만족할 때, 우리는 $C$가 $A, B$를 Block했다고 하며, $A \\perp\\!\\!\\!\\!\\perp B | C$이다.\n\n예를 들어 아래와 같은 두 경우를 예를 들어볼 수 있다.\n\n![ml-d-seperation](/images/ml-d-seperation.jpg)\n\n왼쪽의 경우 C의 parent가 A에서 B로 가는 경로에서 head-to-head를 중계하고 있다. 따라서, A와 B는 Conditionally Independence를 만족하지 않는다. 반면, 오른쪽의 경우 C가 A에서 B로 가는 경로에서 head-to-tail 관계를 중계하고 있으므로, A와 B는 Conditionally Independence를 만족한다. 여기서 재밌는 점은 A와 B는 두 경우 모두 Marginal Independence를 만족한다는 점이다. 왜냐하면, A에서 B로 가는 경로가 순방향만으로는 이루어지지 않기 때문이다.\n\n마지막으로, Bayesian Network도 **factorization**이 가능하다 A, B의 **Causality**가 $P(A|B)P(B)$를 의미한다는 점을 활용해서 우리는 다음과 같은 형태로 정의하는 것이 가능하다.\n즉, 초기 시작 점은 자신만을 가지는 factor를 가지고, head-to-head 관계는 하나로 통일하며, 나머지 관계(head-to-head, 등)는 별도로 factor를 분리한다. 즉, 다음과 같은 형태를 가진다.\n\n![ml-factor-graph-2](/images/ml-factor-graph-2.jpg)\n\n```plaintext\n 🤔 Markov Blankets\n\n Markov Blanket은 특정 Node에 대한 정보(관계)가 있는 모든 Node를 의미한다. \n 즉, 특정 Random Variable의 확률이 궁금하다면, 이 Markov Blanket만 가지면 된다. \n 그 중에서도 가장 작은 크기로 모든 필요한 정보를 담은 subset을 Markov Boundary라고 한다. \n Markov Boundary는 Markov Random Field에서는 Neighbor이고,\n Bayesian Network에서는 Parent, Child, Co-Parent이다.\n```\n\n![ml-markov-boundary](/images/ml-markov-boundary.jpg)\n\n### Factor Graph\n\n앞 서 본 두 가지 Graph 표현 방법은 각 각 장단점을 가지고 있다.\n\n1. Markov Random Field는 Joint Probability를 Potential이라는 임의의 변수를 통해서 추정하는 것이 가능하다. 따라서, 명확성이 떨이지지만, Conditional Independence를 파악하는 것은 더 분명하고 쉽다.\n2. Bayesian Network는 Joint Probability를 명확하게 판별할 수 있다. 하지만, Conditional Independence를 판별하는 것이 더 어렵고 복잡하다.\n\n이러한 장단점을 모두 살릴 수 있는 방법으로 제시된 것이 Factor Graph이다. 위에서 각 각 Factor Graph를 표현하는 방법에 대해서는 제시하였으므로 여기서는 다루지 않는다. Factor Graph는 근본적으로 Graph의 요소들을 인수분해(Factorization)하여 인수(Factor)로 분리해낸 것이다. 그렇기에 더 명확한 구분이 가능하다. 각 Node는 Factor와 기존 Node에 해당하는 값이 두 개 다 존재하고, Factor는 꽉 찬 네모, 기존 Node(Variable)는 비어있는 동그라미로 표현하는 것이 일반적이다.\n\n그리고, 여기서는 Joint Probability를 다음과 같이 정의한다.\n\nVariable Node는 $\\{X_{1},X_{2}, \\cdots, X_{N}\\}$이고, Factor Node가 $\\{f_{1},f_{2}, \\cdots, f_{M}\\}$일 때, $f_{j}$와 이웃한 Variable Node의 집합을 $\\mathcal{N}_{j}$라고 하자.\n\n$$\nP(X_{1}, X_{2}, \\cdots, X_{N}) = \\prod_{j=1}^{M}{f_{j}(\\cap_{X \\in \\mathcal{N}_{j}} X)}\n$$\n\n이렇게 표현하는 것은 확실히 Markov Random Field에서는 명확하다. 하지만, Bayesian Network에서는 표현할 수 있는 정보를 어느정도 잃었다고 볼 수도 있다. 어차피 Conditional Probability인데, 다르게 표현한 것이기 때문이다. 하지만, 이를 이용하게 되면 기존에 문제였던 Conditional Independence를 쉽게 파악할 수 있다. 왜냐하면 Factor Graph에서는 Conditional Independence를 확인하기 위해서 해당 집합으로 이어지는 모든 경로에서 중간에 하나라도 Variable Node가 껴있는지만 확인해도 충분하다.\n\n![ml-factor-graph-3](/images/ml-factor-graph-3.jpg)\n\n따라서, 앞으로의 과정에서는 Factor Graph를 Main으로 하여 설명을 진행하도록 하겠다.\n\n## Message Passing\n\n우리는 앞의 Graph 표현을 통해서 Feature를 Factor로 압축하는 과정을 익혔다. 이 역시 엄청난 계산 효율을 가져온다. 하지만, 이를 더 효과적으로 활용할 수 있는 방법이 있다. 그것은 Message Passing 방법이다. 우선 우리가 해결하고자하는 문제를 정의해보자. 우리는 Joint Probability($P(X_{1}, X_{2}, \\cdots, X_{N})$)가 주어졌을 때, 다음 값을 구하고 싶을 수 있다.\n\n1. <mark>**Marginalization**</mark>  \n   Marginal Probability는 Joint Probability에서 구하고자 하는 Random Variable을 제외한 모든 경우의 수를 더한 것이다.\n   $$\n   \\begin{align*}\n   P(X_{i}) &= \\sum_{X_{j}}P(X_{i}, X_{j}) \\\\\n   &= \\sum_{X_{j}, X_{k}}P(X_{i}, X_{j}, X_{k}) \\\\\n   &= \\cdots \\\\\n   &= \\sum_{X_{-i}}P(X_{1}, X_{2}, \\cdots, X_{N})\n   \\end{align*}\n   $$  \n   즉, 이를 일반적인 방법으로 풀고자하면 Random Variable($X_{i}$)이 각 각 $\\mathbb{R}^{L}$로 정의된다고 할 때, $L^{N-1}$번의 합연산이 필요하다.\n2. <mark>**Maximization**</mark>  \n   Joint Probability의 최댓값을 갖게 하는 경우의 수($\\hat{X}$)를 구하고자 한다면 다음을 구해야 한다.  \n   $$\n   \\hat{X} = \\argmax_{X_{1}, X_{2}, \\cdots, X_{N}} P(X_{1}, X_{2}, \\cdots, X_{N})\n   $$  \n   이 또한 무식하게 풀고자하면, $L^{N-1}$번의 max 연산이 필요하다.\n\n그렇다면, 이를 한 번 가장 간단한 형태인 일자형 Factor Graph로 표현해보자.\n\n![ml-bp-1](/images/ml-bp-1.jpg)\n\n여기서 $P(X_{2})$를 알고 싶다고 해보자. 그 경우 다음과 같이 식이 정리되는 것을 볼 수 있다.\n\n$$\n\\begin{align*}\nP(X_{2}) &= \\sum_{X_{1}}P(X_{1},X_{2}) \\\\\n&= \\sum_{X_{1}, X_{3}, X_{4}, X_{5}}P(X_{1}, X_{2}, X_{3}, X_{4}, X_{5}) \\\\\n&= \\sum_{X_{1}}\\sum_{X_{3}}\\sum_{X_{4}}\\sum_{X_{5}}P(X_{1}, X_{2}, X_{3}, X_{4}, X_{5}) \\\\\n&= \\sum_{X_{1}}\\sum_{X_{3}}\\sum_{X_{4}}\\sum_{X_{5}}f_{a}(X_{1}, X_{2})f_{b}(X_{2}, X_{3})f_{c}(X_{3}, X_{4})f_{d}(X_{4}, X_{5}) \\\\\n&= (\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{3}}\\sum_{X_{4}}\\sum_{X_{5}}f_{b}(X_{2}, X_{3})f_{c}(X_{3}, X_{4})f_{d}(X_{4}, X_{5})) \\\\\n&= (\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{3}}\\sum_{X_{4}}f_{b}(X_{2}, X_{3})f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\\\\n&= (\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{3}}f_{b}(X_{2}, X_{3})\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5}))\n\\end{align*}\n$$\n\n이것이 의미하는 바는 무엇일까? 이는 단순하게 순서를 바꾸어 재조합하는 것만으로 Computing을 줄일 수 있음을 보여줬다. 먼저, 앞의 $\\sum$연산만 단독으로 할 때, $L$번의 연산이 필요하고, 뒤에 연속해서 나오는 3번의 $\\sum$을 구하기 위해서는 결국 $L^{3}$의 연산이 필요하다. 즉, $L + L^{3}$의 합연산으로 marginalization 결과를 구할 수 있다는 것이다. 그렇기에 더 효율적인 연산이 가능한 것이다. 이는 특히 Graph의 중앙에 있는 값을 구할 때 더 도드라지게 나타난다. 전체 marginalization 결과를 나타내면 다음과 같다.\n\n$$\n\\begin{align*}\nP(X_{1}) &= \\sum_{X_{2}}f_{a}(X_{1}, X_{2})\\sum_{X_{3}}f_{b}(X_{2}, X_{3})\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5}) \\rightarrow L^{4} \\\\\nP(X_{2}) &= (\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{3}}f_{b}(X_{2}, X_{3})\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\rightarrow L^{3} + L \\\\\nP(X_{3}) &= (\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\rightarrow 2L^{2} \\\\\nP(X_{4}) &= (\\sum_{X_{3}}f_{c}(X_{3}, X_{4})\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\rightarrow L^{3} + L \\\\\nP(X_{5}) &= \\sum_{X_{4}}f_{d}(X_{4}, X_{5})\\sum_{X_{3}}f_{c}(X_{3}, X_{4})\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\sum_{X_{1}}f_{a}(X_{1}, X_{2}) \\rightarrow L^{4}\n\\end{align*}\n$$\n\n이것이 끝이 아니다. 우리는 중복된 연산을 별도로 저장해두어서 더 빠른 연산을 수행하는 것도 가능하다. 예를 들어 다음과 같은 과정이라고 할 수 있다.\n\n$$\n\\begin{align*}\nP(X_{2}) &= \\underbrace{(\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))}_{\\red{\\mu_{a\\rightarrow2}(X_{2})}}(\\sum_{X_{3}}f_{b}(X_{2}, X_{3})\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\\\\nP(X_{3}) &= \\underbrace{(\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\red{\\mu_{a\\rightarrow2}(X_{2})})}_{\\red{\\mu_{b\\rightarrow3}(X_{3})}}(\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\\\\nP(X_{4}) &= \\underbrace{(\\sum_{X_{3}}f_{c}(X_{3}, X_{4})\\red{\\mu_{b\\rightarrow3}(X_{3})})}_{\\red{\\mu_{c\\rightarrow4}(X_{4})}}(\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\\\\nP(X_{5}) &= \\underbrace{\\sum_{X_{4}}f_{d}(X_{4}, X_{5})\\red{\\mu_{c\\rightarrow4}(X_{4})}}_{\\red{\\mu_{d\\rightarrow5}(X_{5})}}\n\\end{align*}\n$$\n\n![ml-bp-2](/images/ml-bp-2.jpg)\n\n즉, 이전 Marginalization에서 계산했던 $\\mu_{\\text{factor}\\rightarrow\\text{variable}}(X_{\\text{variable}})$를 저장해서, 다음 Marginalization 연산 시에 사용할 수 있기 때문에 전체 Marginalization을 구하는데에도 더 빠른 연산이 가능하다. 이 방식은 역으로 진행하는 것도 가능한데 다음과 같다.\n\n$$\n\\begin{align*}\nP(X_{4}) &= (\\sum_{X_{3}}f_{c}(X_{3}, X_{4})\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))\\underbrace{(\\sum_{X_{5}}f_{d}(X_{4}, X_{5}))}_{\\blue{\\mu_{d\\rightarrow4}(X_{4})}} \\\\\nP(X_{3}) &= (\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))\\underbrace{(\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\blue{\\mu_{d\\rightarrow4}(X_{4})})}_{\\blue{\\mu_{c\\rightarrow3}(X_{3})}} \\\\\nP(X_{2}) &= (\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))\\underbrace{(\\sum_{X_{3}}f_{b}(X_{2}, X_{3})\\blue{\\mu_{c\\rightarrow3}(X_{3})})}_{\\blue{\\mu_{b\\rightarrow2}(X_{2})}} \\\\\nP(X_{1}) &= \\underbrace{\\sum_{X_{2}}f_{a}(X_{1}, X_{2})\\blue{\\mu_{b\\rightarrow2}(X_{2})}}_{\\blue{\\mu_{a\\rightarrow1}(X_{1})}}\n\\end{align*}\n$$\n\n![ml-bp-3](/images/ml-bp-3.jpg)\n\n이를 합쳐서 표현하면 다음과 같은 형태를 가지는 것을 알 수 있다.\n\n$$\n\\begin{align*}\nP(X_{1}) &= \\blue{\\mu_{a\\rightarrow1}(X_{1})} &= \\red{\\mu^{-}(X_{1})}\\blue{\\mu^{+}(X_{1})} \\\\\nP(X_{2}) &= \\red{\\mu_{a\\rightarrow2}(X_{2})}\\blue{\\mu_{b\\rightarrow2}(X_{2})}&= \\red{\\mu^{-}(X_{2})}\\blue{\\mu^{+}(X_{2})} \\\\\nP(X_{3}) &= \\red{\\mu_{b\\rightarrow3}(X_{3})}\\blue{\\mu_{c\\rightarrow3}(X_{3})}&= \\red{\\mu^{-}(X_{3})}\\blue{\\mu^{+}(X_{3})} \\\\\nP(X_{4}) &= \\red{\\mu_{c\\rightarrow4}(X_{4})}\\blue{\\mu_{d\\rightarrow4}(X_{4})}&= \\red{\\mu^{-}(X_{4})}\\blue{\\mu^{+}(X_{4})} \\\\\nP(X_{5}) &= \\red{\\mu_{d\\rightarrow5}(X_{5})}&= \\red{\\mu^{-}(X_{5})}\\blue{\\mu^{+}(X_{5})} \\\\\n&\\therefore P(X_{i}) = \\red{\\mu^{-}(X_{i})}\\blue{\\mu^{+}(X_{i})} \\\\\n\\end{align*}\n$$\n\n![ml-bp-4](/images/ml-bp-4.jpg)\n\n$\\mu^{+}$와 $\\mu^{-}$의 방향이 헷갈릴 수 있는데, 이는 자신($X_{i}$)을 기준으로 큰 쪽에서 왔는지 작은 쪽에서 왔는지를 표시한다고 생각하면 쉽다. 따라서, $\\mu$는 다음과 같이 정의되어질 수 있다.\n\n$$\n\\begin{align*}\n\\mu^{-}(X_{1}) &= 1,\\, \\mu^{+}(X_{N}) = 1 \\text{이고,}\\\\\n\\mu^{-}(X_{i}) &= \\sum_{X_{i-1}}f_{i}(i-1, i)\\mu^{-}(X_{i-1}) \\\\\n\\mu^{+}(X_{i}) &= \\sum_{X_{i+1}}f_{i}(i, i+1)\\mu^{+}(X_{i+1}) \\\\\n\\end{align*}\n$$\n\n여기서 $\\mu$가 바로 <mark>**Message**</mark>를 의미한다. 즉, 우리가 마치 운동장에서 사람 수를 세기 위해서 앞 사람이 말한 수 + 1을 반복하면서 진행하는 것처럼 Message를 전달하며 전체 확률을 구해나가는 것이다. 이러한 방법을 **Message Passing**이라고 하며, 이 방법을 통해서 우리는 Marginal Probability를 더 효과적으로 구할 수 있다. 왜냐하면, $\\mu^{-}(X_{i})$를 구하기 위한 연산량이 $(i-1) \\times L$이라는 것과, $mu^{+}(X_{i})$를 구하기 위한 연산량이 $(N-i) \\times L$이라는 것을 알고 있다. 따라서, 각 각의 Marginalization을 구하기 위한 연산량이 $L^{N-1}$에서 $(N-1)L$로 줄어들었다.\n\n여기까지 우리는 Line으로 되어있는 가장 간단한 Factor Graph에서의 <mark>**Sum-Product Belief Propagation**</mark>을 알아본 것이다. 이제부터 우리는 더 복잡한 상황에서의 Belief Propagation(BP)을 살펴볼 것이다. Belief Propagation과 Message Passing은 대게 비슷한 의미로 사용되어 진다(일부는 Message Passing 후에 데이터를 가공하는 작업을 분리하고 이를 통합하여 Belief Propagation이라고 하기도 한다.)\n\n### Sum-Product Belief Propagation\n\n합의 곱을 통해서 Marginal Probability를 구하는 방법으로, 앞 서 보았던 Linear Factor Graph 뿐만 아니라 Tree형태의 Factor Graph에서도 사용할 수 있다. 물론 Cycle이 존재하는 Factor Graph가 존재할 수도 있지만, 이 경우에 대해서는 특별한 알고리즘을 별도로 적용하는 것이 일반적이다. 따라서, 여기서는 Tree형태의 Factor Graph에서 일반적으로 적용할 수 있는 방법을 제시한다.\n\n우선 아래 그림을 통해서 대략적인 이해를 해보도록 하자.\n\n![ml-sum-product-bp-1](/images/ml-sum-product-bp-1.jpg)\n\n우리는 위에서 Line Factor Graph에서 어떻게 Marginal Probability를 어떻게 구하는지를 보았다. Tree 구조에서도 동일하게 결국 Marginal Probability를 자신과 이웃한 Factor Node들로 부터 전달된 Message의 곱이라고 할 수 있다. 단지 다른 점은 이웃한 factor가 복수 개라는 것이다.  \n(factor 또는 variable에 해당하는 Node 중에서 index가 i인 Node와 인접한 Node(Node i가 factor라면 variable, variable이라면 factor이다.)들의 index 집합을 $\\mathcal{N}_{i}$ 라고하고, 값은 종류의 Node의 index를 모아둔 집합 I가 있을 때 $X_{I} = \\{X_{i}\\}_{i \\in I}$라고 하자.)\n\n$$\nP(X_{i}) = \\prod_{p \\in \\mathcal{N}_{i}}\\mu_{p \\rightarrow i}(X_{i})\n$$\n\n그렇다면, 여기서 $\\mu_{p \\rightarrow i}(X_{i})$를 각 각 어떻게 구할 수 있을까? 그러기 위해서 빨간색 부분을 자세히 봐보자.\n\n![ml-sum-product-bp-2](/images/ml-sum-product-bp-2.jpg)\n\n여기서도 기존 Linear Factor Graph와 다른 점은 Factor Node역시 여러 개의 Variable Node와 연결된다는 점이다. 이 부분만 떼어서 자세히 보면 다음과 같다.\n\n![ml-sum-product-bp-3](/images/ml-sum-product-bp-3.jpg)\n\n그렇기에 이전 Variable Node로 부터 오는 Message들과 factor 값을 함께 곱하는 과정이 필요하다. 여기서, Varaible Node에서 factor Node로 오는 Message를 $\\nu$라고 정의한다면, 다음과 같이 표현할 수 있다. 여기서 주의할 점은 Factor Node와 이웃한 Variable Node 중에서 Message를 전달할 Variable Node는 연산에서 제외해야 한다는 점이다.\n\n$$\n\\mu_{u \\rightarrow i}(X_{i}) = \\sum_{X_{\\mathcal{N}_{u}\\backslash\\{i\\}}}f_{u}(X_{\\mathcal{N}})\\prod_{j \\in \\mathcal{N}\\backslash\\{i\\}}{\\nu_{j \\rightarrow u}(x_{j})}\n$$\n\n그리고 마지막으로 $\\nu$를 구하는 과정은 다음과 같다.\n\n![ml-sum-product-bp-4](/images/ml-sum-product-bp-4.jpg)\n\n$$\n\\nu_{j \\rightarrow u}(X_{j}) = \\prod_{v \\in \\mathcal{N}_{j}\\backslash\\{u\\}}\\mu_{v \\rightarrow j}(X_{j})\n$$\n\n따라서, Marginal Probability($P(X_{i})$)를 구하고자 할 때 우리는 Leaf Node에서 부터 시작해서 차례차례 값을 구하면서, $\\mu_{\\mathcal{N}_{i} \\rightarrow i}$를 모두 구할 때까지 연산을 수행해야 한다.\n\n```plaintext\n 🤔 Loopy Sum-Product BP\n\n 우리는 Sum-Product BP를 Tree에서만 쓸 수 있다고 제한하였지만, \n 사실 Cycle이 존재하는 Factor Graph에서도 동일한 BP를 사용할 수 있다.\n 하지만, 이 경우에는 비례 관계를 통해서 나타낼 수 밖에 없기 때문에\n 결과값에 대해서 100% 확신할 수는 없다.\n```\n\n### Max Product Belief Propagation\n\nBelief Propagation은 앞 서 살펴보았던 Marginal Probability를 구할 때에도 사용할 수 있지만, Maximization 문제를 풀 때에도 사용할 수 있다. 마찬가지로 가장 간단한 예시인 Linear Factor Graph를 가정해보자.\n\n![ml-bp-1](/images/ml-bp-1.jpg)\n\n$$\nP(X_{1}, X_{2}, X_{3}, X_{4}, X_{5}) = f_{a}(X_{1}, X_{2})f_{b}(X_{2}, X_{3})f_{c}(X_{3}, X_{4})f_{d}(X_{4}, X_{5})\n$$\n\n이 경우에 $P(X_{1}, X_{2}, X_{3}, X_{4}, X_{5})$를 최대로 만드는 $\\hat{x_{1}}, \\hat{x_{2}}, \\hat{x_{3}}, \\hat{x_{4}}, \\hat{x_{5}}$를 찾아보자.  \n\n$$\n\\begin{align*}\n&\\max_{X_{1}, X_{2}, X_{3}, X_{4}, X_{5}} f_{a}(X_{1}, X_{2})f_{b}(X_{2}, X_{3})f_{c}(X_{3}, X_{4})f_{d}(X_{4}, X_{5})\\\\\n&= \\max_{X_{1}}\\{f_{a}(X_{1}, X_{2})\\max_{X_{2}}\\{\\max_{X_{3}}\\{f_{b}(X_{2}, X_{3}) \\max_{X_{4}}\\{f_{c}(X_{3}, X_{4})\\max_{X_{5}}\\{f_{d}(X_{4}, X_{5})\\}\\}\\}\\}\\}\\\\\n&= \\max_{X_{2}}\\{\\max_{X_{1}}\\{f_{a}(X_{1}, X_{2})\\} \\max_{X_{3}}\\{f_{b}(X_{2}, X_{3}) \\max_{X_{4}}\\{f_{c}(X_{3}, X_{4})\\max_{X_{5}}\\{f_{d}(X_{4}, X_{5})\\}\\}\\}\\}\\\\\n&= \\max_{X_{3}}\\{\\max_{X_{2}}\\{f_{b}(X_{2}, X_{3})\\max_{X_{1}}\\{f_{a}(X_{1}, X_{2})\\}\\} \\max_{X_{4}}\\{f_{c}(X_{3}, X_{4})\\max_{X_{5}}\\{f_{d}(X_{4}, X_{5})\\}\\}\\}\\\\\n&= \\max_{X_{4}}\\{\\max_{X_{3}}\\{f_{c}(X_{3}, X_{4})\\max_{X_{2}}\\{f_{b}(X_{2}, X_{3})\\max_{X_{1}}\\{f_{a}(X_{1}, X_{2})\\}\\}\\}\\max_{X_{5}}\\{f_{d}(X_{4}, X_{5})\\}\\}\\\\\n&= \\max_{X_{5}}\\{f_{d}(X_{4}, X_{5})\\max_{X_{4}}\\{\\max_{X_{3}}\\{f_{c}(X_{3}, X_{4})\\max_{X_{2}}\\{f_{b}(X_{2}, X_{3})\\max_{X_{1}}\\{f_{a}(X_{1}, X_{2})\\}\\}\\}\\}\\}\\\\\n\\end{align*}\n$$\n\nMarginalization과 굉장히 유사하다고 하다. 이를 이전에 사용한 Tree 구조에 반영해도 동일하다.\n\n![ml-sum-product-bp-1](/images/ml-sum-product-bp-1.jpg)\n\n단, 여기서 Message와 최종 결과를 다음과 같이 재정의하면 끝난다.\n\n$$\n\\begin{align*}\n\\max P(X_{1}, X_{2}, \\cdots, X_{N}) &= \\max_{X_{i}}\\{\\prod_{p\\in\\mathcal{N}_{i}}\\mu_{p \\rightarrow i}(X_{i})\\} \\\\\n\\mu_{u \\rightarrow i}(X_{i}) &= \\max_{\\mathcal{N}_{u}\\backslash\\{i\\}}\\{f_{u}(\\mathcal{N}_{i})\\prod_{j \\in \\mathcal{N}_{u}\\backslash\\{i\\}}\\nu_{j \\rightarrow u}(X_{j})\\} \\\\\n\\nu_{j \\rightarrow u}(X_{j}) &= \\prod_{v\\in\\mathcal{N}_{j}\\backslash\\{u\\}}\\mu_{v \\rightarrow j}(X_{j})\n\\end{align*}\n$$\n\n추가적으로 Max Sum Belief Propagation을 소개하겠다. 이는 Maximization 문제를 풀 때, $\\log$를 취한 결과도 동일하다는 점을 활용하여 문제를 푸는 것이다. 따라서, 다음과 같이 식이 조금 변화한다. 이 방식을 쓰면, 너무 작은 probability로 인한 문제를 피할 수 있다.\n\n$$\n\\begin{align*}\n\\max \\red{\\log} P(X_{1}, X_{2}, \\cdots, X_{N}) &= \\max_{X_{i}}\\{\\red{\\sum_{p\\in\\mathcal{N}_{i}}}\\mu_{p \\rightarrow i}(X_{i})\\} \\\\\n\\mu_{u \\rightarrow i}(X_{i}) &= \\max_{\\mathcal{N}_{u}\\backslash\\{i\\}}\\{\\red{\\log} f_{u}(\\mathcal{N}_{i}) + \\red{\\sum_{j \\in \\mathcal{N}_{u}\\backslash\\{i\\}}}\\nu_{j \\rightarrow u}(X_{j})\\} \\\\\n\\nu_{j \\rightarrow u}(X_{j}) &= \\red{\\sum_{v\\in\\mathcal{N}_{j}\\backslash\\{u\\}}}\\mu_{v \\rightarrow j}(X_{j})\n\\end{align*}\n$$\n\n## Construction from Data\n\n앞에서는 Graph를 통해서 연산 과정을 Optimization하는 방법을 알아보았다면, 여기서는 실제 관측 data를 이용해서 어떻게 Graph를 구조화할 수 있는지에 대해서 알아볼 것이다. 이를 수행하기 위해 많은 Algorithm이 존재하지만 가장 기본이 될 수 있는 Algorithm인 **Chow-Liu Algorithm**만 살펴보도록 하겠다.\n\n### Chow-Liu Algorithm\n\n제일 먼저 구해야할 것은 **Joint Probability**이다. 이는 Empirical distribution을 이용하여 구할 수 있다. 아래는 feature가 N개인 총 K개의 data가 있을 때, 다음과 같이 **Joint Probability**를 구한 것이다.\n\n$$\np(X_{1}=x_{1}, X_{2}=x_{2}, \\cdots, X_{N}=x_{n}) = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{1}[x^{(k)}=(x_{1}, x_{2}, \\cdots, x_{n})]\n$$\n\n위와 같이 **Joint Probability**가 주어졌을 때, **Chow-Liu Algorithm**에서는 **Second Order Conditional Probability**(총 두개의 Random Variable로 구성된 Conditional Probability. 즉, Condtion도 하나이고, 확률을 구하고자 하는 변수도 하나이다.)와 **Marginal Probability**로 Graph를 가정하고 **Bayesian Network**를 구성한다. 이 경우에는 형태가 Tree 형태로 만들어지기 때문에 결론적으로 head-to-head 관계가 만들어지지 않는다.(각 각의 node는 하나의 parent만 갖기 때문이다.)\n\n![ml-chow-liu-1](/images/ml-chow-liu-1.jpg)\n\n따라서, 위와 같은 Graph로 추정했다면, 확률은 다음과 같아진다.\n\n$$\np(x_{1}, x_{2}, \\cdots, x_{n}) = p(x_{6}|x_{5})p(x_{5}|x_{2})p(x_{4}|x_{2})p(x_{3}|x_{2})p(x_{2}|x_{1})p(x_{1})\n$$\n\n여기서 이제 우리는 다음과 같은 문제만 풀면 끝이다. Empirical distribution으로 구한 Joint Probability($p$)와 우리가 추정한 Graph에서의 Joint Probability($p_{\\intercal}$)사이의 차이가 최소가 되도록 하면 된다. 이를 위해서 사용하는 것이 **KL Divergence**이다. 따라서, 우리가 구하고 싶은 **Bayesian Network**는 다음과 같이 구할 수 있다.\n\n$$\n\\argmin_{\\intercal\\text{:tree}} KL(p||p_{\\intercal})) = \\argmin_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}p(x^{(k)})\\log \\frac{p(x^{(k)})}{p_{\\intercal}(x^{(k)})}\n$$\n\n그렇다면, 좀 더 면밀하게 $p_{\\intercal}$을 정의해보자.\n\n$$\n\\begin{align*}\np_{\\intercal}(x_{1}, x_{2}, \\cdots, x_{N}) &= \\prod_{i=1}^{N}p(x_{i}|x_{\\text{parent}(i)})\\, (\\because \\text{Bayesian Network Definition})\\\\\n&= p(x_{root})\\prod_{(i,j) \\in E}p(x_{j}|x_{i})\n\\end{align*}\n$$\n\n여기서 V는 node의 집합을 의미하고, E는 edge를 저장하며 각 tuple(i,j)는 (parent, child)를 의미한다. 그리고, Tree에서는 단 하나의 Node만 Root이고 parent가 없기 때문에 해당 Root만 marginal Probability를 가지는 것을 알 수 있다.\n\n$$\n\\begin{align*}\np_{\\intercal}(x_{1}, x_{2}, \\cdots, x_{N}) &= p(x_{root})\\prod_{(i,j) \\in E}p(x_{j}|x_{i})\\\\\n&= p(x_{root})\\prod_{(i,j) \\in E} \\frac{p(x_{j},x_{i})}{p(x_{i})}\\\\\n&= \\red{p(x_{root})}\\prod_{(i,j) \\in E} \\frac{p(x_{j},x_{i})\\red{p(x_{j})}}{p(x_{i})p(x_{j})}\\\\\n&= \\prod_{i\\in V}p(x_{i}) \\prod_{(i,j) \\in E} \\frac{p(x_{j},x_{i})}{p(x_{i})p(x_{j})}\\\\\n\\end{align*}\n$$\n\n마지막이 좀 애매할 수 있는 tree이기 때문에 가능한 것이다. 특정 node로 가는 path는 단 하나이기 때문에 $j$로 끝나는 edge도 하나일 수 밖에 없다. 따라서, $p(x_{root})\\prod_{(i,j) \\in E} p(x_{j}) = \\prod_{i=V}p(x_{i})$일 수 있는 것이다.\n\n이것이 정의되면, 우리는 이제 최적의 Tree인 $\\intercal_{*}$를 찾을 수 있다.\n\n$$\n\\begin{align*}\n\\intercal_{*} &= \\argmin_{\\intercal\\text{:tree}} KL(p||p_{\\intercal})\\\\\n&= \\argmin_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}p(x^{(k)})\\log \\frac{p(x^{(k)})}{p_{\\intercal}(x^{(k)})}\\\\\n&= \\argmin_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}\\cancel{p(x^{(k)})\\log{p(x^{(k)})}} -p(x^{(k)})\\log{p_{\\intercal}(x^{(k)})}\\\\\n&= \\argmax_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}p(x^{(k)})\\log{p_{\\intercal}(x^{(k)})}\\\\\n&= \\argmax_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}p(x^{(k)})\\log({\\prod_{i\\in V}p(x_{i}^{(k)}) \\prod_{(i,j) \\in E} \\frac{p(x_{j}^{(k)},x_{i}^{(k)})}{p(x_{i}^{(k)})p(x_{j}^{(k)})}})\\\\\n&= \\argmax_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}\\sum_{i\\in V}p(x^{(k)})\\log(p(x_{i}^{(k)})) + \\sum_{x^{(k)} \\in \\mathcal{D}}\\sum_{(i,j) \\in E} p(x^{(k)})\\log({\\frac{p(x_{j}^{(k)},x_{i}^{(k)})}{p(x_{i}^{(k)})p(x_{j}^{(k)})}})\\\\\n&= \\argmax_{\\intercal\\text{:tree}} \\sum_{i\\in V}\\sum_{x^{(k)} \\in \\mathcal{D}}p(x^{(k)})\\log(p(x_{i}^{(k)})) + \\sum_{(i,j) \\in E}\\sum_{x^{(k)}_{i}, x^{(k)}_{j}} p(x^{(k)}_{i}, x^{(k)}_{j})\\log({\\frac{p(x_{j}^{(k)},x_{i}^{(k)})}{p(x_{i}^{(k)})p(x_{j}^{(k)})}})\\, (\\because \\text{marginalization})\\\\\n&= \\argmax_{\\intercal\\text{:tree}} \\cancel{\\sum_{i\\in V}-H(X_{i})} + \\sum_{(i,j) \\in E}I(X_{i}, X_{j})\\, (\\because H(X_{i})\\text{는 constant이다.})\\\\\n&= \\argmax_{\\intercal\\text{:tree}}\\sum_{(i,j) \\in E}I(X_{i}, X_{j})\\\\\n\\end{align*}\n$$\n\n마지막 marginalization은 헷갈린다면, 해당 Posting의 Sum-Product BP 부분을 다시 보고오도록 하자.\n\n자, 이제 우리가 얻은 결론은 다음과 같다. 결국, 최적의 Tree는 $I(X_{i}, X_{j})$가 최대가 되는 Tree이다. I(Mutual Information)이 헷갈린다면, [Information Theory](/posts/ml-base-knowledge#Information-Theory) 정리해놓은 Posting을 다시 보고 오자. 결국, $X_{i}, X_{j}$간의 모든 Mutual Information을 구해서 weighted graph를 구축한다음에 Kruskal Algorithm을 통해서 최적 Tree를 찾으면 되는 것이다.\n\n따라서, 과정은 다음과 같다.\n\n1. 가능한 모든 (i,j) 쌍에 대하여 $I(X_{i}, X_{j})$를 구하여, Weighted Graph를 구성한다.\n2. Kruskal Algorithm을 수행한다.\n   1. weight의 내림차순으로 Edge를 정렬한다.\n   2. 하나씩 Edge를 뽑으면서, Cycle이 생기는지 확인하여 생기면 버리고, Cycle이 생기지 않으면 Tree에 추가한다.(Cycle 여부는 동일한 Node가 두 개 다 존재하는지 확인)\n   3. 모든 Node를 뽑았다면 종료하고, 그렇지 않다면 2번을 반복 시행한다.\n\n이렇게 Graph를 만들게 되면, 우리는 Joint Probability를 이전에 배운 Optimization 방법을 통해서 쉽게 구할 수 있다. 그리고, 이를 Model에 직접 적용할 수 있다. 예를 들면, 우리가 Classification을 수행할 때이다.\n\n$$\n\\argmax_{\\mathcal{l} \\in \\{0,1,2,\\cdots, 9\\}} p_{\\mathcal{l}} \\times p(x^{\\text{new}}|\\mathcal{l}^{\\text{new}}=\\mathcal{l}) \\propto \\argmax_{\\mathcal{l} \\in \\{0,1,2,\\cdots, 9\\}} p_{\\mathcal{l}} \\times p_{\\intercal}(x^{\\text{new}})\n$$\n\n위와 같이 추정하여 계산을 획기적으로 줄일 수 있다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Medium[Chullin], Graphical Model이란 무엇인가요?, <https://medium.com/@chullino/graphical-model%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80%EC%9A%94-2d34980e6d1f>\n- Wiki, Markov Random Field, <https://en.wikipedia.org/wiki/Markov_random_field>\n- Adaptive Computation and Machine Learning, Thomas G. Dietterich\n- <https://cedar.buffalo.edu/~srihari/CSE574/Chap8/Ch8-PGM-Inference/Ch8.3.2-FactorGraphs.pdf>\n","slug":"ml-graphical-model","date":"2022-11-14 13:08","title":"[ML] 8. Graphical Model","category":"AI","tags":["ML","GraphicalModel","ConditionalIndependence","MarkovRandomField","BayesianNetwork","FactorGraph","D-Seperation","Factorization","MarkovProperty","MessagePassing","BeliefPropagation","Chow-LiuAlgorithm"],"desc":"Machine Learning은 주어진 data를 가장 잘 설명할 수 있는 pattern(Model)을 찾는 것이 목표라고 하였다. 그렇다면, \"data가 가지는 여러가지 정보(feature)들 중에서 어떤 feature를 중점적으로 보고 이용할 수 있을까?\" 그리고, \"만약 여러 feature들이 서로 연관이 있다면 이를 연산의 최적화를 위해 이용할 수 있지 않을까?\" 라는 접근이 가능하다. 여기서 Graphical Model은 이러한 관계를 시각적으로 표현할 수 있으며, 이를 통해서 연산 최적화에 대한 insight를 얻을 수 있다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n여태까지 ML을 수행할 수 있는 여러 가지 방법론을 살펴보았다. 그렇다면, 어떤 Model을 선택하고, 학습과 추정을 해야할지 결정해야 한다. 따라서, 여기서는 어떤 것이 좋은 Model이고, 각 Model 간에 어떻게 비교를 수행할 것인지 그리고 더 나아가 Model을 혼합하는 방법에 대해서 알아볼 것이다.\n\n## What is Good Model?\n\n우리가 사람 image를 입력받아서 긴 머리를 가진 사람인지 여부를 판단하는 classifier를 만든다고 하자. 이때 어떤 Model이 좋은 Model이 될 수 있을까?\n\n가장 쉽게 생각할 수 있는 Model은 Fully Connected Neural Network(FCNN)를 구성하는 것이다. 이를 위해서 Image의 각 pixel을 일렬로 줄 세워 입력할 수 밖에 없다. 하지만, 이는 pixel들 간의 인접 관계를 사용할 수 없게 한다는 단점 때문에 높은 성능을 내기가 어려웠다. 따라서, 이를 극복하기 위헤서 제시된 방법이 Convolutional Neural Network(CNN)를 사용하는 것이다. 이는 FCNN을 적용하기 이전에 Image에 Filter를 적용하여 특정 구간을 대표하는 값을 뽑아내서 더 효율적인 학습을 하는 것을 목표로 한다.(물론 더 자세히 다루면 Pooling Layer 등 더 자세한 설명이 필요하지만, 여기서는 자세히 다루지 않는다. 해당 글을 참고하도록 하자. [🔗 CNN(Convolutional Neural Networks) 쉽게 이해하기](https://halfundecided.medium.com/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-cnn-convolutional-neural-networks-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-836869f88375))\n\n우리의 뇌에서도 Image를 인식하고 처리하기 위해서, color와 motion 그리고 윤곽 등을 따로 따로 처리한다고 한다. 즉, CNN은 이러한 Domain Knowledge를 활용한 훌륭한 예시 중 하나라고 할 수 있다. 즉, 여기서 말하고자 하는 바는 결국 모든 환경에서 최고의 성능을 보여줄 수 있는 Model은 없다는 것이며, Good Model은 우리가 하고자 하는 일에 따라서 Domain Knowledge를 충실하게 활용하여 최고의 성능을 낼 수 있는 Model이라고 할 수 있다.\n\n```plaintext\n 🤔 Data Augmentation\n \n Domain Knowledge를 활용하여 Model의 성능을 높일 수 있는 방법은 \n 단순히 Model 자체를 바꾸는 것 뿐만 아니라 Domain Knowledge를 바탕으로 \n Data를 추가적으로 더 만들어내는 방법이 있다. 이러한 방법을 \n Data Augmentation이라고 한다.\n\n Image data 같은 경우에는 원본 Image를 약간 회전시키거나 확대하거나 \n Noise를 주는 등의 작업을 하여 전체 데이터의 크기를 늘릴 수 있다.\n\nText 같은 경우에는 동의어를 활용하여 문장 데이터의 크기를 효과적으로\n늘리는 것도 가능하다.\n```\n\n![ml-data-augmentation](/images/ml-data-augmentation.png)\n\n## Comparison between Models\n\n여기서 만약 우리가 얻을 수 있는 Model의 종류가 다양하다면 이들을 어떻게 비교하여 하나의 Model을 선택할 수 있을까? 이 역시 중요한 문제이다.\n\n사실 우리가 학습했던 data를 그대로 평가할 때 사용하는 것은 굉장히 불공평하다고 할 수 있다. 우리가 만들고자 하는 Model은 일반적으로 어느 상황에 두어도 그리고 안본 data일지라도 올바르게 분류하기를 원한다. 즉, 우리의 Model이 **Generalization**을 수행할 수 있기를 바란다.\n\n이러한 Model의 **Generalization** 성능을 측정하기 위해서 자주 사용되는 것이 Dataset을 Train과 Test set으로 나누는 것이다. 하지만, 이것도 부족할 때가 있다. 특정 Model이 특정 Train set에서만 성능이 높을 수도 있기 때문이다. 따라서, 우리는 **Cross Validation**이라는 방식을 도입한다. 이는 우리가 가진 dataset을 골고루 test와 train set으로 활용하는 방법이다. 즉, 여러 번의 training을 수행하며, test를 수행하기를 반복하는 것이다. 그리고, 이를 평균을 내서 전체적인 Model 성능을 평가하는 방법이다.\n\n![ml-k-fold-cross-validation](/images/ml-k-fold-cross-validation.png)\n\n위와 같이 공평하게 k개로 나누는 방식을 k fold cross validation이라고 하며, 해당 예시는 $k=4$인 경우이다. 즉, 위와 같이 Validation을 하기 위해서는 Model의 수가 $N$개라고 할 때, 총 $N \\times k$번의 Training과 Evaluation이 필요하다.\n\n하지만, 여기서 또 간과한 사실은 hyperparameter가 각 model마다 큰 영향을 미친다는 사실이다. 즉, Hyper Parameter를 정하는 과정 역시 필요한데, 이는 각 각의 Model 내부에서 어떤 Hyper Parameter를 사용할지에 대한 합의가 필요한 것이다. 이를 확인하기 위해서 어쩔 수 없이 우리는 Training과 Evaluation을 수행해야 하며, 이를 위한 data를 별도로 분리해야 한다. 따라서, 우리가 가지는 dataset을 다음과 같이 세개로 나누어야 한다는 것이다.\n\n![ml-dataset](/images/ml-dataset.png)\n\n여기서 더 정당하게 하고 싶다면, 아래와 같은 과정을 반복해야 한다.\n\n![ml-nested-cross-validation](/images/ml-nested-cross-validation.png)\n\n하지만, 이는 굉장히 비용이 커질 수 있다. validation set을 고를 때, $k^{\\prime}$개가 필요하다고 한다면, 우리는 $N \\times k^{\\prime} \\times k$번의 Training과 Evaluation이 필요한 것이다. 굉장히 비용이 커지기 때문에 대게 validation set까지 cross validation하는 nested cross validation은 상황에 따라 사용되기도 하고, 사용되지 않기도 한다.\n\n## Combining Simple Models\n\n좋은 Model을 만들 수 있는 방법 중에서 가장 쉽게 생각할 수 있는 것 중에 하나가 여러 개의 Model을 활용하는 방법이다. 쉽게 집단 지성을 활용한다고 볼 수 있다. 이러한 방식을 **Ensemble**(앙상블)이라고 부르고, 이를 활용할 수 있는 방법은 여러 가지가 있다.\n\n1. 서로 다른 여러 개의 Model, 또는 Hyperparameter만을 변경하거나 또는 feature를 다르게 변형하여 Model을 여러 개 생성하고 평균 또는 최댓값을 취하는 방법 (**Voting**)\n2. 여러 개의 Model을 혼합하지만, 각 단계에 따라서 Model을 선택하는 방법 (**Stacking**)\n3. dataset을 여러 번 sampling하여 각 각의 Model을 만들고, 각 Model의 결과를 평균 또는 최댓값을 취하는 방법 (**Bagging**, **Pasting**)\n4. 이전과는 달리 앞 서 진행한 Model의 결과를 반영하여 다음 Model에 적용하기를 반복하며, 여러 Model을 제작하고 취합하는 방법 (**Boosting**)\n\n크게는 이렇게 3가지로 나눌 수 있다. 여기서 각각을 자세히 다루지는 않고, **Boosting** 방식 중에서도 많이 사용되는 방법 중에 하나인 **AdaBoost**에 대해서 좀 더 자세히 다뤄보도록 하겠다.\n\n### AdaBoost\n\nAdaptive Boosting의 약자인 AdaBoost는 이름에서 볼 수 있듯이 반복적인 작업을 통해서 최종 Model의 성능을 높이는 것을 목표로 한다. 우선 Boosting 방법 자체가 동시에 Model을 학습시키는 것이 아니고, 순차적으로 학습시키면서 성능을 높이는 방법이다. 그렇다면, 우리가 이전 Model들의 학습 과정에서 다음 Model에게 넘겨줄 수 있는 특별한 정보는 무엇일까? 이는 바로 자신들이 잘못 분류한 데이터에 대한 정보이다. 자신들이 잘못 분류한 data들에게 더 높은 가중치를 부여하도록 하여 다음 Model에서는 이를 중심적으로 분류할 수 있도록 하는 방식으로 최종 Model의 성능을 높여보자는 것이 Idea이다.\n\n그렇다면, 이것이 어떻게 가능할까? 매우 간단한 이진 분류기를 기반으로 이를 설명하도록 하겠다. 우리가 만약 특정 임계값($\\theta_{t}$)보다 작으면 -1, 그렇지 않으면 1이라고 분류하는 아주 간단한 분류기(weak classifier, decision stump)를 가지고 있다고 하자.\n\n$$\nf_{t}(x) = \\begin{cases} -1 & \\text{if } x < \\theta_{t} \\\\ 1 & \\text{otherwise} \\end{cases}\n$$\n\n이제 우리는 이 간단한 분류기 T개를 합쳐서 복잡한 분류 문제를 해결할 분류기를 제작할 것이다. 이 때, 각 분류기는 다음과 같은 가중치($\\alpha_{t}$)를 가지게 된다.\n\n$$\n\\begin{align*}\n\\text{output} = \\text{sign}(F_{T}(x)) \\\\\nF_{T}(x) = \\sum_{t=1}^{T} \\alpha_{t} f_{t}(x)\n\\end{align*}\n$$\n\n그렇다면, 우리는 위 식에서 어떻게 하면, 현명하게 $\\theta_{t}, \\alpha_{t}$를 결정할 수 있을까? 이에 대한 해답으로 **AdaBoost**는 이전 $F_{t-1}$에 의해 발생한 **error**에 집중한다.\n\n우선 $F_{t}$의 Error($E(F_{t})$)를 아래와 같다고 하자.\n\n$$\nE(F_{t}) = \\sum_{i=1}^{N} \\exp(-y^{(i)}F_{t}(x^{(i)}))\n$$\n\n즉, 예측이 맞다면 error는 $1 \\over e$, 틀리다면 $e$만큼 error가 증가한다.  \n여기서 우리는 현재 학습할 Model 이전까지의 Model의 하나의 데이터에 대한 Error를 $\\gamma_{t}^{(i)}$라고 정의해보자.\n\n$$\n\\gamma_{t}^{(i)} = \\exp(-y^{(i)}F_{t-1}(x^{(i)})),\\quad \\gamma_{1}^{(i)} = 1\n$$\n\n다시 한 번 $\\gamma_{t}^{(i)}$의 의미를 정의하면, 간단하게 이전까지의 Model의 합으로 만든 Model이 잘 분류했다면, $e$ 그렇지 않다면, $1 \\over e$가 된다.\n\n그렇다면, 계속해서 Error 식을 정리해보자.\n\n$$\n\\begin{align*}\nE(F_{t}) &= \\sum_{i=1}^{N}\\{\\exp(-y^{(i)}F_{t-1}(x^{(i)})) \\times \\exp(-y^{(i)}\\alpha_{t}f_{t}(x^{(i)}))\\} \\\\\n&= \\sum_{i=1}^{N} \\gamma_{t}^{(i)} \\exp(-y^{(i)}\\alpha_{t}f_{t}(x^{(i)})) \\\\\n&= \\sum_{i:y^{(i)}=f_{t}(x^{(i)})}\\gamma_{t}^{(i)}\\exp(-\\alpha_{t}) + \\sum_{i:y^{(i)}\\neq f_{t}(x^{(i)})}\\gamma_{t}^{(i)}\\exp(\\alpha_{t}) \\\\\n&= \\sum_{i=1}^{N}\\gamma_{t}^{(i)}\\exp(-\\alpha_{t}) + \\sum_{i:y^{(i)}\\neq f_{t}(x^{(i)})}\\gamma_{t}^{(i)}(\\exp(\\alpha_{t})-\\exp(-\\alpha_{t})) \\\\\n&= \\exp(-\\alpha_{t})\\sum_{i=1}^{N}\\gamma_{t}^{(i)} + (\\exp(\\alpha_{t})-\\exp(-\\alpha_{t}))\\sum_{i:y^{(i)}\\neq f_{t}(x^{(i)})}\\gamma_{t}^{(i)}\n\\end{align*}\n$$\n\n여기서 Error를 가장 작게 할 수 있는 $\\theta_{t}, \\alpha_{t}$를 찾기 위한 방법은 각 각 다음과 같다.\n\n1. 식에서 $\\theta_{t}$가 바꿀 수 있는 것은 $f_{t}$밖에 없다. 즉 $\\sum_{i:y^{(i)}\\neq f_{t}(x^{(i)})}\\gamma_{t}^{(i)}$를 조정하는 것이다.  \n   즉, $\\gamma_{t}^{(i)}$는 이전 분류기($F_{t-1}$)가 잘 분류했다면 $e$, 그렇지 않다면 $1 \\over e$가 되는데, 이들의 합이 최소가 되도록 하는 임계값 $\\theta_{t}$를 찾는 것이다.  \n   즉, 기존 분류기가 잘못 분류한 data에 대해서 더 중점적으로 분류할 수 있도록 가중치를 부여하여 다시 분류한다는 것이다.\n2. Error를 $\\alpha_{t}$에 대한 미분을 하여, 0이 되도록 하는 $\\alpha_{t}$를 찾으면 된다. 이 과정은 다음과 같다.\n\n$$\n\\alpha_{t} = \\frac{1}{2}\\ln\\frac{1-\\varepsilon_{t}}{\\varepsilon_{t}},\\quad \\varepsilon_{t} = \\frac{\\sum_{i:y^{(i)}\\neq f_{t}(x^{(i)})}\\gamma_{t}^{(i)}}{\\sum_{i}^{N}\\gamma_{t}^{(i)}}\n$$\n\n여기서 $\\varepsilon_{t}$를 자세히 보면, 분모는 decision stump의 최대 Error이고 분자는 현재 decision stump의 Error를 의미한다. 이것이 직접적으로 $\\alpha_{t}$에 영향을 미치는 것이다.\n\n따라서 이르 조금 더 정리하자면 다음과 같다.\n\n1. $\\varepsilon_{t} \\gt \\frac{1}{2} \\rArr \\alpha_{t} \\lt 0$  \n   $\\varepsilon_{t} \\gt \\frac{1}{2}$라는 것은 사실 $f_{t}$의 성능이 선택지 두 개지 하나를 Random하게 고르는 경우의 확률 $\\frac{1}{2}$보다 못하다는 것이다. 이 경우에 $\\alpha_{t}$를 음수로 설정하여 적용하는 것이 반대로 확률을 적용하는 것이고, 이것이 전체 성능을 높일 수 있기에 타당하다.\n2. $\\varepsilon_{t} = \\frac{1}{2} \\rArr \\alpha_{t} = 0$  \n   만약, 성능이 딱 $\\frac{1}{2}$라면, 더 이상 개선의 여지가 없어진다. 즉, $\\alpha_{t}$를 0으로 설정하여 적용하게 되면, $F_{t}=F_{t-1}$이 된다. 즉, 더 이상의 Model 중첩은 무의미하다는 것을 의미하므로 해당 단계에 도달하면 학습을 중단한다.\n3. $0 \\lt \\varepsilon_{t} \\lt \\frac{1}{2} \\rArr \\alpha_{t} \\gt 0$  \n   일반적인 경우로, 새롭게 만든 분류기가 기존 분류기($F_{t-1}$)를 보완할 만큼 잘 예측을 하고 있기에 $\\alpha_{t}$를 양수로 설정하여 적용한다.\n4. $\\varepsilon_{t} \\rarr 0 \\rArr \\alpha \\rarr \\infin$  \n   $\\varepsilon_{t}$가 0에 가까워지면, 즉, $f_{t}$가 모든 data를 정확하게 분류한다면, 사실상 기존 분류기들은 더 이상 의미가 없다. 하나의 $decision stump$로 완벽하게 분류되는 문제였기 때문이다. 즉, $F_{t} = f_{t}$가 된다.\n\n### Decision Tree\n\n앞 선 **AdaBoost**에서는 Decision Stump를 다루었지만, 더 다양한 분류기를 이용해서 Decision Tree를 구성하는 것도 가능하다. 실제 Stacking 또는 Bagging 등의 작업을 할 때에는 단순한 Decision Stump의 합 같은 형태가 아니라 Tree형태로 구성되는 경우가 많다(Decision을 할 때마다 가지치기를 하며 나뉘는 형태). 그리고 실제로도 이 형태가 인간의 사고 과정도 매우 유사하다. 따라서, 대게의 경우 성능도 좋은 뿐만 아니라 직관적이기 때문에 이러한 방식을 사용해서 여러 Model을 혼합하는 경우도 있다. 이 안에서 Decision을 수행할 때 복잡한 Deep Learning을 수행할 수도 있고, 단순하게 Decision Stump를 사용할 수도 있는 것이다.\n\n![ml-decision-tree](/images/ml-decision-tree.png)\n\n그렇다면, 이러한 Decision Tree를 어떻게 학습하는 게 좋을지를 조금만 살펴보도록 하겠다. 가정을 하나 해보자. 우리가 분류하고자 하는 Category가 10개이고, feature가 100개이다. 이때, 어떤 Feature를 이용한 어떤 Model을 사용한 것을 우선으로 적용해야할까? 이것이 사실 가장 중요한 문제이다. 이를 해결하기 위해서 여러 알고리즘(ID3, CART, 등)이 제시되었다. 하지만, 결국 핵심은 각 각의 단계에서 데이터를 가장 적절하게 나누는 것이 중요한 것이다. 따라서, Model(f)에 대해서 <mark>**얻을 수 있는 정보의 양**(**IG**, Information Gain)</mark>이 많을 수록 좋은 Model이라고 칭하는 것이다. 이를 식으로 표현하면 다음과 같다.\n\n$$\nIG(\\mathcal{D}, f) = I(\\mathcal{D}) - \\sum_{j=1}^{J} \\frac{D_{j}}{D}I(\\mathcal{D}_{j})\n$$\n\n여기서, 또 그렇다면, I는 무엇인지 궁금할 수 있다. 이는 Impurity(정보의 혼탁도)를 의미하며, 이를 표현하는 지표는 아래와 같은 것들이 있다.\n\n1. Gini Impurity\n2. Entropy\n3. Classification Error\n\n위 중에서 우리가 [🔗 ML Base Knowledge(Information Theory)](/posts/ml-base-knowledge#Information-Theory)에서 다루었던 **Entropy**에 기반한 방법이 가장 즐겨서 사용되어진다.\n\n즉, Entropy에 기반한 설명을 하자면, 우리는 IG(정보 획득량)를 최대화하기 위한 선택을 하게 되면, 해당 결정의 Child들은 적은 Entropy를 가지게 되고 이 과정을 반복해 나가면서 최적화를 수행하는 것이다.\n\n즉, Decision Tree를 생성할 때에는 여러 가지 feature와 Model을 적용하며 각 Model이 가지는 IG를 기반으로 하여 Tree의 Root에서부터 Model을 선택하며 내려오는 것이다.\n\n## Cutting down a Compex Model\n\n또한, 좋은 Model을 만들기 위해서 아이러니하게도 일부 정보를 삭제하는 것이 도움이 될 때가 있다. 대게 Deep Learning 환경에서 많이 발생하는 경우인데, **over fitting**으로 인한 문제를 해결하기 위해서 일부 edge를 제거하는 **dropout**을 수행한다. 이러한 방법은 **over fitting**을 방지할 뿐만 아니라 학습의 속도 역시 개선할 수 있기 때문에 자주 사용되어진다. 실제로 model의 성능이 증가할 수 있는지에 대해 다룬 논문이 별도로 있으니 참고할 수 있다면 해보도록 하자. 만약 시간이 된다면 이에 대해서도 다룰 수 있도록 하겠다.\n\n- Frankle, Jonathan, and Michael Carbin. \"The lottery ticket hypothesis: Finding sparse, trainable neural networks.\" ICRL 2019\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- [🔗 CNN(Convolutional Neural Networks) 쉽게 이해하기](https://halfundecided.medium.com/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-cnn-convolutional-neural-networks-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-836869f88375)\n","slug":"ml-model-selection","date":"2022-11-08 16:07","title":"[ML] 7. Model Selection","category":"AI","tags":["ML","ModelSelection","CrossValidation","Boosting","AdaBoost","DecisionTree","NetworkPruning"],"desc":"여태까지 ML을 수행할 수 있는 여러 가지 방법론을 살펴보았다. 그렇다면, 어떤 Model을 선택하고, 학습과 추정을 해야할지 결정해야 한다. 따라서, 여기서는 어떤 것이 좋은 Model이고, 각 Model 간에 어떻게 비교를 수행할 것인지 그리고 더 나아가 Model을 혼합하는 방법에 대해서 알아볼 것이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n우리가 NL을 제대로 분석하기 위해서 각 단어가 가진 의미를 알아야하며, 이를 넘어서 문장이 가지는 의미를 파악해야 한다. 결론적으로 이 과정이 고도화된 NLP를 위한 핵심 단계이다. 이를 위해서는 Raw한 형태로 주어진 text를 처리해서 더 나은 형태의 구조를 만들 필요가 있다. 따지고 보면 하나의 전처리 과정이라고 볼 수 있다. 그치만 이전 text processing chapter과 다른 점은 문장 구분과 같은 간단한 과정이 아닌 Linguistic 단계에 따른 처리 과정을 수행한다고 볼 수 있다. 또한, 각 단계 역시 NLP 중에 하나라고 할 수 있으므로 이 또한 ML과 DL을 통해서 고도화하는 것도 가능하다. Morphology 단계부터 시작하여 Syntax, Semantic까지 어떻게 다루게 되는지를 살펴보도록 하겠다.\n\n## POS tagging\n\nMorphology 단계에서 가장 기본이되는 요소이기 때문에 이를 먼저 살펴보도록 하겠다. Part of Speech라는 단어의 뜻 자체가 \"품사\"이다. 이는 단어의 문법적인 기능이나 형태 등을 표현하기 위해서 제시되었다. 이를 구분하려는 시도는 디오니소스 이전부터 있었지만 근본적인 형태를 제시한 것은 디오니소스가 첫 번째이다. 그는 기원전 100년에 지금과 굉장히 유사한 형태의 8개의 품사를 제시하였다. 지금도 8개지만, 감탄사와 형용사 등이 추가되고 몇몇 요소가 빠졌다. 이를 NLP 과정에서 input으로 활용하게 되면 언어의 모호성을 해결하는데 도움을 줄 수 있다. 품사를 통해서 단어가 가지는 뜻의 범위가 더 줄어들 수 있기 때문이다. 따라서, 이를 각 단어마다 표시하는 절차를 preprocessing으로 진행하는 경우도 많다.\n\n우선 POS의 일반적인 종류는 다음과 같다.\n\n- Noun(명사)\n- Verb(동사)\n- Adjective(형용사)\n- Adverb(부사)\n- Preposition(전치사)\n- Conjunction(접속사)\n- Pronoun(대명사)\n- Interjection(감탄사)\n\n하지만, Computer Science에서는 이를 좀 더 명확하게 표현하기 위해서 더 많은 분류(tag)를 사용하는 것이 일반적이다. 대표적인 예시가 [🔗 Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)이다. 여기서는 36개의 종류를 활용하여 표기한다. 이외에도 Brown Corpus 등 다양한 tagging 방법이 있다. 또한, 언어에 따라서는 별도의 품사를 정의하는 경우도 많기 때문에 언어마다 적절한 방식을 사용해주는 것이 좋다.\n\ntag를 정할 때 일반적인 규칙은 우리가 중/고등학교 시간에 배웠을 문법 요소를 적용한 것이 많다는 점을 기억하면 된다. NNS 같은 경우는 복수명사 뒤에 붙은 s를 포함하는 tag를 의미하고, VBD는 동사 과거형을 의미한다. 이와 같은 형태로 품사를 좀 더 세분화한 것 외에는 차이가 없다.\n\n### How can I get?\n\n그렇다면, 어떻게 하면 POS tagging된 데이터를 얻을 수 있을지가 궁금할 것이다. 신기하게도 가장 쉬운 추론을 하더라도 90%의 정확도를 가질 수 있다. 다음과 같은 방법이다.\n\n1. 단어가 가지는 품사 중 가장 빈도가 높은 것을 표기한다.\n2. 못 본 단어인 경우 Noun(명사)로 표기한다.\n\n이것이 가능한 이유는 사실상 대부분의 word는 모호하지 않다는 점이다. 대부분의 word는 품사 앞에서는 그렇게 변화무쌍하지 않다. **하지만,** 특정 word는 사람 조차도 헷갈리는 경우가 있다. 대게 통계적으로 11%정도는 사람 조차도 헷갈릴 수 있는 형태의 품사가 주어진다고 한다. 그래서, 이를 해결하기 위해서 Statistic Inference를 활용하는 경우가 있고, 우리가 앞 서 배웠던 HMM을 활용하면 97%, MaxEnt를 활용하면 99% 정확도를 가지는 tagger를 만들 수 있다. 물론 더 복잡한 Deep Learning을 활용한다면 더 높은 성능도 가능은 할 것이다.\n\n## Morphology\n\nMorphology 단계에서 POS tagging이 중요하긴 하지만 더 나아갈 필요가 있다. 결국 우리가 원하는 것은 단어의 의미를 더 완벽하게 찾는 것이다. 따라서, 대게의 경우 POS tagging을 포함하는 Morphology tagging을 수행한다. 특정 단어를 사전형 기본형(lemma) 또는 더 나아가 가장 뿌리가 되는 요소 root와 stem으로 나누고 여기에 품사를 덧붙이는 형태이다. 우리가 얻은 품사(tag)와 lemma만 갖고도 우리는 원래 단어를 만드는 것이 가능하고, 뜻의 범위를 더 한정할 수 있다. 더 나아가 root와 stem으로 나누게 되면 보지 못한 데이터에 대해서도 더 면밀한 의미 파악이 가능해진다. 이를 구현할 때에는 대게 4가지 방법 중에 하나를 수행하는 것이 일반적이다.\n\n1. Word form list  \n   간단하게 생각하면, word list에서 단어를 조회하는 방식이다. 대게 key, value보다는 Trie 형태로 담는 것을 선호한다. Trie는 각 node가 sequence 데이터의 요소 하나하나가 되는 tree를 의미하며, sequence 데이터의 조회를 위해 사용된다.\n2. Direct coding  \n   root와 stem을 찾는 과정은 사실 영어에서는 간단하다. 앞 뒤에서 부터 진행하면서 대표적인 stem을 제거해 나가면, root만 남기 때문이다. 하지만, 일부 일본어와 같은 경우에는 이것이 불가능한 경우도 있다. 이 경우에는 다른 방식을 적용해야 한다.\n3. Finite state machinery  \n   각 단어의 형태를 FSM으로 정의하여 변할 수 있는 형태와 이에 따른 품사 등을 미리 표현하여 정의하는 방법이다.\n4. CFG, DATR, Unification  \n   언어학에 기반한 분석법이다.\n\n사실 이러한 방법을 직접 구현하는 것은 한계가 있을 수 있다. 따라서, 이미 구현되어 있는 POS tagger를 사용하는 것이 현명할 수 있다. 일반적으로 가장 많이 사용되는 POS tagger는 다음과 같은 것들이 있다.\n\n| Library | Language | ProgrammingLanguage |\n| :------ | :------- | :------------------ |\n| NLTK    | English  | Python              |\n| spaCy   | English  | Python              |\n| KoNLPy  | 한글     | Python              |\n\n## Syntactic Analysis\n\nMorphology 단계에서는 각 word의 뜻을 다루었다면, 이 단계는 word의 결합으로 이루어지는 문장 구조를 분석하는 단계이다. 문장 구조를 분석(구문 분석)하는 방법은 크게 두 가지로 나뉘어진다.\n\n1. <mark>**Phrase Structure**</mark>  \n   문장을 Phrase(구) 단위로 나누어 구조화 시키는 방법이다. 단어 각 각의 품사에서 부터 시작하여 이들을 묶어서 하나의 문장 요소(대게 phrase)를 만들어 하나의 문장을 만드는 구조를 가진다.\n2. <mark>**Dependency Structure**</mark>  \n   문장에서 각 단어가 가지는 의존 관계를 나타낸 구조이다.\n\n각 구조는 둘다 Tree 형태로 이루어지며, 분석하는 방법도 서로 매우 다르다. 각 방법은 밑에서부터 자세히 다루도록 하겠다.\n\n### Phrase Structure\n\n문장을 이루는 요소들과 요소들의 구조화 규칙을 정의해야 우리는 이를 분석할 수 있을 것이다. 따라서, 이를 정의한 것을 Grammar라고 한다. 그리고 이를 위해서 대표적으로 사용되는 것이 **CFG**이다. **CFG**는 Context Free Grammar의 약자로, 모든 잘 구조화된 문장들을 정의할 수 있는 규칙들을 의미한다. 각 각의 Rule은 왼쪽에는 문법적 type이 주어지고, 오른쪽에는 이를 이루는 요소들이 정의되어진다. 각 요소는 하위 문법적 type 또는 이전에 제시한 POS가 될 수 있다.\n\n가장 기본적으로 사용되어지는 문법적 type들은 다음과 같다. 이외에도 기술하지 않은 POS도 사용이 가능하다.\n\n| Symbol  | Mean               | Korean   |\n| :------ | :----------------- | :------- |\n| NP      | Noun Phrase        | 명사 구  |\n| VP      | Verb Phrase        | 동사 구  |\n| S       | Sentence           | 문장     |\n| DET(DT) | Determiner         | 관사     |\n| N       | NOUN               | 명사     |\n| V       | Verb               | 동사     |\n| PREP    | Preposition        | 전치사   |\n| PP      | Preposition Phrase | 전치사구 |\n\n이에 따라 대표적인 Rule은 다음과 같다.\n\n- S -> NP VP\n- NP -> (DT) N\n- NP -> N\n- VP -> V (NP)\n\n위에 제시된 Rule은 가장 기본적인 규칙으로 여기서 더 확장된 규칙을 만들어서 Parsing을 수행할 수 있다. 하지만, 이렇게 규칙을 만들어서 수행을 하게 되면 문제가 발생할 수 있다. 바로 여러 개의 Parsing Result가 만들어졌을 때 이 중에서 어떤 것이 가장 적절한지를 알 수 없다는 것이다. 즉, 너무 구체적인 Rule을 만들기에는 Parsing이 하나도 되지 않는 문장이 만들어질 가능성이 높고, 그렇다고 너무 적은 Rule을 적용하게 되면 Parsing이 너무 많이 만들어지게 된다.\n\n따라서, 결론적으로 말하자면 위와 같은 형태의 CFG로는 phrase structure를 구조화하는데 한계가 있다는 결론을 내리게 된다. 결국 아래와 같은 두 개의 문제점에 직면하게 되고 이를 해결하기 위한 방법이 각 각 제시된다.\n\n1. Repeated work  \n   문장 구조가 동일한 경우 결국 동일한 작업을 반복하게 된다. 이를 해결하기 위해서 Treebank라는 구조를 도입하고, 이것의 일부를 Dynamic Programming의 Memoization처럼 저장해두었다가 쓰는 방식을 적용한다. 즉, 기존에는 Rule만을 저장하고, 때에 따라 이를 적용하였다면, 이제는 모든 단어의 품사와 구조를 기록해두는 것이다. 이를 통해서 이미 나왔던 작업의 경우 빠른 처리가 가능해진다.\n2. Choosing the correct parse  \n   위에서 말했던 것처럼 우리는 결국 <mark>가장 적절할 거 같은 parsing result를 선택해야 한다.</mark> Rule에 기반한 방식으로는 한계가 있지만 우리가 Statistic한 방식을 활용한다면 이를 극복할 수 있다. 따라서, 우리는 CFG에서 나아가 PCFG(Probabilistic CFG)를 적용하여 이를 처리할 수 있다. 이러한 Statistical한 결과를 얻기 위해서도 Treebank 구조가 필요하다.\n\n![nlp-cfg-treebank](/images/nlp-cfg-treebank.jpg)\n\n이제부터는 실제로 PCFG를 어떻게 수행할 수 있는지를 자세히 다뤄보도록 하겠다.\n\n#### PCFG\n\n앞 서 얘기한 것처럼 Probabilistic CFG로, 각 Rule마다 Probability를 적용하는 것이다. 여기서 유의할 것은 다음 내용이다.\n\n1. 기존 정의한 문법적 type을 만드는 Rule에 각 각의 확률을 정의한다.\n2. 이때 각 문법적 type을 만들 수 있는 Rule의 확률의 합은 반드시 1이다.\n3. 또한, 각 단어가 특정 POS일 확률도 같이 구해야 한다.  \n   ex. N -> fish (0.5), V -> fish (0.1)  \n   (실제로 이렇게 크게 나오지 않는다. N 또는 V 일 때, Fish일 확률이므로 굉장히 작은 값이 나오는 것이 일반적이다.)\n\n따라서, 어떤 treebank가 더 적절한 지는 각 각의 treebank의 모든 Rule의 확률의 곱을 구해서 비교하면 된다. 굉장히 쉽게 이 과정이 가능한 것이다. 아래는 간단한 예시이다.\n\n![nlp-pcfg](/images/nlp-pcfg.jpg)\n\n이렇게 주어졌을 때, $p(t_1)$과 $p(t_2)$는 아래와 같이 구할 수 있다.\n\n$$\n\\begin{align*}\np(t_{1}) &= 1.0 \\\\\n&\\times 0.3 \\times 0.6 \\\\\n&\\times 0.4 \\\\\n&\\times 0.5 \\times 0.1 \\times 0.4 \\times 1.0 \\times 0.4  \\\\\n&= 0.000576 \\\\\n\\\\\np(t_{2}) &= 1.0 \\\\\n&\\times 0.3 \\times 0.4 \\\\\n&\\times 1.0 \\\\\n&\\times 0.4 \\\\\n&\\times 0.5 \\times 0.6 \\times 1.0 \\times 1.0 \\times 0.4 \\\\\n&= 0.00576 \\\\\n\\\\\n\\therefore p(t_{1}) &\\lt p(t_{2})\n\\end{align*}\n$$\n\n따라서, $t_{2}$ 형태가 더 적절하다고 판별할 수 있는 것이다.\n\n```plaintext\n 🤔 Chomsky Normal Form\n\n 기존 CFG의 형태의 모호함을 제거하고, 좀 더 명확한 형태로 정의하는 것을 의미한다.\n 대표적으로 모호한 내용이 Sentence안에 Sentence를 포함하는 경우(n-ary)라든지,\n 명령문과 같은 문장을 위한 주어 삭제(unary/empty) 등이 존재한다.\n 이를 해결하기 위한 recursive 형태나 empty 형태 등을 제거하는 것을 의미한다.\n\n 결론상 PCFG에서는 확률 표기시에 모호한 표기를 제거할 수 있다는 장점이 있다.\n```\n\n#### CKY Parsing\n\n앞 서 우리가 treebank 중에서 더 큰 확률곱 값을 가지는 것이 최적값이라는 것을 알 수 있었다. 하지만, 사실 이 과정이 그렇게 쉽지는 않다. 왜냐하면, 우리가 가지는 Parsing Result는 굉장히 많을 수도 있기 때문이다. 그렇다면, 이를 연산하는 비용이 굉장히 비싸진다. 이를 효과적으로 연산하기 위한 알고리즘으로 제시된 것이 CKY Parsing이다.\n\nPseudo code는 다음과 같다.\n\n```javascript\nfunction CKY(words, grammar) returns [scores, backpointers]\n  // score[i][j] = \n  // 모든 Symbol(문법적 type, ex. S, NP, VP)에 대하여 \n  // i부터 j까지 word를 사용했을 때의 최댓값을 저장\n  score = new double[#(words) + 1][#(words)+1][#(Symbol)]\n  // back[i][j] = \n  // 모든 Symbol(문법적 type, ex. S, NP, VP)에 대하여 \n  // i부터 j까지 word를 사용했을 때의 최댓값을 만드는 요소의 위치를 저장\n  // (=back pointer)\n  back = new Pair[#(words)+1][#(words)+1][#(Symbol)]\n  for (i=0; i < #(words); i++)\n    // 초기화 단계로 각 단어가 Symbol일 확률을 입력\n    for (A in Symbol)\n      if A -> words[i] in grammar\n        score[i][i+1][A] = P(A -> words[i])\n    // unary 즉 생략되어서 표현되는 경우를 위해서 확률 재계산\n    // ex. Stop!! (S->VP,VP->V)\n    boolean added = true\n    while (added)\n      added = false\n      for A, B in Symbol\n        if score[i][i+1][B] > 0 && A -> B in grammar\n          prob = p(A -> B) * score[i][i+1][B]\n          if prob > score[i][i+1][A]\n            score[i][i+1][A] = prob\n            back[i][i+1][A] = B\n            added = true\n  for (span = 2 to #(words))\n    for (begin = 0 to #(words) - span)\n      // 일반적인 두 항의 합으로 이루어지는 경우를 계산\n      end = begin + span\n      for (split = begin + 1 to end-1)\n        for (A, B, C in Symbol)\n          prob = score[begin][split][B] * score[split][end][C]*P(A -> B C)\n          if prob > score[begin][end][A]\n            score[begin][end][A] = prob\n            back[begin][end][A] = new Triple(split, B, C)\n      // unary인 경우를 고려해서 재계산\n      boolean added = true\n      while (added)\n        added = false\n        for (A, B in Symbol)\n          prob = P(A -> B) * score[begin][end][B]\n          if prob > score[begin][end][A]\n            score[begin][end][A] = prob\n            back[begin][end][A] = B\n            added = true\n  return score, back\n```\n\n전체적인 동작과정은 그림을 통해서 이해할 수 있다. 먼저초기 score 할당부터 첫 단계에 데이터를 저장하기까지는 아래 그림으로 이해할 수 있다.\n각 그림을 다음을 의미한다.\n\n1. score를 위한 공간 할당\n2. score에 가장 기본이 되는 Symbol -> word 확률 입력\n3. unari case를 확인해서 확률 입력\n\n![nlp-cky-1](/images/nlp-cky-1.png)\n\n그 다음 단계로는 단계적으로 관계를 적립한다.\n\n1. 같은 형광펜으로 칠해진 데이터간 관계가 최댓값을 가진다.\n2. unari case도 확인한 결과 S->VP가 초기화 된다.\n\n![nlp-cky-2](/images/nlp-cky-2.png)\n\n마지막에서 다시 관계를 정리할 때, 유의할 점이 있다. 바로 score\\[0\\]\\[3\\]와 score\\[1\\]\\[4\\]도 중요하지만 score\\[0\\]\\[2\\]와 score\\[2\\]\\[4\\]에 의한 관계도 반드시 유의해서 보아야 한다.\n\n![nlp-cky-3](/images/nlp-cky-3.png)\n\n#### modeling\n\n원래라면, modeling 단계도 다루어야하지만, 해당 단계에서는 넘어가도록 한다. 이를 수행하기 위해서는 간단하게는 단순히 빈도를 확인하는 것부터 EM algorithm을 활용하여 업데이트 하는 방식이 있다. 하지만 여기서는 자세히 다루지 않겠다.\n\n### Dependency Structure\n\n문장에서 각 단어의 의존 관계를 나타내는 Dependency Structure는 중심 의미를 가지는 word로 부터 이에 의존하는 word들의 관계로 확장되며 표기된다. 따라서, 문장에서는 대게 동사가 중심이 되고, 그리고 그 다음으로는 전치사, 명사 등이 뒤를 잇게 된다. 이를 파악하게 되면, 단어가 연관성과 전체적인 구조의 안정성 등을 파악하는데 도움을 줄 수 있다.\n\n이 형태를 얻기 위해서 할 수 있는 대표적인 방법은 다음과 같은 방법이 있다.\n\n1. Dynamic Programming  \n   아주 쉽게 생각할 수 있는 방법으로 **PCFG를 활용**하는 것이다. 일반적으로 PCFG를 활용하여 tree 형태를 구축하면 이를 이용하여 Dependency Structure를 쉽게 구할 수 있다. 단순히 tree의 아래서 부터 의존 관계를 가진 단어를 고르면서 root까지 올라오면 이것으로 충분하다. 하지만, 이 과정은 시간적 비용이 많이 든다는 단점이 있다.\n2. Graph Algorithm  \n   **가장 정확도가 높은 방식**으로 Sentence에 대한 Minimum Spanning Tree를 구성하고, 이를 활용하여 ML classifier를 제작하여 구현할 수 있다. 가장 높은 정확도를 원한다면 해당 방식을 활용하는 경우가 많다.\n3. Constraint Satisfaction  \n   모든 경우의 수를 만들고 거기서 제한사항을 만족하지 않는 구조를 제거하는 방식이다. 이 또한 많이 사용되지는 않는다.\n4. Deterministic Parsing  \n   Greedy algorithm에 기반하여 구현된 방식으로 매우 높지는 않지만 적절한 정확도에 **빠른 속도**를 가지기 때문에 많이 사용되어진다.\n\n#### Malt Parser\n\n여기서는 Deterministic Parsing 중에서 가장 쉬운 방법 중에 하나인 Malt Parser를 좀 더 다뤄보도록 하겠다.\n\n이는 3개의 자료 구조와 4개의 action을 통해서 정의되는 알고리즘이다.  \n먼저 자료구조는 다음과 같다.\n\n1. stack($\\sigma$)  \n   dependency tree의 상위 요소를 저장해두는 공간으로, 처음에는 ROOT라는 요소를 갖고 시작한다.\n2. buffer($\\beta$)  \n   input sequence를 저장하는 공간으로, 처음에는 input sequence를 전체를 저장하고 있다.\n3. arcs($A$)  \n   최종으로 만들고자 하는 dependency tree를 의미한다. 처음에는 비어 있는 상태로 시작한다.\n\naction은 다음과 같다.\n\n1. Reduce  \n   stack($\\sigma$)에서 word를 pop한다.\n2. Shift  \n   buffer($\\beta$)에서 stack($\\sigma$)으로 word를 push한다. 이때 문장의 앞의 단어부터 차례대로 전달한다.\n3. Left-Arc  \n   stack($\\sigma$)의 현재 word가 buffer($\\beta$)의 다음 word에 의존하는 경우, 이 관계를 연결하여 arcs($A$)에 저장한다.  \n   결론상 stack($\\sigma$)에서는 pop이 되고, buffer($\\beta$)는 그대로 유지되며, arcs($A$)에는 depdendency가 하나 추가된다.\n4. Right-Arc  \n   buffer($\\beta$)에서 다음 word를 stack($\\sigma$)에 push하고, 기존 stack($\\sigma$)의 이전 word에 의존하는 관계를 arcs($A$)에 추가한다.\n5. Finish  \n   buffer($\\beta$)에 더 이상 word가 없다면, 모든 연산을 마무리할 수 있다.\n\n이 또한 예시를 통해서 알아보는 것이 명확하다.\n\n우리가 `Happy children like to play with their friends.`를 분석하고 싶다고 하자. 그렇다면, 절차는 다음과 같이 진행된다.\n\n| Index | Action    | Stack($\\sigma$)                   | Buffer($\\beta$)        | Arcs($A$)                                      |\n| :---- | :-------- | :-------------------------------- | :--------------------- | :--------------------------------------------- |\n| 0     |           | [ROOT]                            | [Happy, children, ...] | $\\empty$                                       |\n| 1     | Shift     | [ROOT, Happy]                     | [children, like, ...]  | $\\empty$                                       |\n| 2     | LA(amod)  | [ROOT]                            | [children, like, ...]  | {amod(children, happy) = $A_{1}$}              |\n| 3     | Shift     | [ROOT, children]                  | [like, to, ...]        | $A_{1}$                                        |\n| 4     | LA(nsubj) | [ROOT]                            | [like, to, ...]        | $A_{1} \\cup ${nsubj(like, children)} = $A_{2}$ |\n| 5     | RA(root)  | [ROOT, like]                      | [to, play, ...]        | $A_{2} \\cup ${root(ROOT, like)} = $A_{3}$      |\n| 6     | Shift     | [ROOT, like, to]                  | [play, with, ...]      | $A_{3}$                                        |\n| 7     | LA(aux)   | [ROOT, like]                      | [play, with, ...]      | $A_{3} \\cup ${aux(play, to)} = $A_{4}$         |\n| 8     | RA(xcomp) | [ROOT, like, play]                | [with, their,...]      | $A_{4} \\cup ${xcomp(like, play)} = $A_{5}$     |\n| 9     | RA(prep)  | [ROOT, like, play, with]          | [their, friends, .]    | $A_{5} \\cup ${prep(play, with)} = $A_{6}$      |\n| 10    | Shift     | [ROOT, like, play, with, their]   | [friends, .]           | $A_{6}$                                        |\n| 11    | LA(poss)  | [ROOT, like, play, with]          | [friends, .]           | $A_{6} \\cup ${poss(friends, their)} = $A_{7}$  |\n| 12    | RA(pobj)  | [ROOT, like, play, with, friends] | [.]                    | $A_{7} \\cup ${pobj(with, friends)} = $A_{8}$   |\n| 13    | Reduce    | [ROOT, like, play, with]          | [.]                    | $A_{8}$                                        |\n| 14    | Reduce    | [ROOT, like, play]                | [.]                    | $A_{8}$                                        |\n| 15    | Reduce    | [ROOT, like]                      | [.]                    | $A_{8}$                                        |\n| 16    | RA(punc)  | [ROOT, like, .]                   | []                     | $A_{8} \\cup${punc(like, .)} = $A_{9}$          |\n| 17    | Finish    | [ROOT, like, .]                   | []                     | $A_{9}$                                        |\n\n자 이런 예시를 보았다면, 당연히 궁금해할 것은 어떻게 Action을 고를 것인가이다. 이는 Discriminative classifier 즉, Maxent나 여타 Machine Learning 방법을 동원하여 결정한다. PCFG를 활용하는 방식보다는 성능이 약간 낮을지라도 이를 활용하면 매우 빠르게 parsing이 가능하다는 장점이 있다.\n\n```plaintext\n 🤔 Projectivity\n \n 사실 여태까지 우리는 연속되어 있는 word간의 의존성을 파악하는 과정을 살펴보았다.(특히 PCFG)\n 하지만, 그렇지 않은 경우도 분명히 존재한다. 대표적인 예시가 아래이다.\n Who did Bill buy the coffee from yesterday?\n 여기서 from은 Who와 관계가 있지만, 우리가 여태까지 살펴본 PCFG와 Malt Parser로 \n 이 관계를 밝히는데에는 한계가 있다.\n 따라서, 이를 해결하기 위해서 후처리나 추가적인 action을 Malt Parser에 더하거나 \n 아니면 아예 다른 방식을 앙상블하여 해결하기도 한다.\n```\n\n## Semantics\n\n자세히 여기서 다루지 않지만, 구문 분석을 통해 얻은 Tree를 통해서 어떻게 의미를 추출할 수 있는지를 알아보겠다. 먼저, 우리는 전체 요소를 다시 한 번 두 가지로 나눈다.\n\n1. Entities  \n   특정 의미를 가지는 하나의 주체이다. 주로 NP가 모두 여기에 속한다.\n2. Functions  \n   Entity 또는 다른 Function에게 동작, 특성, 등을 적용한다. 형용사, 동사 등이 여기에 속한다.\n\n따라서, 우리가 `Every nation wants George to love Laura.`라는 문장을 갖고 있다면, 우리는 아래와 같이 Tree를 그릴 수 있고, 이를 이용해서 의미 분석이 가능하다.\n\n![nlp-semantic](/images/nlp-semantic.jpg)\n\n위 Tree를 아래에서부터 연결해서 나가면 다음과 같이 구조화되는 것을 알 수 있다.\n\n| Index | Expression                                                |\n| :---- | :-------------------------------------------------------- |\n| 1     | love(x, Laura)                                            |\n| 2     | love(x, Laura)                                            |\n| 3     | love(George, Laura)                                       |\n| 4     | want(x, love(George, Laura))                              |\n| 5     | present(want(x, love(George, Laura)))                     |\n| 6     | Every(nation)                                             |\n| 7     | present(want(Every(nation), love(George, Laura)))         |\n| 8     | assert(present(want(Every(nation), love(George, Laura)))) |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Penn Treebank POS tagging, <https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html>\n- spaCy, <https://spacy.io/>\n- NLTK, <https://www.nltk.org/>\n- KoNLPy, <https://konlpy.org/ko/latest/index.html>\n- NLP CFG, <https://tildesites.bowdoin.edu/~allen/nlp/nlp1.html>\n","slug":"nlp-language-parsing","date":"2022-11-07 15:05","title":"[NLP] 8. Language Parsing","category":"AI","tags":["NLP","POS","PCFG","Morphology","Syntax","Semantics"],"desc":"우리가 NL을 제대로 분석하기 위해서 각 단어가 가진 의미를 알아야하며, 이를 넘어서 문장이 가지는 의미를 파악해야 한다. 결론적으로 이 과정이 고도화된 NLP를 위한 핵심 단계이다. 이를 위해서는 Raw한 형태로 주어진 text를 처리해서 더 나은 형태의 구조를 만들 필요가 있다. 따지고 보면 하나의 전처리 과정이라고 볼 수 있다. 그치만 이전 text processing chapter과 다른 점은 문장 구분과 같은 간단한 과정이 아닌 Linguistic 단계에 따른 처리 과정을 수행한다고 볼 수 있다. 또한, 각 단계 역시 NLP 중에 하나라고 할 수 있으므로 이 또한 ML과 DL을 통해서 고도화하는 것도 가능하다. Morphology 단계부터 시작하여 Syntax, Semantic까지 어떻게 다루게 되는지를 살펴보도록 하겠다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n해당 Posting에서는 Maximum Entropy를 이용하여 최적의 parameter를 찾아나가는 Machine Learning 접근법에 기반한 NLP 방식을 제안한다. 이를 위해서 NL를 수학적인 형태로 변형하기 위한 기술 중 하나인 word2vec에 대한 설명도 같이 진행한다.\n\n## MaxEnt Model\n\nMaximum Entropy Model(MEM)의 약자로, 이것의 의미는 주어진 dataset을 표현할 수 있는 가장 적절한 분포는 Prior Knowledge를 만족하는 분포들 중에서 가장 높은 Entropy를 가지는 분포라는 것이다.  \n\n이는 다음과 같은 관측에 의해서 정의된 것이다.\n\n1. 다양한 물리현상들은 시간이 지남에 따라 Entropy를 최대화하려는 방향으로 이동하려는 경향이 있다.\n2. 더 적은 수의 논리로 설명이 가능한 경우, 많은 수의 논리를 세우지 말라 (오컴의 면도날)\n\n다소 억지같아 보이는 논리일지라도 후에 가서 살펴보면, Machine Learning의 Logistic Regression에 연결되는 것을 볼 수 있다. 우선은 이 정도 논리로 사용하겠다는 정도로 이해해보자.\n\n따라서, 우리가 풀어야할 식은 다음과 같다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & H(p) = - \\sum_{i=1}^{N}p_i\\log p_i &\\\\\n  \\text{subject to} \\quad & p_i \\geq 0, & i = 1, ..., N \\\\\n                          & \\sum_{i=1}^{N}p_i = 1 &\\\\\n                          & \\text{Other Prior Knowledge}\n\\end{align*}\n$$\n\n이를 이용해서 문제를 세 개 정도 풀어보면 감이 잡을 수 있는데 한 번 따라와보도록 하자.\n\n### Example\n\n> <mark>**1. 주사위 던지기**</mark>\n\n1부터 6까지의 눈이 있는 주사위가 있다고 할 때, 주사위의 각 눈이 나올 확률을 알고 싶다고 하자. 이때 우리는 간단하게 $1\\over6$이라고 말할 것이다. 이것도 Maximum Entropy에 기반한 추론 방법 중에 하나라고 할 수 있다. 다음 식을 보자.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & H(p) = - \\sum_{i=1}^{N}p_i\\log p_i &\\\\\n  \\text{subject to} \\quad & p_i \\geq 0, & i = 1, ..., N \\\\\n                          & \\sum_{i=1}^{N}p_i = 1 &\n\\end{align*}\n$$\n\n정말 아무런 정보가 없을 때에는 위의 식을 Lagrangian을 쓰지 않고도 uniform distribution이라는 것을 알 수 있다. 이는 [🔗 [ML] Base Information Theory](/posts/ml-base-knowledge#Information-Theory)에서 살펴보았었다.\n\n그렇다면, 좀 더 복잡한 경우를 고려해보자. 아래는 Duke University ECE587 수업 PPT의 예제이다.\n\n> <mark>**2. 평균이 주어졌을 때의 추론**</mark>\n\n우리가 만약 평균 데이터를 알고 있다면, 이를 Maximum Entropy로 어떻게 추정할 수 있는지를 살펴볼 것이다. 아래는 어느 fastfood점의 메뉴라고 하자.\n\n| Item    | Price | Calories |\n| :------ | :---- | :------- |\n| Burger  | $1    | 1000     |\n| Chicken | $2    | 600      |\n| Fish    | $3    | 400      |\n| Tofu    | $8    | 200      |\n\n그리고 특정 학생이 이 가게에서 하루에 하나씩 먹는다고 할 때, 평균 소비 가격이 $2.5라고 하자. 그렇다면, 이 학생이 가장 많이 먹는 메뉴는 무엇일지를 추론해보는 것이다.  \n즉, 이를 식으로 정리하면 다음과 같다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & H(p) = - \\sum_{i=1}^{N}p_i\\log p_i &\\\\\n  \\text{subject to} \\quad & p_i \\geq 0, & i = 1, ..., N \\\\\n                          & \\sum_{i=1}^{N}p_i = 1 &\\\\\n                          & E[\\text{price}] = 2.5 &\n\\end{align*}\n$$\n\n이를 Lagrangian 방식을 이용해서 표현하면 다음과 같이 나타낼 수 있다.\n\n$$\n\\mathcal{L} = - \\sum_{i}^{N}p_{i}\\log{p_{i}} + \\lambda_{0}(\\sum_{i=1}^{N}p_{i} - 1) + \\lambda_{1}(\\sum_{i=1}^{N}\\text{price}_{i}\\times{p_{i}} -2.5)\n$$\n\n위 식을 각 각의 $p_{i}$에 대해서 미분하면 다음과 같다.\n\n$$\n{\\partial \\mathcal{L}\\over\\partial p_{i}} = -\\log{p_{i}} -1 + \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i}\n$$\n\n따라서, $p_{i}$는 다음과 같다.\n\n$$\n\\begin{align*}\n0 &= -\\log{p_{i}} -1 + \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} \\\\\n\\log{p_{i}} &= \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} - 1 \\\\\np_{i} &= e^{\\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} - 1}\n\\end{align*}\n$$\n\n여기서 나오는 모든 식과 제한 조건을 정리하면 다음과 같다.\n\n- $p(Burger) = e^{\\lambda_{0} + \\lambda_{1} - 1}$, $p(Chicken) = e^{\\lambda_{0} + 2\\lambda_{1} - 1}$, $p(Fish) = e^{\\lambda_{0} + 3\\lambda_{1} - 1}$, $p(Tofu) = e^{\\lambda_{0} + 8\\lambda_{1} - 1}$\n- $p(Burger) + p(Chicken) + p(Fish) + p(Tofu) = 1$\n- $p(Burger) + 2p(Chicken) + 3p(Fish) + 8p(Tofu) = 2.5$\n\n위의 식을 연립해서 풀면, $\\lambda_{0} = 1.2371$, $\\lambda_{1}=0.2586$이고, 전체 확률은 다음과 같다.\n\n| Item    | p      |\n| :------ | :----- |\n| Burger  | 0.3546 |\n| Chicken | 0.2964 |\n| Fish    | 0.2478 |\n| Tofu    | 0.1011 |\n\n> <mark>**3. 주사위의 눈의 합**</mark>\n\n1번에서 보았던 주사위를 n개 던져서 나온 눈의 합을 알 때, 주사위의 비율을 추정한다고 해보자.\n\n이때 우리는 다음과 같은 변수를 정의할 수 있다.\n\n- 주사위의 갯수 : $n$\n- i개의 눈을 가진 주사위의 갯수 : $n_{i}$\n- 전체 눈의 수의 합 : $n\\alpha$\n- 추가되는 Prior Knowledge : $\\sum_{i=1}^{6}{i n_{i}} = n\\alpha$\n\n이를 Maximum Entropy를 이용해서 풀게 되면 다음과 같은 결론을 얻을 수 있다.\n\n$$\np_{i} = {e^{\\lambda_{i}}\\over{\\sum_{i=1}^{6}{e^{\\lambda_{i}}}}}\n$$\n\n## Generalization\n\nMaximum Entropy를 위의 식을 통해서 구하는 것도 문제는 없지만 우리는 좀 더 일반화된 식을 원한다. 따라서, 이를 표현하기 위해서 다음과 같은 상황을 고려해보는 것이다. 우리가 마지막 보았던 예시가 사실은 우리가 하고자 하는 과정을 대표하는 하나의 예시이다. 우리가 가진 사전 지식은 이전에 관측한 데이터와 이것의 class이다. 따라서, 우리는 관측 결과의 가짓수(class)가 $K$개이고, 데이터의 input과 결과를 $(X, Y)$ 쌍이 라고 할 때, 특정 input data($X_{i}$)가 class k일 확률은 다음과 같이 표현할 수 있다.\n\n$$\np(Y_{i} = k) = {e^{w^{\\top}_{k}X_{i}}\\over{\\sum_{k^{\\prime}=1}^{K}{e^{w_{k^{\\prime}}^{\\top}X_{i}}}}}\n$$\n\n여기서 가장 중요한 Point가 발견된다. 바로 이 식이 **softmax** 함수라는 것이다. <mark>즉, Maximum Entropy를 통한 classification의 의미는 사실상 multinomial logistic regression의 다른 이름일 뿐이다.</mark> (logistic regression에 대한 내용은 [🔗 [ML] 3. Logistic Regression](/posts/ml-logistic-regression)에서 다루었다.)\n\n따라서, 여기서는 별도로 Modeling, Estimation, Smoothing 절차를 다루지 않는다. machine learning의 방법과 동일하기 때문이다.\n\n## Features\n\nNL의 가장 큰 특징은 data가 sparse하다는 것이다. domain마다 사용되는 언어와 빈도가 너무나 천차만별이기 때문에 sparse 현상이 필연적으로 발생한다. 이를 극복하기 위해서 대게의 data는 domain 별로 따로따로 수집하는 것이 일반적이다. 또한, data에서 올바른 feature를 추출하는 것이 굉장히 중요하다.  \n이를 위해 NL에서 전통적으로 쓰던 방식은 대소문자 여부, 억양 표기, 품사, 문장구조, 뜻 등을 단어에 미리 적용하기도 하여 이를 이용하는 방법도 있다. 그런데 이러한 품사, 뜻 등을 찾아내는 과정도 Statistical Inference가 필요하다. 따라서, 앞으로 chapter에서는 품사와 문장구조 뜻을 정의하기 위한 기술들과 이를 어떻게 찾을 수 있는지를 알아볼 것이다.\n\n또한, Word자체를 Vector로 치환하여 사용하는 Word2Vec방식에 대해서도 살펴보도록 하겠다.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Maximum Entropy 자료 참고, <https://www2.isye.gatech.edu/~yxie77/ece587/Lecture11.pdf>\n","slug":"nlp-maxent","date":"2022-11-07 10:02","title":"[NLP] 7. MaxEnt","category":"AI","tags":["NLP","MaximumEntropyModel","softmax"],"desc":"해당 Posting에서는 Maximum Entropy를 이용하여 최적의 parameter를 찾아나가는 Machine Learning 접근법에 기반한 NLP 방식을 제안한다. 이를 위해서 NL를 수학적인 형태로 변형하기 위한 기술 중 하나인 word2vec에 대한 설명도 같이 진행한다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n이전까지 특정 word를 기반으로 하여 modeling을 수행하는 방법을 알아보았다. 하지만, 우리가 특정 word의 sequence를 통해서 각 word에 대한 classification을 한 번에 하고 싶은 경우는 어떻게 할까?(예를 들어, 각 단어의 품사를 지정하는 일) 일반적으로 각 단어가 특정 해당 class일 확률로 구하는 방법이 일반적일 것이다. 하지만, 문맥을 고려하여 확률을 구할 방법은 없을까? 그 방법은 바로 bigram을 이용하면 될 것이다. 그렇다면, 사실 우리가 사용하는 문맥이 단어 자체보다는 이전 class가 더 영향이 크다면, 이는 어떻게 해야할까? 이를 위한 해결책이 HMM이다. NLP 뿐만 아니라 여러 분야에서 넓게 사용되고 있지만, 여기서는 NLP 분야에서 어떻게 이를 사용하는지를 알아볼 것이다.\n\n## Markov Model\n\nHMM을 알아보기전에 Markov Model을 알아야 한다. 이는 특정 sequence의 확률을 추정하는 방법이다. 즉 우리에게 state sequence ($S= {s_{0}, s_{1}, ..., s_{N}}$)가 주어질 때, 각 state에서 다음 state로 전이(이동)할 확률을 이용해서 state sequence의 확률을 구하는 방법이다.\n\n![nlp-markov-model-1](/images/nlp-markov-model-1.jpg)\n\n위의 그림이 state 각 각에서 다음 state로 전이할 확률을 나타낸 것이라면, 우리는 아래 그림과 같은 그림으로 sequence의 확률을 추론할 수 있는 것이다.\n\n![nlp-markov-model-2](/images/nlp-markov-model-2.jpg)\n\n따라서, 위의 그림에서 우리가 만약 $(s_{0}, s_{1}, s_{0}, s_{2})$으로 이루어진 sequence의 확률을 얻기를 바란다면, 그 확률은 아래와 같아진다.\n$$\n\\begin{align*}\np(s_{0}, s_{1}, s_{0}, s_{2}) &= p(s_{0}| \\text{start}) \\times p(s_{1}|s_{0}) \\times p(s_{0}|s_{1}) \\times p(s_{2}|s_{1}) \\times p(end|s_{2}) \\\\\n&= \\pi_{0} \\times p_{01} \\times p_{10} \\times p_{12} \\times 1\n\\end{align*}\n$$\n\n이를 잘 살펴보니 bigram에서의 Likelihood를 구하는 공식과 똑같다. 즉, state 각 각을 word라고 본다면, Markov Model을 통해서 구할 수 있는 확률은 bigram의 Likelihood인 것이다.\n\n그리고 이를 일반화하면 다음과 같다.\n\n$$\np(seq) = \\prod_{i=1}^{N}p(seq_{i}|seq_{i-1})\n$$\n\n그런데, 여기서 n이 3 이상인 ngram을 적용하고 싶다면, 각 state를 n-1 gram으로 설정하면 된다.\n\n$$\n\\begin{align*}\nX_{i} &= (Q_{i-1}, Q_{i}) \\text{라면, }\\\\\nP(X_{i} | X_{i-1}) &= P(Q_{i-1}, Q_{i} | Q_{i-2}, Q_{i-1}) \\\\\n&= P(Q_{i} | Q_{i-2}, Q_{i-1})\n\\end{align*}\n$$\n\n따라서, trigram을 적용해보면 아래와 같다.\n\n$$\n\\begin{align*}\np((start, w_{0}), (w_{0}, w_{1}), (w_{1}, w_{0}), (w_{0}, w_{2})) &= p(w_{0}| \\text{start}, \\text{start}) \\times p(w_{1}|\\text{start}, w_{0}) \\times p(w_{0}|w_{0}, w_{1}) \\times p(w_{2}|w_{1}, w_{0}) \\times p(end|w_{0}, w_{2}) \\\\\n&= \\pi_{0} \\times p_{01} \\times p_{10} \\times p_{12} \\times 1\n\\end{align*}\n$$\n\n## Hidden Markov Model\n\nHidden Markov Model은 state를 하나 더 만든다는 것이 핵심이다. 그래서, 우리가 직접 관측하는 state(**observed state**)와 직접적으로 관측하지 않지만, 관측한 state들에 의존하는 state(**hidden state**) 총 두 개의 state를 사용한다. 일반적인 예시가 text가 입력되었을 때 우리는 각 단어를 observed state라고 한다면, 각 단어의 품사를 hidden state라고 정의할 수 있다.\n\n![nlp-markov-model-3](/images/nlp-markov-model-3.jpg)\n\n위의 예시는 우리가 관측하는 데이터($O$)가 3개의 state를 가지고, 이 사건에 의존적인 또 다른 사건($H$)이 3개의 state를 가지는 경우이다. 이를 이용해서 기존 Markov Model보다 복잡한 작업을 수행하는 것이 가능하다.\n\n### Estimation\n\n우리가 할 수 있는 작업은 크게 두 가지이다. 일반적인 Markov Model에서 할 수 있던 방식이 **Trellis** 방식이고, 또 다른 방식이 **Viterbi** 방식이다.\n\n1. $(o_{0}, o_{1}, o_{0}, o_{2})$의 확률이 궁금할 때(**Trellis**)\n2. $(o_{0}, o_{1}, o_{0}, o_{2})$가 주어질 때, 이것의 hidden state의 sequence 중 가장 유력한 sequence를 찾고자할 때(**Viterbi**)\n\n위의 경우를 각각 풀어보도록 하자.\n\n> <mark>**1. Trellis**</mark>\n\n우리가 직접 관측한 데이터의 sequence 자체의 확률이 궁금할 때이다. 따라서, 이에 대한 분석은 $(o_{0}, o_{1}, o_{0}, o_{2})$의 확률을 분석해보면서 설명하겠다.\n\n$$\n\\begin{align*}\np(o_{0}, o_{1}, o_{0}, o_{2}) &= p(o_{0}, o_{1}, o_{0}) \\times p(o_{2} | o_{0}, o_{1}, o_{0}) \\\\\n&= p(o_{0}, o_{1}, o_{0}) \\times \\{p(o_{2} | h_{0})p(h_{0} | o_{0}, o_{1}, o_{0}) + p(o_{2} | h_{1})p(h_{1} | o_{0}, o_{1}, o_{0}) + p(o_{2} | h_{2})p(h_{2} | o_{0}, o_{1}, o_{0})\\} \\\\\n&= p(o_{0}, o_{1}, o_{0}) \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n&= p(o_{0}, o_{1}) \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n&= p(o_{0}) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n&= \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) }\n\\end{align*}\n$$\n\n이를 그림으로 표현하면 다음과 같다.\n\n![nlp-hidden-markov-model-1](/images/nlp-hidden-markov-model-1.jpg)\n\n또한, 이 식을 다음과 같이 축소가 가능하다.\n\n$$\n\\begin{align*}\n  &\\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}\\alpha_{0 i} \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}{\\alpha_{1 i} } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}{\\alpha_{2 i} } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}{\\alpha_{3 i} }\n\\end{align*}\n$$\n\n우리는 이를 통해서, Markov Model의 특징을 하나 배울 수 있다. 그것은 바로 복잡한 sequence 전체의 확률에서 벗어나서 바로 직전의 확률값만 으로 다음 확률을 추론할 수 있다는 것이다. 이것이 Markov Chain이라는 이론이고, 이를 이용했기 때문에 Markov Model라고 부르는 것이기도 하다.\n\n따라서, $\\alpha$는 다음과 같이 정의할 수 있다.\n\n$$\n\\alpha(t, i) = \\sum_{k=1}^{N}{\\alpha(t-1, k)p(h_{i}|h_{k})p(o = s_{t}|h_{i})} \\quad (s_{t} = \\text{input으로 들어온 sequence의 t번째 값})\n$$\n\n또, 이를 반대로 할 경우에는 다음과 같은 식을 얻을 수 있다.\n\n![nlp-hidden-markov-model-2](/images/nlp-hidden-markov-model-2.jpg)\n\n$$\n\\begin{align*}\n  &\\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{\\beta_{3i}} \\\\\n  =& \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{\\beta_{2i}} \\\\\n  =& \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{\\beta_{1i}} \\\\\n  =& \\sum_{i=0}^{2}{\\beta_{0i}} \\\\\n\\end{align*}\n$$\n\n$$\n\\beta(t, i) = \\sum_{k=1}^{N}{\\beta(t+1, k)p(h_{k}|h_{i})p(o = s_{t}|h_{i})} \\quad (s_{t} = \\text{input으로 들어온 sequence의 t번째 값})\n$$\n\n위의 처럼 앞에서부터 풀이를 해나가면서, $\\alpha$의 합으로 끝이 나도록 푸는 방법을 forwarding 방식이라하고, 반대로 뒤에서부터 풀이하면서 $\\beta$의 합으로 푸는 방법을 backwarding 방식이라고 한다. 사실 이 경우는 HMM이 굳이 아니더라도, MM으로 구할 수 있으니 굳이 필요는 없다. 하지만, 이것은 후에 modeling 단계에서 사용하기 때문에 알아두어야 한다.\n\n> <mark>**2. Viterbi**</mark>\n\n이는 observed state의 sequence에 의해서 파생되는 가장 적절한 hidden sequence를 구하는 것이 목표이다. 이를 통해서 할 수 있는 대표적인 것이 sequence classification이다.\n\n그렇다면 가장 유력한 hidden state의 sequence를 $\\hat{s}^{(H)}$라고 하자. 이는 다음과 같다.\n\n$$\n\\begin{align*}\n\\hat{s}^{(H)} &= \\argmax_{s^{(H)} \\in S^{(H)}}P(s^{(H)}|s^{(O)}) \\\\\n&= \\argmax_{s^{(H)} \\in S^{(H)}}P(s^{(O)}|s^{(H)})P(s^{(H)}) \\\\\n&= \\argmax_{{h_{1}, h_{2}, ..., h_{N}} \\in S^{(H)}}\\underbrace{P(o_{1}, o_{2}, ... , o_{N}|h_{1}, h_{2}, ... , h_{N})}_{\\text{Markov Model}}\\underbrace{P(h_{1}, h_{2}, ... , h_{N})}_{\\text{Markov Model}} \\\\\n&= \\argmax_{{h_{1}, h_{2}, ..., h_{N}} \\in S^{(H)}}\\prod_{i=1}^{N}p(o_{i}|h_{i})p(h_{i}|h_{i-1})\n\\end{align*}\n$$\n\n![nlp-hidden-markov-model-3](/images/nlp-hidden-markov-model-3.jpg)\n\n즉, 각 layer에서 단 하나의 가장 큰 output만 살아남을 수 있게 되는 것이다. 이 과정이 사실상 HMM의 본질적인 목표이다. sequence를 입력해서 sequence 형태의 classification 결과를 얻는 것이다.\n\n### Modeling\n\n여태까지 HMM을 활용하여 sequential class를 어떻게 estimation 하는지 알아보았다. 그렇다면, 이제는 이를 위해서 사용되는 확률값을 구해야한다. 필요한 확률값은 다음과 같다.\n\n- $p(h_{i}|h_{i-1})$ : Hidden State에서 Hidden State로 넘어가기 위한 확률이다.\n- $p(o_{i}|h_{i})$ : 방출 확률로 특정 Hidden State에서 다음 State의 Observed State로 넘어가는 방법이다.\n- $\\pi_{i}$\n\nTrelli 방식에서 만들었던, $\\alpha$와 $\\beta$의 의미를 이해해야 한다. 각 각은 해당 과정까지 오면서 누적해온 확률이라고 할 수 있다. 그리고, 우리가 원하는 것은 입력으로 주어진 데이터를 잘 반영할 수 있는 확률 값을 찾는 것이다. 그렇다면, 우리가 생각할 수 있는 방법은 평균을 활용하는 것이다. 이를 구하는 과정을 먼저 살펴보자.\n\n$$\n\\begin{align*}\n  c(i, j, k) &= h_{i}\\text{에서 } h_{j}\\text{로 넘어가고, } o_{k}\\text{가 관측될 확률의 합} \\\\\n  &= \\sum_{t=2}^{T} \\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j) \\\\\n  \\\\\n  c(i,j) &= h_{i}\\text{에서 } h_{j}\\text{로 넘어갈 확률의 합} \\\\\n  &= \\sum_{k=1}^{K}\\sum_{t=2}^{T}{\\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j)} \\\\\n  \\\\\n  c(i) &= h_{i}\\text{에서 상태를 변경하는 확률의 합} \\\\\n  &= \\sum_{j=1}^{N}\\sum_{k=1}^{K}\\sum_{t=2}^{T}{\\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j)} \\\\\n\\end{align*}\n$$\n\n위의 값을 통해서 우리는 우리가 가지고 있던 확률을 업데이트할 수 있다.\n\n$$\n\\begin{align*}\np(h_{j}|h_{i}) &= {c(i,j)\\over c(i)} \\\\\np(o_{k}|h_{i}) &= {c(i,j,k)\\over c(i,j)}\n\\end{align*}\n$$\n\n즉, 우리는 다음 과정을 수행하여 Modeling을 수행할 수 있는 것이다.\n\n1. 초기값 ($p(h_{i}|h_{i-1})$, $p(o_{i}|h_{i})$, $\\pi_{i}$)을 초기화 한다.  \n2. Trelli를 통해서 $\\alpha$, $\\beta$를 계산한다.\n3. $p(h_{i}|h_{i-1})$, $p(o_{i}|h_{i})$를 업데이트 한다.  \n   ($pi_{i}$같은 경우는 발생 빈도로 업데이트 한다.)\n4. 임계치에 도달할 때까지 2,3번을 반복한다.\n\n이 과정을 대게 10번 정도만 하면 수렴하게 되고, 이를 확률로 사용하는 것이다.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-hmm","date":"2022-10-21 21:55","title":"[NLP] 6. Hidden Markov Model","category":"AI","tags":["NLP","MarkovModel","HMM","HiddenMarkovModel"],"desc":"이전까지 특정 word를 기반으로 하여 modeling을 수행하는 방법을 알아보았다. 하지만, 우리가 특정 word의 sequence를 통해서 각 word에 대한 classification을 한 번에 하고 싶은 경우는 어떻게 할까?(예를 들어, 각 단어의 품사를 지정하는 일) 일반적으로 각 단어가 특정 해당 class일 확률로 구하는 방법이 일반적일 것이다. 하지만, 문맥을 고려하여 확률을 구할 방법은 없을까? 그 방법은 바로 bigram을 이용하면 될 것이다. 그렇다면, 사실 우리가 사용하는 문맥이 단어 자체보다는 이전 class가 더 영향이 크다면, 이는 어떻게 해야할까? 이를 위한 해결책이 HMM이다. NLP 뿐만 아니라 여러 분야에서 넓게 사용되고 있지만, 여기서는 NLP 분야에서 어떻게 이를 사용하는지를 알아볼 것이다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nNaive Bayes Model은 가장 쉽게 Classification을 수행할 수 있는 Model이지만, 성능이 다른 Model에 비해 뛰어나지는 않다. 그럼에도 Naive Bayes는 가장 기본이 되는 Model이기에 비교 대상으로 많이 사용되고, Classification의 insight를 키우는데 많은 도움을 줄 수 있다. 여기서는, 전반적인 개념과 이를 직접 Spam Filtering에서 어떻게 사용하는지 살펴본다.\n\n## Naive Bayes Model\n\n특정 class에서 해당 데이터가 얼마나 자주 발생되는지와 실제로 해당 class의 빈도를 활용하여, classification을 수행하는 것이다. 우선 이를 수식적으로 표현하기 위해서 다음 변수들을 먼저 정의해보자.\n\n- **documents($D$)**: 여러 개의 Document를 의미하며, 하나의 Document는 대게 여러 개의 words를 포함한다. 각 document는 $d_{i} \\in D$의 형태로 표현한다.\n- **classes($C$)**: class는 두 개 이상을 가진다. 각 클래스는 $c_{i} \\in C$의 형태로 표현된다.\n- **labeled dataset**: 이는 (document($d_{i}$), class($c_{i}$))가 하나씩 mapping된 형태로 존재한다. 우리가 가지는 dataset으로 학습, 평가 시에 사용한다. 대게 평가에 사용되는 데이터는 학습 시에 사용하는 것을 금지하기 때문에 별도로 분리하여 사용한다.\n- **word($w$)**: 하나의 word를 의미하며 NLP 학습 시에 사용하는 가장 작은 단위이다. 대게 document 하나에 있는 단어의 수는 N으로 표기하고, unique한 단어의 수는 V(size of vocabulary)로 표시한다.\n\n따라서, 우리가 찾고자 하는 가장 높은 확률을 가진 class는 다음을 통해서 구할 수 있다.\n\n$$\n\\begin{align*}\nc_{MAP} &= \\argmax_{c \\in C}{P(c|d)} \\\\\n&= \\argmax_{c \\in C}{p(d|c)p(c)\\over p(d)} \\\\\n&= \\argmax_{c \\in C}{p(d|c)p(c)} \\\\\n&= \\argmax_{c \\in C}{p(w_{1}, w_{2}, ... , w_{N} | c)p(c)} \\\\\n&= \\argmax_{c \\in C}{\\prod_{i=1}^{N}p(w_{i}|c)p(c)} \\\\\n&= \\argmax_{c \\in C}{\\log(\\prod_{i=1}^{N}p(w_{i}|c)p(c))} \\\\\n&= \\argmax_{c \\in C}{\\sum_{i=1}^{N}\\log p(w_{i}|c) + \\log{p(c)}} \\\\\n\\end{align*}\n$$\n\n여기서 우리가 language model을 무엇으로 정했는지가 중요하다. 위에서는 uni-gram이라고 가정해서 풀이했지만, bi-gram인 경우 document의 형태가 $d={(w_{1}, w_{2}), (w_{2}, w_{3}), ... , (w_{N-1}, w_{N})}$이다. 따라서, 전체적인 크기와 vocabulary자체도 바뀌게 된다.\n\n즉, 우리는 train set을 통해서 vocabulary를 완성한다. 그리고, 각 word의 count 및 필요에 따라 필요한 word sequence의 count를 수집하여 $p(w_i)$를 구한 후 위에 방법을 통해서 특정 class를 추측할 수 있는 것이다.\n\n이제 구체적인 Naive Bayes의 동작 절차는 Spam Filtering이라는 Case Study를 통해서 자세히 살펴보도록 하자.\n\n## Case Study. Spam Filtering\n\n초기 NLP가 가장 많이 사용되었던 예시 중에 하나이다. 여러 개의 메일에 spam인지 ham인지를 labeling한 데이터를 갖고 후에 input으로 mail 데이터가 들어왔을 때, 이를 filtering하는 것이다. 위에서 살펴보았던 확률을 그대로 적용하면 된다. 예측에 필요한 확률을 습득하고, 예측하는 방법과 이를 평가하는 방법의 순으로 설명하겠다.\n\n### 0. Preprocessing\n\n사실 mail data의 형태가 이상할 수도 있다. Subject부터 시작하여 날짜 데이터 그리고 특수 문자 등이 존재할 수 있는데, 이를 먼저 처리해서 후에 있을 Modeling 단계에서 잘 사용할 수 있도록 형태를 변형해주어야 한다.\n\n[🔗 이전 Posting(Text Processing)](/posts/nlp-text-processing)에서 배웠던 기술들을 활용하여 이를 해결할 수 있다.\n\n대표적으로 해줄 수 있는 작업들은 다음과 같다.\n\n1. 대소문자 통일\n2. alphabet이 하나라도 들어있지 않은 데이터는 삭제\n3. date, 참조 등을 의미하는 데이터 삭제\n\n### 1. Modeling\n\nParameter Estimation / Learning / Modeling 등으로 불리는 단계이다. 일단 우리는 train set으로부터 우리가 원하는 확률을 추출해야 한다. 그 전에 우리가 어떤 language model을 이용할지 선택해야 한다. 먼저 uni-gram인 경우에는 다음과 같은 방법으로 train set이 정의된다.\n$$\n\\text{TrainSet} = {(d_{1}, c_{1}),  (d_{2}, c_{2}), ..., (d_{N}, c_{N})}\n$$\n$$\nd_{i} = \\begin{cases}\n  {w_{1}, w_{2}, ... , w_{M_{i}}} \\quad&\\text{unigram} \\\\\n  {(<s>, w_{1}), (w_{1}, w_{2}), ... , (w_{M_{i}}, </s>)} \\qquad&\\text{bigram}\n\\end{cases}\n$$\n\n이제 우리가 원하는 parameter, 즉 확률은 다음과 같은 데이터이다.\n\n> **unigram**\n\n$$\n\\begin{align*}\np(w_{i}|c_{j}) &= {\\text{count}(w_{i}, c_{j}) \\over \\sum_{w \\in V} \\text{count}(w, c_{j})} \\\\\np(c_{j}) &= {\\sum_{i = 1}^{N}{1[c_{i} = c_{j}]} \\over N}\n\\end{align*}\n$$\n\n> **bigram**\n\n$$\n\\begin{align*}\np(w_{i}|w_{i-1},c_{j}) &= {\\text{count}((w_{i-1}, w_{i}), c_{j}) \\over \\sum_{(w^{(1)}, w^{(2)}) \\in V} \\text{count}((w^{(1)}, w^{(2)}), c_{j})} \\\\\np(c_{j}) &= {\\sum_{i = 1}^{N}{1[c_{i} = c_{j}]} \\over N}\n\\end{align*}\n$$\n\n여기서 우리는 반드시 Smoothing을 해주어야 한다. 왜냐하면, spam mail에서 안 본 단어가 나올 가능성이 너무나 높기 때문이다. 따라서, 실제 $p(w_{i}|c_{j})$는 아래와 같이 변경된다. (간단한 예시를 들기 위해서 Add-1 방식을 사용했다. - 해당 내용이 기억이 나지 않는다면, [🔗 이전 포스팅](/posts/nlp-language-modeling)을 다시 보고 오자.)\n\n$$\np(w_{i}|c_{j}) = {\\text{count}(w_{i}, c_{j}) + 1 \\over \\sum_{w \\in V} \\text{count}(w, c_{j}) + |V|}\n$$\n\n주의할 점은 다시 한 번 강조하지만, $V$는 후에 Estimation에서 input으로 사용하는 단일 document까지 포함한 Vocabulary이다.\n\n### 2. Estimation\n\n이제 우리가 얻은 parameter를 이용해서 실제 input data에 대한 estimation을 수행할 수 있다.\n\n이 경우 다음과 같은 과정을 수행할 수 있다.\n\n$$\n\\hat{c} = \\argmax_{c \\in C} p(c)\\prod_{w \\in d_{\\text{input}}}p(w|c)\n$$\n\n물론 어떤 n-gram을 쓰냐에 따라 $d_{\\text{input}}$도 형태가 달라질 것이다.\n\n### 3. Evaluation\n\n이제 평가를 수행할 것이다. 평가는 우리가 알아봤던 Accuracy와 F1 Score를 추출할 수 있다. Binary Classification이기 때문에 쉽게 구할 수 있을 것이다.\n\n| prediction\\answer | True                                                                       | False                                                                     |\n| :---------------- | :------------------------------------------------------------------------- | :------------------------------------------------------------------------ |\n| Positive          | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} = c, c = \\text{spam}]$    | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} \\neq c, c = \\text{ham}]$ |\n| Negative          | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} \\neq c, c = \\text{spam}]$ | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} = c, c = \\text{ham}]$    |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-naive-bayes","date":"2022-10-21 15:37","title":"[NLP] 5. Naive Bayes","category":"AI","tags":["NLP"],"desc":"Naive Bayes Model은 가장 쉽게 Classification을 수행할 수 있는 Model이지만, 성능이 다른 Model에 비해 뛰어나지는 않다. 그럼에도 Naive Bayes는 가장 기본이 되는 Model이기에 비교 대상으로 많이 사용되고, Classification의 insight를 키우는데 많은 도움을 줄 수 있다. 여기서는, 전반적인 개념과 이를 직접 Spam Filtering에서 어떻게 사용하는지 살펴본다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n이전 Posting에서는 sentence의 적절성을 확인한다든지 다음 단어를 유추한다든지 오타를 정정하는 등에 필요한 기본적인 Language Modeling 방식을 살펴보았다. 이번에는 실제로 가장 많이 사용되는 예제인 Classification을 Language Model을 이용하여 어떻게 구현하는지를 다룬다.\n\n## Classification\n\nClassification은 input이 들어왔을 때, 이를 알맞은 분류로 나누는 것이 목표이다. 단순히 Rule에 기반하여 이를 수행할 수도 있지만, Statistic한 Language Modeling을 이용하면, 더 정확도가 높은 분류를 수행할 수 있다. 결국 Statistic Prediction을 수행하기 위해서 우리는 3개(Estimation, Modeling, Evaluation)를 중점적으로 봐야 하는 것은 Classification도 동일하다. 따라서, 이에 대해서 살펴볼 것이고, 그 전에 먼저 Classification Model 의 종류를 살펴보도록 하겠다.\n\n## Generative Model vs Discriminative Model\n\nClassification에서 이용되는 Model을 크게 두가지로 나눌 수 있는데 이에 대해서 먼저 알아보도록 하자.\n\n1. **Generative Model(생성 Model)**\n   1. Naive Bayes\n   2. Hidden Markov Model(HMM)\n2. **Discriminative Model(판별 Model)**\n   1. Logistic Regression\n   2. K Nearest Neighbors\n   3. Support Vector Machine\n   4. Maximum Entropy Model(MaxEnt)\n   5. Neural Network(Deep Learning)\n\n두 Model의 가장 큰 차이점은 추론의 과정이다. 우리가 원하는 데이터 $P(\\text{class}=c | \\text{input} = \\text{data})$(특정 data가 주어졌을 때, 각 class의 속할 확률)를 얻는 과정이 서로 다르다.\n\n**첫 번째**로, $P(\\text{class}=c, \\text{input} = \\text{data})$일 확률을 구하여 **간접적**으로 구하는 방법이다.\n\n$$\n\\begin{align*}\nP(\\text{class}=c | \\text{input} = \\text{data}) &= {{P(\\text{class}=c, \\text{input} = \\text{data})}\\over{P(\\text{input} = \\text{data})}} \\\\\n&\\propto {P(\\text{class}=c, \\text{input} = \\text{data})}\n\\end{align*}\n$$\n\n이런 식으로 생성하여 추론하는 방식을 <mark>Generative Model</mark>이라고 한다. 이 방식은 결국 Conditional Probability를 추론하기 위해서 Joint Probability를 이용하는 방식이기 때문에 어느정도 한계가 존재한다는 점을 유의하자.\n\n**두 번째**로는, $P(\\text{class}=c | \\text{input} = \\text{data})$를 **직접적**으로 구하는 방법이 있다. 이를 위해서, 마친 Conditional Probability를 구한 것과 유사한 효과를 내는 **Discriminant Function(판별 함수)**이라는 특별한 함수를 input에 적용하는 방법이다. 이 함수 중에서 가장 대표적인 것이 Softmax function이다. 우리가 만약 input을 softmax function에 입력하게 되면, 이 값은 [0, 1] 사이의 값으로 표현된다. 이를 통해서 우리는 해당 input이 class인 경우 1에 가깝게, 그렇지 않은 경우 0에 가깝게 표현하여 여러 데이터에 적용하면, class의 inpuut에 따른 분포 양상을 확인할 수 있다. 그리고, 이 분포 양상을 확률로 즉각적으로 표현할 수 있기 때문에 softmax function을 취한 결과가 $P(\\text{class}=c | \\text{input} = \\text{data})$과 비례한다는 결론을 낼 수 있다. 자세한 설명이 필요하다면, [🔗 Logistic Regression](/posts/ml-logistic-regression#Logistic-Regression)을 참고하도록 하자. 이러한 방식을 우리는 <mark>Discriminative Model</mark>이라고 한다.\n\n위에서 제시한 방법들 중 대표적인 방법들은 별도의 Posting을 통해서 정리하였다. 해당 링크를 참조하여 확인해보도록 하자.\n\n- **Generative Model(생성 Model)**\n  - [🔗 Naive Bayes](/posts/nlp-naive-bayes)\n  - [🔗 Hidden Markov Model(HMM)](/posts/nlp-hmm)\n- **Discriminative Model(판별 Model)**\n  - [🔗 Maximum Entropy Model(MaxEnt)](/posts/nlp-maxent)\n  - [🔗 Logistic Regression](/posts/ml-logistic-regression)\n\n## Estimation\n\n어떤 Model을 선택했다고 하더라도 결국 우리가 Class를 결정하는 과정을 동일하다. 위의 과정을 통해서 어찌되었든 다음 값을 찾으면 된다.\n\n$$\n\\begin{align*}\nc^{\\prime} &= \\argmax_{c \\in C}{P(\\text{class}=c | \\text{input} = \\text{data})}\n\\end{align*}\n$$\n\n## Modeling\n\nModel을 만드는 과정, 즉 학습하는 과정은 결국 Model의 구현마다 천차 만별이다. Naive Bayes는 단순하게 data의 word와 count를 활용하고, HMM은 EM algorithm을 활용하며, Linear Regression은 Gradient Descent를 활용한다. 따라서, 여기서는 자세히 다루지 않고 위에서 제시한 링크를 따라가서 각 Model마다의 학습법을 확인해보도록 하자.\n\n## Evaluation\n\nClassification의 성능을 평가하는 것 역시 중요한 일이다. 가장 쉬운 Binary Classification부터 알아보자.\n\nbinary classificaiton의 결과는 아래와 같이 4개 중 하나로 결정된다.\n\n| prediction\\answer | True           | False          |\n| :---------------- | :------------- | :------------- |\n| Positive          | true positive  | false positive |\n| Negative          | false negative | true negative  |\n\n이를 쉽게 이해할려면, 병(코로나)의 양성/음성 판정이 row에 해당하고, 실제 병의 여부를 column으로 생각하면 쉽다. 또한, 각 cell의 값이 헷갈릴 수 있는데, 우리가 원하는 것이 예측의 정확도를 확인하는 것이기 때문에 예측 결과는 그대로 보여주면서, 이것이 틀렸는지 맞았는지를 앞에 true/false로 표현했다고 생각하면 쉽다.\n\nclassification의 성능을 측정하는 지표는 대표적으로 4 가지가 있다.\n\n1. **Accuracy(정확도)**  \n   가장 쉽게 그리고 일반적으로 생각하는 지표다. 위의 표에서는 전체 경우의 수를 더하여 옳게 예측한 것(true postive, true negative)의 합을 나누는 것이다.\n   $tp + fn \\over tp + fp + fn + tn$  \n   하지만, 이 방식은 한계가 있다. 바로, 데이터가 한쪽으로 치우쳐져있을 때이다. 예를 들어, 우리가 진짜를 진짜라고 맞출확률은 높지만, 가짜를 가짜라고 맞출 확률이 낮다고 할 때, 이를 제대로 반영하기가 어렵다. 그런데 데이터에서 진짜가 가짜보다 압도적으로 많을 경우 정확도는 좋은 지표로 쓰기 어렵다는 것이다.\n2. **Precision(정밀도, 정답률)**  \n   쉽게 정답 자체를 맞힐 확률입니다.  \n   $tp \\over tp + fn$\n3. **Recall(재현율)**  \n   예측이 맞을 확률을 의미합니다.  \n   $tp \\over tp + fp$\n4. **F1 Score**  \n   좀 더 세분화된 평가지표이다. 조화 평균에 기반하여 모델의 성능을 정확하게 평가할 때 사용한다.  \n   ${2\\over{{1\\over\\text{Precision}} + {1\\over\\text{Recall}}}} = 2 \\times {\\text{Precision} \\times \\text{Recall} \\over \\text{Precision} + \\text{Recall}}$\n\n여기까지 봤으면, 슬슬 multi class의 경우에는 어떻게 해야할지 궁금할 것이다. 대게 두 가지 방법을 통해서 수행할 수 있다.\n\n> **1. Micro Average**\n\n전체 class를 하나의 binary table로 합치는 것이다. 즉, 클래스가 A, B, C 3개가 있다면, 각 클래스 별로 예측 성공도를 binary로 표시하고, 이를 하나의 테이블로 합치는 것이다. 그 후에는 binary에서 계산하는 식을 그대로사용할 수 있다.  \n\n> **2. Macro Average**\n\nmulti class의 경우에도 별로 다를 것은 없다. 단지 Precision과 Recall 그리고 Accuracy가 어떻게 바뀌는지만 알면 쉽게 이해할 수 있을 것이다.  \n\n| prediction\\answer | c1            | c2            | c3            | c4            |\n| :---------------- | :------------ | :------------ | :------------ | :------------ |\n| c1                | true positive | x             | x             | x             |\n| c2                | x             | true positive | x             | x             |\n| c3                | x             | x             | true positive | x             |\n| c4                | x             | x             | x             | true positive |\n\n- Precision: $c_{ii} \\over \\sum_{j}c_{ij}$\n- Recall: $c_{ii} \\over \\sum_{j}c_{ji}$\n- Accuracy: $c_{ii} \\over \\sum_{i}\\sum_{j}c_{ij}$\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-classification","date":"2022-10-21 13:37","title":"[NLP] 4. Classification","category":"AI","tags":["NLP","Classification","Generative","Discriminative","ModelEvaluation"],"desc":"이전 Posting에서는 sentence의 적절성을 확인한다든지 다음 단어를 유추한다든지 오타를 정정하는 등에 필요한 기본적인 Language Modeling 방식을 살펴보았다. 이번에는 실제로 가장 많이 사용되는 예제인 Classification을 Language Model을 이용하여 어떻게 구현하는지를 다룬다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n이제 통계적인 관점에서 NL을 input으로 하는 문제를 해결할 방법을 찾을 것이다. 이를 Language Modeling이라고 하며, 이를 위해서 또는 이를 더 효과적으로 하기 위한 방법들을 소개할 것이다.\n\n## Noisy Channel\n\n일반적으로 우리는 원하는 결과가 있다. 그 결과를 얻기 위해서 우리는 말을 하거나 행동을 하거나 글을 쓴다. 그 과정은 우리가 갖고 싶은 A라는 것을 얻기 위해서 B라는 행동을 대신하는 것과 같다. 즉, 우리는 이를 A에 noise가 껴서 B라는 것이 생성되었다고 생각하는 것이다. 그리고 우리가 관측할 수 있는 것은 B밖에 없는 것이다.\n\n이는 우리가 사용하는 NL에서도 동일하다. 우리가 원하는 결과값 A를 얻기 위해서 우리는 B라는 문장, 음성을 제시한다. 그 결과가 원하는 결과로 될 수 있는 확률을 얻어서 최종 결과를 예측하는 것이 우리의 목표인 것이다.\n\n이 과정을 수식으로 표현하면 다음과 같아진다.\n\n$$\nP(A|B) = {P(B|A)P(A)\\over{P(B)}}\\quad(\\text{Bayes Rule})\n$$\n\n우리가 얻고 싶은 $P(A|B)$ 를 얻기 위해서, $P(A)$와 $P(B|A)$ 를 통해서 구할 수 있는 것이다. 이에 대한 더 자세한 내용은 ML을 이해하는 것이 좋을 것이다. 추천하는 Posting은 [🔗 [ML] Parametric Estimation](/posts/ml-parametric-estimation)이다.\n\n## Language Modeling\n\n결국 우리가 얻고 싶은 것은 특정 문장의 할당된 확률인 것이다. 따라서, Language Model은 input으로 word sequence과 들어왔을 때, 확률을 계산하는 것이다.\n그리고, 이 계산을 수행하기 위해서 필요한 parameter를 찾는 과정을 Language Modeling이라고 한다.\n\n## Input(N-gram)\n\n대게 이러한 모델은 문장 또는 word의 배열이 다음과 같이 주어질 때, $W = w_{1}\\ w_{2}\\ w_{3}\\ ...\\ w_{n}$ 아래와 같은 형태로 표현하는 것이 일반적이다.\n\n- **Single word probability**  \n  하나의 단어의 확률을 나타낼 때 단순하게 아래와 같이 표현한다.  \n  $P(w_{i})\\quad(w_{i} \\in W)$\n- **Sequence of Words probability**  \n  일반적으로 sentence의 확률을 나타낼 때, 여러 문장을 한꺼번에 가지는 확률이므로 아래와 같이 표현하는 것이 일반적이다.  \n  $P(W) = P(w_{1}, w_{2}, w_{3}, ..., w_{n})$\n- **single word probability with context**  \n  일반적으로 우리는 이전에 사용한 단어가 문맥이라고 이해할 수 있다. 따라서, 구체적인 단어들 이후에 특정 단어가 나오는 것은 문맥을 반영한 확률이라고 생각할 수 있다.  \n  $P(W) = P(w_{5}| w_{1}, w_{2}, w_{3}, w_{4})$\n\n위의 식을 보게 되면, 우리는 다시 한번 sentence의 확률을 다시 정리할 수 있다.\n\n$$\nP(W) = P(w_{1}) \\times P(w_{2}|w_{1}) \\times P(w_{3}|w_{1},w_{2}) \\times ... \\times P(w_{n}| w_{1},w_{2},..., w_{n-1})\n$$\n\n위의 식을 보게되면, W가 짧다고 하더라도 굉장히 많은 처리가 필요하고, 저장을 위해 많은 공간이 필요하다는 것을 알 수 있다. 따라서, 우리는 현재 단어를 기준으로 너무 오래된 단어에 대해서는 무시를 하도록 하는 방법을 취하는 것이다.(**Markov Chain**) 이를 \"n 번째까지 허락\"했을 때, 이를 n-gram 이라고 부른다.\n\n$$\np(W) = \\prod_{i=1}^{n}{p(w_{i}|w_{i-n+1},w_{i-n+2},...,w_{i-1})}\n$$\n\n그렇다면, n-gram에서 적절한 n이란 무엇일까? 일반적으로는 n이 크다는 것은 context를 많이 받아들일 수 있다는 의미로 받아들여질 수 있다. 따라서, n이 클수록 성능의 최적화 가능성이 더 높다. 하지만, Vocabulary의 사이즈가 커지는 경우를 예를 들어보자. 여기서는 $|V| = 60$k 라고 해보자.\n\n| n-gram          | p(w_{i})                         | # of parameters   |\n| :-------------- | :------------------------------- | :---------------- |\n| 0-gram(uniform) | ${1\\over\\vert V\\vert}$           | 1                 |\n| 1-gram(unigram) | $p(w_{i})$                       | $6\\times10^4$     |\n| 2-gram(bigram)  | $p(w_{i}\\vert w_{i-1})$          | $3.6\\times10^9$   |\n| 3-gram(trigram) | $p(w_{i}\\vert w_{i-2}, w_{i-1})$ | $2.16\\times10^14$ |\n\nn이 커질 수록 가능한 조합의 수는 굉장히 커지기 때문에 우리가 보지 못하는 경우의 수도 굉장히 증가하게 되어 data자체의 빈도가 적어지는 현상(sparse)이 발생한다. 따라서, 대게의 경우 최대 n의 크기는 3정도로 하는 것이 일반적이다.\n\n```plaintext\n 🤔 주의\n\n 실제 데이터를 가공할 때에는 bigram부터는 문장의 시작과 끝을 표시해주어야 한다. \n 그렇지 않으면, 첫번째 문자의 확률을 구할 때, 이전 단어의 영향을 받을 수 없다.\n 정해진 규칙은 없지만, 대게 <s></s>를 이용한다.\n ex.  bigram : <s> w1 w2 w3 w4 </s>\n     trigram : <s> <s> w1 w2 w3 w4 </s> </s>\n```\n\n```plaintext\n 🤔 Google N-gram\n\n 구글에서 2006년에 N-gram을 직접 구성한 것이 있다. \n 총 1,024,908,257,229개의 단어가 존재하고, 40회 이상 등장하는 5-gram이 1,176,470,663개 존재한다. \n 총 Vocabulary의 size는 200번 이하로 등장하는 것은 제외하면, 13,588,391개이다. \n```\n\n## Estimation\n\nML에서는 Estimation을 수행할 때, continuous하게 추정하였다. 즉, 보지 못한 데이터에 대한 처리를 수행하기 위해서 continuous한 분포의 parameter만 추정하면 되었다. 하지만, NLP에서는 다르다. NL를 continuous하게 표현할 마땅한 방법이 없다. 따라서, 우리는 결국 모든 확률을 discrete하게 구해야 한다. 따라서, 우리는 특정 단어의 확률을 구하는 방법은 단 하나가 된다.\n\n$$\n|T| = \\text{count of observed tokens}\n$$\n$$\nc(w_{i}) = \\text{count of observed } w_{i}\n$$\n$$\n\\begin{align*}\n&P(w_{i}) = {{c(w_{i})}\\over{|T|}} \\\\\n&P(w_{i}| w_{i-1}) = {{c(w_{i-1}, w_{i})}\\over{c(w_{i-1})}} \\\\\n&P(w_{i}| w_{i-2}, w_{i-1}) = {{c(w_{i-2}, w_{i-1}, w_{i})}\\over{c(w_{i-2}, w_{i-1})}} \\\\\n\\end{align*}\n$$\n\n이때, 반드시 sequence의 순서를 유의하도록 하자. 순서가 바뀌면 다른 종류이다.\n\n> **Small Example**\n\n데이터가 다음과 같이 주어진다고 하자.\n\n```plaintext\n He can buy the can of soda.\n```\n\n이때 각 n-gram을 이용한 model의 확률들을 살펴보자.\n\n| model    | probability                                                                                                                                                     |\n| :------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| uni-gram | $p(He)=p(buy)=p(the)=p(of)=p(soda)=p(.) = 0.125$<br /> $p(can)=0.25$                                                                                            |\n| bi-gram  | $p(He\\vert <s>)=p(can\\vert He)=p(the\\vert buy)=p(can\\vert the)=p(soda\\vert of)=p(.\\vert sode) =p(</s> \\vert .) = 1$<br /> $p(buy\\vert can)=p(of\\vert can)= 0.5$ |\n| tri-gram | $p(He\\vert <s>, <s>)=p(can\\vert <s>, He)=p(the\\vert He, buy)=...=p(</s>\\vert ., </s>) =1$                                                                       |\n\n## Evaluation\n\n평가할 때는 ML과 결국은 동일하다. 우리가 확률분포를 구할 때, 사용한 데이터 외에 데이터를 이용해서 잘 적용이 되었는지를 확인할 수 있다. 하지만, word의 갯수와 데이터의 수가 굉장히 많은 NL의 특성상 이 Evaluation 단계에만 굉장히 많은 시간을 소모할 수 있다. 따라서, 즉각적인 평가를 위해서 사용하는 척도가 있다.\n\n> **Perplexity**\n\ntrain set을 통해 학습을 하고, test set을 통해서 평가를 수행할 때, train set을 통해 구한 확률이 실제 test set에서 어느정도의 Entropy를 발생시키는지를 확인하는 것이다. 원래의 식은\n$PP = 2^{H}$이지만, 이를 변형하여 다음과 같이 나타낼 수 있다.\n\n$$\n\\begin{align*}\nPP(W) &= \\sqrt[N]{1 \\over P(w_1, w_2, ..., w_N)} \\\\\n&= \\sqrt[N]{\\prod_{i=1}^{N}{P(w_i|w_{i-n+1}, w_{i-n+2}, ..., w_{i-1})}}\n\\end{align*}\n$$\n\n이를 통해서, 실제로 해당 문제가 너무 어렵지는 않은지, 선택한 model이 잘못되지는 않았는지를 판단한다.\n\n| model    | probability                                                                                                                                                     | entropy |\n| :------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------ |\n| uni-gram | $p(He)=p(buy)=p(the)=p(of)=p(soda)=p(.) = 0.125$<br /> $p(can)=0.25$                                                                                            | 2.75    |\n| bi-gram  | $p(He\\vert <s>)=p(can\\vert He)=p(the\\vert buy)=p(can\\vert the)=p(soda\\vert of)=p(.\\vert sode) =p(</s> \\vert .) = 1$<br /> $p(buy\\vert can)=p(of\\vert can)= 0.5$ | 0.25    |\n| tri-gram | $p(He\\vert <s>, <s>)=p(can\\vert <s>, He)=p(the\\vert He, buy)=...=p(</s>\\vert ., </s>) =1$                                                                       | 0       |\n\n위의 예시를 가져와서 봐보자. 물론 동일한 dataset에서 perplexity를 측정하기는 했지만, n이 커질 수록 점점 entropy가 작아지는 것을 볼 수 있다. 그렇다면, 이런 data가 좋은 걸까? 이는 좋은 게 아니다. 왜냐하면, 해당 dataset에서만 잘 작동하도록 되어있기 때문이다. 일명 **overfitting**이다.\n\n## Smooting\n\n위에서 말한 overfitting을 어느정도 해소할 뿐만 아니라 정말 큰 문제가 될 수 있는 probability가 0이 되는 문제(우리가 trainset을 통해 학습한 확률 분포에서 testset에 들어오는 데이터에 해당하는 확률값이 없을 때, 즉 해당 확률이 0일때)를 해결하기 위해서는 smoothing이 필수적이다. probability가 0이 된다는 것은 후에 위에서 구한 확률로 Prediction을 할 때 모든 예측을 망치는 요인이 된다. 왜냐하면, 추정확률의 최적 Entropy를 의미하는 Cross Entropy를 $\\infin$로 만들기 때문이다. (최적 Entropy가 무한대라는 것은 추정이 불가능하다는 것이다.)\n\n따라서, 우리는 probability가 0이 되지 않게 하는 방법으로 기존의 확률의 일부를 나누어주도록 하는 방법을 제시한다. 이것이 smoothing이다.\n\n이때 반드시 유의할 점은 smoothing을 하건 안하건 각 확률의 총합은 1이 되도록 보장해야 한다는 것이다.\n\n$$\n\\sum_{w \\in \\Omega}p(w) = 1\n$$\n\n대략 6가지의 대표적인 smoothing 방식들을 소개하겠다.\n\n> <mark>**1. Add-1(Laplace)**</mark>\n\n가장 간단한 방법의 smoothing 방법이지만, 실용적인면은 다소 떨어지는 방법이다. 아이디어는 간단하다. prediction을 수행할 때, 현재 들어온 input까지 포함하여 만든 $|V|$를 분모에 더하고, 분자에 1을 더해주는 방법이다. 이 방법을 사용하면, 설사 count가 0이 더라도 확률이 0이 되지는 않는다.\n\n$$\np(w) = {X \\over Y} \\rArr p^{\\prime}(w) = {X + 1 \\over Y + |V|}\n$$\n\n여기서, $|V|$ 값이 정말 헷갈렸는데, 아무도 잘 설명을 안하는 것 같아서 짚고 넘어가면, 우리가 확률값을 얻기 위해서 사용했던 dataset과 현재 prediction을 하기 위해서 들어온 input 둘에서 발생한 모든 unique한 단어의 수를 의미한다. (# of vocabulary)\n\n그렇게 해야만 $\\sum_{w \\in \\Omega}p(w) = 1$을 만족하는 값이 나온다.\n\n따라서, 이를 각 각의 n-gram에 대입하면 다음과 같다.\n\n| n    | $p(w_{i})$                                                 | $p^{\\prime}(w_{i})$                                                            |\n| :--- | :--------------------------------------------------------- | :----------------------------------------------------------------------------- |\n| $1$  | $c(w_{i}) \\over \\vert T \\vert$                             | $c(w_{i}) + 1 \\over \\vert T \\vert + \\vert V \\vert$                             |\n| $2$  | ${{c(w_{i-1}, w_{i})}\\over{c(w_{i-1})}}$                   | ${{c(w_{i-1}, w_{i}) + 1}\\over{c(w_{i-1})} + \\vert V \\vert} $                  |\n| $3$  | ${{c(w_{i-2}, w_{i-1}, w_{i})}\\over{c(w_{i-2}, w_{i-1})}}$ | ${{c(w_{i-2}, w_{i-1}, w_{i}) + 1}\\over{c(w_{i-2}, w_{i-1})} + \\vert V \\vert}$ |\n\n> <mark>**2. Add-k**</mark>\n\n1이라는 숫자가 경우에 따라서는 굉장히 큰 영향을 줄 때가 있다. 특히 기존 데이터의 수가 적은 경우에 더욱 그렇다. 따라서, 이를 해결하기 위해서 1보다 작은 임의의 값(k)을 쓰는 방법이다.\n\n$$\np(w) = {X \\over Y} \\rArr p^{\\prime}(w) = {X + 1 \\over Y + k|V|}\n$$\n\n하지만, 위와 같은 방식은 결국 어떤 확률 값이든지 분자에 1을 더하기 때문에 불평등하게 값을 나눠준다고 할 수 있다. 왜냐하면, 애초에 count(분자)가 큰 데이터에게 1은 별로 영향을 안주겠지만, 분자가 처음부터 작았던 경우에는 이로 인해서 받는 영향이 굉장히 크기 때문이다. 따라서, 이러한 한계점을 극복할 수 있는 방법들이 아래와 같은 방법들이다.\n\n> <mark>**3.Good Turing**</mark>\n\n이를 이해하기 위해서는 우리는 새로운 feature의 데이터를 가져와야 한다. 바로 word의 frequency의 frequency이다.\n\n$$\nN_{k} = \\sum_{i=1}^{n}1[c(w_{i}) = k]\n$$\n\n아마 예시를 봐야 이해가 빠를테니 하나의 예시를 보자.\n\n```plaintext\n sam I am I am sam I do not eat\n```\n\n이 경우 우리는 다음과 같이 count를 구할 수 있다.\n\n$$\n\\begin{align*}\n  &c(I) &=\\ 3 \\\\\n  &c(sam) &=\\ 2\\\\\n  &c(am) &=\\ 2\\\\\n  &c(do) &=\\ 1\\\\\n  &c(not) &=\\ 1\\\\\n  &c(eat) &=\\ 1\\\\\n\\end{align*}\n\\quad\\rArr\\quad\n\\begin{align*}\n  & N_{1} = 3 \\\\\n  & N_{2} = 2 \\\\\n  & N_{3} = 1\n\\end{align*}\n$$\n\n여기서 Good Turing은 한 번도 안본 데이터에게는 한 번만 보는 경우의 수를 전체 경우의 수로 나눈값만큼의 확률을 나누어주고, 기존 데이터들에게는 laplace처럼 1을 더해주는 것이 아니라 비례하는 만큼을 곱해주어 적절한 확률을 가져갈 수 있게하였다.\n\n따라서, 식은 다음과 같다.\n$$\np(w_{i}) = {(c(w_{i})+1)N_{c(w_{i})+1}\\over{N_{c(w_{i})}}} \\times {1\\over |T|},\\quad (N_{0} = 1)\n$$\n\n대게 ${(c(w_{i})+1)N_{c(w_{i})+1}\\over{N_{c(w_{i})}}}$이 부분을 $c^{*}$라고도 부른다.\n\n> <mark>**4. Kneser-Ney**</mark>\n\n가장 널리 쓰이는 Smoothing 방식으로 기억해두는 것이 좋다. 이를 이해하기 위해서는 먼저, Absolute Discounting을 먼저 이해해야 한다.  \nGood-turing 방식을 사용했을 때 $c$와 $c^{*}$사이에 차이가 경험적으로 특정 상수만큼씩 차이가 난다는 것을 발견하여,\n\n$$\nc^{*} = c - d\n$$\n\nChurch과 Gale은 이를 Absolute Discounting 확률이라며 다음 식을 제시한다.\n\n$$\nP(w_{i}|w_{i-1}) = {c(w_{i-1}, w_{i}) -d \\over c(w_{i-1})} + \\lambda(w_{i-1})P(w)\n$$\n\n여기서 뒷에 부분 $\\lambda(w_{i-1})P(w)$은 discounting으로 발생한 오차를 매꾸기 위한 값이다.\n\n여기서 Kneser-Ney problem은 더 넓은 범위로 확장시킬 수 있는 범위로 확장시킨 것이다. 기존에는 bigram으로 제한되어 있던 Absolute Discounting의 식은 다음과 같이 변형된다.\n\n$$\nP_{KN}(w_{i}|w_{i-n+1}^{i-1}) = {\\max(c(w_{i-1}, w_{i}) -d, 0) \\over c(w_{i-n+1}^{i-1})} + \\lambda(w_{i-n+1}^{i-1})P_{KN}(w_{i}|w_{i-n+2}^{i-1})\n$$\n\n(위의 식에 대해서 정확하게 이해를 하지 않았지만, 그렇구나 하고 넘어가도 충분할 것 같다.)\n\n> <mark>**5. Backoff & Interpolation**</mark>\n\n상황에 따라서 unigram, bigram, trigram을 가중치만큼 더해서 사용하는 방식이다. 결국 n-gram에서 n이 작아질 수록 detail을 신경쓸 수 없지만, 신뢰도 자체는 늘어날 수 있다. 따라서, 이를 적절히 섞어쓰면 좋은 결과가 나온다는 이론이다. 하지만, 어떤 것을 더 중점으로 직접 정해주어야 한다.\n\n$$\np^{\\prime}(w_{i}|w_{i-2}, w_{i-1}) = \\lambda_{3}p(w_{i}|w_{i-2}, w_{i-1}) + \\lambda_{2}p(w_{i}|w_{i-1}) + \\lambda_{1}p(w_{i}) + {\\lambda_{0}\\over|V|}\n$$\n\n이를 정할 때는 대게 held-out data를 활용해서 구한다.(validation set이라고 부른다.) 즉, 전체 corpus를 (train, validation, test)로 적절히 나누어 쓰라는 것이다. 그래서 성능을 측정할 때는 testset을 쓰고, $\\lambda$를 추정할 때에는 validation(heldout)set을 사용하라는 것이다.\n\n## Word Class\n\nSmoothing 방식을 이용해서 unseen data를 처리해주었는데 좀 더 효과적으로 이를 처리하는 방법을 고려한 것이다. class단위로 word를 grouping하는 것이다. 그래서 존재하지 않는 단어였다고 하더라도 특정 group에 속한다면, 이를 활용해서 어느정도 확률을 부여할 수 있다는 것이다. 이 방식을 활용하면, 실제로 보지 않은 데이터에 대해서도 현실적인 추정이 가능하지만 detail에 대한 성능은 감소할 수 있다.\n\n$$\np(w_{i}|w_{i-1}) \\rArr p(w_{i}|c_{i-1}) ={ c(c_{i}, w_{i}) \\over c(c_{i-1}) }\n$$\n\n위의 식을 보면, 이전 단어의 context를 보는 것이 아니라 이제는 class를 보고 다음 단어를 probability를 계산하도록 바뀐 것이다. 그리고 이 확률은 class내부에서 해당 단어의 빈도를 이전 class의 빈도로 나누었다고 보면 되겠다.\n\n$$\np(w_{i}|c_{i-1}) = p(w_{i}|c_{i}) \\times p(c_{i}|h_{i})\n$$\n\n즉, class 단위로 단어를 묶고 class에서 단어의 발생 확률에 class에서의 n-gram을 곱한 값이 되는 것이다. 일반적인 Bayes Decison Rule에 기반하여 선택한다고 보면 되겠다.\n\n## Example. Spelling Correction\n\n여태까지 배운 내용을 활용하여 Spelling 오류를 정정해주는 application을 제작한다고 해보자. 먼저, Spelling Error의 종류부터 알아보도록 하자.\n\n- **Non word Error**  \n  잘못된 spelling에 의해서 전혀 뜻이 없는 단어가 만들어진 경우이다.  \n  해결을 위해서는 사전에서 유사한 단어를 찾아서 가장 가능성이 높은 것 또는 이전에 배웠던 shortest edit distance를 찾는 것이다.\n- **Real word Error**  \n  잘못된 spelling 또는 유사한 발음 때문에 뜻이 있는 단어가 만들어졌지만, 오류가 의심되는 경우이다.  \n  해결책은 비슷한 발음 또는 spelling의 모든 단어를 찾아서 해당 단어와 함께 language model에 넣어서 가장 높은 가능성을 가지는 값을 찾는 것이다.\n\n먼저, **Non Word Error** 같은 경우는 오타 데이터에 원래 쓰려고 했던 값을 labeling해서 모아두고 다음 값을 학습시키는 것이다.\n(*x=오타데이터, w=사전에있는단어)\n\n- $P(x|w)$ = x가 w일 가능성\n- $p(w)$ = w의 확률\n\n그리고 나서, 다음을 실행시켜서 가장 적절한 $\\hat{w}$를 찾으면 된다.\n$$\n\\begin{align*}\n\\hat{w} &= \\argmax_{w \\in V}P(w|x) \\\\\n&= \\argmax_{w \\in V}{P(x|w)P(w)}\n\\end{align*}\n$$\n\n**Real Word Error**의 경우에는 결국 이전 단어 sequence를 활용해야 한다. 전체 corpus를 학습해서 tri-gram을 추출해놓고, 번갈아가면서 후보 단어들을 집어넣어서 가장 높은 확률이 나오는 단어를 사용하는 것이다. 예를 들어, 후보 단어가 다음과 같이 정해졌다고 하자. ($\\bold{w}_{3} = {w_3^{(1)}, w_3^{(2)}, w_3^{(3)}, ...}$) 이때 우리가 원하는 w는 다음과 같이 구할 수 있다.\n\n$$\n\\hat{w}_{3} = \\argmax_{w_{3}^{(i)} \\in \\bold{w}_{3}} P(w_{3}^{(i)}| w_{1}, w_{2})\n$$\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-language-modeling","date":"2022-10-21 12:15","title":"[NLP] 3. Language Modeling","category":"AI","tags":["NLP","NoisyChannel","Ngram","LanguageModeling","Smoothing","WordClass"],"desc":"이제 통계적인 관점에서 NL을 input으로 하는 문제를 해결할 방법을 찾을 것이다. 이를 Language Modeling이라고 하며, 이를 위해서 또는 이를 더 효과적으로 하기 위한 방법들을 소개할 것이다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n우리는 Linear Regression, Logistic Regression, SVM을 거치며 data로 부터 유의미한 pattern을 발견하는 과정을 알아보았다. 이 과정은 우리에게 명확한 식 하나를 제시하였고, 모든 과정을 우리가 제어할 수 있게 하였다. 하지만, 실제 데이터를 우리가 모두 명확하게 이해할 수 있는 형태로 분류할 수 있는 것인지는 의문이 들 수 있다. 그렇다면, 우리가 이해하지는 못하지만, 알아서 최적의 결과를 가져오게 할 수 있는 방법이 있을까? 이런 마법같은 일에 대한 아이디어를 제시하는 것이 Neural Network이다.\n  게 알지 못하지만 input이 들어왔을 때, 이를 처리해서 output을 전달하는 시스템을 우리의 신체에서 찾게 된다. 바로 우리 몸을 이루는 신경망이다. 예시로 우리는 눈을 통해 빛이라는 input을 받으면, 우리 눈과 뇌에서 무슨 일이 발생하는지는 모르지만 결과적으로 우리는 물체를 볼 수 있다. 이 과정을 추측의 과정에 도입하면 어떻게 될까?\n\n## Perceptron\n\nPerception(인지 능력) + Neuron(신경 세포)의 합성어이다. 중고등학교 생명 수업을 들었다면, 우리의 모든 신경은 뉴런이라는 단위 세포로 이루어진다는 것을 배웠을 것이다. 즉, 우리의 신경 세포를 컴퓨터 공학에서 활용하기 위해서, 수학적으로 변환한 것이다. 형태를 먼저 살펴보자.\n\n$$\ny = sign(\\bold{w}^{\\top}\\bold{x} + b)\n$$\n\n![nn-perceptron-1](/images/nn-perceptron-1.jpg)\n\n대단한 것을 기대했다면 실망하겠지만, simple한 것이 최고라는 연구의 진리에 따라서 위의 식은 꽤나 합리적이다. 우리가 Linear Regression과 Logistic Regression을 배웠으니 알 것이다. 이는 사실 Linear Regression을 이용해서 우리가 Classification을 수행할 때 사용했던 식이다. 즉, perceptron 하나는 input을 선형으로 구분할 수 있도록 하는 decision boundary를 찾는 것과 같다.\n\n> **Optimization**\n\n그렇다면, 해당 perceptron을 통해서 모든 데이터를 구분하기 위해서는 다음을 만족하는 $\\bold{w}$를 찾아야 한다.\n\n$$\ny_{n} =\n\\begin{cases}\n1  &\\text{ if  } \\bold{x}_{n} \\in \\mathcal{C}_{1} \\\\\n-1 &\\text{ if  } \\bold{x}_{n} \\in \\mathcal{C}_{2} \\\\\n\\end{cases}\n$$\n$$\ny_{n}\\bold{w}^{\\top}\\bold{x}_{n} \\gt 0, \\forall n\n$$\n\n결국 Loss 함수는 perceptron의 잘못된 classification 결과를 최소화하는 것이다.\n\n$$\n\\mathcal{J}(\\bold{w}) = - \\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{w}^{\\top}\\bold{x}_{n}} \\quad( \\mathcal{M}(\\bold{w}) = \\{ n : y_{n}\\bold{w}^{\\top}\\bold{x}_{n} \\} )\n$$\n$$\n\\nabla_{\\bold{w}}\\mathcal{J}(\\bold{w}) = - \\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{x}_{n}}\n$$\n\n따라서, 우리가 사용할 수 있는 Gradient Descent식은 다음과 같다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} + \\alpha\\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{x}_{n}}\n$$\n\n간단한 예시로 AND, OR Gate를 percentron을 통해 표현해보자.\n\n![nn-and-gate](/images/nn-and-gate.jpg)\n![nn-or-gate](/images/nn-or-gate.jpg)\n\n하지만, 우리가 다루는 데이터는 항상 완벽하게 선으로 나뉘어지지는 않는다. 하나의 perceptron으로는 아래의 XOR조차도 구분해낼 수 없다.\n\n![nn-multi-line-example](/images/nn-multi-line-example.jpg)\n\n## Multilayer Perceptron\n\n위의 문제를 해결하기 위해서 나온 것이 perceptron을 다층으로 쌓아서 해결하는 방법이다. 이제는 하나의 신경세포였던 perceptron을 진짜 신경망처럼 연결해보자는 것이다.\n\n먼저 추상적인 예시를 생각해보자. 우리가 XOR Gate를 만들기 위해서는 어떤 Gate를 결합해야할까?\n\n$$\na \\oplus b = ab + \\bar{a}\\bar{b}\n$$\n\n우리는 AND Gate 2개 연산을 수행하고, 해당 결과값을 이용해서 OR Gate 연산을 수행하면 XOR Gate를 표현할 수 있다는 것을 알고 있다. 그렇다면, 각 Gate는 우리가 perceptron으로 나타낼 수 있었는데 그냥 이것을 gate로 표현하듯이 똑같이 나타내면 풀 수 있지 않을까?\n\n그래서 직접 수행해보면 다음과 같은 값을 구할 수 있다.\n\n![nn-xor-gate](/images/nn-xor-gate.jpg)\n\n```plaintext\n 🤔 Insight\n\n 위의 과정을 보다보면 놀라운 것을 하나 발견할 수 있다. 바로 왼 쪽 그림의 변화이다. \n 첫번째, 두 개의 perceptron을 통해서 만들어진 output이 이루는 결과값의 형태로 feature를 변환하면, \n 하나의 perceptron으로 decision boundary를 그릴 수 있다는 것이다. \n 이는 마치 이전 linear regression에서 배웠던 basis function(ϕ)이 했던 역할이다.\n\n 그렇다면, 이를 더욱 확장해보자. \n 만약 해당 Layer가 더 깊어진다고 해도, 출력 직전의 layer는 단순히 이전 모든 layer는 입력 데이터를 가공해서\nfeature를 변환하는 하나의 basis function(ϕ)를 취한 것으로 이해할 수 있다.\n```\n\n결론적으로 우리는 더 복잡하고, 어려운 문제의 경우에도 더 깊게 신경망을 구성하면 결국은 문제를 풀 수 있다는 것이다.\n\n> **Universal Approximation Theorem**\n\n위와 같은 깊은 신경망 구조를 이용하자는 주장도 있지만, 이와 유사하게 넓은 신경망을 쓰자는 주장도 존재했다.  \n\n![nn-universal-approx-theorem-1](/images/nn-universal-approx-theorem-1.jpg)\n\n만약, 우리가 하나의 Layer와 output에서 최종 output perceptron만 갖고 처리를 한다면, 결국 여러 perceptron의 weighted 합으로 볼 수 있다. 그 경우 우리는 계단 함수의 weighted 합으로 생각할 수 있는데 perceptron이 많아질 수록 촘촘해지며 정답과 유사한 추론이 가능해진다.(마치 적분의 개념과 유사하다. 물론 이는 추상적인 설명이기 때문에 실제로는 계단함수의 합이기 때문에 좀 다르다.)\n\n![nn-universal-approx-theorem-2](/images/nn-universal-approx-theorem-2.jpg)\n\n위의 그림을 보면 이해할 수 있다. 하지만, 이 방식은 결국 모든 함수 형태를 기억하는 것이다.(**memorizer**) 이것은 input data가 많아질 수록 복잡도가 급격하게 증가하기 때문에 학습과 예측과정에 굉장히 많은 시간을 소모한다.\n\n> **Multilayer Optimization(Backpropagation)**\n\n그렇다면, 넓은 신경망이 한계가 있으니 선택지는 input과 output 사이의 layer(**hidden layer**)의 갯수를 늘려서 깊은 신경망을 만드는 것이다. 하지만, 우리가 사용하고 있는 perceptron은 sign함수로 감싸져있기 때문에 미분 시에 기울기가 0이라는 문제를 갖는다. 또한, 그렇다고 정답의 갯수를 이용하기에는 각 perceptron의 영향을 전달하기에 부족하다는 것이 명확하다. 따라서, 우리는 perceptron에 있는 정답을 판별하는 함수 sign을 다른 함수로 대체하기로 한다.\n\n![nn-perceptron-2](/images/nn-perceptron-2.jpg)\n\n여기서 이 함수를 우리는 **activation function**이라고 부르고 대표적으로는 같은 종류가 있다.\n\n- **sigmoid**  \n  우리가 가장 쉽게 생각할 수 있는 함수이다. logistic regression에서 사용해본만큼 기울기값을 효과적으로 가질 수 있다.\n- **tanh**  \n  sigmod와 굉장히 유사한 함수이다. 따라서, 비슷한 용도로 사용될 수 있다.\n- **ReLU**  \n  출력 시점에서는 사용하지 않지만, 각 각의 hidden layer에서 이를 사용하는 경우가 많다. 왜냐하면, sigmoid 함수는 출력값의 형태가 [0, 1], tanh는 [-1, 1]이기 때문에 반복해서 적용하면, gradient가 사라지는 현상이 발생할 수 있다. 따라서, 기울기를 있는 그대로 적용할 수 있는 이러한 형태를 출력 이전에는 많이 사용한다.\n- **Leaky ReLU**  \n  ReLU가 음수값을 완전히 무시하는데 Leaky ReLU는 이러한 데이터가 조금이라도 의미 있는 경우에 사용할 수 있다.\n- **ELU**  \n  Leaky ReLU와 비슷한 이유이다.\n\n  ![activation-functions](/images/activation-functions.png)\n  \n자료가 보이지 않는다면 [🔗 wikipedia](https://en.wikipedia.org/wiki/Activation_function)를 참고하자.\n\n---\n\n자 이제 실제로 어떻게 optimization을 수행할지를 알아보도록 하자.\n\n먼저, Loss는 가장 마지막 layer(output layer)의 output과 실제 값과의 차이가 될 것이다. 따라서, 다음과 같이 정의할 수 있다.\n\n(아래서 $\\bold{h}_{L}$은 L번째 layer의 output을 의미한다.)\n\n$$\n\\begin{align*}\n\\mathcal{L} &= \\sum_{n=1}^{N}{\\ell(y_n, \\bold{h}_L)} \\\\\n&= \\sum_{n=1}^{N}{(y_{n} - \\bold{h}_{L})^2} \\\\\n&= \\sum_{n=1}^{N}{(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))^2} \\\\\n&= \\sum_{n=1}^{N}{(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\sigma(\\bold{w}_{L-1}^{\\top}\\bold{h}_{L-2} + b_{L-2}) + b_{L-1}))^2} \\\\\n&= ...\n\\end{align*}\n$$\n\n여기서 중요한 것은 우리는 전체 $\\bold{W}$를 학습시켜야 한다는 것이다. 우리는 출력층만 학습하는 게 아니라 전체 모든 layer의 $\\bold{w}_{i}$를 업데이트해야 한다는 것이다.\n\n그러기 위해서는 우리는 ${{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{i}}}$를 모두 구해야 한다는 것이다. 아마 가장 습관적으로 하는 행위는 숫자가 작은 값부터 편미분하면서 진행하는 것이다. 하지만, 그렇게 하지말고 반대 순서로 미분을 하라는 것이 **backpropagation**의 main idea이다.\n\n$$\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L}}} = \\sum_{n=1}^{N}\\{({\\partial \\bold{h}_L \\over \\partial \\bold{w}_{L}} )\\times\\red{-2(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))}\\}\n$$\n\n$$\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-1}}} = \\sum_{n=1}^{N}\\{({\\partial \\bold{h}_{L} \\over \\partial \\bold{w}_{L-1} })\\times\\red{-2(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))}\\}\n$$\n\n즉, 다음과 같은 chain rule을 이용하는 것이다.\n\n$$\n\\begin{align*}\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L}}} &= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L}}} \\\\\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-1}}} &= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n&= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{h}_{L-1}}} {{\\partial\\bold{h}_{L-1}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n&= \\blue{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L-1}}} } {{\\partial\\bold{h}_{L-1}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-2}}} &= \\blue{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L-1}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L-2}}} \\\\\n\\end{align*}\n$$\n\n우리는 빨간색과 파란색 부분의 연산을 재활용할 수 있다는 것이다. 또한, ${{\\partial\\bold{h}_{l}}\\over{\\partial \\bold{w}_{l}}}$은 굉장히 쉬운 연산이기에 우리가 신경 써서 계산해야 할 값은 매단계를 연결해줄 $ {{\\partial\\bold{h}_{l}}\\over{\\partial \\bold{h}_{l-1}}}$이다.\n\n![ml-backpropagation](/images/ml-backpropagation.jpg)\n\n## Loss Function\n\n우선 KL-Divergence, Entropy, Cross Entropy에 대한 약간의 이해가 필요하니 이전 Posting([🔗 Base Knowledge](posts/ml-base-knowledge))을 살펴보고 오자.\n\n위에서는 자연스럽게 Loss를 계산할 때, Squared Error를 사용하였다. 하지만 경우에 따라서는 다양한 함수를 사용할 수 있다. multiclass classification에서는 **Cross Entropy Loss**를 사용한다.\n\n우선 Cross Entropy Loss는 대게 L2 Loss(Squared Error)와 같이 비교되어진다. 우선 우리가 이전 [🔗 Parametric Estimation](posts/ml-parametric-estimation)에서 MLE를 다룰 때, KL-Divergence를 통해서 MLE가 최적 parameter를 찾을 것이라는 걸 증명한 적이 있다. 그렇다면, 우리가 [🔗 Logistic Regression](/posts/ml-logistic-regression)에서 Squared Error를 통해서 Loss를 구했던 공식을 확인해보자.(Gradient Asecent Part)\n\n여기서 우리는 다음과 공식을 봤었다.\n\n$$\n\\argmax_{\\bold{w}} \\sum_{n=1}^{N}y_{n}\\log{\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}) + (1-y_{n})\\log{(1-\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}))} }\n$$\n\n이 공식을 Cross Entropy를 통해서 설명할 수 있다.\n\n$$\n\\begin{align*}\nH_{q}(p) &= - \\sum_{x \\in \\Omega}q(x)\\log_{2}p(x) \\\\\n&= \\sum_{n=1}^{N}{[-y_{n}\\log\\hat{y}_{n} - (1- y_{n})\\log(1-\\hat{y}_{n})]}\n\\end{align*}\n$$\n\n즉, 여기서 우리가 얻을 수 있는 insight는 Cross Entropy는 sigmoid를 취한 binary classification에서 Squared Error와 같고, 이러한 Cross Entropy를 Squared Error가 할 수 없는 Multiclass에는 적용할 수 있을 것이라는 점이다. 왜냐하면, multiclass classification에 사용되는 Softmax Function을 이용해서 Sigmoid function을 유도하기 때문이다. 잠시 까먹었을까봐 Softmax 함수를 다시 적는다.\n\n$$\n\\hat{y}_{k} = {{\\exp(\\bold{w}_{k}^{\\top}\\bold{x})}\\over{\\sum_{i=1}^{K}{\\exp(\\bold{w}_{i}^{\\top}\\bold{x})}}}\n$$\n\n따라서, Cross Entropy Loss를 대입하여 다음과 같은 Loss를 얻을 수 있다.\n\n$$\n\\mathcal{L} = \\sum_{n=1}^{N}\\sum_{k=1}^{K}[-y_{k,n}\\log\\hat{y}_{k,n}],\\quad y_{k,n} = p(x_{n} \\in C_{k}| x_{n})\n$$\n\n여기서 $y_{k,n}$은 one-hot encoding된 데이터로, 정답인 class만 1이고 나머지는 모두 0으로 되어 있다. 따라서, multiclass classification에서는 위와 같은 Loss를 주로 사용한다.\n\n이 두가지 뿐만 아니라 여러가지 Loss Function이 이미 존재한다. 예전에 잠깐 설명했던 L1 Loss부터 시작해서 NLLLoss, KLDivLoss 등등 존재하며, data의 특성과 output의 형태에 따라서 우리는 스스로 Loss Function을 새로 정의할 수도 있다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- activation function, wikipedia, <https://en.wikipedia.org/wiki/Activation_function>\n","slug":"ml-nn","date":"2022-10-20 09:00","title":"[ML] 6. Neural Network","category":"AI","tags":["ML","NeuralNetwork","Perceptron","Backpropagation","CrossEntropyLoss"],"desc":"우리는 Linear Regression, Logistic Regression, SVM을 거치며 data로 부터 유의미한 pattern을 발견하는 과정을 알아보았다. 이 과정은 우리에게 명확한 식 하나를 제시하였고, 모든 과정을 우리가 제어할 수 있게 하였다. 하지만, 실제 데이터를 우리가 모두 명확하게 이해할 수 있는 형태로 분류할 수 있는 것인지는 의문이 들 수 있다. 그렇다면, 우리가 이해하지는 못하지만, 알아서 최적의 결과를 가져오게 할 수 있는 방법이 있을까? 이런 마법같은 일에 대한 아이디어를 제시하는 것이 Neural Network이다.  게 알지 못하지만 input이 들어왔을 때, 이를 처리해서 output을 전달하는 시스템을 우리의 신체에서 찾게 된다. 바로 우리 몸을 이루는 신경망이다. 예시로 우리는 눈을 통해 빛이라는 input을 받으면, 우리 눈과 뇌에서 무슨 일이 발생하는지는 모르지만 결과적으로 우리는 물체를 볼 수 있다. 이 과정을 추측의 과정에 도입하면 어떻게 될까?","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nNLP를 수행할 때, 우리는 sequence of character를 처리하는 방식을 제대로 알아야 제대로 된 전처리와 후처리 등이 가능하다. 따라서 해당 chapter에서는 어떻게 원본 문자/단어/문장을 처리하는 방식을 다룰 것이다.\n\n## Regular Express\n\n아주 기본적인 문자열 처리 방법이다. 이를 알고 있어야 실질적인 처리가 가능하다. 해당 내용은 별도의 Posting으로 분리하여 다루었다. ([🔗 Regex](/posts/regex))를 살펴보도록 하자.\n\n## Text Normalization\n\n우리가 사용할 NL은 정제되어 있지 않아서 여러 전처리를 수행해야 한다. 그 중에서 대중적으로 좋다고 알려진 방법들을 살펴볼 것이다. 기본적으로는 아래 단계를 처리하는 것이 일반적이다.\n\n1. Word Tokenization  \n   말 그대로 NL 데이터가 입력되었을 때, 이를 단어 단위로 쪼개는 것이다.\n2. Word Reformating  \n   단어를 나누었다면, 각 단어의 형태를 처리하기 쉬운 형태로 Normalizing하는 것이다.  \n3. Sentence Segmentation  \n   문장 단위로 구분해는 과정이다.\n\n이제 각 단계를 세부적으로 다뤄보겠다.\n\n### 1. Word Tokenization\n\n우선 쉽게 생각할 수 있는 것은 단순히 띄어쓰기를 기준으로 구분하는 것이다. 그렇게 하면, 우리는 입력으로 주어진 Corpus에서 token을 추출할 수 있다.\n\n하지만, \"San Francisco\"와 같은 단어가 두 개의 token으로 나누는 것이 아니라 하나의 token으로 처리되기를 원할 수 있다. 뿐만 아니라 일부 언어들(특히 중국어와 일본어)의 경우 띄어쓰기 없이 작성하는 언어들의 경우 문제는 더 커질 수 있다. 이 경우에는 **Word Segmenting**이라는 알고리즘을 활용할 수도 있는데, 원리는 매우 간단하다. 언어의 모든 단어를 포함하는 사전을 기반으로 문장에서 사전에 일치하는 가장 긴 문자열을 찾을 수 있을 때까지 token을 연장해서 만드는 방식이다.\n\n그러나 이 방식도 결국은 특정 언어(중국어, 등)에서는 잘 작동하지만, 일부 언어(영어 등)에서는 잘 작동하지 않는 경우가 많다. 따라서, 최근에는 확률에 기반하여 같이 등장하는 횟수가 많을 경우 하나의 token으로 묶는 형태의 tokenization을 더 선호한다.\n\n이 과정에서 우리가 추가적으로 수행하는 것이 바로 word의 갯수를 추출하는 것이다. 대게 우리가 관심 있어하는 수는 총 3가지이다.\n\n1. **number of tokens**  \n   즉, 띄어쓰기로 나뉘어지는 token들의 총 갯수를 의미한다.\n2. **number of types(Vocabulary)**  \n   띄어쓰기로 나뉘어진 token들의 중복을 제거한 종류들의 갯수를 의미한다. 대게 이러한 type들의 모음을 Vocabulary라고 한다.\n3. **number of each type's tokens**  \n   각 종류의 token이 얼마나 많이 등장했는지를 의미한다.\n\n여기서 이러한 token이나 type이 서로 같나는 것을 어떻게 구분할 수 있을까? 이를 위해서 Word Reformating을 수행하여 좀 더 일반적인 형태로 변형하여 위의 수들을 파악하기도 한다.\n\n### 2. Word Normalization and Stemming(Word Reformating)\n\n대게의 언어는 word의 형태가 여러 개로 존재한다. 이 과정에서 우리가 고려해야 할 것이 정말 많다. 그 중에서 가장 기본적으로 수행되어야 할 내용은 다음과 같다. 해당 내용은 영어에 중심을 둔 설명이다.\n\n1. **Uppercase**  \n   영어에서 첫 글자는 대문자로 시작한다는 규칙이 있다. 또는 강조하고 싶은 단어를 대문자로 표현하기도 한다. 그 결과 token의 종류를 추출하는 과정에서 문제를 일으키기도 한다. 따라서, 이를 모두 lowercase로 바꿔버리는 것이다. 하지만, 모든 경우에 이를 적용할 수 잇는 것은 아니다. 가장 대표적인 예시로 US와 us의 의미가 다르다는 것이다. 또, 고유 명사인 General Motors와 같은 경우도 다르게 처리하는 것이 좋다. 따라서, 이를 고려해서 먼저 처리한 이후에 전체 데이터를 lowercase로 변환하는 방식을 수행한다.  \n2. **Lemmatization**  \n   Lemma(기본형, 사전형)로 단어를 변환하는 것이다. 가장 기본적인 것은 am, are, is와 같은 be동사를 모두 be로 변환하거나 car, cars, car's를 모두 기본형태인 car로 바꾸는 것이다. 대게의 경우에는 이 과정에서 의미를 일부 잃어버리기 때문에 lemma + tag로 기존 token을 복구할 수 있도록 하는 tag를 포함하는 것이 좋다.\n3. **Stemming**  \n   morpheme(형태소)은 중심 의미를 가지는 stem과 핵심 의미는 아니지만 stem에 추가 의미를 더해주는 affixes로 나누어 word를 나눌 수 있다. 따라서, 각 token을 가장 core의 의미를 가지는 stem으로 나타내는 방식이다. 대표적인 예시가 automate, automatic, automation을 automat으로 변환하는 것이다. 이는 lemmatization보다 넓은 범위의 word를 하나로 묶기 때문에 세부의미가 더 손실될 수 있다. 따라서, 기존 의미로 복구할 수 있는 tag를 포함하는 것이 좋다.\n\n### 3. Sentence Segmentation\n\n문장을 구분할 수 있는 도구로 우리는 \"?\", \"!\", \".\"을 활용한다. \"?\"와 \"!\" 같은 경우는 문장의 끝을 의미하는것이 대게 자명하다. 하지만, \".\"은 꽤나 애매할 수 있다. 소수점, Abbreviation(Mr., Dr., Inc.)와 같은 경우에 빈번하게 사용되기 때문이다. 따라서, 이를 판단하기 위해서 Decision Tree를 만들어서 이를 수행한다. 아래와 같이 사람이 직접 규칙을 정할 수도 있지만 현재는 대게 통계 기반으로 수행한다.\n\n![nlp-sentence-segmentation](/images/nlp-sentence-segmentation.jpg)\n\n## Collocation(연어) processing\n\nText Normalization을 통해서 우리는 sentence를 구분하고, word를 추출할 수 있었다. 하지만, 단순히 하나의 word를 기반으로 처리하는 것이 아니라 주변 단어를 활용하여 처리해야만 얻을 수 있는 정보들이 있다. 우리는 이를 Collocation(연어)를 활용하여 수행한다. 이는 특정 단어쌍이 높은 빈도로 같이 붙어 사용되는 현상을 말한다. **\"모든 단어는 이를 동반하는 주변 단어에 의해 특성 지어진다.\"** 따라서, 우리는 이 collocation을 co-ocurrence로 생각할 수 있다. 이를 통해서 우리는 다음과 같은 것들을 할 수 있다.\n\n1. **lexicography(사전 편찬)** : 같가니 유사한 뜻을 가지는 단어는 빈번하게 붙어서 사용되는데 이를 이용해서 하나의 단어의 뜻을 안다면, 이를 통해서 다른 단어의 뜻을 추론하며 확장해나갈 수 있다.\n2. **language modeling** : NL를 통해서 원하는 결과를 얻기 위해서 특정 parameter를 추정해내는 것을 language modeling이라고 하는데 이 과정에서 collocation을 활용하는 것이 단일 단어를 활용하는 것보다 context를 활용할 수 있다는 점에서 장점을 발휘할 수 있다.\n3. **NL generation** : 우리는 문맥상 매끄러운 문장을 원한다. 즉, \"감을 잡다\"를 \"감을 붙잡다\"라고 했을 때, 뜻을 이해할 수는 있지만 어색하다고 느낀다. 따라서, 이 관계를 활용해서 NL을 생성해야 하기 때문에 collocation을 고려해야 한다.\n\n그렇다면, 이러한 Collocation을 어떻게 찾을 수 있을까?\n\n1. **Frequency**  \n   가장 간단하게 단순히 동시 발생 빈도를 확인하는 것이다. 정확한 파악을 위해서는 빈번하게 등장하는 의미 없는 단어를 먼저 filtering할 필요가 있다. 대표적인 예시로 a, the, and 등이 있다.\n2. **Hypothesis Testing**  \n   가설 검증으로 우리가 가정한 collocation을 지정하고, 이 사건이 일어날 가능성을 굉장히 낮게 하는 가설을 반대로 가정한 후에 이것이 불가능하다는 것을 증명하는 Null Hypothesis를 이용한 증명으로 타당성을 확보하는 것이다. 따라서, 우리가 보이고자 하는 것은 word1, word2가 있을 때, 두 단어가 서로 의존적이라는 것을 증명하고 싶은 것이다. 따라서, Null Hypothesis로 두 단어는 독립이다라고 지정하면, 우리는 다음 식을 얻을 수 있다.  \n   $p(w_{1}, w_{2}) = p(w_{1})p(w_{2})$  \n   이를 바탕으로 t-검증을 다음과 같이 수행할 수 있다.  \n   $t = {{p(w_{1}, w_{2}) - p(w_{1})p(w_{2})} \\over \\sqrt{p(w_{1}, w_{2})\\over{N}}} $  \n   t값이 이제 커질 수록 우리는 해당 가설이 틀렸음을 증명하여 collocation임을 주장할 수 있다.\n\n## Minimum Edit Distance\n\n단어 또는 문장 간 유사도를 측정할 때, 사전을 기반으로 수행할 수도 있지만 참고할 corpus가 마땅하지 않거나 더 추가적인 수치가 필요하다면, Minimum Edit Distance로 유사도를 측정하기도 한다. 즉, 두 문자열이 같아지기 위해서 어느정도의 수정이 필요한지를 수치화한 것이다. 여기서 연산은 새로운 문자 추가, 삭제, 대체만 가능하다.\n\n```plaintext\nS - N O W Y  | - S N O W - Y\nS U N N - Y  | S U N - - N Y\ndistance : 3 | distance : 5\n```\n\n다음과 같이 표현이 가능하다.\n\n1. 문자열 x, y가 있을 때, $E(i, j)$는 x의 0\\~i까지를 포함하는 문자열과 y의 0~j까지를 포함하는 문자열의 distance라고 하자.\n2. 이렇게 되면, $E(i,j)$에서 우리는 끝문자의 규칙을 볼 수 있다.\n\n    오른쪽 끝 문자가 가질 수 있는 조합은 3가지 밖에 없다.\n\n    ```plaintext\n    x[i]        | -           | x[i]\n    -           | y[j]        | y[j]\n    distance: 1 | distance: 1 | distance: 0 or 1\n    ```\n\n3. 그렇다면 우리는 하나의 사실을 알게 된다.\n\n    $E(i,j)$는 다음 경우의 수 중 하나여야만 한다.\n\n    - $E(i-1, j) + 1$\n    - $E(i, j-1) + 1$\n    - $E(i-1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )$\n4. 따라서, 다음과 같은 식을 유도할 수 있다.\n\n    $E(i, j) =\\min( \\\\\n      \\quad E(i-1, j) + 1, \\\\\n      \\quad E(i, j-1) + 1,  \\\\\n      \\quad E(i - 1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )\\\\\n    )$\n\n만약, 각 연산의 비용이 다를 경우라면, 1 대신에 그 값을 넣어주면 충분히 풀 수 있으며, 추가적으로 최적의 이동형태를 알고 싶다면, back pointer 하나를 추가하는 것으로 충분하다.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-text-processing","date":"2022-10-19 21:59","title":"[NLP] 2. Text Processing","category":"AI","tags":["NLP","Regex","Tokenization","Collocation","MinimumEditDistance"],"desc":"NLP를 수행할 때, 우리는 sequence of character를 처리하는 방식을 제대로 알아야 제대로 된 전처리와 후처리 등이 가능하다. 따라서 해당 chapter에서는 어떻게 원본 문자/단어/문장을 처리하는 방식을 다룰 것이다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nNatural Language(자연어, 사람이 사용하는 통상 언어)를 input으로 활용하고자 하는 노력은 컴퓨터의 등장부터 시작하여 여러 번 시도되어 왔다. 지금까지도 완벽하게 이를 처리하는 것은 힘들다. 왜 Natural Language를 다루는 것은 어렵고, 이를 해결하기 위해서 NLP에서는 어떤 방식을 활용할지에 대한 개략적인 overview를 제시한다. 또한, Natural Language의 특성과 분석 단계를 이해하기 위해서 Linguistics(언어학)을 간략하게 정리한다.\n\n## NLP\n\nNatural Language Processing의 약자로 사람이 사용하는 언어를 input으로 하여 원하는 값을 추출해내는 것이 목표이다. 이를 위해서 우리는 사람의 언어를 이해하거나 다룰 수 있는 능력을 컴퓨터에게 부여해야 한다.\n\n먼저, 이러한 필요가 있는 대표적인 usecase를 살펴보면 다음과 같다.\n\n### Usecase\n\n- **Spam Detection**  \n  가장 간단한 예시로 mail에서 spam 여부를 확인하는 기능이다.\n- **POS tagging / NER**  \n  특정 단어 단위의 처리를 수행하게 되는데 단어의 품사와 대략적인 의미를 가진 category로 분류로 tagging하는 과정이다. 이를 기반으로 하여 다른 usecase에서 활용하는 경우가 많다. 품사와 category는 단어의 뜻을 추론하는데 큰 도움을 주며, 이것이 문장의 이해 등에 도움을 주기 때문이다.\n- **Sentiment Analysis**  \n  감정/여론 분석 등의 영역을 의미하며, 텍스트 또는 대화에서의 긍정/부정 여부를 판단하거나 평점 등을 추출하는 기능이다.\n- **Conference Resolution**  \n  \"he\", \"she\" 등 대명사, 생략 단어 등을 원래의 단어로 대체하거나 채우는 과정을 수행한다. 이 역시도 여러 영역에서 이를 기반으로 추가적인 작업을 할 수 있다.  \n- **Word Sense Disambiguation(WSD)**  \n  특정 단어가 주어졌을 때, 동의어, 동음이의어 등에서 가르키는 진짜 의미를 헷갈리지 않게 명확하게 다시 한 번 처리한다. 이 역시도 다른 NLP usecase에서 두루 사용된다.\n- **Parsing**  \n  문장에서 단어들을 의미를 가지는 단위(구, 절, 문장)로 다시 grouping한다. 이 과정을 잘 수행하기 위해서는 이전 단계에서 WSD와 Conference Resolution, POS tagging, NER이 이루어지면 좋다. 이 과정을 통해서 문장의 개략적인 의미를 파악할 수 있다.\n- **Machine Translation(MT)**  \n  특정 언어를 또 다른 Natural Language로 변경하는 기능이다.\n- **Information Extraction(IE)**  \n  특정 문장에서 사용자에게 의미있을만한 데이터를 추출하는 것이다.\n- **Q&A**  \n  특정 사용자가 질문을 하였을 때, 이 뜻을 이해하고, 이에 적절한 대답을 수행하는 방식이다.\n- **Paraphrase**  \n  문장의 뜻을 이해하고, 더 쉬운 형태의 표현으로 변환하는 기능이다.\n- **Summarization**  \n  여러 문장으로 이루어진 글의 의미를 이해하고, 적절한 내용으로 요약하는 기능이다.\n- **Dialog**  \n  Natural Language를 사용하는 사람과 1:1로 담화를 주고 받는 것이다. 의미를 이해할 뿐만 아니라 자신이 내보낼 output에 대해서도 적절하게 생성할 수 있는 능력이 필요하다.\n\n위와 같이 많은 usecase가 있는데 이를 구현하는 것은 지금까지도 굉장히 challenge한 부분이다. 그것은 Natural Language가 가지는 몇몇 특징 때문이다.\n\n### Why is NLP difficult?\n\n여기서는 Natural Language 중에서 영어를 기반으로 한 설명이지만, 한국어도 매우 유사하다.\n\n- **non-standard** : Natural Language를 사용하는 사람들이 표준을 항상 따르지는 않는다는 것이다. 우리는 약어를 사용하거나 문법에 맞지 않는 비문을 사용하여 의사소통을 하기 때문에 이것을  시스템이 이해하게 하는 것은 어렵다.\n- **segmentation** issues : 의미를 가지는 단어 단위로 묶는 것이 어렵다는 것이다. 우리는 문장의 띄어쓰기를 어디로 받아들이냐에 따라서 의미가 달라지는 것을 본 경우가 있을 것이다.\n- **idioms** : 관용구의 사용은 NLP에서 예외처리로 해주어야 하는 것이다. 단어 그대로의 의미와 다른 의미를 가지기 때문이다.\n- **neologisms** : 신조어는 계속해서 생겨나기 때문에 이를 계속해서 업데이트 해주는 것도 부담이 된다.\n- **world knowledge** : 사전 지식을 알고 있어야 이해할 수 있는 단어, 문장이 존재한다. 즉, 어떤 지식을 가지고 있느냐에 따라서 해석이 달라진다는 것이다.\n- **tricky entity names** : 고유 명사 중에서 특히 contents(노래, 그림, 소설) 등의 제목이 해석 시에 헷갈리게 한다. 예를 들면, \"Let it be\"라는 비틀즈의 노래는 문장 중간에 들어가면, 하나의 문장으로 받아들여지게 되는데 이를 잘 해결할 수 있도록 해야 한다.\n\n위의 내용을 요약하자면, 다양한 단어가 다양한 현상과 다양한 법칙(Grammer)의 영향을 받기에 어려우며, 단어가 가지는 모호성이 문제를 야기한다는 것이다.\n\n### Solutions\n\n이러한 문제를 해결하기 위해서 크게 두 가지 방식을 사용할 수 있다.\n\n- Rule based approach  \n  Gammer와 같은 법칙을 모두 적용해서 prgoramming을 하는 것이다. 하지만, 이 방식은 비문과 같은 문장을 제대로 처리할 수 없을 뿐만 아니라 정확한 형태의 문장이라도 여러 의미로 해석되는 문장에서 경향성과 문맥을 전혀 파악할 수 없다.\n- Statistic based approach  \n  그래서 최근에는 경향성과 문맥을 파악할 수 있도록 AI 기술, ML, Deep Learning을 이용하여 NLP를 수행하는 것이 하나의 trend로 자리 잡았다. 그렇다면, 어떻게 통계적인 접근법이 경향성과 문맥을 포함할 수 있을까? 이는 통계가 가지는 경향성이라는 특징과 conditional probability를 사용할 때의 문맥을 포함한 경향성을 파악할 수 있다는 점을 활용해서 가능하다.\n\n## Linguistics\n\n결국 앞으로 통계적인 방식을 활용하더라도 우리는 최소한의 언어학적인 기본이 필요하다. 왜냐하면, 통계에 사용할 데이터를 처리하기 위해서이다. 우리가 사용할 데이터는 text 또는 음성이다. 이를 적절하게 처리하여 통계에 사용할 유의미한 데이터로 변환하는 과정이 필요하다. 이를 위해서 언어학에 대한 이해가 필요한 것이다.\n\n일반적으로 언어를 분석할 때, 사용할 수 있는 도구는 **Grammar**이다. 이는 특정 language에서 허용되는 규칙의 집합을 정리한 것이다. 이것의 종류는 크게 두 가지로 나뉜다.\n\n- **Classic Grammar**  \n  사람이 실제로 언어를 사용함에 있어 발생하는 이상한 습관과 같은 언어 표현이다. 이러한 법칙들은 대게 예제들을 통해서 정의되는데 이런 것을 명확하게 구분할 수 있는 명백한 도구가 존재하지는 않는다. 예를 들면, 감탄사와 같은 것들이 여기에 포함되겠다. 이는 이러한 변칙적인 형태 때문에 programming적으로 표현하는 것이 불가능하다.\n- **Explicit Grammar**  \n  명백하게 정의되어 있는 언어 규칙을 의미한다. 이는 Programm으로 구현할 수 있으며, 여러 Grammar 정리 내용이 이미 정리되어 있다. (CFG, LFG, GPSG, HPSG, ....)  \n  이를 문법적으로 분석하기 위해서 우리는 6단계의 순차적인 처리가 필요하다.\n\n### 6 Layers in Language\n\n각 단계는 input과 output을 가진다. 단계적으로 진행되기 때문에 이전 단계의 output이 다음 단계의 input이 되며, 때때로 몇 단계는 생략될 수 있기에 유연하게 생각하도록 하자.\n\n각 단계에서 실제로 특정 문장이 처리되는 과정을 이해하기 위해서 \"Astronomers saw stars with telescope\"라는 문장이 음성 또는 text로 들어왔을 때를 가정하여 각 단계에는 무엇을 하고 이를 통해서 어떻게 이 문장을 바꿀 수 있는지를 확인해보겠다.\n\n> **1. Phonetics/Orthography(음성학/맞춤법)**\n\n먼저 Orthography는 맞춤법 검사를 의미하며, character sequence로 input이 들어오면, 이를 맞춤법에 맞는지를 확인하여 이것이 수정된 sequence로 반환한다.  \n예시 문장에 있는 \"telescope\"는 문법에 맞지 않으므로 \"telescopes\"로 바뀌어야 한다.\n\n| input                                 | output                                 |\n| :------------------------------------ | :------------------------------------- |\n| Astronomers saw stars with telescope. | Astronomers saw stars with telescopes. |\n\nPhonetics는 음성학을 의미하며, 혀와 음성의 영향을 주는 다양한 근육의 위치 형태, 모양, 빈도를 활용하여 자음과 모음을 분류하는 작업을 수행한다. Orthoography와는 달리 억양이라는 것을 추가적으로 활용할 수 있다.\n\n| input                                                       | output                                 |\n| :---------------------------------------------------------- | :------------------------------------- |\n| Astronomers saw stars with telescopes.를 의미하는 음성 신호 | əsˈtrɒnəməz sɔː stɑːz wɪð ˈtɛlɪskəʊps. |\n\n*<https://tophonetics.com/> 을 통해서 변환하여 얻을 수 있다.\n\n> **2. Phonology/Lexicalization(음운론/어휘화)**\n\nPhonology은 음운론으로 소리와 phonemes(음소)사이의 관계를 이용하여, 음소를 특정 word로 변환하고, Lexicalization에서는 해당 단어를 사전에서의 형태로 변환하는 과정을 수행한다.\n\n| input                                  | output                                 |\n| :------------------------------------- | :------------------------------------- |\n| əsˈtrɒnəməz sɔː stɑːz wɪð ˈtɛlɪskəʊps. | Astronomers saw stars with telescopes. |\n\n> **3. Morphology(어형론)**\n\nMorphology는 어형론으로 음소의 구성을 기본형(lemma)의 형태로 변환하며, 각 단어들을 형태학적인 의미를 갖는 카테고리(category, tag)로 분류한다.\n여기서 사용되는 lemma와 category가 무엇인지 좀 더 자세히 살펴보자.\n\n- **lemma**  \n  - 사전에 표기되는 단어의 기본형으로, 사전에서 word를 찾는 pointer가 된다.  \n  - 동음이의어의 경우 특정 뜻을 지칭하고 싶은 경우에는 numbering을 수행하기도 한다.\n  - 더 나아가서는 형태소(morpeheme)까지 구분하기도 한다. 이는 혼자서 쓰일 수 있는 자립 형태소(root)와 의존 형태소(stem)으로 나눌 수 있다.  \n    - 예를 들면, quotations -> quote[root] + -ation[stem] + -s[stem]\n    - 위와 같은 형태로 세분화할 수도 있지만, 대게는 lemma 단위에서 그친다.\n- **categorizing**  \n  - category는 정하기 나름이며, 이미 정해져있는 tagset들도(Brown, Penn, Multext) 많이 존재하고, 억양이나 실제 분류 등을 수행하는 것도 가능하다.\n  - **POS tagging**은 category를 분류하는 방법 중에서 가장 유명한데, 이는 여러 언어에서 거의 호환되기 때문에 이 방식을 활용하여 분석하는 것이 가장 안정적인 방법이라고 할 수 있다. 이는 별도의 Posting에서 더 자세히 다루도록 하겠다.\n\n또한, 단어의 형태는 언어마다 다양하기 때문에 어느정도 언어마다 다른 작업을 해주어야 한다. 크게 구분되는 형태로 언어를 3개의 종류로 나눌 수 있다.\n\n1. **Analytical Language(고립어)**  \n   하나의 단어가 대게 하나의 morpheme을 가진다. 따라서, 하나 이상의 category로 구분되어질 수 있다.  \n   ex. English, Chinese, Italian\n2. **Inflective Fusional Language(굴절어)**  \n   prefix/suffix/infix가 모두 morpheme에 영향을 미치며, morpheme의 정의 자체가 애매해지는 언어 형태  \n   ex. Czech, Russian, Polish, ...\n3. **Agglutinative Language(교착어)**  \n   하나의 단어에는 morpheme이 명확하게 구분되고, prefix/suffix/infix 또한 명확하게 구분 가능하다. 따라서, 각 morpheme에 명확한 category를 mapping하는 것이 가능하다.  \n   ex. Korean, ...\n\n| input                                  | output(based on Brown tagset)                                         |\n| :------------------------------------- | :-------------------------------------------------------------------- |\n| Astronomers saw stars with telescopes. | (astronomer/NNS) (see/VBD) (star/NNS) (with/IN) (telescope/NNS) (./.) |\n\n> **4. Syntax(통사론)**\n\nlemma나 morpheme을 구문의 요소인 S(Subject, 주어), V(Verb, 동사), O(Object, 목적어)와 같은 요소로 분류한다. 이 분류를 수행할 때에는 문장의 구성요소를 알아야 한다. 이를 bottom-up으로 살펴보자.\n\n- **Word(단어)**  \n  사전에 명시된 하나의 단위라고 볼 수 있다. 이는 관용어(dark horse)를 포함한다.\n- **Phrase(구)**  \n  둘 이상의 단어 또는 구의 결합으로 만들어진다. 대게 하나의 문법적인 의미로 변환되어진다.  \n  - 대표적인 예시\n    - Noun : a new book\n    - Adjective : brand new\n    - Adverbial : so much\n    - Prepositional : in a class\n    - Verb : catch a ball\n  - **Elipse(생략)**  \n    대게 단어 또는 구가 생략되는 경우가 많다. 특히 담화의 경우 더욱 그렇다.  \n    이를 추론을 통해서 추가할 수도 있다.\n- **Clause(절)**  \n  절은 주어와 서술어를 갖춘 하나의 문장과 유사하지만, 문장 요소로서 더 상위 문장에 속하는 경우이다.  \n  또한, 영어에서는 특히 접속사로 연결된 절이 아닌 경우에는 해당 절이 지칭하는 대상이 절 내부에서 생략된다. 이를 gap이라고 한다.\n- **Sentense**  \n  하나 이상의 절로 이루어지고, 영어에서는 시작 시에 대문자로 표기하며 종료 시에는 구분자로 .?!로 끝난다.\n\n결국 우리는 이러한 요소를 적절하게 표시해야 하는데, 이를 위해서 tree 구조를 사용하는 것이 일반적이다. 대표적으로 두 가지의 구조가 있다.\n\n1. **phrase structure(derivation tree)**\n   문장을 기점으로 절, 구, 단어로 top-down으로 내려가는 구조를 가진다.  \n   각 단위를 묶을 때에는 ()를 이용하고, 그 뒤애 해당하는 내용이 무슨 구, 절인지를 표기한다.\n2. **dependency structure**  \n   단어 간의 관계에 더 집중하여 나타낸다. 따라서, 사람이 보기에는 불명확해 보일 수 있지만 특정 usecase에서는 유용하다.\n\n| input                                                                 | output (phrase structure)                                                              |\n| :-------------------------------------------------------------------- | :------------------------------------------------------------------------------------- |\n| (astronomer/NNS) (see/VBD) (star/NNS) (with/IN) (telescope/NNS) (./.) | ((astronomer/NNS)NP ((see/VBD)V ((star/NNS)NP ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S |\n\n![nlp-phase-structure](/images/nlp-phase-structure.jpg)\n\n> **5. Semantics(Meaning, 의미론)**\n\n간단하게는 주어, 목적어와 같은 tag나 \"Agent\"나 \"Effect\"와 같은 tag를 적용하며, 전체적인 의미를 유추해낸다.\n\n| input                                                                                  | output                                                                                                |\n| :------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------- |\n| ((astronomer/NNS)NP ((see/VBD)V ((star/NNS)NP ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S | ((astronomer/NNS)NP/agent ((see/VBD)V ((star/NNS)NP/affected ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S |\n\n> **6. Discourse/Pragmatics(담화/화용론)**\n\n실제 대화 등과 같은 목표를 해결하기 위해서 앞서 보았던 문장 구조를 이용한다.\n\n만약, 해당 데이터를 통해서 하고자 하는 것이 이 이야기를 한 사람이 식당 내부에 있는지를 판단하고자 한다고 가정해보자.\n\n| input                                                                                                 | output |\n| :---------------------------------------------------------------------------------------------------- | :----- |\n| ((astronomer/NNS)NP/agent ((see/VBD)V ((star/NNS)NP/affected ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S | False  |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- text to phonetic converter, <https://tophonetics.com>\n","slug":"nlp-linguistics","date":"2022-10-19 09:03","title":"[NLP] 1. Linguistics","category":"AI","tags":["NLP","Languagistics"],"desc":"Natural Language(자연어, 사람이 사용하는 통상 언어)를 input으로 활용하고자 하는 노력은 컴퓨터의 등장부터 시작하여 여러 번 시도되어 왔다. 지금까지도 완벽하게 이를 처리하는 것은 힘들다. 왜 Natural Language를 다루는 것은 어렵고, 이를 해결하기 위해서 NLP에서는 어떤 방식을 활용할지에 대한 개략적인 overview를 제시한다. 또한, Natural Language의 특성과 분석 단계를 이해하기 위해서 Linguistics(언어학)을 간략하게 정리한다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\n이전 Posting에서는 SVM에 대해서 알아보았다. 일반적인 Logistic Regression에서는 softmax function을 통해서 여러 class를 구분할 수 있었지만, SVM의 경우 구분 선이 결국은 hyperplane으로만 표현 가능하다. 이를 해결하기 위한 SVM에서의 여러 해결책을 알아보자.\n\n## Multiclass in SVM\n\n가장 쉽게 생각할 수 있는 것은 SVM을 결합해서 Multiclass를 구분할 수 있다는 idea이다. 아래에서 곧바로 제시할 아이디어들이 이에 대한 내용이다.\n\n> **1. OvR SVM**\n\nOne vs Rest 의 약자로 다양한 별명이 존재한다. (One vs All, OVA, One Against All, OAA)  \n이름에서부터 느껴지다시피 하나의 class와 그 외에 모든 class를 하나로 묶어서 SVM을 총 class 갯수만큼 만들어서 각 decision boundary로 부터 거리가 양의 방향으로 가장 큰 class를 선택하는 방식이다.\n\n$$\n\\argmax_{k \\in [K]}(\\bold{w}_{(k)}^{\\top}\\phi(\\bold{x})+ b_{(k)})\n$$\n\n![svm-ovr](/images/svm-ovr.jpg)\n\n이 방식은 하나의 큰 문제를 갖고 있는데, 그것은 과도한 데이터 불균형을 유발한다는 것이다. 이러한 문제는 class의 수가 많아질 수록 더 심해진다.\n\n> **2. OvO SVM**\n\nOne vs (Another) One의 약자로, 해당 방식은 1대1로 비교하면서 각 SVM에서 선택한 class 중에서 가장 많은 선택을 받은 class를 최종한다. OvR과는 다르게 각 각의 class를 1대1로 비교하기 때문에 데이터의 불균형에 대한 위협은 덜하다. 하지만, 해당 과정을 수행하기 위해서는 총 K(K-1)/2개의 SVM이 필요하다.\n\n![svm-ovo](/images/svm-ovo.jpg)\n\n또한, 그림에서 \"?\"로 표시된 부분을 어떤 class로 선택할지에 대한 기준이 없다. 왜냐하면, 각 영역에서 한 표씩만 받기 때문이다.\n\n> **3. DAG SVM**\n\n앞 서 보았던 OvO와 OvR의 문제를 해결하기 위해서 장단점을 취하기 위해서 둘을 결합한 방식이다. 계층 형태로 SVM을 구성하기 때문에 OvO보다는 적은 SVM을 사용하면서, OvO에서의 과도한 데이터 불균형을 해결한다.\n\n![svm-multiclass-comparing](/images/svm-multiclass-comparing.jpg)\n\n> **4. WW SVM**\n\nmulticlass 구분을 SVM 최적화 과정에 적용하기 위해서 목적 함수의 형태를 변형하여 구현한 방법으로 자세히 다루지 않지만, 궁금하다면 해당 [🔗 link](https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf)를 통해서 확인할 수 있다.\n\n## Kernel Method\n\n이전까지는 실제로 SVM의 형태를 변형시키거나 SVM을 여러 개 활용하여 multiclass classification을 수행하기 위한 방법을 보았다.\n\n또 다른 방법이 존재한다. 바로 input 공간을 확장하는 것이다. 즉, 더 많은 유의미한 feature를 수집하거나 기존 feature를 가공하여 새로운 feature로 활용하는 것이다. 시스템적으로 해결할 수 있는 방법은 기존 feature를 가공하여 새로운 feature를 활용하는 것이다. 아래의 예시를 보자.\n\n![feature-transposing](/images/feature-transposing.jpg)\n\n왼쪽 공간에서는 SVM은 decision vector를 적절하게 선택하는 것이 어렵다. 하지만, 기존 x 데이터에 절대값을 취하여 나타내어 데이터에 추가하면, 쉽게 decision boundary를 결정하는 것을 볼 수 있다. 그렇다면, 이러한 여러 변환 함수를 적용해보며 여러 feature를 더 추출하는 것이 좋은 해결책을 가져다 줄 것이다.\n\n그렇다면, 우리의 Soft margin SVM의 Dual Problem을 다시 한 번 살펴보자.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & 0 \\leq \\alpha_{i} \\leq C, & i = 1, ..., N\n\\end{align*}\n$$\n\n이것을 feature 변환(basis function을 취한다.)을 통해서 다음과 같이 변형한다는 것이다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\red{\\boldsymbol{\\phi}^{\\top}(\\bold{x}_{i})\\boldsymbol{\\phi}(\\bold{x}_{j})} + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & 0 \\leq \\alpha_{i} \\leq C, & i = 1, ..., N\n\\end{align*}\n$$\n\n하지만, 우리가 새로운 feature를 생성할 수록, 그리고 기존 feature를 복잡하게 사용할 수록 $\\boldsymbol{\\phi}(\\bold{x}_{i})$를 연산하는 비용이 커질 수 밖에 없다.  \n\n따라서, 우리는 일종의 trick을 하나 사용하도록 한다. 바로, 매 bayese update 마다 변하지 않고 재사용되는 값인 $\\boldsymbol{\\phi}^{\\top}(\\bold{x}_{i})\\boldsymbol{\\phi}(\\bold{x}_{j})$를 다른 값으로 대체하면 어떨까? 그렇게 하면 우리는 $\\boldsymbol{\\phi}(\\bold{x}_{i})$를 계산하고 구성하는 수고를 덜 수 있다.\n\n이것이 kernel method(trick)의 핵심 아이디어이다.\n\n가장 대표적인 예시로 아래와 같은 복잡한 $\\phi$ 가 주어졌을 때,\n\n$$\n\\boldsymbol{\\phi}(x) = \\exp[{{-x^{2}}\\over{2\\sigma^{2}}}](1, \\sqrt{1\\over{1!\\sigma^{2}}}x, \\sqrt{1\\over{2!\\sigma^{4}}}x^{2}, \\sqrt{1\\over{3!\\sigma^{6}}}x^{3}, \\cdots)\n$$\n\n아래의 (RBF) kernel로 대체가 가능해진다.\n\n$$\n\\kappa(x,x^{\\prime}) = \\exp(-{{(x - x^{\\prime})}\\over{2\\sigma^{2}}}) = \\boldsymbol{\\phi}^{\\top}(x)\\boldsymbol{\\phi}(x^{\\prime})\n$$\n\n대게 우리가 표현하고자 하는 형태의 $\\boldsymbol{\\phi}$는 이미 특정 kernel 함수로 매핑되고 있으니 직접 $\\boldsymbol{\\phi}$를 계산하기 전에 찾아보는 것이 도움이 될 것이다.[🔗 link](https://dataaspirant.com/svm-kernels/#t-1608054630726)\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- A Comparison of Methods for Multi-class Support Vector Machines, Chih-Wei Hsu and Chih-Jen Lin, <https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf>\n- SEVEN MOST POPULAR SVM KERNELS, <https://dataaspirant.com/svm-kernels/#t-1608054630726>\n","slug":"ml-multiclass-classification-in-svm","date":"2022-10-18 23:19","title":"[ML] 5. Multiclass Classification in SVM","category":"AI","tags":["ML","SVM","KernelMethod"],"desc":"이전 Posting에서는 SVM에 대해서 알아보았다. 일반적인 Logistic Regression에서는 softmax function을 통해서 여러 class를 구분할 수 있었지만, SVM의 경우 구분 선이 결국은 hyperplane으로만 표현 가능하다. 이를 해결하기 위한 SVM에서의 여러 해결책을 알아보자.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n우리는 Classification을 하기 위해서 Logistic Regression을 수행하였다. 그 결과 결국 Classification도 결국은 선을 긋는 것이라는 결론을 내리게 되었다. 하지만, 여기서 그치지 않고 하나 더 고민해 볼 수 있는 것이 있다. 바로 주어진 데이터에 대해서 완벽하게 구분하는 decision boundary가 여러 개 있을 때, 어떤 것이 가장 좋은 것일까? 이것에 대한 아이디어를 제시하는 것이 SVM이다. 해당 Posting에서는 이에 대해서 살펴보도록 하겠다.\n\n## (Hard Margin) SVM\n\nSoft Vector Machine의 약자로, 위에서 제시한 문제를 해결하기 위해서 Margin이라는 것을 도입하였다.\n\n> **Margin**\n\n**Margin**이란 decison boundary와 가장 가까운 각 class의 두 점 사이의 거리를 2로 나눈 값이다.\n\n![svm-1](/images/svm-1.jpg)\n\n위의 그림은 똑같은 데이터 분포에서 대표적인 decision boundary 두 개를 제시한 것이다. 여기서 우리는 굉장히 많은 decision boundary를 그릴 수 있다. 그 중에서도 파란색 실선이 직관적으로 가장 적절한 decision boundary가 될 것이라고 짐작할 수 있다. 그 이유는 필연적으로 data는 noise에 의한 오차가 발생할 수 있는데 실제 데이터의 오차의 허용 범위를 우리는 **margin**(=capability of unexpected noise)만큼 확보할 수 있다는 의미로 이를 해석할 수 있다. 따라서, 이 margin을 크게 하면 할 수록 좋은 성능을 가지는 선을 그을 수 있을 것이라는 결론을 내릴 수 있다.\n\n이것이 SVM의 핵심 아이디어이다.\n\n그렇다면, margin을 수학적으로 정의해보자. 우리가 decision boundary를 $f(\\bold{x}) := \\bold{w}^{\\top}\\bold{x} + b = 0$이라고 한다면, 점($\\bold{x}_{i}$)과 vector 직선 vector 사이의 거리 공식을 통해서 ${{|f(\\bold{x}_{i})|}\\over{||\\bold{w}||^{2}}}$라는 것을 알 수 있다.\n\n따라서 margin은 수학적으로 다음과 같다.\n\n$$\n\\min_{i}{{|f(\\bold{x}_{i})|}\\over{||\\bold{w}||^{2}}}\n$$\n\n```plaintext\n 🤔 Canonical(법칙까지는 아니지만 사실상 표준화된) SVM\n\n SVM에서는 f(x) = 0인 등식 형태를 같는다. 즉 f(x)에 어떤 값을 곱해도 똑같다는 것이다.\n 그런데 margin의 크기를 구할 때에는, w와 b에 어떤 값이 곱해진다면 이 값이 굉장히 달라지게 된다.\n 따라서, 일반적으로 우리는 margin에서의 |f(x)| = 1이 될 수 있도록 설정한다. \n 이렇게 하면 계산이 굉장히 쉬워진다.\n```\n\n![svm-2](/images/svm-2.jpg)\n\n따라서, 우리는 위의 그림과 같은 형태로 $\\bold{x}^{-}$와 $\\bold{x}^{+}$를 찾을 수 있는 것이다.\n\n이제 마지막으로 margin을 정의해보자.\n\n$$\n\\begin{align*}\n\\rho &= {1\\over2}\\{ {{|f(\\bold{x}^{+})|}\\over{||\\bold{w}||^{2}}} - {{|f(\\bold{x}^{-})|}\\over{||\\bold{w}||^{2}}}  \\} \\\\\n&= {1\\over2}{1\\over{||\\bold{w}||^{2}}}\\{\\bold{w}^{\\top}\\bold{x}^{+} - \\bold{w}^{\\top}\\bold{x}^{-}\\} \\\\\n&= {1\\over{||\\bold{w}||^{2}}}\n\\end{align*}\n$$\n\n> **Optimization**\n\n그렇다면, 이제 우리는 문제를 해결할 준비가 된 것이다. 우리가 하고자 하는 것은 margin을 최대화하면서도, 모든 data를 오류없이 분류하는 것이다. 이는 다음과 같은 Constraint Optimization 형태로 변환할 수 있다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & {1\\over{||\\bold{w}||^{2}}} &\\\\\n  \\text{subject to} \\quad & y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\geq 1, & i = 1, ..., N\n\\end{align*}\n$$\n\nConditional Optimization은 이전 Posting([[ML] 0. Base Knowledge](/posts/ml-base-knowledge))에서 다룬바 있다. 해당 내용에 대해 미숙하다면 한 번 살펴보고 오도록 하자.\n\n위 내용을 숙지하였다면, 위의 폼이 다소 바뀌어야 한다는 것을 알 것이다. 해당 형태를 바꾸면서, minimize 형태를 미분이 간편할 수 있도록 바꾸도록 하겠다.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}||^{2} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\leq 0, & i = 1, ..., N\n\\end{align*}\n$$\n\n우선 lagrangian은 다음과 같다.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}\\alpha_{i}(1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b))\n$$\n\n이것에 KKT Condition을 적용하여 정리하면 다음과 같은 등식을 얻을 수 있다.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n이를 $\\mathcal{L}$에 대입하여 식을 정리하면, 다음과 같다.\n\n$$\n\\mathcal{L} = -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i}\n$$\n\n이제 이것을 이용해서 Dual Problem을 정의하면 다음과 같다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & \\alpha_{i} \\geq 0, & i = 1, ..., N\n\\end{align*}\n$$\n\n이 식에서 눈여겨 볼점은 바로 constraint 부분이다. 이 과정을 통해서 결론적으로 constraint 부분이 부등식에서 등식이 되었다. 이는 연산 과정을 매우 간단하게 한다. 뿐만 아니라 $\\bold{x}_{i}^{\\top}\\bold{x}_{j}$는 한 번 계산하면, 전체 과정에서 계속해서 재사용할 수 있기 때문에 컴퓨팅 시에는 굉장한 이점을 발휘할 수 있다. 따라서, 실제로 값을 구할 때에는 이것을 이용하여 값을 구하는 것이 가장 일반적이다.\n\n## (Soft Margin) SVM\n\nSVM의 모든 절차를 살펴본 것 같지만, 우리가 간과한 사실이 하나 있다. 바로 그것은 우리는 data가 하나의 선을 통해서 완벽하게 나뉘어진다고 가정했다. 하지만, 실제 데이터는 그렇지 않을 가능성이 크다. 따라서, 우리는 어느 정도의 오차를 허용할 수 있도록 해야 한다. 이를 slack($\\zeta$)이라고 한다.\n\n![svm-2](/images/svm-2.jpg)\n\n이를 적용하면, 우리의 목적함수와 제약 조건을 변경해야 한다. 이를 변경하는 방법은 두 가지가 존재하는데 각 각 slack variable의 L2-norm을 목적함수에 더하는 방식과 L1-norm을 더하는 방식이다.\n\n> **L2-norm Optimization**\n\n먼저 L2-norm을 더하는 방식을 알아보자\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i}^{2} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, & i = 1, ..., N\n\\end{align*}\n$$\n\n여기서 $C$는 margin 최대화와 slackness 정도의 상대값을 의미한다. 만약, slackness보다 margin의 최대화가 중요하다면, C값은 커지고 반대라면 이 값은 작아진다.\n\n우선 lagrangian을 먼저 구하면 다음과 같다.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}||^{2} + {C\\over2}\\sum_{i=1}^{N}\\zeta_{i}^{2} + \\sum_{i=1}^{N}\\alpha_{i}(1 - \\zeta_{i} - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b))\n$$\n\nKKT condition을 이용하여 주요 값들을 구하면 다음과 같은 등식을 얻을 수 있다.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n$$\n\\boldsymbol{\\zeta} = {\\alpha\\over{C}}\n$$\n\n마지막으로 이를 Dual Problem으로 재정의하면 다음과 같아진다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\bold{x}_{i}^{\\top}\\bold{x}_{j} + {1\\over{C}}\\delta_{ij}) + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & \\alpha_{i} \\geq 0, & i = 1, ..., N\n\\end{align*}\n$$\n\n여기서 $\\delta_{ij}$는 단위행렬이다. 기존 hard margin svm과 비교했을 때, ${1\\over{C}}\\delta_{ij}$ 외에는 바뀌지 않는 것을 알 수 있다.\n\n> **L1-norm Optimization**\n\n그 다음은 L1-norm이다.\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, & \\\\\n  & \\zeta_{i} \\geq 0 & i = 1, ..., N\n\\end{align*}\n$$\n\n여기서는 slack variable이 반드시 0보다 크거나 같다는 것을 주의하자.\n\nlagrangian은 다음과 같다.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i} + \\sum_{i=1}^{N}\\alpha_{i}(1 - \\zeta_{i} - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b)) -  \\sum_{i=1}^{N}\\beta_{i}\\zeta_{i}\n$$\n\nKKT condition을 이용하여 주요 값들을 구하면 다음과 같은 등식을 얻을 수 있다.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n$$\n\\sum_{i=1}^{N}\\beta_{i} = C\n$$\n\n마지막으로 이를 Dual Problem으로 재정의하면 다음과 같아진다.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & 0 \\leq \\alpha_{i} \\leq C, & i = 1, ..., N\n\\end{align*}\n$$\n\n결국 기존 Hard margin과 비교했을 대는 마지막 constraint에 $\\alpha_{i} \\leq C$가 추가된 것 밖에 없다.\n\n---\n\n마지막으로 여기서 하나의 insight를 더 얻을 수 있다.  \nL1-norm의 optimization으로 돌아가보자.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, & \\\\\n  & \\zeta_{i} \\geq 0 & i = 1, ..., N\n\\end{align*}\n$$\n\n목적 함수의 slack variable에 constraint의 값을 대입하여, 다음과 같이 변환이 가능하다.\n\n$$\n\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}max\\{ 0, 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\}\n$$\n\n이 형태는 logistric regression에 regularization을 수행한 것과 동일한 형태를 가지게 된다. 즉, 이전 logistic regression에서 regularization을 다루지 않았는데, 결국은 soft margin svm의 L1-norm 목적함수가 logistic regression 중에서도 hinge function이라는 것을 이용했을 때의 regularization이 되는 것이다.\n\n## Generalization\n\n여태까지 살펴본 Regression을 통해서 우리는 General한 Classification 방식을 지정할 수 있다. 우선 아래 식을 살펴보자.\n\n- Linear Regression(Quadratic Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}{1\\over2}(1 - y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})) )^{2}$\n- Logit Regresion(Log Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}\\log( 1 + \\exp[-y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})]) )$\n- Binary SVM(Hinge Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}max\\{ 0, 1 - y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})) \\}$\n\n여태까지 나온 식들을 살펴보면 위와 같다. 우리는 여기서 아래와 같은 일반적인 형태의 Classification을 제시할 수 있다.\n\n- General Classification  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}\\varepsilon\\log( 1 + \\exp[-y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})]) )$\n\n여기서 $\\varepsilon$이 1이면 바로 logistic regression이 되고, $\\varepsilon$이 0에 수렴할 수록 SVM이 된다. 아래 그림을 보면 이를 알 수 있다.\n\n![compare-regressions](/images/compare-regressions.jpg)\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-svm","date":"2022-10-18 17:29","title":"[ML] 4. SVM","category":"AI","tags":["ML","SVM","GeneralClassifier"],"desc":"우리는 Classification을 하기 위해서 Logistic Regression을 수행하였다. 그 결과 결국 Classification도 결국은 선을 긋는 것이라는 결론을 내리게 되었다. 하지만, 여기서 그치지 않고 하나 더 고민해 볼 수 있는 것이 있다. 바로 주어진 데이터에 대해서 완벽하게 구분하는 decision boundary가 여러 개 있을 때, 어떤 것이 가장 좋은 것일까? 이것에 대한 아이디어를 제시하는 것이 SVM이다. 해당 Posting에서는 이에 대해서 살펴보도록 하겠다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\n이전까지 우리는 input data가 들어왔을 때, continuos한 output을 얻는 것을 목표로 했다. 하지만 현실에서는 대게 정확한 수치보다는 특정 분류로 나누는 것이 효과적인 경우가 많다. 예를 들어, spam 필터링, object detection, 등 등. 따라서, 해당 포스팅에서는 classification을 위해서 사용할 수 있는 logistic regression에 대해서 살펴볼 것이다.\n\n## Classification\n\n**Classification**이란 결국 특정 input이 들어왔을 때, 이를 하나의 Class라는 output을 내보내는 것이다. 즉, output은 연속적이지 않고, descret하다. 대게 Classification에서는 Class의 갯수를 K라고 표기하고, $C_k$는 k 번째 Class라는 의미로 사용되어진다.\n\n그렇다면, 어떻게 Class를 나눌 수 있는 것일까? 매우 단순하게도 이는 **Decision Boundary**라는 선을 그어서 해결 할 수 있다.\n\n![decision-boundary](/images/decision-boundary.jpg)\n\n위의 예시처럼 우리는 선을 하나 그어서 $\\red{\\text{x}}$와 $\\blue{\\text{o}}$를 구분할 수 있다. 이를 통해서 우리는 Class 1에 해당할 것이라고 예측하는 구간 $R_1$이 만들어지고, Class 2라고 예측하는 구간 $R_2$를 구성할 수 있다.\n\n즉, classification을 수행하기 위해서 해야할 일은 기존의 Regression 과정과 마찬가지로 선을 찾는 것이다.\n\n결국 찾고자 하는 것이 선이라면, 이것을 Linear Regression으로 해결할 수 있을 것이다. 따라서, 우리는 다음과 같은 식으로 간단히 Linear Regression을 바꿔서 생각할 수 있다.\n\n- 예측값($\\hat{y}$, $h(\\bold{x})$)  \n  $h(\\bold{x}) = \\text{sign}(\\bold{w}^{\\top}\\bold{x}) = \\begin{cases} +1 & \\bold{w}^{\\top}\\bold{x} \\geq 0 \\\\ -1 & \\text{otherwise}\\end{cases}$\n- Least Squared Error(LS, MLE)  \n  실제로 parameter를 구할 때에는 sign을 취하지 않는데, sign을 취하게 되면 모두 LS는 결국 오답의 갯수 정도로 취급된다. 즉, 얼마나 예측이 잘못되었는지를 반영할 수 없다는 것이다. 따라서, 이는 기존 Linear Regression의 LS를 구하는 방법과 동일하게 수행한다.  \n  $\\argmin_{w} {1\\over2}\\sum_{n=1}^{N}{(y_n - (\\bold{w}^{\\top}\\bold{x}))^2}$\n\n이렇게 Linear Regression을 적용하면 문제가 없을 거 같다. 하지만, 실제로는 문제가 있다. 바로, 데이터가 불균형할 때이다. 만약 데이터가 decision boundary를 기준으로 대칭(symmetric)인 형태로 존재한다면, 문제가 없다. 하지만, 비대칭(asymmetric)인 경우 제대로 동작하지 않는다. 왜냐하면, linear regression은 최적에서 데이터의 평균을 반영하는데 불균형한 경우 데이터의 평균이 Decision Boundary가 되는 것은 문제가 있다.\n\n![linear-in-classification](/images/linear-in-classification.jpg)\n\n## Logistic Regression\n\n위에서 제시한 문제를 해결하기 위해서 Classification에서는 Linear Regression이 아닌 Logistic Regression을 활용한다. 이를 이해하기 위해서 기반이 될 요소들을 먼저 살펴보자.\n\n> **Discriminant Function**\n\n판별함수(Discriminant Function, Score Function) 등으로 불리는 해당 함수는 특정 data가 특정 class에 속할 가능성(likelihood, probability, score)을 나타내는 함수이다. 즉, input으로 data를 받고, output으로 class에 속할 확률을 내보낸다.\n\n이를 통해서 우리는 다음과 같은 과정을 할 수 있다.\n\n만약, $f_k(\\bold{x}) \\gt f_j(\\bold{x})$이라면, $\\bold{x}$의 class는 $C_k$이다.\n\n따라서, 우리는 다음과 같은 식으로 여러 개의 Class가 있는 공간에서 data를 분류할 수 있다.\n\n$$\nh(\\bold{x}) = \\argmax_{k}f_{k}(\\bold{x})\n$$\n\n그렇다면, Discriminant Function으로 어떤 값을 쓰면 좋을까? 이에 대한 해결책을 Bayes Decision Rule에서 제시한다.\n\n> **Bayes Decision Rule**\n\n만약 우리가 특정 data가 특정 Class에 속할 확률을 구한다고 하자. 우리는 먼저 Likelihood를 생각할 수 있다. $P(x|C = k), P(x|C = j)$를 구하여 각 Class에 속할 확률을 비교할 수 있을까?  \n물론 비교는 가능하다 하지만, 반쪽짜리 비교라고 할 수 있다. 만약, class k에 속하는 데이터보다 class j에 속하는 데이터가 훨씬 많다고 하자. 그러면, 일반적으로 class j가 발생할 확률 자체가 높다. 하지만, likelihood는 이러한 경향을 반영하지 않는다. 간단한 예시를 들어보자.\n\n```plaintext\n 🤔 어떤 동물의 털에 존재하는 색의 갯수가 주어졌을 때, 고양이일 확률과 호랑이일 확률이라고 하자.\n\n  그리고, input data는 털에 존재하는 색의 수라고 하자. (호랑이는 대게 3가지 색, 백호 = 2가지 색, 고양이는 매우 다양)\n  그렇다면, P(털의 색 = 3|C = 호랑이), P(털의 색 = 3|C = 고양이)를 비교했을 때, 우리는 당연히 전자가 크다고 생각할 것이다.\n  하지만, 여기서 우리가 고려하지 않은 것이 있다. 바로 전체 고양이와 호랑이의 비율이다. \n  상대적으로 고양이가 호랑이보다 압도적으로 많다는 것을 고려했을 때, 고양이의 확률이 더 높을 수도 있다. \n\n  즉, 어떤 동물의 털에 존재하는 색의 갯수가 주어졌을 때, 고양이일 확률은 \n  P(C=고양이|털의 색=3) =  P(털의 색 = 3|C = 고양이)P(C=고양이)이다. (분모는 생략함.)\n```\n\n즉, Bayes Rule에 기반하여 우리가 원하는 output은 Posterior라는 것을 명확히 알 수 있다.\n\n$$\n\\begin{align*}\np(C_{k}|\\bold{x}) &= {{p(\\bold{x}| C_{k}) p(C_{k})}\\over{\\sum_{j=1}^{K}{p(\\bold{x}|C_{j})p(C_{j})}}} \\\\\n&\\propto p(\\bold{x}| C_{k}) p(C_{k})\n\\end{align*}\n$$\n\n위의 경우 Class간의 상대 비교에 사용하는 지표로 이를 사용하기 때문에, 분모(Normalization Factor, 확률의 총합이 1이 되도록 하는 역할)를 제외하여도 상관없기에 대게 복잡한 분모 계산을 제외하고 표현하는 것이 일반적이다.\n\n또한, 앞선 예시에서 얻을 수 있는 insight는 편향된 데이터일수록 MLE를 사용할 수 없다는 것이다. 위에서 Linear Regression이 Classification에 부적함한 경우도 데이터의 편향이 있을 경우이다. 이 역시 Linear Regression이 결국은 MLE에 기반하기 때문인 것이다.\n\n우리는 각 Class 자체의 확률(Prior)과 Likelihood를 이용할 수 있는 Discriminant Function을 구해야 한다는 것이다.\n\n> **Logistic Regression**\n\n자 이제 드디어 Logistric Regression을 시작해보자. 우리는 Discriminant Function을 먼저 지정해야 한다. 여러 가지 방법이 있지만, 가장 대표적으로 사용되는 방법은 **Softmax**를 활용하는 것이다. **Softmax**를 활용하여 식을 나타내면 아래와 같다.\n\n$$\np(y_n = k | \\bold{x}_n, \\bold{w}) = {{\\exp(\\bold{w}_{k}^{\\top}\\bold{x}_n)}\\over{\\sum_{j=1}^{K}{\\exp(\\bold{w}_{j}^{\\top}\\bold{x}_n)}}}\n$$\n\n만약, class가 2개인 Binary Classification인 경우에 **Softmax**는 다음과 같아진다. 특히 이를 **Sigmoid**(**Logit**)라고 정의한다.\n\n$$\np(y_n = k | \\bold{x}_n, \\bold{w}) = {1\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}\n$$\n\n이를 유도하는 과정은 생략하지만, 여타 다른 블로그를 더 참고하면 좋다.\n\n이를 Linear Regression과 비교해서 살펴보자.\n\n![logistic-vs-linear](/images/logistic-vs-linear.jpg)\n\nLinear Regression은 특정값을 향해 나아가고 있다. 해당 방식을 보면 x가 대상의 특성을 강하게 가지고 있다면, 명확하게 구분할 수 있는데, 이는 **sigmoid**($\\sigma$) 함수가 [0, 1] 범위 내에서 정의되기 때문에 Regression 과정에서 극단 데이터(outlier)가 가지는 영향력이 Linear Regression보다 극단적으로 적다는 것을 알 수 있다.\n\n자 이것이 가지는 의미를 이전에 살펴본 **Bayes Decision Rule**에 기반해서 생각해보자. **sigmoid**($\\sigma$)는 결국 극단적인 데이터이든, 애매한 데이터이든 거의 비슷한 값으로 변환한다. 그렇다는 것은 기존에는 평균을 구하는데에 input(x)의 값이 큰 영향을 미쳤다면, **sigmoid**($\\sigma$)에서는 특정 class에 속하는 x의 갯수가 많은 영향을 주는 것을 알 수 있다. 이를 통해서 **sigmoid**($\\sigma$)가 완벽하지는 않지만, **Bayes Decision Rule**을 반영했다는 것을 알 수 있다.\n\n마지막으로, MLE를 통해서 Logistic Regression의 parameter를 추정해보자. (MAP는 기존에 살펴본 Linear Regression과 동일하게 regularizer를 더해주는 방식이기 때문에 생략한다.)\n\n$$\n\\begin{align*}\n\\argmax_{w}\\log{p(\\mathcal{D}|\\bold{w})} &= \\argmax_{w}\\sum_{n=1}^{N}{\\log p(y_{n}|\\bold{x}_{n}, \\bold{w})} \\\\\n&= \\argmax_{w}\\sum_{n=1}^{N}{\\log ({1\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}) } \\\\\n&= \\argmax_{w}\\sum_{n=1}^{N}{-\\log (1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})) } \\\\\n&= \\argmin_{w}\\sum_{n=1}^{N}{\\log (1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})) } \\\\\n\\end{align*}\n$$\n\n## Gradient Descent/Ascent\n\n위의 복잡한 식을 봤으면 알겠지만, 안타깝게도 일반식으로 $\\bold{w}_{MLE}, \\bold{w}_{MAP}$ 등을 구할 수는 없다. 따라서, 우리가 믿을 것은 Gradient를 이용한 방식이다.\n\n> **Gradient Descent**\n\n먼저, 위에서 봤겠지만, Loss는 다음과 같다.\n\n$$\n\\mathcal{L} = \\sum_{n=1}^{N}{\\log(1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n}))}\n$$\n\n이제 이를 미분해서 Gradient를 구하면 다음과 같다.\n\n$$\n\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}) = \\sum_{n=1}^{N}{{{-y_{n}\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}\\bold{x}_{n}}\n$$\n\n따라서, Gradient Descent 방식은 다음과 같이 진행된다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\alpha\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}_{t})\n$$\n\n> **Gradient Ascent**\n\n위의 방식이 가장 일반적이지만, 우리가 sigmoid의 class값으로 $y \\in \\{-1, 1\\}$ 대신 $y \\in \\{0, 1\\}$을 사용했을 경우 다른 식으로도 접근이 가능하다.\n\n이 경우에는 Loss라기 보기 어렵지만, 다른 형태의 optimization 형태가 만들어진다. (여기서 $\\sigma$는 sigmoid 함수를 의미한다.)\n\n$$\n\\argmax_{\\bold{w}} \\sum_{n=1}^{N}y_{n}\\log{\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}) + (1-y_{n})\\log{(1-\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}))} }\n$$\n\n이를 똑같이 미분하여 사용하지만, 반대로 이 경우에는 maximization 이기 때문에 Gradient Ascent를 수행해야 한다.\n\n우선 미분 결과 얻는 Gradient는 다음과 같다.\n\n$$\n\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}) = \\sum_{n=1}^{N}{[y_{n} - \\sigma(\\bold{w}^{\\top}\\bold{x}_{n})]\\bold{x}_{n}}\n$$\n\n굉장히 간단하게 정리가 되어지는 것을 볼 수 있다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\alpha\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}_{t})\n$$\n\n따라서, 아래와 같이 Gradient Ascent를 활용하여 계산하는 것도 충분히 가능하다.\n\n> **Newton Method**\n\n이러한 형태로 넘어오게 되면, 굉장히 많은 연산이 각 update마다 필요하다는 것을 알 수 있다. 따라서, 우리는 이 과정을 축약할 방법을 찾게 된다. 그 아이디어는 바로 gradient를 업데이트 할 때, linear 하게 update하는 것이 아니라 Quadratic하게 update하는 것이다. 이를 위한 방법론이 **Newton Method**이다. 이 방식을 Logistic Regression에 적용하였을 때, 이를 IRLS(Iterative Re-weighted Least Squared) Algorithm 이라고 부른다.\n\n![newton-method](/images/newton-method.jpg)\n\n위 그래프에서 f(x)가 Loss 라고 할 때, 우리는 $x_k$에서 직선형의 gradient를 사용하는 것보다 quadratic 형태를 사용하는 것이 더 빠르게 수렴값을 찾을 수 있다는 것을 알 수 있다.\n\n이를 사용하기 위해서는 다음 2가지에 대한 사전 이해가 필요하다.\n\n- Taylor Series  \n  smooth한 형태를 가진 x에 대한 함수를 x에 대한 급수의 형태로 변환한 것이다. 따라서 이를 식으로 나타내면 다음과 같다.  \n  $T_{\\infin}(x) = \\sum_{k=0}^{\\infin}{f^{(k)}(x_{0})\\over{k\\!}}(x-x_{0})^{k} $  \n  즉, sine 함수와 같은 형태의 그래프도 x의 급수 형태로 변환이 가능하다는 것이다. Newton Method에서는 무한대까지는 사용하지 않고, 대게 K=2까지를 쓴다.\n- Hessian Matrix  \n  특정 함수 $f(\\bold{x})$를 각 feature에 대해서 이중 편미분한 결과를 저장한 행렬이다. 식은 다음과 같다.  \n  $\n  H = \\nabla^{2}f(x) =\n  \\left[\n    \\begin{array}{ccc}\n      \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{1}^{2}} & \\cdots & \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{1} \\partial x_{D}} \\\\\n      \\vdots & \\ddots & \\vdots \\\\\n      \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{D} \\partial x_{1}} & \\cdots & \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{n}^{2}}\n    \\end{array}\n  \\right]\n  $\n\n이를 이용해서, Newton Method의 결과값을 정리하면 결과는 다음과 같다.\n\n$$\n\\bold{w}^{(k+1)} = \\bold{w}^{(k)} - [\\nabla^{2}\\mathcal{J}(\\bold{w}^{(k)})]^{-1}\\nabla\\mathcal{J}(\\bold{w}^{(k)})\n$$\n\n자 이제 이것을 실제로 Logistic Regression 식에 대입해보자.\n\n$$\n\\begin{align*}\n  \\nabla\\mathcal{J}(w) &= - \\sum_{n=1}^{N}(y_{n}-\\hat{y}_{n})x_{n} \\\\\n  \\nabla^{2}\\mathcal{J}(w) &= \\sum_{n=1}^{N}\\hat{y}_{n}(1-\\hat{y}_{n})\\bold{x}_{n}\\bold{x}_{n}^{\\top}\n\\end{align*}\n$$\n\n여기서, 아래와 같이 변수를 정의하면,\n\n$$\nS =\n  \\begin{bmatrix}\n    \\hat{y}_{1}(1-\\hat{y}_1)  & \\cdots  & 0                         \\\\\n    \\vdots                    & \\ddots  & \\vdots                     \\\\\n    0                         & \\cdots  & \\hat{y}_{N}(1-\\hat{y}_N)  \\\\\n  \\end{bmatrix},\n\n\\bold{b} =\n  \\begin{bmatrix}\n    {{y_{1} - \\hat{y}_{1}}\\over{\\hat{y}_{1}(1-\\hat{y}_{1})}} \\\\\n    \\vdots \\\\\n    {{y_{N} - \\hat{y}_{N}}\\over{\\hat{y}_{N}(1-\\hat{y}_{N})}}\n  \\end{bmatrix}\n$$\n\n결과적으로 다음과 같은 형태를 얻을 수 있다.\n\n$$\n\\begin{align*}\n\\bold{w}_{k+1} &= \\bold{w}_{k} + (XS_{k}X^{\\top})^{-1}XS_{k}\\bold{b}_{k} \\\\\n&= (XS_{k}X^{\\top})^{-1}[(XS_{k}X^{\\top})\\bold{w}_{k} + XS_{k}\\bold{b}_{k}] \\\\\n&= (XS_{k}X^{\\top})^{-1}XS_{k}[X^{\\top}\\bold{w}_{k} + \\bold{b}_{k}]\n\\end{align*}\n$$\n\n이는 결코 계산 과정이 단순하다고는 할 수 없지만, 빠르게 수렴할 수 있기 때문에 가치있는 방법이다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-logistic-regression","date":"2022-10-18 09:58","title":"[ML] 3. Logistic Regression","category":"AI","tags":["ML","LogisticRegression","Classification","SigmoidFunction","SoftmaxFunction","NewtonMethod"],"desc":"이전까지 우리는 input data가 들어왔을 때, continuos한 output을 얻는 것을 목표로 했다. 하지만 현실에서는 대게 정확한 수치보다는 특정 분류로 나누는 것이 효과적인 경우가 많다. 예를 들어, spam 필터링, object detection, 등 등. 따라서, 해당 포스팅에서는 classification을 위해서 사용할 수 있는 logistic regression에 대해서 살펴볼 것이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nRegression(회귀)이라는 단어는 \"원래의 상태로 돌아간다\"로 돌아간다는 의미를 가진다. 결국 어떤 일련의 Event로 인해서 데이터에 Noise가 발생할 수 있어도 결국은 하나의 \"보편\"으로 시간이 지나면 수렴(회귀)할 것이라는 생각에 기반하는 것이다.  \n따라서, 우리는 이러한 \"보편\"을 찾기 위해서 우리가 알고 있는 독립 데이터 X를 통해서 알고자 하는 값 Y를 보편적으로 추론할 수 있다. 이 과정을 우리는 Regression이라고 부른다. 또한, X에 의해 독립적이지 않고 종속적인 Y의 관계가 Linear하게 표현될 때 이를 우리는 Linear Regression이라고 한다.  \n따라서, 해당 Posting에서는 Linear Regression을 바탕으로 Machine Learning이 어떻게 동작하는지를 이해하는 것이 목표이다.\n\n## Regression\n\n> **정의**\n\n독립 변수 X로 부터 종속 변수 Y에 대응되는 함수 f를 생성하는 과정을 의미한다.\n\n$$\n\\bold{y} = f(\\bold{x}) + \\epsilon\n$$\n\n$$\nf(\\bold{x}) = \\bold{w}^{\\top}\\bold{x},\n(\\bold{w} = \\begin{bmatrix} w_{0} \\\\ w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{N} \\\\ \\end{bmatrix}, \\bold{x} = \\begin{bmatrix} 1 \\\\ x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{N} \\\\ \\end{bmatrix} )\n$$\n\n여기서 각 변수 $x$, $y$, $\\epsilon$, $w$은 다음과 같이 여러 이름으로 불려진다.\n\n- $x$ : input, 독립 변수, predictor, regressor, covariate\n- $y$ : output, 종속 변수, response\n- $\\epsilon$ : noise, 관측되지 않은 요소\n- $w$ : weight, 가중치, parameter\n\n> <mark>**성능 평가(MSE)**</mark>\n\n우리가 만든 Regression이 얼마나 데이터를 잘 반영하는지를 알고 싶을 때, 즉 평가하고자 할 때, 우리는 Mean Squared Error(MSE)를 사용한다. 이는 이전 포스팅인 [Parametric Estimation](/posts/ml-parametric-estimation)에서도 살펴보았었다.\n\n그렇다면, MSE를 최소로 하는 f(x)는 무엇일까? 이를 통해서 또, 하나의 식견을 넓힐 수 있다. 한 번 MSE 식을 정리해보자.\n\n$$\n\\begin{align*}\n\\Epsilon(f) &= E[||\\bold{y}_*-f(\\bold{x})||^2] \\\\\n&= \\int\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{x}, \\bold{y}_*)d\\bold{x}d\\bold{y}_* \\\\\n&= \\int\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{x})p(\\bold{y}_* | \\bold{x})d\\bold{y}_*d\\bold{x} \\\\\n&= \\int p(\\bold{x}) \\red{\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}d\\bold{x}\n\\end{align*}\n$$\n\n여기서 중요한 것은 바로 빨간색으로 색칠한 부분이다. 우리가 바꿀 수 있는 값은 f(x)를 구성하는 w밖에 없다 즉, 위 식을 최소화하는 것은 빨간색 부분을 최소화하는 것과 같아진다.  \n따라서, 이 부분을 미분해서 최솟값을 구할 수 있는데 이를 확인해보자.\n\n$$\n\\begin{align*}\n&{\\partial\\over{\\partial{f(\\bold{x})}}}({\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}) = 0 \\\\\n&{\\partial\\over{\\partial{f(\\bold{x})}}}({\\int||\\bold{y}_*^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} - 2f(\\bold{x}){\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} + f(\\bold{x})^2{\\int p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}) = 0 \\\\\n&-2{\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} + 2f(\\bold{x}) = 0 \\\\\n&f(\\bold{x}) = {\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} = E[\\bold{y}_*|\\bold{x}]\n\\end{align*}\n$$\n\n즉, 우리가 구하고자 하는 Linear Regression 함수는 data x에 따른 실제 y 값의 평균을 의미한다. Regression의 정의를 생각했을 때, 어느정도 합리적이라는 것을 알 수 있다. \"보편\"적이다는 의미에서 \"평균\"을 쓰는 경우가 많이 있기 때문이다.\n\n위에서는 MSE를 이용해서 분석하였지만, MAE(Mean Absolute Error)를 활용하여 구할 수 있는데 이 경우에는 Regression의 형태가 또 달라진다. 즉, MSE를 최소화하는 방식은 우리가 \"보편\"적인 답을 구하는데 있어 \"평균\"을 활용한 것이고, MAE를 사용한다면, 또 다른 방식을 사용한다는 것을 알게 될 것이다.\n\n## MLE of Linear Regression\n\n이제 Linear Regression에서 $\\bold{w}$를 어떻게 찾아 나갈지에 대해서 살펴볼 것이다. 순서는 이전 Posting [Parametric Estimation](/posts/ml-parametric-estimation)에서 살펴봤던 것과 마찬가지로 MLE, MAP 순으로 살펴볼 것이다. 그리고 이것이 왜 MLE고, MAP랑 관련이 있는지도 살펴볼 것이다.\n\n들어가기에 앞 서, 표기법과 용어를 몇 개 정리할 필요가 있다.\n\n- $\\bold{x}, \\bold{y}, \\bold{w}$ 등 굵은 선 처리되어 있는 변수는 vector를 의미한다.\n- $\\bold{X}$ 등 굵고 대문자로 처리되어 있는 변수는 Matrix를 의미한다.\n- $\\bold{w^{\\top}}$, $\\bold{X}^{\\top}$ 에서 T는 Transpose를 의미한다.\n- feature : input 데이터의 각 각 분류 기준들을 의미한다. 수식으로는 $x_1, x_2, x_3$ 이런 식으로 표현된 input들 중에 각 각의 input을 feature라고 하며, 실제 예시로는 데이터 수집 시에 각 데이터의 column(나이, 성별, 등 등)이 될 것이다.\n\n위의 용어 정리에 의해서 다음과 같은 사실을 다시 한 번 확인하자.\n\n먼저, 단일 Linear Regression이다.\n\n$$\n\\hat{y} = f(\\bold{x}) = \\bold{w}^{\\top}\\bold{x} = \\bold{x}^{\\top}\\bold{w}\n$$\n\n이번에는 여러 개의 데이터를 한 번에 추측한 결과값 $\\hat{\\bold{y}}$ 이다.\n\n$$\n\\hat{\\bold{y}} = \\bold{X}\\bold{w}\n$$\n\n각 의미를 곱씹어보면 어떻게 생겼을지 어렵풋이 짐작이 올 것이다.\n\n> **basis function**\n\n여기서 또 하나 짚어볼 것은 바로 $\\bold{x}$를 변형하는 방법이다. 바로, 우리는 데이터로 입력 받은 데이터를 바로 사용할 수도 있지만, 해당 input 값을 제곱해서 사용해도 되고, 서로 더해서 사용해도 되고, 나누어서 사용할 수도 있다. 예를 들어서 우리가 구하고 싶은 값이 대한민국 인구의 평균 나이라고 하자. 이때, 우리가 사용하는 데이터의 값이 가구 단위로 조사되어 부,모,자식1, 자식2, ... 로 분류되어 나이가 적혀있다고 하자. 이때 우리가 필요한 것은 결국 전체 인구의 나이 데이터임으로 모두 하나의 feature로 합쳐버릴 수도 있다.\n\n이러한 과정을 위해서 우리는 basis function($\\phi(\\bold{x})$)이라는 것을 이용한다. 단순히 input data를 합성해서 하나의 input을 생성하는 것이다.\n\n따라서, 우리는 필요에 따라 input data를 가공하여 사용하며 여러 $\\phi$를 적용하여 나타낼 경우 linear regression은 다음과 같은 형태가 된다.\n\n$$\nf(\\bold{x}) = \\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x})\n$$\n\n대표적인 Basis function을 살펴보자.\n\n- Polynomial basis : 하나의 input feature에 대해서 n-제곱형태의 vector로 변환하는 형식이다. 따라서, 다음과 같이 표기 된다.  \n  $\\boldsymbol{\\phi}(\\bold{x}) = \\begin{bmatrix} 1 \\\\ x \\\\ x^{2} \\\\ \\vdots \\\\ x^{n} \\\\ \\end{bmatrix}$, $\\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x}) = w_{0} + w_{1}x + w_{2}x^{2} + ... + w_{n}x^{n}$  \n  대게 이러한 형태로 변형한 Linear Regression을 Polinomial Regression이라고 부르는데, 이를 통한 결과 값이 마치 다항식의 형태를 띄기 때문이다. 하지만, feature의 값이 polynomial이 되었더라도 $\\bold{w}$가 선형임을 잊어서는 안된다.  \n  이를 사용하게 되면, 우리는 1차원의 input 공간에서 선형으로는 나눌 수 없던 분류를 수행할 수 있다.\n- Gaussian basis : 가우시안 분포로 변환하는 것으로 특정 feature를 gaussian으로 변환하게 되면, 데이터의 경향성이 파악된다. 이는 후에 더 자세히 다룰 기회가 온다.  \n- Spline basis: 특정 구간마다 다른 Polynomial 형태의 feature를 적용하도록 하는 방식이다. 대게 구간마다 다른 확률 분포를 적용하고자 할 때 사용한다.\n- Fourier basis, Hyperbolic tangent basis, wavelet basis 등 여러 가지 방식이 존재한다.\n\n> **Design Matrix**\n\n마지막으로, 이렇게 만들어진 $\\phi(\\bold{x})$를 하나의 Matrix로 합친 것을 Design Matrix라고 한다. N개의 데이터를 L개의 서로 다른 basis function으로 변환한 데이터를 행렬로 표현하면, 다음과 같다.\n\n$$\n\\Phi =\n  \\begin{bmatrix}\n    \\phi_1({\\bold{x_1}})  & \\phi_2(\\bold{x_1})  & \\cdots  & \\phi_L(\\bold{x_1})  \\\\\n    \\phi_1({\\bold{x_2}})  & \\phi_2(\\bold{x_2})  & \\cdots  & \\phi_L(\\bold{x_2})  \\\\\n    \\vdots                & \\vdots              & \\ddots  & \\vdots              \\\\\n    \\phi_1({\\bold{x_N}})  & \\phi_2(\\bold{x_N})  & \\cdots  & \\phi_L(\\bold{x_N})  \\\\\n  \\end{bmatrix}\n$$\n\n이를 통해서 표현한 모든 데이터에 대한 Linear Regression은 다음과 같다.\n\n$$\n\\hat{\\bold{y}} = \\Phi\\bold{w}\n$$\n\n자 이제부터 우리는 본론으로 들어와서 우리의 Linear Regression의 Weight(Parameter, $\\bold{w}$)를 어떻게 추정할 수 있을지를 알아보자.\n\n우리는 최종적으로 우리의 Linear Regression이 정답과 매우 유사한 값을 내놓기를 원한다. 따라서, 이때 우리는 Least Square Error를 사용할 수 있다. 이는 모든 데이터에서 얻은 예측값(Linear Regression의 output)과 실제 y의 값의 Square Error의 합을 최소화하는 것이다.\n\n$$\n\\varepsilon_{LS}(\\bold{w}) = {1\\over2}\\sum_{n=1}^{N}(y_n - \\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x_n}))^2 = {1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2\n$$\n\n이제 $\\argmin_{\\bold{w}}\\varepsilon_{LS}(\\bold{w})$을 풀기 위해서 미분을 해보자.\n\n$$\n\\begin{align*}\n&{\\partial\\over\\partial\\bold{w}}{1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2 = 0 \\\\\n&\\Phi^{\\top}(\\bold{y}_* - \\Phi\\bold{w}) = 0 \\\\\n&\\Phi^{\\top}\\Phi\\bold{w} = \\Phi^{\\top}\\bold{y_*} \\\\\n&\\bold{w} = (\\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y_*} \\\\\n&\\bold{w} = \\Phi^{\\dagger}\\bold{y_*}\n\\end{align*}\n$$\n\n이를 통해서, 위와 같은 식을 얻을 수 있다.\n\n---\n\n그럼 이 식이 왜 MLE랑 관련이 있는 것일까? 그것은 다음의 과정을 통해서 증명할 수 있다.\n\n우리는 각 data마다 존재하는 error(noise, $y_* - \\hat{y}$, $\\varepsilon$)가 그 양이 많아짐에 따라 정규 분포를 따른다는 것을 알 수 있다. (Central Limit Theorem)\n\n$$\n\\begin{align*}\n\\varepsilon &= y_* - \\hat{y} = y_*-\\phi(\\bold{x}) \\\\\ny_* &= \\phi(\\bold{x}) + \\varepsilon\n\\end{align*}\n$$\n\n이를 좌표 평면 상에서 나타내면 다음과 같다고 할 수 있다.\n\n![gaussian-error](/images/gaussian-error.jpeg)\n\n또한, $\\varepsilon$의 확률을 정의하면 다음과 같은 확률을 얻을 수 있다.\n\n$$\n\\begin{align*}\np(\\varepsilon) &= {1\\over{\\sqrt{2\\pi}\\sigma}}\\exp{[-{\\varepsilon^2\\over{2\\sigma^2}}]} \\\\\np(\\varepsilon) &= {1\\over{\\sqrt{2\\pi}\\sigma}}\\exp{[-{(y_*-\\phi(\\bold{x}))^2\\over{2\\sigma^2}}]}\n\\end{align*}\n$$\n\n여기서 우리는 $p(\\varepsilon)$을 $p(y_*|\\bold{x}; \\theta)$라고 볼 수 있다. ($\\theta = (\\bold{w}, \\phi, \\sigma)$)\n\n우리는 이를 이용해서 Likelihood를 구할 수 있다.\n\n$$\n\\begin{align*}\n\\mathcal{L} &= \\log{p(\\bold{y}_*|\\bold{X}; \\theta)} = \\sum_{i=1}^{N}{\\log{p(y_{*(i)}|\\bold{x}_{(i)}; \\theta)}} \\\\\n&= N\\log{{1\\over{\\sqrt{2\\pi}\\sigma}}} + \\sum_{i=1}^{N}{-{(y_*-\\phi(\\bold{x}))^2\\over{2\\sigma^2}}} \\\\\n&= N\\log{{1\\over{\\sqrt{2\\pi}\\sigma}}} - {1\\over{\\sigma^2}}\\red{{1\\over{2}}\\sum_{i=1}^{N}{(y_*-\\phi(\\bold{x}))^2}}\n\\end{align*}\n$$\n\n우리가 변경할 수 있는 데이터는 $\\phi(\\bold{x})$ 밖에 없다. 따라서, 빨간색을 제외한 부분은 Likelihood의 최댓값을 구할 때, 고려하지 않아도 되는 상수로 볼 수 있다. 그렇다면, 우리는 Likelihood의 최댓값을 구하기 위해서 빨간색 표시된 부분을 최소화해야 한다는 것을 알 수 있다. 그리고, 이는 우리가 앞에서 살펴봤던, Least Squared Error와 같다.\n\n<mark>즉, $\\bold{w}_{LS}=\\bold{w}_{MLE}$ 라는 것이다.</mark>\n\n## MAP of Linear Regression\n\n이번에는 Linear Regression에서 $\\bold{w}$를 찾아나가는 과정에서 MAP를 활용하는 과정을 알아볼 것이다.\n\n> **overfitting**\n\n우리가 MLE를 통해서 Linear Regression을 찾는 것이 충분하다고 생각할 수 있다. 하지만, 우리는 어쩔 수 없이 **overfitting**이라는 문제에 직면하게 된다.\n\n![over-fitting-example](/images/over-fitting-example.jpg)\n\n**overfitting**이란 데이터를 통해서 구할 수 있는 분포가 학습에 사용된 데이터에 대해서는 에러가 거의 없는 형태로 예측하지만, 그 외에 데이터에 대해서는 에러가 크게 발생하는 경우를 의미한다. 위의 예시에서 처럼 데이터가 전체 Sample space보다 턱없이 적은 경우에 발생하기 쉽다.\n\n이러한 문제는 사실 basis function을 잘 선택하면 해결할 수 있다. 하지만, 우리가 어떻게 매번 적절한 basis function을 찾기 위해서 iteration을 반복하는 것이 올바를까? 그리고 이는 실제 적합한 값을 찾기 위한 수학적 식도 존재하지 않는다.\n\n> **Regularization**\n\n따라서, 우리는 **regularization**을 수행한다. 위의 overfitting된 그래프를 보면 하나의 insight(번뜩이는 idea?)를 얻을 수 있다. 바로, 급격한 기울기의 변화는 overfitting과 유사한 의미로 볼 수 있다는 것이다. 즉, 그래프의 형태가 smooth 해야한다는 것이다.\n\n따라서, 우리는 하나의 error에 대해서 다음과 같이 재정의해서 smoothing(regularization)을 수행할 수 있다.\n\n$$\n\\varepsilon = {1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2 + {\\lambda\\over{2}}||\\bold{w}||^2\n$$\n\n$\\bold{w}$의 L2 norm을 error에 추가하여 $\\bold{w}$의 크기가 작아지는 방향으로 예측을 할 수 있도록 하는 것이다. (물론 L1 norm을 사용할 수도 있다. 이 또한, 후에 다룰 것이니 여기서는 넘어가겠다. 추가로 이렇게 L2 norm을 이용하면 **Ridge Regression**, L1 norm을 이용하면 **Lasso Regression**이라고 한다.)\n\n자 이제 위의 식을 미분해서 최소값이 되게 하는 $\\bold{w}$를 찾아보자. 과정은 연산이 그렇게 어렵지 않으므로 넘어가고 결과는 아래와 같다.\n\n$$\n\\bold{w}_{ridge} = (\\lambda I + \\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y}_*\n$$\n\n---\n\n그럼 이 역시 MAP를 통해서 해석해보도록 하자.\n\n위에서 살펴본 바와 같이 우리는 w값이 작을 확률이 높을 수록 좋은 성능을 가질 것이라는 Prior를 얻을 수 있다.\n\n즉,$p(\\bold{w})$가 zero-mean gaussian(표준정규분포)형태를 이루기를 바랄 것이다.\n\n$$\np(\\bold{w}) = \\mathcal{N}(\\bold{w}|0, \\Sigma)\n$$\n\n그리고, 이전에 MLE를 구할 때, Likelihood를 다음과 같이 정의했다.\n\n$$\n\\begin{align*}\np(\\bold{y}_*|\\bold{X}; \\theta) &= \\mathcal{N}(\\bold{y}_*-\\Phi\\bold{w}, \\sigma I) \\\\\np(\\bold{y}_*|\\Phi, \\bold{w}) &= \\mathcal{N}(\\bold{y}_*-\\Phi\\bold{w}, \\sigma I)\n\\end{align*}\n$$\n\n따라서, 우리는 이를 이용해서 posterior를 추론할 수 있다.\n\n$$\np(\\bold{w}|\\bold{y}_*, \\Phi) = {{p(\\bold{y}_*| \\Phi, \\bold{w})p(\\bold{w})}\\over{p(\\bold{y}_*|\\Phi)}}\n$$\n\n여기서 MAP를 구할 때에는 Lemma 정리(두 정규분포의 conditional Probability를 구하는 공식)를 이용하면 편하다. 따로 연산은 수행하지 않지만 결과 값은 아래와 같다.\n\n$$\n\\bold{w}_{MAP} = (\\sigma^2\\Sigma^{-1}+\\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y}\n$$\n\n여기서 만약 우리가 $\\Sigma = {\\sigma^2\\over{\\lambda}}I$라고 가정하면, 위의 MAP 식은 Ridge Regression과 동일해지는 것을 알 수 있다.\n\n즉, Ridge Regression은 MAP의 한 종류라고 볼 수 있는 것이다.\n\n$$\n\\bold{w}_{ridge} \\in {(\\bold{w}_{MAP}, \\Sigma)}\n$$\n\n## Gradient Descent\n\n여태까지 우리는 Loss를 정의하고, 이 Loss가 최솟값을 갖는 $\\bold{w}$를 찾는 것을 목표로 하였다. 하지만, 우리가 다루는 모든 Loss가 미분이 항상 쉬운 것은 아니다. 뿐만 아니라, Loss의 미분 값이 5차원 이상의 식으로 이루어진다면, 우리는 이를 풀 수 없을 수도 있다. 5차원 이상의 polynomial에서는 선형대수적인 해결법(근의 방정식)이 없다는 것이 증명되어있다.(Abel-Ruffini theorem)\n\n따라서, 우리는 Loss가 0이 되는 지점을 찾기 위해서, w의 값을 점진적으로 업데이트하는 방식을 활용한다. 이때, 우리는 w의 값이 계속해서 Loss를 감소시키기를 원한다. 따라서, 우리는 현재 $\\bold{w}$에서 Gradient를 현재 $\\bold{w}$에 빼준다. 이를 우리는 **Gradient Descent**라고 한다.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\gamma((\\nabla L)(\\bold{w}_{t}))^{\\top}\n$$\n\n여기서 $\\gamma$는 step size(learning rate)라고 하며, 기울기값을 얼마나 반영할지를 의미한다.\n\n---\n\n이제부터는 Gradient Descent를 더 효과적으로 진행하기 위한 3가지의 기술들을 추가적으로 제시한다.\n\n> **1. optimize stepsize**\n\nstepsize($\\gamma$)가 특정 상수로 제시된 게 아니라 변수로 표현된 이유는 linear regression마다 적절한 $\\gamma$가 다르기 때문이다. 하나의 예시를 들어보자.\n\n![loss-divergence](/images/loss-divergence.jpg)\n\n위는 Loss function이 convex할 때, 최솟값을 찾아나가는 과정이다. 만약, $\\gamma$가 크다면, Loss가 특정값으로 수렴하는 것이 아니라 발산하는 것을 알 수 있다. 이를 막기 위해 $\\gamma$를 굉장히 작은 수로 하는 경우에는 Loss의 최솟값을 찾기도 전에 특정 지점에서 멈춰버릴 수도 있다. 또한, Loss의 graph형태는 data마다 달라지기 때문에 절대적인 $\\gamma$역시 존재하지 않는다.\n\n따라서, 우리는 매 update마다 적절한 $\\gamma$를 찾을려고 노력한다. 여기서는 자세히 다루지 않지만 후에 더 다룰 기회가 있을 것이다. 간단히 프로그래밍적으로(systemical) 생각하면, 업데이트 이후 loss가 만약 그전 Loss보다 커진다면, 이를 취소하고 더 작은 $\\gamma$를 사용하도록 하고, 업데이트 된 후의 Loss와 그전 Loss가 같다면, 진짜 수렴하는지를 확인하기 위해서 $\\gamma$를 키워볼 수도 있다.\n\n> **2. momentum**\n\n우리가 Gradient Descent를 진행하다보면, 다음과 같은 현상을 자주 마주하게 된다.\n\n![momentum-example-1](/images/momentum-example-1.jpg)\n\n우리가 찾고자 하는 Loss를 찾아가는 과정에서 매 업데이트마다 반대방향으로 기울기가 바뀌는 경우이다.(진동한다) 이는 최종으로 찾고자 하는 값을 찾는 과정이 더 오래 걸리게 한다. 따라서, 우리는 이러한 진동을 막기 위해서 Momentum을 사용한다. 즉, 이전 차시에서의 gradient를 저장해두고, 이를 더해서 진동하는 것을 막는 것이다.\n\n$$\n\\bold{w}_{i+1} = \\bold{w}_{i} - \\gamma_{i}((\\nabla L)(\\bold{w}_{i}))^{\\top} + \\alpha \\Delta \\bold{w}_i ,( \\alpha \\in [0, 1] )\n$$\n\n$$\n\\Delta \\bold{w}_i = \\bold{w}_{i} - \\bold{w}_{i-1} = \\alpha \\Delta \\bold{w}_{i-1} - \\gamma_{i-1}((\\nabla L)(\\bold{w}_{i-1}))^{\\top}\n$$\n\n즉, 그림으로 표현하면, 다음과 같다.\n\n![momentum-example-2](/images/momentum-example-2.jpg)\n\n이전 변화량과 현재 변화량을 합하여 이동하기 때문에 위에 새로 추가된 것처럼 진동하지 않고, 진행하는 것을 볼 수 있다.\n\n> **3. Stochastic Gradient Descent**\n\n우리의 Gradient Descent의 가장 큰 문제는 바로 Global Minimum을 찾을 거라는 확신을 줄 수 없다는 것이다. 아래 그림을 보자.\n\n![gradient-descent-example](/images/gradient-descent-example.jpg)\n\n여기서 우리는 초기 w 값을 어떻게 정하냐에 따라서, **local minimum**을 얻게 되거나 **global minimum**을 얻게 된다. 즉, 초기값이 결과에 굉장히 큰 영향을 준다는 것이다.\n\n이를 해결할 수 있으며, 학습 효율도 높일 수 있는 것이 Stochastic Gradient Descent이다. 원리는 Loss를 구하기 위해서 전체 데이터(모집단)를 사용했었는데 그러지말고 일부 데이터를 랜덤하게 추출(sampling)해서(표본 집단) 이들을 통해서 Loss function을 구하기를 반복하자는 것이다.\n\n이 방식을 통해서 구한 Gradient의 평균이 결국은 전체 batch의 평균과 같다는 것은 Central Limit Theorem(중심 극한 정리)에 의해 증명이 된다. 따라서, 우리는 이를 통한 gradient descent도 특정 minimum을 향해 나아가고 있음을 알 수 있다.\n\n그렇지만, 표본 집단을 이용한 평균을 구했을 때에 우리는 noise에 의해서 local minimum으로만 수렴하는 현상을 막을 수 있다. 즉, gradient descent를 반복하다보면, 다른 local minimum으로 튀어나가기도 하며 global minimum을 발견할 확률을 높일 수 있는 것이다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- [Probabilistic interpretation of linear regression clearly explained](https://towardsdatascience.com/probabilistic-interpretation-of-linear-regression-clearly-explained-d3b9ba26823b), Lily Chen\n","slug":"ml-linear-regression","date":"2022-10-17 09:46","title":"[ML] 2. Linear Regression","category":"AI","tags":["ML","LinearRegression","BasisFunction","Regularization","GradientDescent","Momentum","StochasticGradientDescent"],"desc":"Regression(회귀)이라는 단어는 \"원래의 상태로 돌아간다\"로 돌아간다는 의미를 가진다. 결국 어떤 일련의 Event로 인해서 데이터에 Noise가 발생할 수 있어도 결국은 하나의 \"보편\"으로 시간이 지나면 수렴(회귀)할 것이라는 생각에 기반하는 것이다.  따라서, 우리는 이러한 \"보편\"을 찾기 위해서 우리가 알고 있는 독립 데이터 X를 통해서 알고자 하는 값 Y를 보편적으로 추론할 수 있다. 이 과정을 우리는 Regression이라고 부른다. 또한, X에 의해 독립적이지 않고 종속적인 Y의 관계가 Linear하게 표현될 때 이를 우리는 Linear Regression이라고 한다.  따라서, 해당 Posting에서는 Linear Regression을 바탕으로 Machine Learning이 어떻게 동작하는지를 이해하는 것이 목표이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nMachine Learning은 특정 목표를 달성하기 위해서 데이터로 부터 pattern 또는 가정 등을 유도해내는 방법이다.\n이를 위한 가장 일반적인 방법은 여러 개의 확률분포와 이것의 parameter의 조합(probabilistic model)들 중에서 측정된 데이터들을 가장 잘 나타내는 하나를 찾아내는 것이다.\n그 중에서, 확률 분포를 결정한 상태에서 parameter를 찾아나가는 형태의 접근법을 우리는 Parametric Estimation이라고 한다. 그 외에도 Nonparametric, Semi-parametric 방식도 존재하지만 이는 여기서는 다루지 않는다.\n\n## Small Example\n\n간단한 예시를 통해서 Parametric Estimation의 흐름을 익혀보자.\n\n한 학급에서 학생들의 형제자매 수에 대한 예측을 하고 싶다고 하자.  \n그렇다면, 우리는 먼저 조사(관측)를 수행해야 한다. 이를 통해서 다음과 같은 데이터를 얻게 되었다고 하자.\n\n| x        | 1    | 2    | 3    | 4    | 5    | 6    | x$\\geq$7 |\n| :------- | :--- | :--- | :--- | :--- | :--- | :--- | :------- |\n| $p(X=x)$ | 17   | 59   | 15   | 6    | 2    | 0    | 1        |\n\n여기서 우리는 여러 사전 지식을 활용하여 해당 데이터를 보았을 때, 해당 분포가 Poisson 분포의 형태라는 것을 알 수 있다.  \n따라서, 우리는 해당 분포를 Poisson이라고 가정한 다음에는 단순히 해당 분포에 대입하며, 가장 적절한 parameter만 찾으면 된다.  \n\n이 과정과 단순히 각 x에서의 확률값을 구하는 방식이랑 무엇이 다른지를 알아야지 해당 과정의 의의를 알 수 있다.\n먼저, 우리가 하고자 하는 일이 형제자매의 평균 수를 구한다고 하자. 이때의 평균 값과 Poisson 분포에서의 확률값은 다를 수 밖에 없다.\n\n이렇게 확률 분포를 구하는 것의 의미는 이것말고도 보지 않은 데이터(unseen data)를 처리함에 있다. 우리가 만약 모든 가능한 경우의 수를 모두 알고 있고, 이를 저장할 공간이 충분하다면,\n이러한 확률 분포를 구할 필요가 없다. 하지만, 우리가 원하는 추측은 unseen data에 대해서도 그럴사해야 한다. 이를 위해서는 결국 확률 분포가 필요하다.\n\n위의 예시에서 만약, 형제자매가 3명인 경우의 데이터가 없다고 하자. 이 경우에도 확률분포를 통한 추측을 한다면, 우리는 유의미한 값을 구할 수 있는 것이다.\n\n## Parametric Estimation\n\n> **정의**\n\nsample space $\\Omega$에서 통계 실험의 관측 결과를 통해서 얻은 sample $X_1$, $X_2$, ... , $X_n$이 있다고 하자. 각 sample에 대한 확률 분포를 우리는 $p_\\theta$라고 한다.\n여기서 $\\theta$는 특정 확률 분포에서의 parameter를 의미한다. 만약, bernoulli 라면, 단일 시행에 대한 확률이 될 것이고, binomial이라면, 단일 시행의 확률과 횟수가 해당 값이 될 것이다.\n\n> **성능 평가**\n\n여기서 우리가 찾기를 원하는 것은 전체 sample space $\\Omega$를 모두 잘 표현할 수 있는 $\\theta_{*}$(실제 true $\\theta$)를 찾는 것이다.(이미 확률 분포의 형태(함수, ex. Bernoulli, Binomial)는 이미 정의되어 있다.)  \n그렇다면, 실제 $\\theta_*$와 추측을 통해 만든 $\\hat{\\theta}$ 사이의 비교를 위한 지표도 필요할 것이다. 즉, 우리가 만든 확률 분포의 예측 성능평가가 필요하다는 것이다. 이를 측정하기 위해서 우리는 **Risk**라는 것을 사용한다.  \n간단하게도 실제 $\\theta_*$와 $\\hat{\\theta}$의 Mean Square Error를 계산한다.\n\n$$\n\\begin{align*}\nRisk &= E[(\\hat{\\theta} - \\theta_*)^2] = E[\\hat{\\theta}^2 - 2\\hat{\\theta}\\theta_* + \\theta_*^2] \\\\\n&= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 \\\\\n&= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 + (E^2[\\hat{\\theta}] - E^2[\\hat{\\theta}]) \\\\\n&= (E[\\hat{\\theta}] - \\theta_*)^2 + E[\\hat{\\theta}^2] - E^2[\\hat{\\theta}] \\\\\n&= {Bias}^2 + Var[\\hat{\\theta}]\n\\end{align*}\n$$\n\n해당 식을 분석해보면, 이와 같은 의미로 해석하는 것이 가능하다. 우리가 특정 확률 분포의 파라미터를 단 하나로 단정하고 Risk를 계산하는 경우는 Variance 값은 0이다. 즉, 해당 확률 분포가 가지는 Risk는 단순히 해당 parameter와 실제 parameter가 얼마나 찾이가 나는가를 의미한다.\n\n하지만, parameter를 특정하지 않고, 범위로 지정한다면, (예를 들어, 주사위를 던져 3이 나올 확률은 1/6 ~ 1/3이다.) 해당 확률의 평균과 Variance가 영향을 미칠 것이다.  \n다소 처음에는 헷갈릴 수 있지만, 해당 식에서 평균이 의미는 잘 확인하자. 특정 확률 분포를 가지도록 하는 $\\theta$가 $\\theta_*$ 에 얼마나 근접한지를 확인하기 위한 식이라는 것을 다시 한 번 기억하자.\n\n> **Estimation**\n\n이제부터는 앞에서 살펴보았던, parameteric estimation에서 어떻게 $\\hat{\\theta}$를 구할 수 있는지를 다룰 것이다. 확률/통계 이론에서는 크게 3가지로 나눌 수 있다고 볼 수 있다. 각 각을 살펴보도록 하자.\n\n<mark>**1. MLE**</mark>\n\nMaximum Likelihood Estimation의 약자이다. 여기서, Likelihood는 가능성이라는 뜻을 가지며, 확률/통계 이론에서 이는 확률을 해당 사건이 발생할 가능성으로 해석하는 것이다. 이를 이용해서 우리가 풀고자 하는 문제, 우리가 추측한 $\\theta$가 우리가 가진 Dataset를 만족시킬 가능성을 확인하기 위해 사용한다. 아래 수식을 보자.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta;\\mathcal{D}) &= p(\\mathcal{D}|\\theta) = p(x_1, x_2, ..., x_n|\\theta) \\\\\n&= \\prod_{i=1}^{n}{p(x_i|\\theta)}\n\\end{align*}\n$$\n\n(위 식을 이해하려면, 먼저 Dataset의 각 data들은 서로 independent하다는 사실을 기억하자.)  \n결국 $\\theta$가 주어졌을 때, Dataset일 확률을 구하는 것이다. 이를 다시 생각하면, $\\theta$가 얼마나 데이터셋의 확률을 잘 표현할 수 있는가와 같다.\n\n이것을 직관적으로 이해하려면 하나의 예시를 보면 좋다.\n\n![MLE example](/images/MLE-example.png)\n\n첫 번째 그래프는 같은 가우시안 분포 함수를 쓰면서, parameter만 다르게 한 경우이고, 아래는 실제 데이터의 분포라고 하자.(빨간색 선 하나 하나가 데이터를 의미)  \n이때, Likelihood를 각 각 구하면 각 x에서의 확률분포의 확률값을 모두 곱하면 된다. 그 경우 어떤 것이 제일 클지는 분명하다. 바로 파란색 분포일 것이다.  \n\n그렇다면, 우리가 원하는 것은 무엇인가? 바로 가장 높은 가능성을 가지게 하는 $\\theta$를 찾는 것이다. 따라서, 이를 식으로 표시하면 아래와 같다.\n\n$$\n\\hat{\\theta}_{MLE} = \\argmax_{\\theta}\\mathcal{L}(\\theta;\\mathcal{D})\n$$\n\n여기서 하나 문제가 있을 수 있다. 바로, 컴퓨터로 연산하게 되면 underflow가 발생하는 것이다. 특정 언어가 계산할 수 있는 소수점 범위를 벗어난다면, 제대로 된 결과를 얻을 수 없다. 이와 같은 문제를 **vanishing likelihood**라고 한다.  \n따라서, 우리는 log를 취했을 때와 log를 취하지 않았을 때의 경향성이 같음을 바탕으로 likelihood에 log를 취한 값을 이용하여 MLE를 구하는 것이 일반적이다. 이 방식을 maxmum log likelihood estimation 이라고 부른다.\n\n$$\n\\mathcal{l}(\\theta;\\mathcal{D}) = \\sum_{i=1}^{n}{\\log{(p(x_i|\\theta))}}\n$$\n\n이 방식을 이용하게 되면, 곱셈이 모두 덧셈으로 바뀌기 때문에 계산에서도 용이하다.\n\n여기까지 살펴보면, 하나의 의문이 들 수도 있다. 바로, $p(\\theta|\\mathcal{D})$도 측정 기준으로 사용할 수 있지 않냐는 것이다. 이 역시도 Dataset이 주어질 때, $\\theta$일 확률이라고 볼 수 있다.  \n어찌보면, 사람의 생각으로는 이게 더 당연하게 느껴질 수도 있다. 이는 바로 다음 MAP에서 다룰 것이다. 우선 MLE를 먼저한 이유는 이것이 더 구하기 쉽기 때문임을 기억해두자.\n\n```plaintext\n 🤔 증명\n\n (*해당 내용은 정보 이론에 기반한 MLE에 대한 추가적인 이해를 위한 내용입니다. 해당 내용은 자세히 알 필요까지는 없습니다.)\n\n 두 확률 분포 간 information Entropy의 차이를 나타내는 KL divergence의 최솟값을 구하는 것이 우리의 목표라고 정의할 수 있다.  \n 따라서, 우리가 결국 얻고자 하는 것은 확률 분포 함수가 주어졌을 때,  \n n이 무한대로 갈 때, 경험적 확률(empirical probability)에 가장 근사하는 parameter를 찾는 것이다.  \n 따라서, 우리는 KL divergence의 최솟값을 구하면 된다.\n```\n\n$$\n\\begin{align*}\n\\argmin_\\theta KL(\\tilde{p}||p_\\theta) &= \\argmin_\\theta \\int\\tilde{p}(x)\\log{\\tilde{p}(x)\\over{p_\\theta(x)}}dx \\\\\n&=\\argmin_\\theta[-\\int\\tilde{p}(x)\\log{\\tilde{p}(x)dx} - \\int\\tilde{p}(x)\\log{p_\\theta(x)dx}] \\\\\n&= \\argmax_\\theta\\int{\\tilde{p}(x)\\log{p_\\theta(x)}dx} \\\\\n&= \\argmax_\\theta\\sum_{i=1}^{n}{\\log{p_\\theta(x_i)}} \\\\\n&= \\theta_{MLE}\n\\end{align*}\n$$\n\n<mark>**2. MAP**</mark>\n\nMaximum A Posteriori의 약자이다. Posteriori는 사후 확률이라고도 부르며, dataset이 주어졌을 때, $\\theta$일 확률을 구하는 것이다.  \n이를 바로 구하는 것은 다소 어렵다. 왜냐하면, Dataset이 조건으로 들어가는 형태이기 때문이다. ($p(\\theta|\\mathcal{D})$)  \n따라서, 우리는 Bayes' Theorem에 따라서 이전에 배운 Likelihood와 parameter의 확률, 그리고 Dataset의 확률을 활용히여 풀어낼 것이다.\n\n$$\np(\\theta|\\mathcal{D}) = {p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}}\n$$\n\n여기서 주의해서 볼 것은 바로 $p(\\theta|\\mathcal{D})$와 $p(\\theta)$의 관계이다. dataset이 주어질 때의 parameter의 확률을 구하기 위해서 원래 parameter의 확률이 필요하다는 것이다.  \n어찌보면 굉장히 모순되어 보일 수 있지만, 우리가 이것을 사전 확률(priori)로 본다면 다르게 볼 여지가 있다.  \n예를 들면, 우리가 수상한 주사위로 하는 게임에 참가한다고 하자. 이때, 우리는 수상한 주사위의 실제 확률은 알 수 없지만, 주사위 자체의 확률은 모두 1/6이라는 것을 알고 있다. 따라서, $p(\\theta={1\\over6}) = \\alpha, p(\\theta\\neq{1\\over6}) = \\beta$ 라고 할 수 있다. 만약 정말 수상해보인다면, 우리는 $\\alpha$가 점점 작아진다는 식으로 표현할 수 있고, 하나도 수상해보이지 않는 일반 주사위라면, $\\alpha=1, \\beta=0$으로 할 수도 있다. 이 경우에는 likelihood 값에 상관없이 다른 모든 값이 0이기 때문에 결국은 $p(\\theta|\\mathcal{D}) = p(\\theta)$ 가 되는 것을 알 수 있다.\n\n최종적으로, MAP도 결국은 Dataset을 얼마나 parameter가 잘 표현하는가에 대한 지표로 사용할 수 있다.\n따라서, 이를 최대로 만드는 parameter는 $\\theta_*$와 굉장히 근접할 것이다.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{MAP} &= \\argmax_{\\theta}p(\\theta|\\mathcal{D}) \\\\\n&= \\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}} \\\\\n&=\\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)} \\\\\n&=\\argmax_\\theta{[\\red{\\log{p(\\mathcal{D}|\\theta)}} + \\blue{\\log{p(\\theta)}}]}\n\\end{align*}\n$$\n\nMLE와 마찬가지로 이 또한 연산 및 **vanishing**을 막기 위해서 log를 취한다. 사실상 likelihood와 사전 확률의 합을 최대로 하는 $\\theta$를 찾는 것이다.\n\n<mark>**3. Bayesian Inference**</mark>\n\n이제 마지막 방법으로 제시되는 Bayesian Inference이다. 이는 대게 Bayesian Estimation이라고 많이 불리는 것 같다. 이전까지 MLE, MAP는 결국 주어진 식을 최대로 하는 확정적 $\\theta$ 하나를 구하는 것을 목표로 했다.\n\nBayesian Inference는 Dataset이 주어졌을 때, $\\theta$의 평균값을 활용한다. 더 자세히 말하면, Posteriori(사후 확률)의 평균을 구하는 것이다.  \n이를 구하는 과정을 살펴보면 이해하는데 도움이 될 것이다. 한 번 살펴보자.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{BE}&= E[\\theta|\\mathcal{D}] \\\\\n&= {\\int_{0}^{1}{{\\theta}p(\\theta|\\mathcal{D})}d\\theta} \\\\\n&= {\\int_{0}^{1}{\\theta}{{p(\\mathcal{D}|\\theta)p(\\theta)}\\over{p(\\mathcal{D})}}d\\theta} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)p(\\mathcal{D}|\\theta)}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\mathcal{D}|\\theta)p(\\theta)}d\\theta}} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\theta)\\prod_{i=1}^{n}p(x_i|\\theta)}d\\theta}} \\\\\n\\end{align*}\n$$\n\n이를 구하는 과정은 이전과는 다르게 상대값이 아닌 평균을 구해야하기 때문에 posteriori(사후 확률,$p(\\theta|\\mathcal{D})$)를 구해야 한다.\n\n하지만, 여기서 잡기술이 하나 존재한다. 바로 **Conjugate Prior**이다.\n\n바로 두 확률 분포 함수(likelihood, prior)에 의한 posterior의 형태가 정해진 경우가 있기 때문이다.\n\n| Prior $p(\\theta \\mid \\alpha)$  | Likelihood $p(\\mathcal{D} \\mid \\theta)$                 | Posterior $p(\\theta \\mid \\mathcal{D}, \\alpha)$                                                                                                                                                                                                                   | Expectation of Posterior                                                                                                                                                       |\n| :----------------------------- | :------------------------------------------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Beta ($\\alpha, \\beta$)         | Benoulli ($\\sum _{i=1}^{n}x_{i}$)                       | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +n-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                                            | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + n}}$                                                                                                                   |\n| Beta ($\\alpha, \\beta$)         | Binomial ($\\sum _{i=1}^{n}N_{i}, \\sum _{i=1}^{n}x_{i}$) | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                         | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + \\sum _{i=1}^{n}N_{i}}}$                                                                                                |\n| Gaussian ($\\mu_0, \\sigma_0^2$) | Gaussian ($\\mu, \\sigma^2$)                              | Gaussian (${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu_{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum_{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right),\\left({\\frac {1}{\\sigma_{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}\\right)^{-1}}$) | ${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu_{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum_{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right)}$ |\n\n이를 이용하면, 우리는 간단하게 Posteriori의 평균을 구할 수 있다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n  ","slug":"ml-parametric-estimation","date":"2022-10-15 11:25","title":"[ML] 1. Parametric Estimation","category":"AI","tags":["ML","MLE","MAP","Bayesian"],"desc":"Machine Learning은 특정 목표를 달성하기 위해서 데이터로 부터 pattern 또는 가정 등을 유도해내는 방법이다.이를 위한 가장 일반적인 방법은 여러 개의 확률분포와 이것의 parameter의 조합(probabilistic model)들 중에서 측정된 데이터들을 가장 잘 나타내는 하나를 찾아내는 것이다.그 중에서, 확률 분포를 결정한 상태에서 parameter를 찾아나가는 형태의 접근법을 우리는 Parametric Estimation이라고 한다. 그 외에도 Nonparametric, Semi-parametric 방식도 존재하지만 이는 여기서는 다루지 않는다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nMachine Learning은 data들로 부터 특정 pattern을 나타내는 function을 만드는 것이라고 할 수 있다. 즉, pattern은 data에 대한 간단한 요약본이라고 볼 수 있다.\n확률/통계 이론 및 선형대수, 미적분, 정보 이론 관련 기본 내용을 해당 포스팅에 정리한다. 여기서 다루는 내용은 대게 많이 추상적인 내용이며, 키워드 중심의 내용이다. 만약, 추가적인 설명이 필요하다면 키워드를 기반으로 더 검색을 하는 것이 좋을 것이다.\n\n## Probability/Statisics\n\n확률과 통계는 대게 거의 동의어처럼 사용되지만, Statistics는 대게 과거를 의미할 때 사용하는 반면 Probability는 미래를 의미하는 용도로 많이 사용되어진다.\n\n### Probability Space\n\n확률 공간을 정의하는 것은 확률을 이해하는 토대가 된다. 확률을 적용하기 위한 공간을 먼저 살펴보자.\n\n- Sample Space($\\Omega$)  \n  가능한 모든 결과값의 집합이다.  \n  ex. 동전을 두 번을 던져서 나올 수 있는 모든 결과값은 $\\Omega = $ $\\{ hh, ht, th, tt \\}$\n- Event($E$)  \n  Sample Space의 Subset이다. Sample Space에서 발생할 수 있는 event라는 의미로 볼 수 있다.  \n  ex. 동전을 두 번을 던져서 모두 같은 면이 나오는 Event는 $E = $ $\\{ hh, tt \\}$\n- Field($\\mathcal{F}$)  \n  Sample Space에서 발생 가능한 모든 Event들의 집합이다.  \n  ex 동전을 두 번 던져서 나오는 결과값의 Field는 $\\mathcal{F} = $ $\\{$ $\\emptyset$, $\\{hh\\}$, $\\{ht\\}$, $\\{th\\}$, $\\{tt\\}$, $\\{hh, ht\\}$, $\\{hh, th\\}$, $\\{hh, tt\\}$, $\\{ht, th\\}$, $\\{ht, tt\\}$, $\\{th, tt\\}$, $\\{hh, ht, th\\}$, $\\{hh, ht, tt\\}$, $\\{hh, th, tt\\}$, $\\{ht, th, tt\\}$, $\\{hh, ht, th, tt\\}$ $\\}$\n- $\\sigma$-field  \n  자신 내부의 원소를 포함하는 합집합을 모두 포함하는 셀 수 있는 field를 sigma field라고 한다.  \n  이 $\\sigma$-field는 일반적인 확률과 특정 domain에서의 확률을 정의하는데 필요하다.  \n  우리가 sample space($\\Omega$)와 $\\sigma$-field $\\mathcal{F} \\subset 2^{\\Omega}$가 주어질 때, 확률P가 다음과 같이 mapping한다고 하자. $P: \\mathcal{F} \\mapsto [0, 1]$ 이때 P는 다음과 같은 특징을 가진다.\n  - $A \\in \\mathcal{F}$인 모든 A에 대해서 $P(A) \\leq 0$ 이다.  \n    $P(\\emptyset) = 0, P(\\Omega) = 1$\n  - $\\{A_i\\}_{i \\in I}$이고, 서로 다른 모든 i, j에 대해 $ A_{i}\\cup A_{j} = \\emptyset$이라면, 아래 식을 만족한다.  \n    $$P(\\cup_{i \\in I}A_i) = \\sum_{i \\in I}P(A_i)$$\n\n### Important properties of Probability\n\n- **Joint Probability**  \n  두 Event의 Joint Probability는 두 Event의 합집합의 확률을 의미한다.\n  $P(A, B) = P(A \\cap B)$\n- **Marginal Probability**  \n  대게 두 개 이상의 Event가 있을 때, 각 각의 Event의 확률을 특정할 때 사용한다.\n  $P(A), P(B)$\n- **Independence**  \n  두 Event가 독립이라는 의미는 서로의 Event가 서로 영향을 받지 않는다는 의미이다. <mark>**주의할 것은 이것이 의미하는 것이 두 Event의 교집합이 없다는 의미가 아니다.**</mark>  \n  예를 들어보면 다음과 같다. 우리가 위에서 예시로 사용한 두 개의 동전을 던진 결과를 보자. 두 개의 동전이 모두 앞면이 나오는 경우와 모두 뒷면이 나오는 경우는 서로 독립일까? 이는 독립이 아니다. 왜냐하면, 동전이 모두 앞면이 나오는 사건은 필연적으로 모두 뒷면이 나오는 사건은 반드시 일어나지 않을 것이라는 증거가 되기 때문이다. 반대로, 모두 앞면이 나오는 사건과 한 번만 앞면이 나오는 사건을 생각해보자. 하나의 사건이 일어났다고, 반드시 그 사건이 일어났거나 안일어났다는 관계를 밝혀낼 수 없다. 따라서, 이러한 경우 두 사건이 독립적이라고 한다.  \n  이를 수학적으로 표현하면, 다음과 같이 표현할 수 있다.  \n  $P(A, B)=P(A)P(B)$  \n  즉 위 공식이 성립하면 독립이며, 독립이라면 위의 식이 성립한다.\n- **Conditional Probability**  \n  두 Event가 있을 때, 하나의 Event가 발생했을 때 다른 하나의 Event가 발생할 확률을 의미한다. 따라서, 이는 다음과 같이 수식으로 표현할 수 있다.  \n  $P(A|B) = {{P(A, B)}\\over{P(B)}}, (P(B) \\neq 0)$  \n  여기서 independence 특성을 더 명확하게 확인할 수 있는데, 만약 A와 B가 독립이라면, $P(A|B) = P(A)$이다.  \n  즉, B가 발생했는지 여부는 A의 결과에 영향을 안준다는 것이다.\n- **Partition**  \n  Sample Space($\\Omega$)를 겹치지 않고, 모두 포함하는 Event의 집합을 의미한다. 따라서, 이를 식으로 다음과 같이 표현할 수 있다.  \n  $\\cup_{i=1}^{n}{P_i} = \\Omega$ 이고, $\\cap_{i=1}^{n}{P_i} = \\emptyset$\n- **Marginalization**  \n  전체 Sample space($\\Omega$)에 대하여 **B**가 이에 대한 partition일 때, 아래 공식이 성립한다.  \n  $P(A) = \\sum_{i=1}^{n}{P(A,B_i)} = \\sum_{i=1}^{n}{P(A|B_i)P(B_i)}$\n- **Bayes' Theorem**  \n  만약 $P(B) \\neq 0$라면, 아래 공식이 성립한다. 간단히 conditional probability를 풀어주면 아래 식을 얻을 수 있다.  \n  $P(A|B) = {P(B|A)P(A)\\over{P(B)}}$  \n  해당 식은 단순히 Joint Probability로 변환하고, 다시 반대 확률로 변경했을 뿐이다. 이 공식이 중요하다기 보다는 이 공식이 가지는 의미를 이해하는 것이 중요하다. 확률을 사건의 발생의 빈도로 이해하는 Frequentist Approach에서는 관측을 통해서 특정 데이터가 발생할 확률을 얻는다. 만약 우리가 원하는 확률이 관측을 통해서는 얻을 수 없는 데이터라고 하자. 이 경우에 우리는 확률의 역연산이 필요하다. 위의 공식을 보면 특이한 것이 보이는데, 바로 $P(A|B)$와 $P(A)$이다. 이는 전체 확률을 통해서 **Conditional Probability**를 찾는 것이다. 그렇기에 우리는 이를 역연산이라고 부르며, 우리가 가지고 있는 기존 **사전 확률**(Priority, 이전까지 맞을 거라고 생각한 확률)을 통해서 데이터가 주어졌을 때의 사건의 확률을 다시 계산해보는 것이다. 이 과정을 **Bayesian Update**라고 하는데 이 과정을 통해서 얻은 $P(A|B)$를 다시 다음 데이터에 대해서는 $P(A)$로써 활용하는 것이다. 이렇게 해서 우리는 점진적으로 $P(A)$를 찾아나갈 수 있다.\n\n### Random Variable\n\nRandom Variable이라는 것은 특정 사건을 수학적으로 표현하기 위해서 변형하는 과정을 의미한다. 우리는 이전 예시에서 두 개의 동전을 동시에 던져서 나온 결과를 Sample Space로 두었고, 이를 $\\Omega = $ $\\{ hh, ht, th, tt \\}$라고 표현했다. 하지만, 이와 같은 표기 방식은 수학적인 연산을 적용하기 어렵다. 따라서, 우리는 앞면이 나온 경우를 $X=1$, 뒷면이 나온 경우를 $X=-1$ 라고 하는 형태로 치환하는 것이다. 여기서 만들어진 X를 우리는 Random Variable이라고 부른다. 이런 치환을 통해서 우리는 확률을 Random Variable에 대한 함수로 표현할 수 있다.\n\n또, Random Variable을 정의하여 다음과 같은 값을 연속적으로 정의할 수 있다.\n\n- **Mean**  \n  Random Variable의 평균 또는 기댓값이라고 부른다.  \n  $\\mu_{X} = E[X] = \\sum_{x}{xP(X=x)}$\n- **Variance**  \n  평균에서 데이터가 떨어진 정도를 표현하는 값으로 분산이라고 부른다.  \n  $\\sigma_{X}^{2} = E[(X-\\mu_{X})^2] = E[X^2] -\\mu_{X}^{2}$\n- **Covariance**  \n  Random Variable X와 Y의 상관관계(Correlation)을 확인하는 척도로 사용한다.  \n  $cov(X, Y) = E[(X-\\mu_{X})(Y-\\mu_{Y})] = E[XY] -\\mu_{X}\\mu_{Y}$  \n  만약, 두 X와 Y가 서로 전혀 상관이 없다(Independent)면, $cov(X, Y) = 0$이다. 그 반대는 성립하지 않지만, 그럴 가능성이 굉장히 높아진다.\n- **Correlation Coefficient**  \n  Covariance보다 더 엄격한 상관관계를 확인하는 척도로 사용되는데, 단순히 Covariance를 각 표준편차($\\sigma$)로 나눈 것이다. 이로 인해 결과 값은 [-1, 1] 사이 값이 된다.  \n  $corr(X, Y) = {cov(X,Y)\\over{\\sigma_{X}\\sigma_{Y}}}$  \n  따라서, <mark>1일 수록 두 Random Variable의 상관성이 높으며 비례하는 관계라는 것을 의미하며, -1일 경우에는 상관이 높지만 반비례하는 관계라는 것을 의미한다. 반대로, 0인 경우는 상관 관계가 아주 낮음으로 독립일 가능성이 높다.</mark> 그렇다고 100%는 아니지만, 단지 그럴 확률이 굉장히 높다는 것이다. 주의할 점은 Correlation Coefficient가 1이라고 X가 Y의 원인이 되는 것은 아니라는 것을 유의해야 한다. 단순히 X가 일어났을 때, Y가 일어날 확률이 높다는 것이다.  \n\n### Law of Large Numbers\n\n경험적 확률과 수학적 확률 사이의 관계를 나타내는 법칙으로, 전체 경우의 수와 이에 따른 확률(모집단)이 있을 때, 관측한 경우의 수와 이에 따른 확률(표본 집단)은 관측 데이터의 크기가 커질 수록 표본 평균이 모평균에 가까워짐을 의미한다.\n\n### 자주 사용되는 Probability Distribution Function\n\n특정 task의 경우 이미 정의된 확률 분포를 통해서 표현할 수 있는 경우가 있다. 따라서, 아래와 같은 대표적인 확률 분포는 알아두는 것은 중요하다.\n\n- **Bernoulli distribution**  \n  하나의 사건이 일어날 확률을 의미한다. 발생하는 경우를 X=1, 그렇지 않은 경우를 X=0으로 random variable로 치환하여 나타낸 확률 분포(probability distribution)이다. 대표적인 사건은 동전 던지기와 같은 두 개의 결과만 갖는 binary event을 표현할 때이다.  \n  따라서, 사건이 일어날 확률을 p라고 할 때, 다음과 같이 Random Variable에 대한 확률을 정의할 수 있다.  \n  $P(X=x) = p^{x}(1-p)^{1-x}$\n  복잡해보이지만, 실상은 X가 0 또는 1이므로, $P(X=0)=1-p$이고, $P(X=1)=p$이다.\n  - 평균\n    $E[X] = p$\n  - 분산  \n    $Var[X] = E[X^2] - \\mu_{X}^2 = p - p^2 = p(1-p)$\n- **Binomial Distribution**  \n  확률이 p인 사건을 n번 수행했을 때, x번 발생할 확률을 의미한다. 따라서, Random Variable X의 범위는 {0, 1, …, n}이 된다. 대표적인 사건은 동전 던지기를 여러 번 던졌을 때, 앞 면이 x번 나올 경우의 수이다.  \n  이에 따라 Random Variable에 대한 확률을 정의하면 다음과 같다.  \n  $P(X=x) = {n \\choose x}p^x(1-p)^{n-x}$  \n  이 또한 복잡해 보이지만, 사실은 독립적인 Bernoulli의 연속 수행으로 볼 수 있다.  \n  - 평균  \n    $E[X] = np$\n  - 분산  \n    $Var[X] = Var[\\sum_{i}X_i]=\\sum_iVar[X_i]=np(1-p)$\n- **Beta Distribution**  \n  $\\alpha, \\beta > 0$를 만족하는 두 parameter를 이용한 probability distribution이다.  \n  이는 [0, 1]에서 continuous한 random variable를 이용할 수 있다. 이에 따른 확률은 다음과 같다.  \n  $P(X=x) \\propto x^{\\alpha-1}(1-x)^{\\beta-1}$  \n  이에 대한 의미를 이해하자면, 확률에 대한 확률 분포이다. 각 $\\alpha - 1$와 $\\beta - 1$를 성공 횟수, 실패 횟수라고 하자.  이는 이미 알고 있는 모집단(전체 집합)의 계산 결과이다. 그리고 random variable을 특정 event의 확률이라고 하자. 예를들면, 동전 던지기를 할 때, 앞면이 나올 확률이 $1\\over2$이라는 것을 이미 알고 있다. 따라서, 우리는 $\\alpha - 1$ = $\\beta - 1$ 라는 것을 알고 있는 것이다. 하지만, 실제로 동전 던지기를 5번 수행했을 때, 4번 앞면이 나왔다고 하자. 그렇다면, 우리가 추측한 해당 event의 확률은 $4\\over5$이 된다. 그렇다면, 실제로 해당 확률이 $4\\over5$일 확률을 얼마나 될까?  \n  이를 측정하기 위한 것이 Beta distribution인 것이다. 이에 따라, Beta distribution을 PDF로 표현하면 ${\\alpha\\over\\alpha+\\beta}$에서 높은 확률값을 가지는 것을 볼 수 있다.\n  - 평균  \n    $E[X] = {\\alpha\\over{\\alpha+\\beta}}$\n  - 분산  \n    $Var[X] = {\\alpha\\beta\\over{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}}$\n\n  위의 평균과 분산을 보면 알 수 있듯이, 만약 이전 모집단에서의 평균값에 대한 믿음이 크다면, 각각  $\\alpha, \\beta$의 비율은 유지하면서 상수배를 수행하여 평균은 동일하지만 분산 값을 더 적게 만들어 뾰족한 형태의 분포를 완성할 수도 있다. 이 경우에는 평균과 맞지 않는 표본집합에서의 평균을 굉장히 확률이 낮은 확률로 식별하는 것이다.\n- **Gaussian Distribution**  \n  $\\mu, \\sigma^2$를 parameter로 갖는 probability distribution이다.\n\n  이는 $[-\\infin, \\infin]$를 구간으로 continuous한 random varible을 이용한다. 이에 따른 확률은 다음과 같다. (단일 random variable인 경우)\n  \n  $P(X) = {1\\over{\\sqrt{2\\pi\\sigma^2}}}\\exp(-{1\\over{2\\sigma^2}}(X-\\mu)^2)$\n\n  이 분포는 굉장히 많은 곳에 사용되는데 <mark>우리가 생각할 수 있는 대게의 자연 발생에 의한 현상들(ex. 사람 키의 분포)이 이 분포를 따르기 때문이다.</mark> 그렇기에 다양한 환경에서 많이 사용되는 분포이다. 뿐만 아니라 (Lindeberg-Levy) **Central Limit Theoriem**에 따르면, 표본에서 얻은 표본 평균을 구하면 구할 수록 점점 Gaussian Distribution을 따라간다. 즉, $n \\rarr \\infin$이면, 표본 평균이 이루는 분포가 Gaussian이라는 것이다.\n  \n  추가적으로 알아볼 것은 바로 여러 개의 Random Variable로 Gaussian Distribution을 더 높은 차원으로 구성할 수 있다는 것이다. 이를 수행하면, Gaussian 분포가 평균과 분산을 포함하기 때문에 두 데이터의 경향성(Covariance)를 어느정도 파악할 수 있다.\n\n## Calculus\n\n일명 미적분학으로, 대게의 미적분 법칙은 모두 알고 있을 것이라고 생각하고 넘어간다.\n\n### Optimization\n\n정말 모두가 알고 있을 거 같지만, 그럼에도 중요하기 때문에 정리하고 넘어가자. 일반적으로 Optimization이란 최적값을 찾는 과정이다. 이 과정에서 대게 사용되는 것이 최솟값 또는 최댓값이다. 우리는 최솟값/최댓값을 미분을 통해서 구할 수 있다.\n\n여기서는 Convex라는 성질에 대해서 자세히 다루지 않는다. 시간이 있다면, 이에대한 개념도 반드시 숙지하기를 바란다.\n\n> **최대/최소 구하기**\n\n모두가 알다시피 함수의 미분은 기울기를 의미한다. 만약 함수의 미분에 특정 값을 대입할 경우 이는 그 지점에서의 기울기를 의미한다.\n\n먼저, 함수의 미분에 대입한 값이 0인 경우에 해당 값(극값)이 가지는 성질을 기억해야 한다. 만약, 값이 0으로 하는 값을 기준으로 좌우 부호가 바뀐다면, 이는 정말 극대, 극소라는 의미를 가진다. 즉, 해당 구간에서 최소와 최대라는 의미를 가지는 것이다.\n\n이를 이해하기 위해서는 함수의 기울기의 부호가 바뀌었다는 의미를 살펴보아야 한다. 이는 함수의 값이 구간 내에서 가장 작은 값(극소) 또는 구간 내에서 가장 큰 값(극대)라는 것을 의미한다. 왜냐하면, 직관적으로 기울기가 0이 되기 전까지는 계속해서 증가/감소해왔다는 것을 알기 때문이다. 따라서, 우리는 기울기가 0인 지점을 모두 찾아 비교하면, 그 안에서 최대/최소를 찾을 수 있을 것이다.\n\n그런데 어떻게 하면, 기울기가 0인 지점이 극대인지 극소인지를 구분할 수 있을까? 이것은 바로 직전의 값을 미분 함수에 대입해보면 쉽게 알 수 있다. 하지만, 이것이 매번 그렇게 쉽게 판별되는 것은 아니다. 따라서, 우리는 이중 미분을 사용한다. 이중 미분 함수에 극값을 대입했을 때 양수라면 이는 극소를 의미하고, 음수인 경우는 극대를 의미한다. 이 또한, 직관적으로 기울기의 변화량이라는 이중 미분의 정의를 알면, 직관적으로 와닿을 수 있다.\n\n이렇게 해서 극대와 극소를 골라내고, 이중에서 가장 큰 값과 가장 작은 값을 찾아내면, 우리는 이것을 함수의 최적화를 수행했다고 한다.\n\n### Constraint Optimization\n\n여기서는 특별한 case를 위한 예시이다. 특정 조건이 주어졌을 때, 이를 만족하면서 특정 함수의 optimization을 수행하는 것이다.\n\n그러면 우리가 최적화하고자 하는 목적함수($\\mathcal{J}(\\bold{x})$)와 등식 제약 조건($h_{j}(\\bold{x})$), 부등식 제약 조건($g_{i}(\\bold{x})$)을 살펴보자.\n\n우리는 모든 최적화 문제를 다음과 같은 형태로 묘사할 수 있다.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & \\mathcal{J}(\\bold{x}) &\\\\\n  \\text{subject to} \\quad & g_{i}(\\bold{x}) \\leq 0, & i = 1, ..., M \\\\\n                          & h_{j}(\\bold{x}) = 0, & j = 1, ..., L\n\\end{align*}\n$$\n\nmaximization인 경우는 음수를 취해서 결과를 구한 후 변환 시에 다시 음수를 취해주면 된다. 그리고 부등호가 반대인 제약 조건인 경우에도 양변에 음수를 취해서 간단하게 뒤집는 것이 가능하다.\n\n이러한 문제를 풀기 위해서는 우리는 식을 **Lagrangian** 형태로 변환해야 한다.\n\n> **Lagrangian**\n\n이는 조건부식에 있는 조건에 변수($\\nu$, $\\lambda$)를 곱한 값의 합과 원래 목적 함수($\\mathcal{J}(\\bold{x})$)의 합이다.\n\n$$\n\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) = \\mathcal{J}(\\bold{x}) + \\sum_{i=1}^{M}{\\nu_{i}g_{i}(\\bold{x})} + \\sum_{j=1}^{K}{\\lambda_{j}h_{j}(\\bold{x})}\n$$\n\n여기서 **Lagrangian** 함수의 optimization이 곧 목적함수의 optimization이다. 증명은 수행하지 않는다. 이에 대한 추가적인 설명이 필요한 경우에는 직접 찾아보아야할 것이다.  \n여기서 만약 등식만 있는 식인 경우에 우리는 간단히 모든 변수에 대해서 편미분이 0이 되는 등식을 이용해서, 최적 $\\bold{x}$를 찾을 수 있다. 위에 식에서 부등식 조건($g_{i}(\\bold{x})$)이 사라진다면, 우리는 미분을 통해서 처리해야하는 값은 총 x의 크기(N)와 L이다. 이는 우리가 편미분해서 구할 수 있는 식의 갯수와 똑같다. 즉, 우리가 모르는 변수는 N+M개 우리가 가진 등식은 N+M개이므로 연립해서 각 값을 구할 수 있는 것이다.\n\n하지만, 부등식인 경우에는 추가적으로 고려해줘야할 것이 있다.\n\n> **KKT Condition**\n\n이는 우리가 최적값($\\bold{x}_{*}$)를 찾았을 때, 다음과 같은 $\\boldsymbol{\\nu_{*}}$, $\\boldsymbol{\\lambda_{*}}$가 존재해야 한다는 정리이다.\n\n1. **Optimality**  \n   $\\nabla\\mathcal{L} = \\nabla\\mathcal{J}(\\bold{x}_{*}) + \\sum_{i=1}^{M}{\\nu_{*(i)}\\nabla g_{i}(\\bold{x}_{*})} + \\sum_{j=1}^{L}{\\lambda_{*(j)}\\nabla h_{j}(\\bold{x}_{*})} = 0$  \n   위에서 보았던 최적화를 수행하는 식이다.\n2. **Feasibility**  \n   $g_{i}(\\bold{x}_{*}) \\leq 0,  i = 1, ..., M$  \n   $h_{j}(\\bold{x}_{*}) = 0,  j = 1, ..., L$  \n   조건이 만족하는지를 확인하는 것이다.\n3. **Complementary slackness**  \n   $\\nu_{*(i)}g_{i}(\\bold{x}_{*}) = 0, i = 1, ..., M\\quad(\\nu_{*(i)} \\geq 0)$  \n   위의 식은 다소 헷갈릴 수 있는데 가장 알아듣기 쉬운 형태는 아래이다.  \n   $g_{i}(\\bold{x}_{*}) \\lt 0\\text{, then } \\nu_{*(i)} = 0$  \n   $g_{i}(\\bold{x}_{*}) = 0\\text{, then } \\nu_{*(i)} > 0$\n\n위의 식을 만족하는 $\\bold{x}_{*}$, $\\boldsymbol{\\nu_{*}}$, $\\boldsymbol{\\lambda_{*}}$를 찾으면, 그것이 최적값에서의 $\\bold{x}_{*}$이다.\n\n> **Lagrangian Dual Problem**\n\n여기서 한 발 더 나아가면, Lagrangian에 다시 한번 Lagrangian을 취할 수 있다.\n\n우리가 만약 Lagrangian의 하한을 $\\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda})$이라 하고,\n\n$$\n\\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) = \\inf_{\\bold{x} \\in \\mathcal{X}}\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda})\n$$\n\n$\\boldsymbol{f}^{*}$을 최적값이라고 한다면, 아래 식이 성립한다.\n\n$$\n\\boldsymbol{f}^{*} \\geq \\min_{\\bold{x}}\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) := \\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda})\n$$\n\n따라서, 우리는 $\\mathcal{G}$의 최댓값을 찾으면 해당 값이 최적해에 근사한다는 것을 알 수 있다.\n\n이는 우리가 풀고자 하는 문제의 형식을 다시 한 번 바꾸게 된다.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & \\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) &\\\\\n  \\text{subject to} \\quad & \\boldsymbol{\\nu}_{i} \\geq 0, & i = 1, ..., M\n\\end{align*}\n$$\n\n이 식을 KKT condition을 이용하여 푸는 것이 기존 식보다 쉽게 풀 수 있는 경우가 많다. 따라서, 이러한 형태로 문제를 풀이할 수도 있다.\n\n## Information Theory\n\n### Entropy\n\n수학에서 정보의 불확실성(uncertainty)을 표현하기 위해서 물리의 entropy 라는 개념을 도입한 것이다. 즉 정보가 가지는 \"surprise\" 정도가 크다면, entropy가 큰 정보를 의미하고, 일반적인 정보라면 이는 entropy가 작은 정보인 것이다.\n\n수학적으로 다시 정의하자면, 다음과 같다.  \nsample space $\\Omega$에서 정의된 random variable $X$와 확률 $p_{X}(x)$이 주어질 때, Entropy를 $H(x)$라 하자.\n\n$$\nH(X) = -\\sum_{x \\in \\Omega}p(x)\\log_{2}p(x)\n$$\n\n위 식에서 log의 밑이 2인 것을 알 수 있는데 computer science에서는 정보가 bit단위로 저장되기 때문에 기본적으로는 2를 사용한다. 하지만, 상황에 따라서는 다른 밑을 사용할 수도 있다.\n\n헷갈릴 수 있는데 표기법이 굉장히 다양하니 유의해서 보도록 하자.\n\n$$\nH(X) = H_{p}(X) = H(p) = H_{X}(p) = H(p_{X})\n$$\n\n> **최댓값과 최솟값**\n\nEntropy는 정보의 불확실성을 나타낸다고 했다. 즉, 정보가 확실할 수록 Entrophy는 0으로 수렴하며, 확실히 아는 정보의 경우 Entropy가 최솟값인 0이 된다.  \n반대로 Entropy의 최댓값의 경우 $|\\Omega| = n$이라고 할 때, $\\log_{2}{n}$이다. 이는 uniform distribution(모든 Random Variable의 확률이 같은 분포)일 경우이다.\n\n$$\n0 \\leq H(x) \\leq log_{2}{|\\Omega|}\n$$\n\n> **$\\bold{\\log_{2}({1 \\over p_{X}(x)})}$의 평균**\n\nEntropy를 random variable x의 확률의 역수의 log를 취한 값으로 해석할 수도 있다.\n\n$$\n\\begin{align*}\nE[\\log_{2}(({1 \\over p_{X}(x)})] &= \\sum_{x \\in X}p_{X}(x)\\log_{2}({1 \\over p_{X}(x)}) \\\\\n&= -\\sum_{x \\in X}p_{X}(x)\\log_{2}(p_{X}(x)) \\\\\n&= H(p_{X})\n\\end{align*}\n$$\n\n여기서 우리는 $\\log_{2}({1 \\over p_{X}(x)})$을 **정보량**이라고 정의한다. 식에서도 알 수 있지만, 정보량과 해당 정보량을 얻을 확률은 반비례한다.\n\n> **Joint, Conditional Entropy**\n\nRandom Variable이 두 개 이상일 때, 이를 적용할 수 있다. 유도 과정은 $H(X)$가 Expectation과 어떤 관계였는지를 떠올려 보면 알 수 있다.\n\n- **Joint Entropy** : $H(X, Y) = -\\sum_{x \\in \\Omega_{X}}\\sum_{y \\in \\Omega_{Y}}p(x, y)\\log_{2}p(x, y)$\n- **Conditional Entropy** : $H(Y|X) = -\\sum_{x \\in \\Omega_{X}}\\sum_{y \\in \\Omega_{Y}}p(x, y)\\log_{2}p(y|x)$\n\n> **properties**\n\n1. **Chain Rule**  \n   $H(X, Y) = H(Y|X) + H(X)$  \n   $H(X, Y) = H(X|Y) + H(Y)$\n2. **Conditional Entropy's Maximum**  \n   $H(Y|X) \\leq H(Y)$  \n   독립일 때 같아지며 그 외에는 항상 Conditional의 Entropy가 더 낮다. 의미를 이해하면 쉽다. 한 마디로 X라는 정보가 Y라는 정보가 발생할 확률에 대한 티끌이라도의 힌트를 준다면, 해당 불확실성은 감소하게 되는 것이다.\n3. **Joint Entropy's Maximum**  \n   $H(X, Y) \\leq H(X) + H(Y)$  \n   동일하게 독립일 때 같아지며, 그 외에는 항상 Joint의 Entropy가 더 낮다. 이 또한, 두 사건이 티끌이라도 겹치는 Event가 있다면, 각 Entropy를 더하는 것보다 당연히 작을 수 밖에 없는 것이다.\n4. **Concave**  \n   Entropy의 그래프는 항상 concave하다.\n\n> **Coding**\n\n정보 이론이 활발하게 사용되는 예시 중에 하나가 바로 데이터의 Encoding/Decoding을 수행하여 bit data로 mapping할 때이다. variable length encoding을 알고 있다면, 이에 대한 이해가 쉬울 것이다. 쉽게 설명하면, 데이터를 bit sequence로 mapping할 때 모든 데이터에게 동일한 bit sequence의 길이만큼을 할당하는 게 아니라 빈도가 높은 데이터부터 짧은 bit sequence 길이를 할당하는 방식이다. 이때 bit sequence의 길이를 Entropy를 이용해서 구할 수 있다. 이 길이는 항상 해당 데이터의 Entropy보다는 커야 한다. 따라서, 해당 Entropy보다 큰 가장 작은 자연수가 해당 데이터의 Bit Sequence 길이의 최적값이다.\n\n### KL divergence(Relative Entropy)\n\nKullback-Leibler Divergence의 약자로, 우리가 구하고자하는 실제 확률(p)과 추측 확률(q) 사이의 오차를 계산할 때 사용하는 지표이다. 따라서, 동일한 Sample Space와 Random Variable에 대한 서로 다른 확률 분포를 비교한다고 생각할 수 있다.\n\n$$\nD(p||q) = KL(p||q) = \\sum_{x \\in \\Omega}p(x)\\log_{2}(p(x)/q(x)) = E_{p}[\\log_{2}({p(x) \\over q(x)})]\n$$\n\n아쉽게도 KL divergence는 거리와 같은 연산을 적용할 수 없다. 즉, 둘 사이의 역연산은 같지 않을 뿐만 아니라($D(p||q) \\neq D(q||p)$), 서로 다른 Random Variable의 KL divergence의 합이 Random Variable의 합의 KL divergence와는 다르다.\n\n### Mutual Information\n\nKL divergence를 활용하여 서로 다른 Random Variable X,Y의 연관성을 유추하는 것도 가능하다.\n\n$$\nI(X, Y) = D(p(x,y) || p(x)p(y))\n$$\n\n$I$값이 의미하는 것은 Y를 아는 것이 X의 추측을 얼마나 쉽게하는지에 대한 지표로 작용한다.\n\n증명 과정은 생략하지만, 위의 식을 정리하면 결국은 아래와 같아지기 때문이다.\n\n$$\n\\begin{align*}\n  I(X, Y) &= H(X) - H(X|Y) \\\\\n  &= H(Y) - H(Y|X)\n\\end{align*}\n$$\n\n### Cross Entropy\n\n우리가 특정 corpus(dataset)를 통해서 확률을 추정했다고 해보자. 그렇다면, 우리는 이 가설을 증명하기 위해서 다른 data에 대해서 해당 추측이 적절한지를 확인하여 정당성을 증명할 수 있다. 그러기 위해서 우리가 만든 확률에서 정보량을 추출하고, 이를 우리가 알고 있는 data의 공간에 넣었을 때의 확률을 추정하기 위해서 Cross Entropy를 사용할 수 있다.\n\n$$\nH_{q}(p) = \\sum_{x \\in \\Omega}q(x)\\log_{2}{1 \\over p(x)}\n$$\n\n간단하게 요약하면, 위 식은 틀릴 수 있는 정보를 갖고 구한 최적의 Entropy로, 정보량에 추측 정보량을 넣고, 확률에는 실제 확률을 넣는 방식이다.\n\n또한, 이는 다음과 같이 변형되기도 한다.\n\n$$\nH_{q}(p) = H(q) + D(q || p)\n$$\n\n또한, 특정 조건이 주어졌을 때의 Conditional Cross Entropy는 다음과 같다.\n\n$$\nH_{q}(p) = - \\sum_{y \\in \\Omega_{Y}}\\sum_{x \\in \\Omega_{X}}q(y,x)\\log_{2}p(y|x)\n$$\n\n하지만, 실제 구현 level에서는 이러한 Cross Entropy를 정석으로 구하는 것은 비용이 크다. 따라서, 이와 근사하는 식을 사용한다.\n\n$$\n\\begin{align*}\n  H_{q}(p) &= - \\sum_{y \\in \\Omega_{Y}}\\sum_{x \\in \\Omega_{X}}q(y,x)\\log_{2}p(y|x) \\\\\n  &= {1\\over{|T_{Y}|}}\\sum_{i=1..|T_{Y}|}{\\log_{2}p(y_{i}|x_{i})}\n\\end{align*}\n$$\n\n위 식을 이용할 때에는 실제 데이터와 비교에 사용해서는 안된다. 대신 두 개의 서로 다른 p,q가 있을 때, s라는 실제 분포에 어떤 것이 더 적절한지를 판명할 때 사용할 수 있다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-base-knowledge","date":"2022-10-14 19:28","title":"[ML] 0. Base Knowledge","category":"AI","tags":["ML","Probability","Calculus","InformationTheory"],"desc":"Machine Learning은 data들로 부터 특정 pattern을 나타내는 function을 만드는 것이라고 할 수 있다. 즉, pattern은 data에 대한 간단한 요약본이라고 볼 수 있다.확률/통계 이론 및 선형대수, 미적분, 정보 이론 관련 기본 내용을 해당 포스팅에 정리한다. 여기서 다루는 내용은 대게 많이 추상적인 내용이며, 키워드 중심의 내용이다. 만약, 추가적인 설명이 필요하다면 키워드를 기반으로 더 검색을 하는 것이 좋을 것이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"AI"}},"__N_SSG":true}