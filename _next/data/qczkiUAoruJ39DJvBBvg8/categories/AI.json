{"pageProps":{"posts":[{"content":"\n## Intro\n\nClusteringê³¼ ê°™ì€ Unsupervised Learningìœ¼ë¡œ Feature Selection ë˜ëŠ” Feature Extraction ë“± ì—¬ëŸ¬ê°€ì§€ ì´ë¦„ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” Dimensionality Reduction ê¸°ë²•ì— ëŒ€í•´ì„œ ë‹¤ë£° ê²ƒì´ë‹¤. íŠ¹ì • dataì—ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” data ì „ì²´ë¥¼ ë³¼ í•„ìš”ê°€ ì—†ë‹¤. ë”°ë¼ì„œ, featureë“¤ì„ ìµœì†Œí•œìœ¼ë¡œ ì¤„ì´ë©´ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ë§Œ ìˆë‹¤ë©´ êµ‰ì¥íˆ íš¨ìœ¨ì ì¸ inferencingê³¼ learningì„ í•  ìˆ˜ ìˆë‹¤.\n\n## Dimensionality Reduction\n\nê°€ì¥ AIê°€ í™œë°œí•˜ê²Œ ì—°êµ¬ë˜ê³  ìˆëŠ” ë¶„ì•¼ëŠ” Visonê³¼ Natural Language ë¶„ì•¼ì¼ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ë“¤ ë¶„ì•¼ì˜ ê³µí†µì ì€ dataì˜ í¬ê¸°ê°€ êµ‰ì¥íˆ í¬ë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìˆ«ìë¥¼ í‘œí˜„í•˜ëŠ” í•˜ë‚˜ì˜ ì´ë¯¸ì§€ê°€ $1920\\times1080$ ì •ë„ì˜ í™”ì§ˆì´ê³ , ê²€ì€ ìƒ‰ ë˜ëŠ” í°ìƒ‰ìœ¼ë¡œ êµ¬ë¶„í•˜ëŠ” bitmap í˜•ì‹ì´ë¼ê³  í•˜ì. ì´ ë•Œ ìš°ë¦¬ê°€ ê° imageì— ì¡´ì¬í•˜ëŠ” ìˆ«ìë¥¼ classificationí•˜ê³  ì‹¶ì€ ê²½ìš°, dataì— ì‚¬ìš©ë˜ëŠ” featureê°€ $1920\\times1080=2,073,600$ì´ë‹¤. ìƒë‹¹íˆ í° ìˆ«ìì´ê³ , í™”ì§ˆì´ ì»¤ì§ˆ ìˆ˜ë¡ ê·¸ë¦¬ê³  ìƒ‰ì´ ìƒê¸¸ ìˆ˜ë¡ ì´ ê°’ì€ ì»¤ì§ˆ ê²ƒì´ë‹¤. Natural Language ë¶„ì•¼ì—ì„œë„ ë§ˆì°¬ê°€ì§€ì´ë‹¤. ì˜ì–´ ë‹¨ì–´ì˜ ì¢…ë¥˜ë§Œ ë”°ì ¸ë„ 170,000ê°œê°€ ë„˜ëŠ”ë‹¤. ì¦‰, ì´ë“¤ ê° ê°ì„ ë‹¨ìˆœ one-hot encodingìœ¼ë¡œ í‘œí˜„í•œë‹¤ë©´, dataì— ì‚¬ìš©ë˜ëŠ” featureì˜ ìˆ˜ëŠ” 170,000ê°œê°€ ë„˜ëŠ”ë‹¤. ì´ë ‡ê²Œ featureì˜ ìˆ˜ê°€ ë§ì•„ì§€ë©´, ìš°ë¦¬ëŠ” dataë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê¸°ê°€ ë§¤ìš° ì–´ë ¤ì›Œì§„ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” featureì˜ ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ë°˜ë“œì‹œ í•„ìš”í•˜ë‹¤.\n\nì´ë¥¼ ìœ„í•´ì„œ, ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ insightê°€ í•„ìš”í•˜ë‹¤. dataì—ì„œ í•„ìš”í•œ ë¶€ë¶„ì´ ëˆˆìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒì´ ì•„ë‹ ìˆ˜ë„ ìˆë‹¤ëŠ” ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìš°ë¦¬ê°€ $10\\times10$ í¬ê¸°ì˜ bitmapì— ìˆ«ìê°€ ì“°ì—¬ìˆë‹¤ê³  í•˜ì.\n\n![ml-observed-dim](/images/ml-observed-dim.png)\n\nì´ë•Œ ìš°ë¦¬ê°€ ê°–ëŠ” ê²½ìš°ì˜ ìˆ˜ëŠ” $2^{100}$ê°œì¼ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ìˆ«ì í˜•íƒœë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” dataê°€ ì‚¬ì‹¤ì€ ë” ë§ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ìš°ë¦¬ëŠ” ì´ ê²½ìš°ì˜ ìˆ˜ë„ ê³ ë ¤í•˜ëŠ” dimensionì—ì„œ inferencingê³¼ learningì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì‹¤ì œë¡œ ê´€ì¸¡í•œ observed dimensionalityë³´ë‹¤ëŠ” ë” í›Œë¥­í•œ true dimensionalityë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. ì´ë¥¼ ìœ„í•´ì„œ, ë‹¤ì–‘í•œ Dimensionality Reduction ê´€ë ¨ ë°©ë²•ë“¤ì´ ì¡´ì¬í•œë‹¤. ê·¸ ì¤‘ì—ì„œ PCAê°€ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆëŠ” ë°©ë²•ì´ë‹¤.\n\n## Principal Component Analysis\n\nPrincipal Component Analysis(PCA)ëŠ” í•µì‹¬ì´ ë˜ëŠ” principal component(basis, ê¸°ì €, ì°¨ì› ì¶•)ë¥¼ ì¬ì •ì˜í•˜ì—¬ ì°¨ì›ì„ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì–´ë–¤ basisê°€ ì¢‹ì€ basisì¸ì§€ë¥¼ ì•Œ ìˆ˜ ìˆì„ê¹Œ? ì•„ë§ˆë„ ì¢‹ì€ basisëŠ” ì´ì „ basisì—ì„œ ê°€ì§€ê³  ìˆë˜ ì •ë³´ë“¤ì„ ìµœëŒ€í•œ ë³´ì¡´í•  ìˆ˜ ìˆë‹¤ë©´ ì¢‹ë‹¤ê³  í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì—¬ê¸°ì„œ ì–´ë–»ê²Œ informationì„ ë§ì´ ë³´ê´€í•  ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ í†µì°°ì´ í•„ìš”í•˜ë‹¤.\n\n![ml-pca-1](/images/ml-pca-1.png)\n\nìœ„ì˜ ê·¸ë¦¼ì„ ë´¤ì„ ë•Œ, ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ ì¤‘ ì–´ë–¤ basisê°€ ë” ì¢‹ì€ basisì¼ì§€ë¥¼ ë³´ì. ì°¨ì›ì„ ì˜®ê¸´ë‹¤ëŠ” ê²ƒì€ ê¸°ì¡´ basisì—ì„œì˜ dataë¥¼ ìƒˆë¡œìš´ basisë¡œ projectioní•˜ëŠ” ê²ƒì´ë‹¤. ë‹¤ì‹œ ë§í•´, í•´ë‹¹ ì¶•ì— ì§ê°ìœ¼ë¡œ dataë¥¼ ë‚´ë ¤ë³´ë‚´ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê²Œ í–ˆì„ ë•Œ, ì™¼ìª½ ê·¸ë¦¼ì´ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ë³´ë‹¤ ë” ë„“ê²Œ dataê°€ í¼ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, ì°¨ì› ì´ë™ ì‹œì— ì¶©ëŒë¡œ ì¸í•´ ì‚¬ë¼ì§€ëŠ” dataì˜ ìˆ˜ê°€ ë” ì ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì´ì— ë”°ë¼ ë” ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì€ dataë¥¼ ìµœëŒ€í•œ ë„“ê²Œ í¼íŠ¸ë¦´ ìˆ˜ ìˆëŠ” basisë¥¼ ê°€ì§€ëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. <mark>**ë”°ë¼ì„œ, PCAëŠ” ê¸°ì¡´ basisë³´ë‹¤ ì ì€ ìˆ˜ì˜ ìƒˆë¡œìš´ basisë¡œ ì˜®ê¸°ëŠ” ê³¼ì •ì—ì„œ ê¸°ì¡´ dimensionì—ì„œì˜ ì •ë³´ë¥¼ ìµœëŒ€í•œ í¬í•¨í•˜ê¸° ìœ„í•´ì„œ varianceê°€ ìµœëŒ€ê°€ ë˜ëŠ” basisë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.**</mark> ê·¸ëŸ¼ ì´ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•´ë³¼ ê²ƒì´ë‹¤. ìš°ì„ ì€ ì´í•´ë¥¼ ìœ„í•´ì„œ ìƒˆë¡œìš´ basisì˜ ìˆ˜ë¥¼ 1ì´ë¼ê³  ê°€ì •í•˜ê³  ìš°ë¦¬ëŠ” sample meanê³¼ varianceë¥¼ ì´ìš©í•´ì„œ ì´ë¥¼ ì¶”ë¡ í•  ê²ƒì´ë‹¤. (í•´ë‹¹ basisê°€ $u_{1}$ì´ë‹¤.)\n\n$$\n\\begin{align*}\nVar[u_{1}^{\\top}x] &= E[(u_{1}^{\\top}x - E[u_{1}^{\\top}x])^{2}] \\\\\n&= E[(u_{1}^{\\top}x - u_{1}^{\\top}\\bar{x})^{2}] \\quad (\\bar{x} = E[x] = \\frac{1}{N}\\sum_{n\\in[N]}x_{n}) \\\\\n&=\\frac{1}{N}\\sum_{n\\in[N]}(u_{1}^{\\top}x_{n} - u_{1}^{\\top}\\bar{x})^{2} \\\\\n&=\\frac{1}{N}\\sum_{n\\in[N]}(u_{1}^{\\top}x_{n} - u_{1}^{\\top}\\bar{x})^{\\top}(u_{1}^{\\top}x_{n} - u_{1}^{\\top}\\bar{x}) \\\\\n&=\\frac{1}{N}\\sum_{n\\in[N]}(u_{1}^{\\top}(x_{n}-\\bar{x})(x_{n} -\\bar{x})^{\\top}u_{1}) \\\\\n&=u_{1}^{\\top} \\times \\{\\frac{1}{N}\\sum_{n\\in[N]}(x_{n}-\\bar{x})(x_{n} -\\bar{x})^{\\top}\\} \\times u_{1} \\\\\n&=u_{1}^{\\top} S u_{1} \\quad (S = \\frac{1}{N}\\sum_{n\\in[N]}(x_{n}-\\bar{x})(x_{n} -\\bar{x})^{\\top})\n\\end{align*}\n$$\n\nê²°êµ­ ìš°ë¦¬ê°€ ì–»ê³ ì í•˜ëŠ” ë‹¤ë¥¸ basisì—ì„œì˜ varianceë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” ê¸°ì¡´ ì°¨ì›ì—ì„œì˜ ëª¨ë“  dataë“¤ì˜ covariance($S_{ij} = Cov[x_{i}, x_{j}]$)ë¥¼ êµ¬í•´ì•¼í•œë‹¤.\n\nì ì´ì œ ì´ë¥¼ maximizationí•˜ëŠ” ë‹µì„ ì°¾ì•„ë³¼ ê²ƒì´ë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œ, ìš°ì„  basisì˜ í¬ê¸° ì—­ì‹œ varianceì˜ ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ unit vectorë¡œ ì œí•œí•´ì•¼ í•œë‹¤. ì´ë¥¼ ì¢…í•©í•˜ë©´ ê²°ë¡ ì ìœ¼ë¡œ dimensionì„ 1ë¡œ ë°”ê¾¸ëŠ” PCAëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\text{maximize }&\\quad u_{1}^{\\top} S u_{1} \\\\\n\\text{subject to }&\\quad u_{1}^{\\top}u_{1} = 1\n\\end{align*}\n$$\n\nìœ„ì˜ maximization problemì„ í•´ê²°í•´ë³´ì. ê·¸ëŸ¬ë©´ ë‹¤ìŒê³¼ ê°™ì€ lagrange functionì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = u_{1}^{\\top} S u_{1} + \\lambda(1-u_{1}^{\\top}u_{1})\n$$\n\nì´ë¥¼ ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n\\frac{\\partial\\mathcal{L}}{\\partial u_{1}} &= 2S u_{1} - 2\\lambda u_{1} = 0 \\\\\nS u_{1} &= \\lambda u_{1}\n\\end{align*}\n$$\n\nì¦‰, ìš°ë¦¬ê°€ ì°¾ê³ ì í•˜ëŠ” basisëŠ” S matrixì˜ eigenvector ì¤‘ í•˜ë‚˜ì˜ eigenvectorì¸ ê²ƒì´ë‹¤. ê°„ë‹¨í•˜ê²Œ eigenvectorì™€ eigenvalueê°€ ë­”ì§€ë¥¼ ì„¤ëª…í•˜ìë©´, ìœ„ì²˜ëŸ¼ ë™ì¼í•œ vectorì— ëŒ€í•´ì„œ, matrixì™€ vectorì˜ ê³±ì´ scalarì™€ vectorê³¼ ë™ì¼í•˜ê²Œ í•˜ëŠ” scalar(eigenvalue)ì™€ vector(eigenvector)ë¥¼ ì˜ë¯¸í•œë‹¤. (í›„ì— ì‹œê°„ì´ ë˜ë©´ í•´ë‹¹ ê°œë…ì„ ë‹¤ë£¨ê² ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” eigenvalueì™€ eigenvectorì— ëŒ€í•œ ê°œë…ì€ ìƒëµí•œë‹¤.)\n\nê·¸ë¦¬ê³  ìš°ë¦¬ê°€ êµ¬í•œ ì‹ì„ maximization ì‹ì— í•œ ë²ˆ ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\text{maximize }& \\quad u_{1}^{\\top} S u_{1} \\\\\n\\text{maximize }& \\quad u_{1}^{\\top} \\lambda u_{1} \\\\\n\\text{maximize }& \\quad \\lambda u_{1}^{\\top} u_{1} \\\\\n\\text{maximize }& \\quad \\lambda\n\\end{align*}\n$$\n\në”°ë¼ì„œ, eigenvalue ì¤‘ ê°€ì¥ í° ê°’ì„ ê°€ì§ˆ ë•Œì˜ eigenvectorê°€ ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” basisê°€ ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ, ê²°ë¡ ì ìœ¼ë¡œ $u_{1}^{\\top} S u_{1} = \\lambda$ì´ê¸° ë•Œë¬¸ì— $\\lambda$ê°€ varianceê°€ ëœë‹¤ëŠ” ê²ƒë„ í¬ì¸íŠ¸ ì¤‘ í•˜ë‚˜ì´ë‹¤.\n\në˜í•œ ìµœì¢…ì ìœ¼ë¡œ ì´ë¥¼ í™•ì¥í•´ì„œ ì´ì œ Mê°œì˜ basisë¥¼ ì‚¬ìš©í•˜ëŠ” PCAë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\text{maximize }\\quad& \\sum_{i \\in [M]} u_{i}^{\\top} S u_{i} \\\\\n\\text{subject to }\\quad& u_{i}^{\\top}u_{j} = \\delta_{ij} \\quad \\forall i,j \\in [M] \\\\\n&(\\delta_{ij} = \\begin{cases} 1 & i=j \\\\ 0 & i \\neq j \\end{cases})\n\\end{align*}\n$$\n\në”°ë¼ì„œ, PCAëŠ” dataì˜ covariance matrixì— ëŒ€í•œ eigenvalue decompositionì„ í†µí•´ ì–»ì€ eigenvalue ì¤‘ì— í° ê°’ì„ ê°€ì§€ëŠ” ê²ƒì„ ì´ Mê°œ ë½‘ê³ , ì´ì— ìƒì‘í•˜ëŠ” eigenvectorë“¤ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³ , ì´ì— ë”°ë¼ì„œ ìš°ë¦¬ê°€ ê°€ì§€ëŠ” Mê°œì˜ basisì˜ eigenvalue($\\lambda$)ì˜ í•©ì€ ìš°ë¦¬ê°€ ì˜®ê¸´ ì°¨ì›ì—ì„œ ê°–ê³  ìˆëŠ” ì´ varianceë¥¼ ì˜ë¯¸í•œë‹¤.\n\n### Other Approaches\n\nìœ„ì—ì„œ ìš°ë¦¬ëŠ” PCAë¥¼ varianceë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” basisë¥¼ ì°¾ëŠ” ê²ƒìœ¼ë¡œ ì •ì˜í–ˆë‹¤. í•˜ì§€ë§Œ, ê´€ì ì„ ë°”ê¿”ì„œ ë¬¸ì œë¥¼ projection error, ê¸°ì¡´ dataì™€ projected data ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•  ìˆ˜ ìˆëŠ” basisë¥¼ ì°¾ëŠ” ê²ƒìœ¼ë¡œ ì •ì˜í•  ìˆ˜ë„ ìˆë‹¤. ì´ ë˜í•œ ìƒê°í•˜ê¸°ì— í•©ë¦¬ì ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆëŠ”ë° ì´ë¥¼ ì‹¤ì œë¡œ ìˆ˜í•™ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ ì•Œì•„ë³´ê² ë‹¤.\n\në¨¼ì €, ìš°ë¦¬ê°€ ì´ Dê°œì˜ unit vectorê°€ ìˆê³ , ì´ ì¤‘ì— ì„ì˜ì˜ Mê°œì˜ vectorë¥¼ basisë¡œ í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ reductionì„ í•œë‹¤ê³  í•´ë³´ì. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Nê°œì˜ dataë“¤ ì¤‘ í•˜ë‚˜ì¸ $x_{n}$ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\n  x_{n} &= \\sum_{i=1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\} \\\\\n  &= \\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\}\n\\end{align*}\n$$\n\nê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” ì˜®ê²¨ì§„ data($\\tilde{x}_{n}$)ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n  \\tilde{x}_{n} &= \\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(\\bar{x}^{\\top}u_{i})u_{i}\\}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆë‹¤. $\\sum_{i=M+1}^{D}$ ë¶€ë¶„ì´ ì™€ë‹¿ì§€ ì•Šì„ ê²ƒì´ë‹¤. ì™œëƒí•˜ë©´, ìš°ë¦¬ëŠ” projected dataê°€ ì‹¤ì œë¡œëŠ” $\\sum_{i=1}^{M}$ ë¶€ë¶„ë§Œì„ ê°€ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ë’· ë¶€ë¶„ì´ ì¶”ê°€ëœ ì´ìœ ëŠ” í•´ë‹¹ Mì°¨ì› dataë¥¼ Dì°¨ì›ì˜ ê³µê°„ì— í‘œí˜„í•  ë•Œ, ì–´ëŠ ë¶€ë¶„ì„ ì¤‘ì ìœ¼ë¡œ í• ì§€ë¥¼ ì •í•œ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì• ì„œ ë³´ì•˜ë˜ ì „ì²´ dataì˜ í‰ê· ë§Œí¼ì„ ë‚¨ì€ ëª¨ë“  ë°©í–¥ì— ë”í•´ì¤€ ê²ƒì´ë‹¤. ì´ ê°’ì€ ëª¨ë“  projected dataì— ë™ì¼í•˜ê²Œ ë”í•´ì§€ëŠ” ìƒìˆ˜ê°’ì´ë¼ê³  ë´ë„ ë¬´ë°©í•˜ë‹¤. ì´ë˜ë„ ì´í•´ê°€ ì¡°ê¸ˆ ì–´ë µë‹¤ë©´ ì•„ë˜ ê·¸ë¦¼ì„ í†µí•´ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n\n![ml-pca-3](/images/ml-pca-3.png)\n\nìœ„ì™€ ê°™ì´ 3ê°œì˜ vectorëŠ” 1ì°¨ì›ì—ì„œëŠ” ë™ì¼í•œ vectorì´ë‹¤. í•˜ì§€ë§Œ, projected dataì™€ ì›ë˜ dataê°„ì˜ ì ì ˆí•œ ê±°ë¦¬ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ì¤‘ì•™ì— ìˆëŠ” í˜•íƒœë¡œ basisë¡œ ì˜®ê²¨ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ í•„ìš”í•œ ê²ƒì´ ë’¤ì˜ ìš”ì†Œì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì•„ë˜ì‹ê³¼ ê°™ì´ ë‘ projected dataì™€ ì›ë˜ dataê°„ì˜ ê±°ë¦¬ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n  x_{n} - \\tilde{x}_{n} &= (\\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\}) - (\\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(\\bar{x}^{\\top}u_{i})u_{i}\\}) \\\\\n  &= \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\} - \\sum_{i = M+1}^{D}\\{(\\bar{x}^{\\top}u_{i})u_{i}\\} \\\\\n  &= \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})u_{i}\\}\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ êµ¬í•˜ê³ ìí•˜ëŠ” ìµœì¢… ëª©ì í•¨ìˆ˜ë¥¼ mean squared errorë¼ê³  í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{J} &= \\frac{1}{N}\\sum_{n=1}^{N}||x_{n} - \\tilde{x}_{n}||^2 \\\\\n&= \\frac{1}{N}\\sum_{n=1}^{N}(\\sum_{i = M+1}^{D}x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})^{\\top}(\\sum_{i = M+1}^{D}x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i}) \\\\\n&= \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{i = M+1}^{D}(x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})^{2}u_{i}^{\\top}u_{i} \\\\\n&= \\sum_{i = M+1}^{D}\\frac{1}{N}\\sum_{n=1}^{N}(x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})^{2}\\\\\n&= \\sum_{i=M+1}^{D} u_{i}^{\\top} S u_{i}\\quad (\\because \\text{ì• ì„œ ì‚´í´ë³¸ ì²« ë²ˆì§¸ ê´€ì ê³¼ ë™ì¼})\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²°ë¡ ì— ë„ë‹¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\text{minimize }\\quad& \\sum_{i=M+1}^{D} u_{i}^{\\top} S u_{i} \\\\\n\\text{subject to }\\quad& u_{i}^{\\top}u_{j} = \\delta_{ij} \\quad \\forall i,j \\in [M] \\\\\n&(\\delta_{ij} = \\begin{cases} 1 & i=j \\\\ 0 & i \\neq j \\end{cases})\n\\end{align*}\n$$\n\nê¸°ì¡´ ì‹ê³¼ ë‹¤ë¥¸ ì ì´ë¼ë©´, maximizationì´ minimizationìœ¼ë¡œ ë°”ë€Œì—ˆê³ , ë²”ìœ„ê°€ $[1, M]$ì—ì„œ $[M+1, D]$ë¡œ ë°”ë€Œì—ˆë‹¤ëŠ” ì ì´ë‹¤. ê·¸ë¦¬ê³ , ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í†µì°°ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. eigenvalueë“¤ ì¤‘ì—ì„œ ê°€ì¥ í° ê°’ë¶€í„° Më²ˆì§¸ë¡œ í° ê°’ì„ êµ¬í•˜ëŠ” ê²ƒê³¼ M+1ë¶€í„° ì‹œì‘í•´ì„œ ê°€ì¥ ì‘ì€ eigenvalueë¥¼ ì°¾ëŠ” ê³¼ì •ì€ ë™ì¼í•˜ë¯€ë¡œ ë‘ ì‹ì€ ì‚¬ì‹¤ìƒ ë™ì¼í•˜ë‹¤.\n\n![ml-pca-4](/images/ml-pca-4.png)\n\n### Very High Dimensional Data\n\nì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ê°€ PCAë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œ eigenvectorë¥¼ êµ¬í•˜ëŠ” ë¹„ìš©ì€ O($D^3$)ì´ë‹¤. í•˜ì§€ë§Œ, ì°¨ì›ì´ dataì˜ ìˆ˜ë³´ë‹¤ í° ê²½ìš°ì— ì•½ê°„ì˜ ê¼¼ìˆ˜ë¥¼ ì“¸ ìˆ˜ ìˆë‹¤. ê°„ë‹¨í•˜ê²Œ Së¥¼ (M x M) matrixì—ì„œ (N x N) matrixë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ì´ìš©í•˜ë©´, O($N^3$)ë¡œ ì‹œê°„ ë³µì¡ë„ë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. ì‹ì€ ì•„ë˜ë¥¼ ì°¸ê³  í•˜ì. ì•„ë˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” $X$ëŠ” ê° í–‰ì´ $(x_{n} - \\bar{x})^{\\top}$ì¸ matrixì´ë‹¤.\n\n$$\n\\begin{align*}\n  Su_{i} &= \\lambda_{i}u_{i} &\\\\\n  \\frac{1}{N}X^{\\top}Xu_{i} &= \\lambda_{i}u_{i} &\\leftarrow X^{\\top}X \\in R^{D\\times D} \\\\\n  X \\times \\frac{1}{N}X^{\\top}Xu_{i} &= X \\times \\lambda_{i}u_{i}& \\\\\n  \\frac{1}{N}XX^{\\top}(Xu_{i}) &= \\lambda_{i}(Xu_{i})& \\\\\n  \\frac{1}{N}XX^{\\top}(v_{i}) &= \\lambda_{i}(v_{i}) &\\leftarrow XX^{\\top} \\in R^{N\\times N}\\\\\n\\end{align*}\n$$\n\n## Probabilistic PCA\n\nPCAë¥¼ í†µí•´ì„œ dataë¥¼ ë‹¤ë¥¸ ì°¨ì›ìœ¼ë¡œ ì˜®ê¸°ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í–ˆë‹¤. ì‚¬ì‹¤ ì´ê²ƒìœ¼ë¡œ ëë‚˜ê¸°ëŠ” ì¡°ê¸ˆ ì•„ì‰½ë‹¤. ì™œëƒí•˜ë©´, ìš°ë¦¬ëŠ” í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” dataì— ëŒ€ì‘í•˜ëŠ” ì°¨ì›ìœ¼ë¡œì˜ ì´ë™ì„ ìˆ˜í–‰í•œ ê²ƒì´ë‹¤. ì¦‰, ìš°ë¦¬ê°€ ì‹¤ì œë¡œ inferencing ë‹¨ê³„ì—ì„œ unseen dataë¥¼ ë³´ê²Œ ë˜ì—ˆì„ ë•Œ ì œëŒ€ë¡œ ë™ì‘í• ì§€ëŠ” ë¯¸ì§€ìˆ˜ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œ, continueousí•œ í˜•íƒœë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ, PCAë¥¼ í™•ë¥ ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤. í›„ì— ë” ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.\n\n## Kernel PCA\n\nì—¬íƒœê¹Œì§€ ì•ì—ì„œ ì‚´í´ë´¤ë˜ PCAëŠ” ìƒˆë¡œìš´ basisê°€ ê¸°ì¡´ Dimenalalityì—ì„œ linearí•˜ê²Œ ë§Œë“¤ì—ˆë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” dataê°€ ì‚¬ì‹¤ì€ ê·¸ë ‡ì§€ ì•Šì€ í˜•íƒœì¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ìš°ë¦¬ê°€ ê´€ì¸¡í•œ íŠ¹ì§•ë“¤ì— ì˜í•œ ì¢Œí‘œ ê³µê°„ì—ì„œ ì„ í˜• í˜•íƒœë¡œ dataê°€ ì¡´ì¬í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ë” ë³µì¡í•œ ê³¡ì„  í˜•íƒœë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ê°€ì¥ ì¼ë°˜ì ì¸ dataê°€ ì›í˜•ìœ¼ë¡œ ì´ë£¨ì–´ì§„ data ë¶„í¬ì´ë‹¤. ì›í˜•ìœ¼ë¡œ ë°˜ì§€ë¦„ì´ 0.5ì´í•˜ì¸ dataì™€ ë°˜ì§€ë¦„ì´ 1ì¸ dataë¥¼ êµ¬ë¶„í•˜ê³  ì‹¶ë‹¤ê³  í•˜ì. ì¼ë°˜ì ì¸ x, y ê³µê°„ì—ì„œ linear basisë¥¼ ì´ìš©í•´ì„œ ì´ë¥¼ ì ì ˆí•˜ê²Œ ë‚˜ëˆ„ë ¤ë©´, dimensionality reductionì—ì„œëŠ” ì •ë³´ë¥¼ ëª¨ë‘ ê±°ì˜ ê· ì¼í•˜ê²Œ ìƒì„ ìˆ˜ ë°–ì— ì—†ë‹¤.\n\n![ml-kernel-pca-1](/images/ml-kernel-pca-1.png)\n\ní•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¤‘ì‹¬ê³¼ ë–¨ì–´ì§„ ì •ë„($x^{2} + y^{2}$)ì™€ ê°™ì€ ê¸°ì¡´ featureë¥¼ non-linearí•˜ê²Œ ì¡°í•©í•˜ì—¬ í™œìš©í•˜ë©´ ë” íš¨ê³¼ì ì¸ êµ¬ë¶„ì´ ê°€ëŠ¥í•  ê²ƒì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. (ì•„ë˜ ê·¸ë¦¼ì€ ë‹¤ë¥¸ ë°©ì‹ì„ ì‚¬ìš©í•œ ê²ƒì´ì§€ë§Œ, ì›ì˜ ì¤‘ì‹¬ê³¼ ë¹„ìŠ·í•˜ë‹¤.)\n\n![ml-kernel-pca-2](/images/ml-kernel-pca-2.png)\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê¸°ì¡´ ì°¨ì›ì—ì„œ ì„ í˜•ìœ¼ë¡œ basisë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë¹„ì„ í˜•ìœ¼ë¡œ basisë¥¼ ì°¾ê³  ì‹¶ì€ ê²ƒì´ë‹¤. ê¸°ì¡´ ì°¨ì›ê³¼ ë¹„êµí–ˆì„ ë•Œ ë¹„ì„ í˜•ì˜ basisë¥¼ í†µí•´ì„œ ë§Œë“¤ì–´ì§€ëŠ” ì°¨ì›ì„ manifoldë¼ê³  í•˜ë©°, ì•„ë˜ ì™¼ìª½ ìœ„ì™€ ê°™ì€ manifoldë¥¼ ë°œê²¬ë§Œ í•œë‹¤ë©´, Dimensionality reductionì„ ê¸°ì¡´ linear ë°©ì‹ê³¼ ë¹„êµí–ˆì„ ë•Œ ë” íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\n![ml-kernel-pca-3](/images/ml-kernel-pca-3.png)\n\nì ê·¸ë ‡ë‹¤ë©´ ì´ ë˜í•œ ì–´ë–»ê²Œ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆì„ê¹Œ? ì´ì „ì— ì¼ë˜ Maximum Variance ë°©ì‹ì„ ì´ìš©í•  ê²ƒì´ë‹¤. ìš°ì„  ìš°ë¦¬ê°€ data($x$)ë¥¼ non-linear ê³µê°„ìœ¼ë¡œ ì°¨ì› ì´ë™ì‹œí‚¨ ê°’ì„ $\\phi(x)$ë¼ê³  ì •ì˜í•˜ê³ , $\\sum_{i=1}^{N}\\phi(x_{i})=0$ì´ ë˜ëŠ” ìƒí™©ì´ë¼ê³  ê°€ì •ì„ í•´ë³´ì.(ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ êµ‰ì¥íˆ ìˆ˜ì‹ì´ ë³µì¡í•´ì§€ê¸° ë•Œë¬¸ì— ìš°ì„  ì´ë ‡ê²Œ ê°€ì •ì„ í•  ê²ƒì´ë‹¤.)\n\n$$\n\\begin{align*}\nVar[u_{1}^{\\top}\\phi(x)] &= E[(u_{1}^{\\top}\\phi(x) - E[u_{1}^{\\top}\\phi(x)])^{2}] \\\\\n&= E[(u_{1}^{\\top}\\phi(x) - u_{1}^{\\top}\\bar{\\phi}(x))^{2}]\\quad (\\bar{\\phi}(x) = E[\\phi(x)] = \\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})) \\\\\n&= E[(u_{1}^{\\top}\\phi(x))^2] \\\\\n&= \\frac{1}{N}\\sum_{n \\in [N]}(u_{1}^{\\top}\\phi(x_{n}))^{2} \\\\\n&= u_{1}^{\\top}\\frac{1}{N}\\{\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top}\\}u_{1} \\\\\n&= u_{1}^{\\top}\\bar{S}u_{1} \\quad (\\bar{S} = \\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê²°ë¡ ìƒ ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\text{maximize}\\quad& \\sum_{i \\in [M]}u_{i}^{\\top}\\bar{S}u_{i} \\\\\n\\text{subject to}\\quad& u_{i}^{\\top}u_{j} = \\delta_{ij}\\quad \\forall i,j \\in [M] \\\\\n\\end{align*}\n$$\n\nì´ë¥¼ í‘¸ëŠ” ê³¼ì •ì€ ì•ì—ì„œ ì‚´í´ë´¤ìœ¼ë‹ˆ ì •ë¦¬ë¥¼ í•˜ìë©´, ê²°êµ­ ë‹¤ìŒê³¼ ê°™ì€ eigenvalue, eigenvectorë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\bar{S}u_{i} = \\lambda_{i}u_{i}\n$$\n\ní•˜ì§€ë§Œ, ì´ë¥¼ í‘¸ëŠ” ê²ƒì€ êµ‰ì¥íˆ ê³¤í˜¹ìŠ¤ëŸ½ë‹¤. ì™œëƒí•˜ë©´, ìš°ë¦¬ëŠ” ëª¨ë“  dataì— ëŒ€í•´ì„œ ì°¨ì› ë³€í™˜ í•¨ìˆ˜ì¸ $\\phi$ë¥¼ ì ìš©í•´ì£¼ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì—°ì‚°ì˜ ë³µì¡ë„ëŠ” ê¸‰ê²©í•˜ê²Œ ì¦ê°€í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ë¥¼ ì°¨ì› ë³€í™˜ í•¨ìˆ˜ $\\phi$ì˜ ì—°ì‚° ê³¼ì •ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤. ì´ê²ƒì´ kernel í•¨ìˆ˜ì´ë‹¤. ì´ëŠ” ì• ì„œ ì‚´í´ë´¤ì—ˆë˜, [ğŸ”— 5. Multiclass Classification](/posts/ml-multiclass-classification-in-svm#Kernel-Method)ì˜ Kernel Methodì™€ ë™ì¼í•˜ë‹¤. ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ìë©´, $\\phi(x_{i})^{\\top}\\phi(x_{j})$ì˜ ê²°ê³¼ì™€ ë™ì¼í•œ $\\kappa(x_{i}, x_{j})$ì˜ ì—°ì‚°ì„ í™œìš©í•´ì„œ ì´ 2ë²ˆì˜ ì°¨ì› ë³€í™˜ê³¼ ê³±ì…ˆ ì—°ì‚°ì„ ë‘ ê°œì˜ ë³€ìˆ˜ë¥¼ ë°›ëŠ” í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•œë‹¤ëŠ” ideaì´ë‹¤. ì´ë¥¼ í†µí•´ì„œ, ì—°ì‚° ë¹„ìš©ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê¸°ì¡´ ì‹ì—ì„œ $\\phi$ë¥¼ ì—†ì• ê³ , kernel ë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ í˜•íƒœë¡œ ë°”ê¿€ ê²ƒì´ë‹¤.\n\nì´ë¥¼ ìœ„í•´ì„œ, ìš°ë¦¬ëŠ” ë¨¼ì € $u_{i}$ë¥¼ ë‹¤ì‹œ í‘œí˜„í•´ë³´ì.\n\n$$\n\\begin{align*}\n\\bar{S}u_{i} &= (\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})u_{i} \\\\\n&= \\frac{1}{N}\\sum_{n \\in [N]}\\{\\phi(x_{n})\\times (\\phi(x_{n})^{\\top}u_{i})\\} \\\\\n&= \\frac{1}{N}\\sum_{n \\in [N]} <\\phi(x_{n}), u_{i}>\\phi(x_{n})\\quad(<\\phi(x_{n}), u_{i}> = \\phi(x_{n})^{\\top}u_{i})  \\\\\n\\\\\n\\lambda_{i}u_{i} &= \\bar{S}u_{i} \\\\\n\\lambda_{i}u_{i} &= \\frac{1}{N}\\sum_{n \\in [N]} <\\phi(x_{n}), u_{i}>\\phi(x_{n}) \\\\\nu_{i} &= \\sum_{n \\in [N]} \\frac{<\\phi(x_{n}), u_{i}>}{N\\lambda_{i}}\\phi(x_{n}) \\\\\n\\therefore u_{i} &= \\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})\n\\end{align*}\n$$\n\ní•´ë‹¹ ê³¼ì •ì„ í†µí•´ì„œ, ìš°ë¦¬ê°€ ì–»ê³ ì í•˜ëŠ” basisëŠ” ì„ì˜ì˜ ìƒìˆ˜ $\\alpha_{in}$ì— ì˜í•´ì„œ ì •ì˜ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì´ì œë¶€í„° ë¬¸ì œëŠ” $\\alpha_{in}$ì„ ëª¨ë“  Nê³¼ Mì— ëŒ€í•´ì„œ ì°¾ê¸°ë§Œ í•˜ë©´ ë˜ëŠ” ê²ƒì´ë‹¤. ì´ì œ ê¸°ì¡´ ì‹ì„ ë‹¤ì‹œ ì •ë¦¬í•´ë³´ë„ë¡ í•˜ì.\n\n$$\n\\begin{align*}\n\\bar{S}u_{i} &= \\lambda_{i}u_{i} \\\\\n(\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})(\\sum_{m \\in [N]} \\alpha_{in}\\phi(x_{m})) &= \\lambda_{i}(\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})) \\\\\n\\phi(x_{\\ell})^{\\top}\\times (\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})(\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{m})) &= \\phi(x_{\\ell})^{\\top}\\times \\lambda_{i}(\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})) \\\\\n\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{\\ell})^{\\top}\\phi(x_{n})\\phi(x_{n})^{\\top}\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{m}) &= \\lambda_{i}\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{\\ell})^{\\top}\\phi(x_{n}) \\\\\n\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{\\ell})^{\\top}\\phi(x_{n})\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{n})^{\\top}\\phi(x_{m}) &= \\lambda_{i}\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{\\ell})^{\\top}\\phi(x_{n}) \\\\\n\\frac{1}{N}\\sum_{n \\in [N]}\\kappa(x_{\\ell}, x_{n})\\sum_{m \\in [N]} \\alpha_{im} \\kappa(x_{n}, x_{m}) &= \\lambda_{i}\\sum_{n \\in [N]} \\alpha_{in}\\kappa(x_{\\ell}, x_{n}) \\\\\n\\sum_{n \\in [N]}\\kappa(x_{\\ell}, x_{n})\\begin{bmatrix} \\kappa(x_{n}, x_{1}) \\\\ \\kappa(x_{n}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{n}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} &= N\\lambda_{i} \\begin{bmatrix} \\kappa(x_{\\ell}, x_{1}) \\\\ \\kappa(x_{\\ell}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{\\ell}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \\\\\n\\begin{bmatrix} \\kappa(x_{\\ell}, x_{1}) \\\\ \\kappa(x_{\\ell}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{\\ell}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix} \\kappa(x_{1}, x_{1}) & \\kappa(x_{1}, x_{2}) & \\cdots & \\kappa(x_{1}, x_{N}) \\\\\\kappa(x_{2}, x_{1}) & \\kappa(x_{2}, x_{2}) & \\cdots & \\kappa(x_{2}, x_{N}) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\kappa(x_{N}, x_{1}) & \\kappa(x_{N}, x_{2}) & \\cdots & \\kappa(x_{N}, x_{N}) \\end{bmatrix}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} &= N\\lambda_{i} \\begin{bmatrix} \\kappa(x_{\\ell}, x_{1}) \\\\ \\kappa(x_{\\ell}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{\\ell}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \\\\\n\\begin{bmatrix} \\kappa(x_{1}, x_{1}) & \\kappa(x_{1}, x_{2}) & \\cdots & \\kappa(x_{1}, x_{N}) \\\\\\kappa(x_{2}, x_{1}) & \\kappa(x_{2}, x_{2}) & \\cdots & \\kappa(x_{2}, x_{N}) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\kappa(x_{N}, x_{1}) & \\kappa(x_{N}, x_{2}) & \\cdots & \\kappa(x_{N}, x_{N}) \\end{bmatrix}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} &= N\\lambda_{i} \\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \\\\\nK\\alpha_{i} &= N\\lambda_{i} \\alpha_{i} \\\\\n\\end{align*}\n$$\n\nê²°êµ­ ë˜ ë‹¤ë¥¸ eigenvalue problemì„ ë§Œë“¤ ìˆ˜ ìˆê²Œ ëœë‹¤. í•˜ì§€ë§Œ, ì—¬ê¸°ì„œ $\\alpha_{i}$ì˜ í¬ê¸°ëŠ” 1ì´ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ, $\\alpha_{i}$ë¥¼ ì •ê·œí™”í•´ì£¼ì–´ì•¼ í•œë‹¤.\n\n$$\n\\begin{align*}\nu_{i}^{\\top}u_{i} &= \\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{m}) \\\\\n&= \\sum_{n \\in [N]}\\alpha_{in} \\sum_{m \\in [N]}\\alpha_{im}\\phi(x_{n})\\phi(x_{m}) \\\\\n&= \\sum_{n \\in [N]}\\alpha_{in} \\sum_{m \\in [N]}\\alpha_{im}\\kappa(x_{n}, x_{m})\\\\\n&= \\sum_{n \\in [N]}\\alpha_{in} \\begin{bmatrix} \\kappa(x_{n}, x_{1}) \\\\ \\kappa(x_{n}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{n}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix}^{\\top} \\begin{bmatrix} \\kappa(x_{1}, x_{1}) & \\kappa(x_{1}, x_{2}) & \\cdots & \\kappa(x_{1}, x_{N}) \\\\\\kappa(x_{2}, x_{1}) & \\kappa(x_{2}, x_{2}) & \\cdots & \\kappa(x_{2}, x_{N}) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\kappa(x_{N}, x_{1}) & \\kappa(x_{N}, x_{2}) & \\cdots & \\kappa(x_{N}, x_{N}) \\end{bmatrix}  \\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix}\\\\\n&= \\alpha_{i}^{\\top}K\\alpha_{i} \\\\\n&= \\alpha_{i}^{\\top}N\\lambda_{i} \\alpha_{i}\\quad(\\because K\\alpha_{i} = N\\lambda_{i} \\alpha_{i}) \\\\\n&= N\\lambda_{i}\\alpha_{i}^{\\top}\\alpha_{i} = 1 \\\\\n\n\\therefore ||\\alpha_{i}||^{2} &= \\frac{1}{N\\lambda_{i}}\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ê²°ë¡ ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” nonlinear PCAë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” $\\lambda_{i}$ë¥¼ í°ê°’ë¶€í„° ì‹œì‘í•˜ì—¬ Më²ˆì§¸ê¹Œì§€ì—ì„œì˜ $\\alpha_{i}$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\nK\\alpha_{i} &= N\\lambda_{i} \\alpha_{i} \\\\\n||\\alpha_{i}||^{2} &= \\frac{1}{N\\lambda_{i}}\n\\end{align*}\n$$\n\nê·¸ë¦¬ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ dataë¥¼ ìš°ë¦¬ê°€ êµ¬í•œ basisë¡œ projectioní•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n<u_{i}, \\phi(x)> &=\n  \n\\end{align*}\n$$\n\n\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Kernel PCA, <https://sebastianraschka.com/Articles/2014_kernel_pca.html>\n","slug":"ml-dimensionality-reduction","date":"2022-12-04 14:19","title":"[ML] 11. Dimensionality Reduction","category":"AI","tags":["ML"],"desc":"Clusteringê³¼ ê°™ì€ Unsupervised Learningìœ¼ë¡œ Feature Selection ë˜ëŠ” Feature Extraction ë“± ì—¬ëŸ¬ê°€ì§€ ì´ë¦„ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” Dimensionality Reduction ê¸°ë²•ì— ëŒ€í•´ì„œ ë‹¤ë£° ê²ƒì´ë‹¤. íŠ¹ì • dataì—ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” data ì „ì²´ë¥¼ ë³¼ í•„ìš”ê°€ ì—†ë‹¤. ë”°ë¼ì„œ, featureë“¤ì„ ìµœì†Œí•œìœ¼ë¡œ ì¤„ì´ë©´ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ë§Œ ìˆë‹¤ë©´ êµ‰ì¥íˆ íš¨ìœ¨ì ì¸ inferencingê³¼ learningì„ í•  ìˆ˜ ìˆë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nì£¼ì–´ì§€ëŠ” dataì— í•­ìƒ feature, labelì´ ì •í™•í•˜ê²Œ ë§¤ì¹­ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ì´ëŸ´ ê²½ìš° ìš°ë¦¬ëŠ” ê° dataì— ëŒ€í•œ labelê³¼ ì£¼ì–´ì§„ dataì™€ labelì„ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” probability distributionì„ ëª¨ë‘ êµ¬í•´ì•¼ í•œë‹¤. ì—¬ê¸°ì„œ labelê³¼ probability distributionì„ ë™ì‹œì— êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ì§€ì— ëŒ€í•œ ë°©ë²• ì¤‘ì—ì„œ ëŒ€í‘œì ì¸ EM Algorithmì— ëŒ€í•´ì„œ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## Problem\n\nì—¬íƒœê¹Œì§€ ìš°ë¦¬ê°€ ì‚´í´ë´¤ë˜ supervised learningì—ì„œëŠ” í•™ìŠµ(learning) ì‹œì—ëŠ” featureì™€ labelì´ ëª¨ë‘ ë™ì‹œì— ì£¼ì–´ì§€ê³ , ì˜ˆì¸¡/ì¶”ë¡ (inference)ì„ ìˆ˜í–‰í•  ë•Œì—ëŠ” featureë§Œ ì¡´ì¬í•˜ëŠ” dataê°€ ì£¼ì–´ì¡Œë‹¤. ë”°ë¼ì„œ, í•™ìŠµ ì‹œì— feature ì •ë³´ë“¤ì„ íŠ¹ì • patternì— ë…¹ì—¬ëƒˆì„ ë•Œ, labelê°’ì„ ì–»ëŠ”ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. í•˜ì§€ë§Œ, labelì´ ì£¼ì–´ì§€ì§€ ì•Šì€ dataë¥¼ í•™ìŠµì‹œí‚¬ ë•Œì—ëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ? ìš°ë¦¬ëŠ” labelì´ ìˆì–´ì•¼ í•´ë‹¹ dataê°€ ê°€ì§„ ì‹¤ì œ ê²°ê³¼ê°’ì„ ì•Œê³  probability distributionì„ ì–¼ë§ˆë‚˜ ìˆ˜ì •í• ì§€ë¥¼ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. í•˜ì§€ë§Œ, ì´ ê°’ì„ ëª¨ë¥´ë‹ˆ probability distributionì„ ë§Œë“¤ ìˆ˜ ì—†ë‹¤. ì •í™•í•œ probability distributionì´ ìˆë‹¤ë©´, ë°˜ëŒ€ë¡œ labelì„ ìƒì„±í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” ì•„ë¬´ê²ƒë„ ì•Œ ìˆ˜ ì—†ë‹¤.\n\nì´ë ‡ê²Œ ë‹µë‹µí•œ ìƒí™©ì—ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì•„ì´ë””ì–´ë¥¼ ë°œìƒí•´ë‚¼ ìˆ˜ ìˆë‹¤. ë§Œì•½, ëŒ€ëµì ì¸ labelì„ ì•ˆë‹¤ë©´, ì´ê²ƒì„ ì´ìš©í•´ì„œ ìµœì ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì°¾ê³ , ì´ í™•ë¥  ë¶„í¬ì— ë§ëŠ” labelì„ ë‹¤ì‹œ ìƒì„±í•˜ê³  ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ í™•ë¥  ë¶„í¬ë¥¼ ì°¾ëŠ”ë‹¤ë©´ ì–´ë–¨ê¹Œ? ì´ë ‡ê²Œ ë°˜ë³µí•˜ë©´ ê½¤ë‚˜ ê·¸ëŸ´ì‹¸í•œ ë¶„í¬ë¥¼ ë§Œë“¤ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ? ì´ ê³¼ì •ì„ ì˜ˆë¥¼ ë“¤ìë©´, ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nê° ê¸° ë‹¤ë¥¸ ë‚˜ë¼(label)ì˜ ë™ì „ 3ì¢…ë¥˜(500ì›, 100cent, 100ì—”)ë¥¼ êµ¬ë¶„í•˜ê³  ì‹¶ë‹¤ê³  í•˜ì. ì´ë•Œ, ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” ë¬´ê²Œ(feature) ë°–ì— ì—†ë‹¤ê³  ê°€ì •í•˜ê² ë‹¤. ì´ë•Œ ìš°ë¦¬ëŠ” ì–´ë–»ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆì„ê¹Œ? ìš°ë¦¬ê°€ ê¸¸ ê±°ë¦¬ì—ì„œ ë¬´ì‘ìœ„ë¡œ ë™ì „ì„ ìˆ˜ì§‘í–ˆë‹¤ê³  í•˜ì. ê° ë™ì „ì€ í ì§‘ë„ ìˆì„ ê²ƒì´ê³  ê³µì¥ë§ˆë‹¤ ì¡°ê¸ˆì”© ë¬´ê²Œê°€ ì°¨ì´ìˆì„ ìˆ˜ ìˆë‹¤. ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ë¶„í¬ê°€ ë‚˜ì™”ë‹¤ê³  í•˜ì.\n\n![ml-em-algorithm-1](/images/ml-em-algorithm-1.jpg)\n\nê·¸ë˜ì„œ ìš°ë¦¬ëŠ” í™•ë¥  ë¶„í¬ê°€ ì•„ë§ˆ Gaussian distributionì´ë¼ê³  ìƒê°í•  ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì„ì˜ì˜ Gaussian Distributionì„ ë”°ë¥´ëŠ” ì„¸ ê°œì˜ ë¶„í¬ë¥¼ ì•„ë˜ì™€ ê°™ì´ ê°€ì •í•´ë³´ëŠ” ê²ƒì´ë‹¤.\n\n![ml-em-algorithm-2](/images/ml-em-algorithm-2.jpg)\n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ì´ ë¶„í¬ì— ë”°ë¼ ê°€ì¥ ì ì ˆí•œ labelì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ì™€ ê°™ì´ ìƒì„±í•  ìˆ˜ ìˆë‹¤.\n\n![ml-em-algorithm-3](/images/ml-em-algorithm-3.jpg)\n\nê·¸ëŸ¬ë©´ ê²°ê³¼ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ labelëœ dataë¥¼ ê°–ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.\n\n![ml-em-algorithm-4](/images/ml-em-algorithm-4.jpg)\n\nì´ë ‡ê²Œ labeling dataë¥¼ ì´ìš©í•´ì„œ ìš°ë¦¬ëŠ” ë” íš¨ê³¼ì ì¸ í™•ë¥  ë¶„í¬ ë³€ìˆ˜ë¥¼ ì°¾ì•„ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ ì´ì „ê³¼ëŠ” ì‚¬ë­‡ ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n![ml-em-algorithm-5](/images/ml-em-algorithm-5.jpg)\n\nê²°ê³¼ì ìœ¼ë¡œ í•´ë‹¹ ë¶„í¬ê°€ ì´ì „ì— ì„ì˜ë¡œ ì¶”ì •í–ˆë˜ ë¶„í¬ë³´ë‹¤ ë” ì ì ˆí•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ ê³¼ì •ì„ ê³„ì†í•´ì„œ ë°˜ë³µí•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ?\n\n![ml-em-algorithm-6](/images/ml-em-algorithm-6.jpg)\n\në°˜ë³µì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” ê·¸ëŸ´ì‹¸í•œ í™•ë¥ ë¶„í¬ë¥¼ ìŠµë“í–ˆë‹¤. ëŒ€ëµ ë¨¸ë¦¿ì†ìœ¼ë¡œëŠ” ê·¸ëŸ´ ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“¤ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ê²ƒì´ ì–´ë–»ê²Œ ê°€ëŠ¥í•˜ë©° ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í• ê¹Œ? ì´ë¥¼ ì´ì œë¶€í„° ìì„¸íˆ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## Base Knowledge\n\në³¸ë¡ ìœ¼ë¡œ ë“¤ì–´ê°€ê¸°ì— ì• ì„œ ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ ì •ì˜ë¥¼ ì•Œì•„ì•¼ EM Algorithmì„ ì¦ëª…í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n\n1. Jensenâ€™s Inequality\n2. Gibb's Inequality\n\nì´ ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ ì•ˆë‹¤ë©´ ë°”ë¡œ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°€ëŠ” ê²ƒì´ ì¢‹ë‹¤. í•˜ì§€ë§Œ, ì•Œì§€ ëª»í•œë‹¤ë©´ ì´ ì •ì˜ì— ëŒ€í•´ì„œ ë¨¼ì € ì•Œì•„ë³´ê³  ê°€ë„ë¡ í•˜ì.\n\n### Jensenâ€™s Inequality\n\nì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë‹¤ìŒ ì„±ì§ˆì„ ë§Œì¡±í•˜ëŠ” ì§‘í•©ì„ Convex setì´ë¼ê³  í•œë‹¤.\n\n$$\n\\lambda x + (1-\\lambda)y \\in C,\\quad \\forall x, y \\in C \\text{ and } \\forall\\lambda\\in[0,1]\n$$\n\nì¦‰, ì§‘í•©ì—ì„œ randomìœ¼ë¡œ ê³ ë¥¸ ë‘ ìˆ˜ ì‚¬ì´ì˜ ìˆ˜ë„ ì§‘í•©ì— í¬í•¨ë˜ëŠ” ì§‘í•©ì´ë¼ëŠ” ê²ƒì´ë‹¤. convex setì´ë¼ê³  ë¶ˆë¦¬ëŠ” ì´ìœ ëŠ” ê²°êµ­ ì´ëŸ¬í•œ ì§‘í•©ì„ 2, 3 ì°¨ì›ìƒì— ê·¸ë ¤ë³´ë©´ ë³¼ë¡í•˜ê²Œ íŠ€ì–´ë‚˜ì˜¤ëŠ” í˜•íƒœë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\n\në˜í•œ, ì•„ë˜ì™€ ê°™ì€ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í•¨ìˆ˜(f)ë¥¼ Convex functionì´ë¼ê³  í•œë‹¤.\n\n$$\nf(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y),\\quad \\forall x,y \\in C(\\text{Convex set}) \\text{ and } \\forall\\lambda \\in [0,1]\n$$\n\nì•„ë˜ë¡œ ë³¼ë¡í•œ í•¨ìˆ˜ì—ì„œëŠ” ìœ„ì™€ ê°™ì€ ê³¼ì •ì´ ë„ˆë¬´ë‚˜ ë‹¹ì—°í•˜ê²Œë„ ì„±ë¦½í•œë‹¤. ì‚¬ì‡ê°’ì˜ í•¨ìˆ˜ê°’ë³´ë‹¤ í•¨ìˆ˜ê°’ì˜ ì‚¬ì‡ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì´ë‹¤.\n\n![ml-convex-function](/images/ml-convex-function.jpg)\n\në°˜ëŒ€ë¡œ concave(ìœ„ë¡œ ë³¼ë¡) í•¨ìˆ˜ì¸ ê²½ìš°ì—ëŠ” ë°˜ëŒ€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.\n\n$$\nf(\\lambda x + (1-\\lambda)y) \\geq \\lambda f(x) + (1-\\lambda)f(y),\\quad \\forall x,y \\in C(\\text{Convex set}) \\text{ and } \\forall\\lambda \\in [0,1]\n$$\n\nì—¬ê¸°ì„œ Jensen's InequalityëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ì´ convexì—ì„œ ì„±ë¦½í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n$$\nE[f(X)] \\geq f(E[X])\n$$\n\nconvex functionì—ì„œëŠ” ì–´ì°Œë³´ë©´ ë‹¹ì—°í•´ë³´ì¸ë‹¤. ê·¸ë ‡ì§€ë§Œ ì´ëŠ” EM Algorithmì—ì„œ í† ëŒ€ë¡œ ì‚¬ìš©ë˜ëŠ” ì•„ì´ë””ì–´ì´ê¸° ë•Œë¬¸ì— ë°˜ë“œì‹œ ê¸°ì–µí•˜ì. ë°˜ëŒ€ë¡œ Concave functionì¸ ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\nE[f(X)] \\leq f(E[X])\n$$\n\n### Gibb's Inequality\n\nKL divergence ì‹ì— Jensen's Inequalityë¥¼ ì ìš©í•˜ì—¬ KL divergenceê°€ í•­ìƒ 0ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ê³ , KL divergenceê°€ 0ì´ ë˜ê¸° ìœ„í•´ì„œëŠ” ë‘ í™•ë¥ ë¶„í¬ê°€ ê°™ì•„ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œ ê²ƒì´ë‹¤.\n\nì´ì— ëŒ€í•œ ì¦ëª…ì„ ê°„ë‹¨í•˜ê²Œ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\nKL(p||q) &= \\sum_{i}{p_{i}\\log\\frac{p_{i}}{q_{i}}} \\\\\n&= -\\sum_{i}p_{i}\\log{\\frac{q_{i}}{p_{i}}} \\\\\n&= E_{p}[-\\log{\\frac{q_{i}}{p_{i}}}] \\geq -\\log{E_{p}[\\frac{q_{i}}{p_{i}}]}\\, (\\because \\text{Jensen's Inequality}) \\\\\n&= -\\log{\\sum_{i}p_{i}\\frac{q_{i}}{p_{i}}} = -\\log{1} = 0\\\\\n\\therefore KL(p||q) &\\geq 0\n\\end{align*}\n$$\n\n## EM Algorithm\n\nìš°ë¦¬ëŠ” parametric estimation ë°©ë²•ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ Likelihoodë¥¼ ê³„ì‚°í–ˆë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) &= \\log p(D| \\theta) \\\\\n\\mathcal{L}(\\theta) &= \\log \\prod_{x\\in D} p(x| \\theta) \\\\\n&= \\sum_{x\\in D} \\log{p(x|\\theta)}\n\\end{align*}\n$$\n\nê·¸ë ‡ì§€ë§Œ ìš°ë¦¬ê°€ ì´ ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤ëŠ” ê²ƒì„ ìœ„ì—ì„œ ì œì‹œí–ˆë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë°”ê¿”ì„œ í’€ì–´ë³´ìëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) &= \\sum_{x\\in D} \\log{p(x|\\theta)} \\\\\n&= \\sum_{x\\in D} \\log{\\int_{z \\in K}p(x, z|\\theta)} dz\n\\end{align*}\n$$\n\nì´ë ‡ê²Œ ë°”ê¾¸ê²Œ ëœë‹¤ê³  ë¬´ìŠ¨ ì´ë“ì´ ìˆì„ê¹Œ? ë‹¨ìˆœíˆ ì‹ì´ ë” ë³µì¡í•´ë³´ì¸ë‹¤. í•˜ì§€ë§Œ, ì´ ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ë°”ê¿€ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) &= \\sum_{x\\in D} \\log{p(x|\\theta)} \\\\\n&= \\sum_{x\\in D} \\log{\\int_{z \\in K} p(x, z|\\theta)} dz \\\\\n&= \\sum_{x\\in D} \\log{\\int_{z \\in K} p(x, z|\\theta) \\frac{p(z|x, \\theta^{\\prime})}{p(z| x, \\theta^{\\prime})} dz} \\\\\n&= \\sum_{x\\in D} \\log{\\int_{z \\in K} p(z|x, \\theta^{\\prime}) \\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})} dz} \\\\\n&= \\sum_{x\\in D} \\log{E_{z|x, \\theta^{\\prime}}{[\\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})}]}} \\geq \\sum_{x\\in D} E_{z|x, \\theta^{\\prime}}{[\\log{\\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})}}]}\\,(\\because \\text{Jensen's Inequality}) = \\mathcal{F}(p_{z|x, \\theta^{\\prime}}, \\theta)\n\\end{align*}\n$$\n\nì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ì„ ìˆ˜í–‰í•´ë³¼ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) - \\mathcal{F}(p_{z|x, \\theta^{\\prime}}, \\theta) &= \\sum_{x\\in D} \\log{p(x|\\theta)} - \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{\\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})}}}\\} \\\\\n&= \\sum_{x\\in D} \\log{p(x|\\theta)} - \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{\\frac{p(z|x, \\theta)p(x|\\theta)}{p(z| x, \\theta^{\\prime})}}}\\} \\\\\n&= \\sum_{x\\in D} \\log{p(x|\\theta)} - \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{p(x|\\theta)}} + \\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{\\frac{p(z|x, \\theta)}{p(z| x, \\theta^{\\prime})}}}\\} \\\\\n&= \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{\\frac{p(z|x, \\theta^{\\prime})}{p(z| x, \\theta)}}}\\} \\\\\n&= \\sum_{x\\in D}{KL(p_{z|x, \\theta^{\\prime}}, p_{z|x, \\theta})}\n\\end{align*}\n$$\n\nìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ê²°êµ­ í•´ë‹¹ ê°’ì˜ minizationì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ëª¨ë“  $x$ì— ëŒ€í•´ì„œ KL-divergenceì˜ ìµœì†Ÿê°’ì„ êµ¬í•´ì•¼ í•œë‹¤(ëª¨ë“  ì‚¬ê±´ì€ ë…ë¦½ì´ê¸° ë•Œë¬¸ì´ë‹¤). KL-divergenceëŠ” 0ê³¼ ê°™ê±°ë‚˜ í° ìˆ˜ì´ê³ , KL-divergenceëŠ” $p_{z|x, \\theta^{\\prime}} = p_{z|x, \\theta}$ì¼ ë•Œ, 0ì´ë¯€ë¡œ ì´ë¥¼ ë§Œì¡±í•  ìˆ˜ ìˆëŠ” ê°’ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ insightë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n1. $\\mathcal{F}(p_{z|x, \\theta^{(t-1)}}, \\theta^{(t)}) \\leq \\mathcal{L}(\\theta^{(t)})$ ì•„ë˜ ì‹ì— ì˜í•´ì„œ ì´ë¥¼ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.  \n   $\\mathcal{F}(p_{z|x, \\theta}, \\theta^{(t)}) \\leq \\mathcal{L}(\\theta^{(t)}) (\\because \\text{Jensen's Inequality})$\n2. $\\mathcal{L}(\\theta^{(t)}) = \\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta^{(t)})$ ë°”ë¡œ ìœ„ì—ì„œ ì‚´í´ë³´ì•˜ ë“¯ì´ $p_{z|x, \\theta^{\\prime}} = p_{z|x, \\theta}$ì¼ ë•Œ, ë“±ì‹ì´ ì„±ë¦½í•œë‹¤. (Gibb's Inequality)\n3. $\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta^{(t)}) \\leq \\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta^{(t+1)})$  \n   ì—¬ê¸°ì„œ $\\theta^{(t+1)}$ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.  \n   $\\theta^{(t+1)} = \\argmax_{\\theta}{\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta)}$\n4. $\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta^{(t+1)}) \\leq \\mathcal{L}(\\theta^{(t+1)})$  \n   ì´ëŠ” 1ë²ˆê³¼ ë™ì¼í•œ ì‹ì´ë‹¤.\n5. $\\mathcal{L}(\\theta^{(t)}) \\leq \\mathcal{L}(\\theta^{(t+1)})$\n\nê²°ë¡ ì ìœ¼ë¡œ, 5ë²ˆì— ì˜í•´ì„œ ìš°ë¦¬ëŠ” ë§¤ë‹¨ê³„ê°€ ì´ì „ë³´ë‹¤ ê°™ê±°ë‚˜ í¬ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ, ê° ë‹¨ê³„ë¥¼ ì°¨ë¡€ëŒ€ë¡œ ì„¤ëª…í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. ì´ì „ í™•ë¥ ê³¼ í˜„ì¬ parameterì˜ ì¶”ì •ì¹˜ë¥¼ ì´ìš©í•´ì„œ êµ¬í•œ $\\mathcal{F}(p_{z|x, \\theta^{(t-1)}}, \\theta^{(t)})$ëŠ” $\\theta^{(t)}) \\leq \\mathcal{L}(\\theta^{(t)})$ë³´ë‹¤ ì‘ë‹¤ëŠ” ê²ƒì„ Jensen's Inequalityì— ì˜í•´ì„œ ì•Œ ìˆ˜ ìˆë‹¤.\n2. ìš°ë¦¬ëŠ” ì´ì œ $\\mathcal{F}(p_{z|x, \\theta}, \\theta^{(t)})$ë¥¼ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ì„œ $p_{z|x, \\theta}$ë¥¼ $p_{z|x, \\theta^{(t)}}$ë¡œ ì—…ë°ì´íŠ¸ í•œë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ ê²°ê³¼ëŠ” ì• ì„œ ë³´ì•˜ë“¯ì´ ì´ ê°’ì€ $\\mathcal{L}(\\theta^{(t)})$ì™€ ë™ì¼í•œ ê²°ê³¼ë¥¼ ê°–ëŠ”ë‹¤.\n3. ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì–»ì€ $ \\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta)$ë¥¼ ìµœëŒ€í™”í•  ìˆ˜ ìˆëŠ” $\\theta^{(t+1)}$ë¥¼ êµ¬í•œë‹¤ë©´, ì´ëŠ” ë‹¹ì—°í•˜ê²Œë„ $\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta^{(t)})$ ë³´ë‹¤ í¬ë‹¤.\n4. ì´ë ‡ê²Œ ì–»ì€ ìƒˆë¡œìš´ parameterì— ì˜í•œ ê²°ê³¼ëŠ” ì—­ì‹œ ë‹¹ì—°í•˜ê²Œë„ 1ë²ˆê³¼ ê°™ì€ ê²°ë¡ ì— ë„ë‹¬í•˜ê²Œ ëœë‹¤.\n5. ê²°êµ­ ìš°ë¦¬ëŠ” 1~4ë²ˆ ê¹Œì§€ì˜ ê³¼ì •ì„ ê±°ì¹˜ë©´ì„œ $\\mathcal{L}(\\theta)$ë¥¼ ê³„ì†í•´ì„œ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆë‹¤.\n\nê²°êµ­ ìš°ë¦¬ê°€ í•´ì•¼í•  ê²ƒì€ ë‹¤ìŒê°’ì„ ë§¤ì°¨ì‹œë§ˆë‹¤ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\theta^{(t+1)} = \\argmax_{\\theta}{\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta)}\n$$\n\nì´ë¥¼ ìœ„í•´ì„œ ë¨¼ì € ìš°ë¦¬ëŠ” $\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta)$ì˜ ì‹ì„ ì¢€ ë” ì •ë¦¬í•´ë³¼ ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{F}(p_{z|x, \\theta^{\\prime}}, \\theta) &= \\sum_{x\\in D} E_{z|x, \\theta^{\\prime}}{[\\log{\\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})}}]} \\\\\n&= \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{\\frac{p(x, z|\\theta)}{p(z| x, \\theta^{\\prime})}}}\\}\\\\\n&= \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{p(x, z|\\theta)}} + \\int_{z|x, \\theta^{\\prime}}{p(z|x, \\theta^{\\prime})}{\\log{\\frac{1}{p(z|x, \\theta^{\\prime})}}}\\} \\\\\n&= \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{p(x, z|\\theta)}} + H(p_{z|x,\\theta^{\\prime}})\\}\n\\end{align*}\n$$\n\nìœ„ ì‹ì—ì„œ ìš°ë¦¬ëŠ” $\\argmax_{\\theta}{\\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta)}$ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ê³¼ì •ì—ì„œ $H(p_{z|x,\\theta^{\\prime}})$ëŠ” í•„ìš”ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{Q}(\\theta; \\theta^{\\prime}) &= \\mathcal{F}(p_{z|x, \\theta^{(t)}}, \\theta) - H(p_{z|x,\\theta^{\\prime}}) \\\\\n&= \\sum_{x\\in D}\\{\\int_{z|x, \\theta^{\\prime}}{p(z|x,\\theta^{\\prime})\\log{p(x, z|\\theta)}}\\} \\\\\n&= \\sum_{x\\in D}\\{ E_{z|x, \\theta^{\\prime}}[\\log{p(x, z|\\theta)}] \\}\n\\end{align*}\n$$\n\nê·¸ ê²°ê³¼ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²°ë¡ ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\theta^{(t+1)} &= \\argmax_{\\theta} \\mathcal{Q}(\\theta; \\theta^{(t)}) \\\\\n&= \\argmax_{\\theta} \\sum_{x\\in D}\\{ E_{z|x, \\theta^{(t)}}[\\log{p(x, z|\\theta)}] \\}\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ êµ¬í•˜ê¸° ìœ„í•´ì„œ EM Algorithmì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ê³ , ë‹¨ê³„ì— ë”°ë¼ ìˆ˜í–‰í•œë‹¤.\n\n1. Expectation Step  \n   ì•ì—ì„œ ì œì‹œí•œ $\\mathcal{Q}(\\theta; \\theta^{(t)})$ì˜ ì‹ì„ êµ¬í•˜ëŠ” ë‹¨ê³„ì´ë‹¤. ì¦‰, ë³€ìˆ˜ë¥¼ $\\theta$ ì™¸ì—ëŠ” ëª¨ë‘ ì—†ì• ëŠ” ë‹¨ê³„ì´ë‹¤. Expectation ë‹¨ê³„ë¼ê³  ë¶€ë¥´ëŠ” ì´ìœ ëŠ” $\\mathcal{Q}(\\theta; \\theta^{(t)})$ê°€ $\\sum_{x\\in D}\\{ E_{z|x, \\theta^{\\prime}}[\\log{p(x, z|\\theta)}] \\}$ì™€ ê°™ì´ Expectationì˜ í•©ì˜ í˜•íƒœë¡œ í‘œí˜„ë˜ê¸° ë•Œë¬¸ì´ë‹¤.  \n   ì´ë¥¼ ì¢€ ë” ì‰½ê²Œ í‘œí˜„í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ì´ ë§í•  ìˆ˜ë„ ìˆë‹¤. ì´ì „ parameter $\\theta^{(t)}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê° ë°ì´í„°ì— ëŒ€í•œ latent variable $z$ì˜ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, $p(z|x, \\theta^{(t)})$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.\n2. Maximization Step  \n   ì´ì œ ì• ì„œ êµ¬í•œ $\\mathcal{Q}(\\theta; \\theta^{(t)})$ë¥¼ $\\theta$ì— ëŒ€í•´ ìµœëŒ€í™”í•˜ì—¬, $\\theta^{(t+1)}$ë¥¼ êµ¬í•˜ëŠ” ë‹¨ê³„ì´ë‹¤.\n\nì´ê²ƒì´ EM Algorithmì˜ ë³¸ì§ˆì´ë‹¤.\n\nê·¸ë˜ì„œ ì• ì„  Clusteringì—ì„œ ì‚´í´ë³´ì•˜ë˜ ê²ƒì²˜ëŸ¼ EM Algorithmì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ë„ ìˆëŠ” ê²ƒì´ë‹¤.\n\n1. ì´ˆê¸° parameter $\\theta^{(0)}$ë¥¼ ì„¤ì •í•œë‹¤.  \n2. ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ dataê°€ í•´ë‹¹ ë¶„í¬ì—ì„œ $z$ì¼ í™•ë¥ ì„ êµ¬í•œë‹¤.\n3. êµ¬í•œ í™•ë¥ ì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ í™•ë¥ ê³¼ dataë¥¼ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ parameter $\\theta^{(t+1)}$ë¥¼ êµ¬í•œë‹¤.\n4. 2, 3ë²ˆ ê³¼ì •ì„ parameterê°€ ì¼ì • ìˆ˜ì¤€ì— ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-em-algorithm","date":"2022-11-24 20:15","title":"[ML] 10. EM Algorithm","category":"AI","tags":["ML","EM-Algorithm","JensensInequality","GibbsInequality"],"desc":"ì£¼ì–´ì§€ëŠ” dataì— í•­ìƒ feature, labelì´ ì •í™•í•˜ê²Œ ë§¤ì¹­ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ì´ëŸ´ ê²½ìš° ìš°ë¦¬ëŠ” ê° dataì— ëŒ€í•œ labelê³¼ ì£¼ì–´ì§„ dataì™€ labelì„ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” probability distributionì„ ëª¨ë‘ êµ¬í•´ì•¼ í•œë‹¤. ì—¬ê¸°ì„œ labelê³¼ probability distributionì„ ë™ì‹œì— êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ì§€ì— ëŒ€í•œ ë°©ë²• ì¤‘ì—ì„œ ëŒ€í‘œì ì¸ EM Algorithmì— ëŒ€í•´ì„œ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nì´ì „ê¹Œì§€ì˜ Postingì—ì„œëŠ” Supervised Learning ì¦‰, ì´ë¯¸ Labelingì´ ì™„ë£Œëœ ë°ì´í„°ì— ì˜í•œ Learningì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ì—ˆë‹¤. ì§€ê¸ˆë¶€í„°ëŠ” Unsupervised Learningì— ëŒ€í•´ì„œ ì¡°ê¸ˆ ë” ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤. ëŒ€í‘œì ì¸ Unsupervised Learningì€ Clustering, Feature Selection(or Dimensionality Reduction), Generative Model ë“±ì´ ì¡´ì¬í•œë‹¤. ì´ë“¤ì— ëŒ€í•´ì„œ ì°¨ê·¼ì°¨ê·¼ ì‚´í´ë³´ë„ë¡ í•˜ê³ , í•´ë‹¹ Postingì—ì„œëŠ” ê°€ì¥ ëŒ€í‘œì ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” Clusteringì„ ë¨¼ì € ì‚´í´ë³´ë©´ì„œ Unsupervised Learningì— ëŒ€í•œ ê³„ëµì ì¸ ì´í•´ë¥¼ í•´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## Clustering\n\nClusteringì€ unlabeled dataë¥¼ dataê°„ ìœ ì‚¬ì„± ë˜ëŠ” ê±°ë¦¬ ì§€í‘œ ë“±ì„ í™œìš©í•˜ì—¬ ë¯¸ë¦¬ ì§€ì •í•œ ìˆ˜ ë§Œí¼ì˜ partitioningí•˜ëŠ” ì‘ì—…ì„ ì˜ë¯¸í•œë‹¤. ì¦‰, ìš°ë¦¬ê°€ í•™ìŠµì„ ì§„í–‰í•¨ì— ìˆì–´ dataëŠ” labelì´ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” dataê°„ì˜ ê´€ê³„ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì„œ ì´ë¥¼ ë¶„ë¥˜í•´ë‚´ëŠ” ê²ƒì´ ëª©í‘œì¸ ê²ƒì´ë‹¤.\n\nì´ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë°©ë²•ì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.\n\n1. **Non-Parametric Approach**  \n   ì´ë¦„ ê·¸ëŒ€ë¡œ í™•ë¥ ì  ë¶„í¬ë¥¼ ê°€ì •í•œ í›„, Parameterë¥¼ ì°¾ì•„ê°€ëŠ” ë°©ì‹ì´ ì•„ë‹Œ ì§ê´€ì ì¸ ë°©ë²•(Huristic Approach)ì„ í™œìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê·¸ë ‡ê¸°ì— í™•ë¥ ì ì¸ í•´ì„ì´ ë’·ë°›ì¹¨ë˜ê¸° ë³´ë‹¤ëŠ” Algorithmì„ í†µí•´ì„œ ì´ë¥¼ ì„¤ëª…í•œë‹¤. ëŒ€í‘œì ì¸ ë°©ë²•ì´ K-Means Clusteringì´ë‹¤.\n2. **Parametric Approach**  \n   í™•ë¥ ì  ë¶„í¬ë¥¼ ê°€ì •í•œ í›„, Parameterë¥¼ ì°¾ì•„ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ, ëŒ€í‘œì ì¸ ë°©ë²•ì´ Gaussian ë¶„í¬ë¥¼ ê°€ì •í•˜ê³  ì°¾ì•„ë‚˜ê°€ëŠ” Gaussian Mixture Model(GMM, or MoG, Mixture of Gaussian)ì´ ìˆë‹¤.\n\në”°ë¼ì„œ, Clusteringì„ ëŒ€í‘œí•˜ëŠ” K-means Clusteringê³¼ GMMì„ ê° ê° ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n### K-Means Clustering\n\nK-Means Clusteringì€ Kê°œì˜ í‰ê· ê°’ì„ í†µí•œ Clusteringìœ¼ë¡œ í•´ì„í•˜ë©´ ì˜ë¯¸ íŒŒì•…ì´ ì‰½ë‹¤. ì¦‰, Kê°œì˜ Partitionì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ Kê°œì˜ í‰ê· ê°’ì„ ì°¾ì•„ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë” ê°€ê¹Œìš´ í‰ê· ê°’ì— ì†í•˜ëŠ” Partitionì— dataë¥¼ ë¶„ë°°í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ êµ¬í•´ì•¼í•  ê°’ì€ ê° dataê°€ ì–´ëŠ Partitionì— ì†í•˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´($\\bold{r}\\leftarrow\\text{one hot vector}$)ì™€ ê° Partitionì˜ í‰ê· ê°’($\\mu$)ì´ë‹¤. ì¦‰, K-means Clusteringì—ì„œëŠ” ê¸°ì¡´ dataë“¤ì„ í†µí•´ì„œ Kê°œì˜ í‰ê· ê°’(K-means)ì„ ì°¾ì•„ì„œ(**Learning**) ì´í›„ì— ì¶”ê°€ë¡œ ë“¤ì–´ì˜¬ dataì— ëŒ€í•´ì„œë„ ë˜‘ê°’ì€ K-meansë¥¼ í†µí•´ì„œ Partitionì„ ì°¾ì„ ìˆ˜ ìˆë‹¤(**Inference**).ë˜ëŠ” ëª¨ë“  dataë¥¼ ì €ì¥í•´ë‘ì—ˆë‹¤ê°€ K-meansë¥¼ ë‹¤ì‹œ ê³„ì‚°í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤(Online K-means).\n\nê·¸ë ‡ë‹¤ë©´, $\\boldsymbol{\\mu}(=\\{\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}\\})$ì™€ $\\bold{R}$(ëª¨ë“  dataì˜ $\\bold{r}$ë¡œ ì´ë£¨ì–´ì§„ Matrix)ì„ ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? ì´ì— ëŒ€í•œ í•´ë‹µì€ ë‹¤ìŒê³¼ ê°™ì€ Cost Functionì„ ì œì‹œí•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}||x_{i} - \\mu_{k}||^{2}\n$$\n\ní˜„ì¬ dataì˜ pointë¡œ ë¶€í„° ê°€ì¥ ê°€ê¹Œìš´ í‰ê· ì„ ì„ íƒí•˜ëŠ” ê²½ìš°ë¥¼ ìµœëŒ€í™”í•´ì•¼ í•´ë‹¹ ê°’ì´ ê°€ì¥ ì‘ì•„ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. <mark>ì¦‰, ì—¬ê¸°ì„œëŠ” í‰ê· ê³¼ì˜ ê±°ë¦¬ë¥¼ ìœ ì‚¬ì„±ì˜ ì§€í‘œë¡œ ì‚¬ìš©í•œ ê²ƒì´ë‹¤.</mark> ì—¬ê¸°ì„œëŠ” Euclidean distance(L2-norm)ë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, Manhatan distance(L1-norm)ì„ í™œìš©í•  ìˆ˜ë„ ìˆê³  ì•„ì˜ˆ ë‹¤ë¥¸ ì§€í‘œë¥¼ í™œìš©í•  ìˆ˜ë„ ìˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€ Cost Functionì´ ì•„ë˜ì™€ ê°™ì€ formì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\times(\\text{Similarity measure})\n$$\n\nê·¸ë ‡ë‹¤ë©´, ì‹¤ì œë¡œ ìœ„ì—ì„œ ì œì‹œí•œ Cost Functionì„ í™œìš©í•˜ì—¬ ì–´ë–»ê²Œ $\\boldsymbol{\\mu}$ì™€ $\\bold{R}$ì„ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? minimizeí•˜ê³ ìí•˜ëŠ” ìš”ì†Œê°€ ë‘ ê°œì´ê¸° ë•Œë¬¸ì— ë¯¸ë¶„ì„ í•˜ê¸°ë„ ë‹¤ì†Œ ë‚œí•´í•˜ë‹¤. ë”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” EM Algorithmì´ë¼ëŠ” ë°©ì‹ì„ ì œì‹œí•œë‹¤. ì´ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒ Postingì— ëŒ€í•´ì„œ ìì„¸íˆ ë‹¤ë£¨ê² ì§€ë§Œ, ê°„ë‹¨íˆ ì„¤ëª…í•˜ìë©´ í•˜ë‚˜ì˜ Variableì„ Randomí•˜ê²Œ ì§€ì •í•˜ê³ , ë‹¤ë¥¸ Variableì˜ ìµœì ê°’ì„ êµ¬í•œ í›„ ì´ë¥¼ ë‹¤ì‹œ ëŒ€ì…í•˜ê³  ë°˜ëŒ€ Variableì„ ìµœì ê°’ìœ¼ë¡œ êµ¬í•˜ê¸°ë¥¼ ë°˜ë³µí•˜ë©´ì„œ ë” ì´ìƒ Variableì´ ìœ ì˜ë¯¸í•˜ê²Œ ë³€ê²½ë˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ë°˜ë³µí•´ì„œ êµ¬í•œ ê°’ì´ ìµœì ê°’ê³¼ ê·¼ì‚¬í•œë‹¤ëŠ” ì ì„ í™œìš©í•œ Algorithmì´ë‹¤. ì§€ê¸ˆì€ ë‹¤ì†Œ ì—‰ëš±í•  ìˆ˜ ìˆì§€ë§Œ, ì§€ê¸ˆì€ í•´ë‹¹ ë°©ë²•ì„ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ë‹¤. ì¦ëª…ì´ ê¶ê¸ˆí•˜ë‹¤ë©´, í•´ë‹¹ Posting([ğŸ”— [ML] 10. EM Algorithm](/posts/ml-em-algorithm))ì„ ì°¸ê³ í•˜ì.\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ ìˆ˜í–‰í•  ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. $\\boldsymbol{\\mu}$ë¥¼ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•œë‹¤.\n2. Assignment step: $\\boldsymbol{\\mu}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $\\bold{R}$ì„ êµ¬í•œë‹¤.  \n   $$\n   R_{ik} = \\begin{cases}\n    1 & \\text{if}\\quad k = \\argmin_{k}||x_{i} - \\mu_{k}||^{2} \\\\\n    0 & \\text{otherwise}\n   \\end{cases}\n   $$\n3. Update step: $\\bold{R}$ì´ ì£¼ì–´ì¡Œì„ ë•Œ, $\\boldsymbol{\\mu}$ë¥¼ êµ¬í•œë‹¤.  \n   ìš°ë¦¬ê°€ ë¶„ë¥˜í•œ $R$ì„ í™œìš©í•˜ì—¬ ê° kì— ì†í•˜ëŠ” dataì˜ í‰ê· ì„ í†µí•´ì„œ $\\boldsymbol{\\mu}$ë¥¼ êµ¬í•œë‹¤.\n   $$\n   \\mu_{k} = \\frac{\\sum_{i=1}^{N}R_{ik}x_{i}}{\\sum_{i=1}^{N}R_{ik}}\n   $$\n4. íŠ¹ì •ê°’ìœ¼ë¡œ $\\boldsymbol{\\mu}$ê°€ ìˆ˜ë ´í•  ë•Œê¹Œì§€ 2ë²ˆ, 3ë²ˆ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.\n\nì•„ë˜ëŠ” ì´ ê³¼ì •ì„ ê·¸ë¦¼ì„ í†µí•´ì„œ í‘œí˜„í•œ ê²ƒì´ë‹¤.\n\n![ml-clustering-1](/images/ml-clustering-1.jpg)\n\nK-means ë°©ì‹ì€ ìœ„ì™€ ê°™ì€ Iteration ì ˆì°¨ë¥¼ ë§ì´ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ë„ ëª‡ë²ˆì˜ ìˆ˜í–‰ë§Œìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤ëŠ” ê²ƒì„ ê´€ì¸¡í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, Assignment ì‹œì—ëŠ” $O(KND)$ì˜ ì‹œê°„ì´ ì†Œëª¨ë˜ê³ , Update ì‹œì—ëŠ” $O(N)$ ë§Œí¼ì˜ ì‹œê°„ì´ ì†Œëª¨ë˜ê¸° ë•Œë¬¸ì— ë¬´ê²ì§€ ì•Šê³ , êµ‰ì¥íˆ ê°„ë‹¨í•˜ë‹¤ëŠ” ì¥ì ì„ ê°–ê³  ìˆë‹¤. í•˜ì§€ë§Œ, ì´ ë°©ë²•ì€ Global Optimalì„ ì°¾ì„ ê²ƒì´ë¼ëŠ” í™•ì‹ ì„ ì¤„ ìˆ˜ ì—†ë‹¤. ê·¸ë ‡ê¸°ì— ì´ˆê¸°ê°’ì„ ì–´ë–»ê²Œ ì¡ëŠëƒì— ë”°ë¼ì„œ ê²°ê³¼ê°€ í¬ê²Œ ë³€í•  ìˆ˜ë„ ìˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼, outlier dataì— ëŒ€í•´ì„œë„ êµ‰ì¥íˆ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ ì‚¬ì§„ì—ì„œ ì™¼ìª½ë³´ë‹¤ ì˜¤ë¥¸ìª½ì´ ë” ì„±ê³µì ì¸ Clusteringì´ë¼ê³  ë§í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n![ml-clustering-2](/images/ml-clustering-2.jpg)\n\nì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ë“¤ì´ ì œì‹œë˜ì—ˆë‹¤.\n\n1. **K-means++**: ì´ˆê¸°ê°’ì„ ì˜ ì„¤ì •í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ, ì´ˆê¸°ê°’ì„ ì˜ ì„¤ì •í•˜ë©´ ìˆ˜ë ´í•˜ëŠ” ì†ë„ê°€ ë¹¨ë¼ì§€ê³ , Global Optimalì— ìˆ˜ë ´í•  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§„ë‹¤.\n2. **K-mendoids**: K-meansì—ì„œëŠ” ì¤‘ì‹¬ì ì„ dataì˜ í‰ê· ìœ¼ë¡œ ì„¤ì •í–ˆì§€ë§Œ, K-mendoidsì—ì„œëŠ” ì¤‘ì‹¬ì ì„ dataì˜ ì¤‘ê°„ê°’ìœ¼ë¡œ ì„¤ì •í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´ outlierì— ë¯¼ê°í•˜ì§€ ì•Šê²Œ ëœë‹¤.\n\n> <mark>**Soft K-means**</mark>\n\në§ˆì§€ë§‰ìœ¼ë¡œ K-means Clusteringì—ì„œ í™•ë¥ ì ì¸ ì ‘ê·¼ì„ ì‹œë„í•œ ë°©ë²• ë˜í•œ ì†Œê°œí•˜ê² ë‹¤. ì• ì„œ ë³¸ (Hard)K-meansì—ì„œëŠ” $\\bold{R}_{ik}$ë¥¼ 0 ë˜ëŠ” 1ë¡œ ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ, ì´ë¥¼ í™•ë¥ ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì— ëŒ€í•´ì„œ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰, ë‹¤ìŒê³¼ ê°™ì´ soft-max functionì„ í™œìš©í•œë‹¤ë©´ í‘œí˜„ì´ ê°€ëŠ¥í•  ê²ƒì´ë‹¤.\n\n$$\n\\bold{R}_{ik} = \\frac{\\exp(-\\beta||x_{i}-\\mu_{k}||^{2})}{\\sum_{l \\in {1, 2, \\cdots, K}} \\exp(-\\beta||x_{i}-\\mu_{l}||^{2})}\n$$\n\nì´ë ‡ê²Œ í™•ë¥ ì ìœ¼ë¡œ í‘œí˜„í•˜ê²Œ ë˜ë©´, ìš°ë¦¬ëŠ” ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ íŠ¹ì • Clusterë¡œ í•´ë‹¹ í™•ë¥ ì´ í¸í–¥ë˜ì–´ ìˆì„ ìˆ˜ë¡ ë” ì¢‹ì€ ë¶„ë¥˜ì¼ ê²ƒì´ë¼ëŠ” ì‚¬ì „ ì§€ì‹(Prior)ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Cost Functionì„ ë³€ê²½í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}||x_{i} - \\mu_{k}||^{2} - \\frac{1}{\\beta}\\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\log\\bold{R}_{ik}\n$$\n\në’· ë¶€ë¶„ì— ìƒˆë¡œ ì¶”ê°€ëœ $-\\frac{1}{\\beta}\\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\log\\bold{R}_{ik}$ëŠ” $R_{ik}$ê°€ í™•ë¥ ì´ ë˜ì—ˆê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ìƒ Entropyë¥¼ ì˜ë¯¸í•œë‹¤. EntropyëŠ” ê· í˜•ì¡íŒ ë¶„í¬ì¼ ìˆ˜ë¡ ì»¤ì§€ê³ , skewëœ ê²½ìš°ì—ëŠ” ì‘ì•„ì§€ê¸° ë•Œë¬¸ì— ì ì ˆí•œ ì§€í‘œë¼ê³  í•  ìˆ˜ ìˆë‹¤. $\\beta$ëŠ” ì´ëŸ¬í•œ priorë¥¼ ì–¼ë§ˆë‚˜ ì‚¬ìš©í• ì§€ì— ëŒ€í•œ hyperparameterì´ë‹¤. $\\beta$ê°€ í´ ìˆ˜ë¡ ì‚¬ì‹¤ìƒ Hard K-meansì™€ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ê²Œ ë˜ê³ , $\\beta$ê°€ ì‘ì„ ìˆ˜ë¡ Entropyë¥¼ ë” ì¤‘ìš”ì‹œí•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê²Œ ëœë‹¤.\n\n### Gaussian Mixture Model\n\nGaussian Mixture Model, ì¼ëª… GMMì€ Finite Mixture Modelì˜ ì¼ì¢…ì´ë‹¤. Finite Mixture Modelì€ ìš°ë¦¬ê°€ ì¶”ì •í•˜ê³ ì í•˜ëŠ” í™•ë¥  ë¶„í¬ê°€ ë‹¤ì–‘í•œ í™•ë¥  ë¶„í¬ ëª‡ ê°œì˜ ì¡°í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë¶„í¬ë¼ê³  ê°€ì •í•˜ê³ , í•´ë‹¹ í™•ë¥  ë¶„í¬ì˜ Parameterë¥¼ í•™ìŠµ(Learning) ë‹¨ê³„ì—ì„œ ì°¾ì•„ë‚´ê³ , ì´ë¥¼ ì´ìš©í•´ì„œ ìƒˆë¡œìš´ dataì— ëŒ€í•´ì„œ ì¶”ì •(Inference)í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n\n$$\np(x) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x, z=k) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x|z=k)p(z=k) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p_{k}(x)p(z=k)\n$$\n\nì—¬ê¸°ì„œ $z$ëŠ” ê´€ì¸¡í•  ìˆ˜ ì—†ëŠ” latent(hidden) variableë¡œ dataê°€ ëª‡ ë²ˆì§¸ í™•ë¥  ë¶„í¬ì— ì†í•  ê²ƒì¸ì§€ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ, $p(z=k)$ kë²ˆì§¸ ë¶„í¬ì— ì†í•  í™•ë¥ ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ëŒ€ê²Œ ì´ê²ƒì´ ì–´ëŠì •ë„ë¡œ í™•ë¥  ë¶„í¬ë¥¼ ì„ëŠ”ì§€ë¥¼ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì— mixing parameterë¼ê³ ë„ ë¶€ë¥¸ë‹¤.\n\nì´ì— ë”°ë¼ GMMì€ ê° $p_{k}(x)$ê°€ Gaussian Distributionì´ë¼ê³  ê°€ì •í•˜ëŠ” Finite Mixture Modelì¸ ê²ƒì´ë‹¤.\n\n![ml-gmm-graphical-form](/images/ml-gmm-graphical-form.jpg)\n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Graphical Model í˜•íƒœë¡œ Finite Mixture Modelì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ $\\pi,\\, \\mu,\\, \\Sigma$ëŠ” Parameterë¥¼ ì˜ë¯¸í•œë‹¤.\n\n- $\\pi_{k} = p(z = k)$\n- $\\mu_{k} = E[x|z=k]$ ì¦‰, Gaussianì˜ ê¸°ëŒ“ê°’ì„ ì˜ë¯¸í•œë‹¤.\n- $\\Sigma_{k} = Cov[x|z=k]$ ì¦‰, Gaussianì˜ ë¶„ì‚°ì„ ì˜ë¯¸í•œë‹¤.\n\nì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” ìœ„ì—ì„œ ì œì‹œí•œ í™•ë¥ ì„ ë‹¤ìŒê³¼ ê°™ì´ ì¬ì •ì˜í•  ìˆ˜ ìˆë‹¤. (Joint Probabilityë¥¼ Bayesian Networkë¡œ í‘¼ ì‹ì´ë‹¤. ëª¨ë¥´ê² ë‹¤ë©´, [ğŸ”— [ML] 8. Graphical Model](/posts/ml-graphical-model#Graphical-Model)ì—ì„œ Bayesian Networkë¥¼ ë‹¤ì‹œ ì‚´í´ë³´ê³  ì˜¤ì.)\n\n$$\n\\begin{align*}\np(x) &= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x, z=k) \\\\\n&= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(z=k| \\pi_{k})p(x|z=k, \\mu_{k}, \\Sigma_{k}) \\\\\n&= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\pi_{k}\\mathcal{N}(x|\\mu_{k}, \\Sigma_{k})\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì‹¤ì œë¡œ ì¶”ì¸¡(Inference)ì„ í•  ë•Œì—ëŠ” $p(z|x)$ê°€ í•„ìš”í•˜ë‹¤. ì´ëŠ” ìš°ë¦¬ê°€ posteriorë¥¼ í™œìš©í•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\hat{k} &=\\argmax_{k}p(z=k|x) \\\\\n&= \\argmax_{k}\\frac{p(x|z=k)p(z=k)}{p(x)} \\\\\n&= \\argmax_{k}p(x|z=k)p(z=k)\\\\\n&= \\argmax_{k}\\pi_{k}\\mathcal{N}(x|\\mu_{k}, \\Sigma_{k})\n\\end{align*}\n$$\n\ní•™ìŠµ(Learning)ì„ í•  ë•Œì—ëŠ” ê²°êµ­ $\\pi,\\, \\mu,\\, \\Sigma$ ì´ ì„¸ ê°œì˜ parameter ê°’ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì´ê²ƒì€ ìš°ë¦¬ê°€ Parametric Estimationì—ì„œ ì¤„ê¸°ì°¨ê²Œ í–ˆë˜ MLEë¥¼ ì´ìš©í•˜ë©´ ëœë‹¤. ì´ë¥¼ ìœ„í•œ LikelihoodëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\pi,\\, \\mu,\\, \\Sigma) &= \\log{p(\\mathcal{D} | \\pi,\\, \\mu,\\, \\Sigma)} \\\\\n&= \\log{\\prod_{i=1}^{N}{p(x_{i} | \\pi,\\, \\mu,\\, \\Sigma)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{p(x_{i}| \\pi,\\, \\mu,\\, \\Sigma)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{\\sum_{k=1}^{K}{\\pi_{k}\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma_{k})}}}\n\\end{align*}\n$$\n\ní•˜ì§€ë§Œ, ì´ê²ƒì„ ë‹¨ìˆœí•œ Optimization Techniqueìœ¼ë¡œëŠ” í’€ ìˆ˜ ì—†ë‹¤. ì™œëƒí•˜ë©´, ë‹¨ìˆœí•œ ë¯¸ë¶„ìœ¼ë¡œ ê° parameterë¥¼ êµ¬í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, EM Algorithmì„ ì´ìš©í•´ì„œ í’€ì–´ì•¼ í•œë‹¤. (ì´ê²ƒì€ [ğŸ”— [ML] 10. EM Algorithm](/posts/ml-em-algorithm)ì—ì„œ ë‹¤ë£¬ë‹¤.)\n\në”°ë¼ì„œ, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì„ì˜ì˜ $\\pi,\\, \\mu,\\, \\Sigma$ë¥¼ ê°€ì •í•œ ìƒíƒœì—ì„œ dataì— ì•Œë§ëŠ” ìµœì ì˜ Cluster setì„ êµ¬í•˜ê³ , dataì— clusterê°€ labelëœ ìƒíƒœì—ì„œ ìµœì ì˜ $\\pi,\\, \\mu,\\, \\Sigma$ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ëŠ” ê²ƒì´ë‹¤.\n\n![ml-gmm-1](/images/ml-gmm-1.jpg)\n\nê·¸ë ‡ë‹¤ë©´, ì´ë¥¼ ì‹¤ì œë¡œ ì–´ë–»ê²Œ í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤. í•˜ì§€ë§Œ, ê·¸ëƒ¥ ëª¨ë“  Gaussian í˜•íƒœë¥¼ ìœ„í•œ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ë‹¤ì†Œ ì‹ì´ ë³µì¡í•´ì§€ê¸° ë•Œë¬¸ì— isotropic Gaussian(ëª¨ë“  ë°©í–¥ì—ì„œ ë¶„ì‚°ì´ ë™ì¼í•œ Gaussian)ì„ ê°€ì •ìœ¼ë¡œ í•˜ê² ë‹¤.\n\në˜í•œ, ë‹¤ìŒê³¼ ê°™ì€ ìš”ì†Œë¥¼ ì¶”ê°€ë¡œ ì •ì˜í•˜ì.\n\n1. $z_{i} \\in \\{1, 2, \\cdots, K\\}$ : $i$ë²ˆì§¸ dataê°€ ì†í•˜ëŠ” clusterì˜ index  \n   $z_{ik} = \\begin{cases} 1 & \\text{if } z_{i} = k \\\\ 0 & \\text{otherwise} \\end{cases}$\n2. $\\theta_{k} = (\\pi_{k},\\, \\mu_{k},\\, \\Sigma_{k})$ : $k$ë²ˆì§¸ clusterë¥¼ ìœ„í•œ parameterì˜ ì§‘í•©  \n   $\\theta = (\\pi,\\, \\mu,\\, \\Sigma)$ : parameterì˜ ì§‘í•©  \n\nì• ì„œ ë§í•œ ë°”ì™€ ê°™ì´ ì´ì œ ìš°ë¦¬ëŠ” $\\theta$ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ $z_{i}$ì— í•´ë‹¹í•˜ëŠ” ì •ë³´ë„ ì•Œê³  ìˆë‹¤. ë”°ë¼ì„œ, Likelihood ì‹ë„ ë³€í˜•ë˜ì–´ì•¼ í•œë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) &= \\log{p(\\mathcal{D} | \\theta)} \\\\\n&\\geq \\log{p(X, Z | \\theta)} \\\\\n&= \\sum_{i=1}^{N}{\\log{p(x_{i}, z_{i}| \\theta)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{p(z_{i} | \\theta) \\times p(x_{i}| z_{i}, \\theta)}} \\\\\n&= \\sum_{i=1}^{N}{\\log{(\\prod_{k=1}^{K}{\\pi_{k}^{z_{ik}}} \\times \\prod_{k=1}^{K}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)^{z_{ik}}})}} \\\\\n&= \\sum_{i=1}^{N}{\\sum_{k=1}^{K}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})^{z_{ik}}}} \\\\\n&= \\sum_{i=1}^{N}{\\sum_{k=1}^{K}z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}} \\\\\n\\end{align*}\n$$\n\nì´ì— ë”°ë¼ì„œ ìš°ë¦¬ëŠ” EM Algorithmì˜ $\\mathcal{Q}$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{Q}(\\theta; \\theta^{\\prime}) &= \\sum_{i=1}^{N}E_{z_{i}|x_{i}, \\theta^{\\prime}}[\\log p(x_{i}, z_{i} | \\theta)] \\\\\n&= \\sum_{i=1}^{N}E_{z_{i}|x_{i}, \\theta^{\\prime}}[\\sum_{k=1}^{K}z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}] (\\because \\text{ìœ„ì˜ ì‹ì—ì„œ 3ë²ˆì§¸ ì¤„ì„ ì°¸ê³ })\\\\\n&= \\sum_{i=1}^{N}\\sum_{k=1}^{K}E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}] \\\\\n&= \\sum_{i=1}^{N}\\sum_{k=1}^{K}E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}] \\\\\n&= \\sum_{i=1}^{N}\\sum_{k=1}^{K}E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}]\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})} \\\\\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê° stepì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n- **E-step**  \n  $\\mathcal{Q}$ì—ì„œ parameter($\\pi,\\, \\mu,\\, \\Sigma$)ë¥¼ ì œì™¸í•˜ê³ , ì•„ì§ ë¯¸ì§€ìˆ˜ë¡œ ë‚¨ì•„ìˆëŠ” ê°’ì€ $E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}]$ì´ë‹¤. ì¦‰, ì´ ê°’ë§Œ êµ¬í•˜ë©´ $\\mathcal{Q}$ë¥¼ êµ¬í–ˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.  \n  $$\n  \\begin{align*}\n  E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}] &= \\sum_{k=1}^{K}{z_{ik}p(z_{i} = k | x_{i}, \\theta^{\\prime})} \\\\\n  &= p(z_{i} = k^{*} | x_{i}, \\theta^{\\prime}) = r_{ik^{*}}\n  \\end{align*}\n  $$  \n  ê²°êµ­ ìš°ë¦¬ê°€ í•´ë‹¹ ë‹¨ê³„ì—ì„œ êµ¬í•  ê²ƒì€ ê´€ì¸¡ ê°€ëŠ¥í•œ dataì™€ ì´ì „ parameterê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì†í•˜ê²Œ ë˜ëŠ” clusterì—ì„œì˜ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì„ ëª¨ë“  dataì— ëŒ€í•´ì„œ êµ¬í•˜ë©´, $\\mathcal{Q}$ì—ì„œ parameterë¥¼ ì œì™¸í•œ ëª¨ë“  ë¶€ë¶„ì„ êµ¬í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì‹ì„ ì¢€ ë” ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ë¡ ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n  $$\n  \\begin{align*}\n  E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}] = r_{ik^{*}} &= \\frac{p(x_{i}, z_{i}=k^{*} | \\theta^{\\prime})}{p(x_{i}|\\theta^{\\prime})} \\\\\n  &= \\frac{\\pi_{k^{*}}^{\\prime}{\\mathcal{N}(x_{i}|\\mu_{k^{*}}^{\\prime}, \\Sigma_{k^{*}}^{\\prime} I)}}{\\sum_{l=1}^{K}{\\pi_{l}{\\mathcal{N}(x_{i}|z_{i} = l, \\mu_{l}, \\Sigma_{l} I)}}}\n  \\end{align*}\n  $$  \n  \n- **M-step**  \n  ê²°ë¡ ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ $\\mathcal{Q}$ì™€ constraintë¥¼ ì–»ì—ˆë‹¤.  \n  $$\n  \\begin{align*}\n  \\text{maximize}&\\quad \\mathcal{Q}(\\theta; \\theta^{\\prime}) = \\sum_{i=1}^{N}\\sum_{k=1}^{K}r_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})} \\\\\n  \\text{subject to}&\\quad \\sum_{k=1}^{K}{\\pi_{k}} = 1\n  \\end{align*}\n  $$  \n  ì´ì œ ìš°ë¦¬ëŠ” ì´ë¥¼ Optimization ë°©ì‹ì„ í™œìš©í•˜ì—¬ í’€ê¸°ë§Œ í•˜ë©´ ëì´ë‹¤. ([ğŸ”— ì°¸ê³ (Base Knowledge)](/posts/ml-base-knowledge))  \n  $$\n  \\begin{align*}\n  \\mu_{k} &= \\frac{\\sum_{i=1}^{N}{r_{ik}x_{i}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\Sigma_{k} &= \\frac{\\sum_{i=1}^{N}{r_{ik}||x_{i} - \\mu_{k}||^{2}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\pi_{k} &= \\frac{1}{N}\\sum_{i=1}^{N}{r_{ik}}\n  \\end{align*}\n  $$\n\n---\n\në§ˆì§€ë§‰ìœ¼ë¡œ ì§šê³  ë„˜ì–´ê°ˆ ê²ƒì€, ë°”ë¡œ K-means Clusteringì€ ì‚¬ì‹¤ GMMì˜ í•˜ë‚˜ì˜ special caseë¼ëŠ” ê²ƒì´ë‹¤. ë§Œì•½, ìš°ë¦¬ê°€ $\\pi_{k},\\, \\Sigma_{k}$ë¥¼ ëª¨ë‘ ê°™ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ë©´, $\\pi_{k} = \\frac{1}{K}$ì´ê³  $\\Sigma_{k} = \\Sigma$ê°€ ëœë‹¤ê³  í•˜ì. ì´ë•Œ EM algorithmì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n- **E-step**  \n  $$\n  r_{ik} = \\begin{cases} 1 & k = \\argmax_{l\\in \\{1, 2, \\cdots, K\\}} p(x_{i}|z_{i} = l, \\mu_{l}, \\Sigma) \\\\ 0 & \\text{otherwise} \\end{cases}\n  $$  \n  ì´ëŠ” ì‚¬ì‹¤ìƒ K-means Clusteringì—ì„œ ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬ë¥¼ í†µí•´ì„œ êµ¬í–ˆë˜ ê²ƒê³¼ ë§¤ìš° ìœ ì‚¬í•œ ì‹ì´ë‹¤.\n- **M-step**  \n  $$\n  \\begin{align*}\n  \\mu_{k} &= \\frac{\\sum_{i=1}^{N}{r_{ik}x_{i}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\pi_{k} &= \\frac{1}{N}\\sum_{i=1}^{N}{r_{ik}}\n  \\end{align*}\n  $$  \n  $\\pi_{k}$ê°€ ì¶”ê°€ë˜ê¸°ëŠ” í–ˆì§€ë§Œ, $\\mu_{k}$ë¥¼ êµ¬í•˜ëŠ” ì‹ì€ ì™„ì „ ë™ì¼í•˜ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-clustering","date":"2022-11-23 09:19","title":"[ML] 9. Clustering","category":"AI","tags":["ML","UnsupervisedLearning","Clustering","K-means","GMM"],"desc":"ì´ì „ê¹Œì§€ì˜ Postingì—ì„œëŠ” Supervised Learning ì¦‰, ì´ë¯¸ Labelingì´ ì™„ë£Œëœ ë°ì´í„°ì— ì˜í•œ Learningì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ì—ˆë‹¤. ì§€ê¸ˆë¶€í„°ëŠ” Unsupervised Learningì— ëŒ€í•´ì„œ ì¡°ê¸ˆ ë” ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤. ëŒ€í‘œì ì¸ Unsupervised Learningì€ Clustering, Feature Selection(or Dimensionality Reduction), Generative Model ë“±ì´ ì¡´ì¬í•œë‹¤. ì´ë“¤ì— ëŒ€í•´ì„œ ì°¨ê·¼ì°¨ê·¼ ì‚´í´ë³´ë„ë¡ í•˜ê³ , í•´ë‹¹ Postingì—ì„œëŠ” ê°€ì¥ ëŒ€í‘œì ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” Clusteringì„ ë¨¼ì € ì‚´í´ë³´ë©´ì„œ Unsupervised Learningì— ëŒ€í•œ ê³„ëµì ì¸ ì´í•´ë¥¼ í•´ë³´ë„ë¡ í•˜ê² ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nMachine Learningì€ ì£¼ì–´ì§„ dataë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” pattern(Model)ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œë¼ê³  í•˜ì˜€ë‹¤. ê·¸ë ‡ë‹¤ë©´, \"dataê°€ ê°€ì§€ëŠ” ì—¬ëŸ¬ê°€ì§€ ì •ë³´(feature)ë“¤ ì¤‘ì—ì„œ ì–´ë–¤ featureë¥¼ ì¤‘ì ì ìœ¼ë¡œ ë³´ê³  ì´ìš©í•  ìˆ˜ ìˆì„ê¹Œ?\" ê·¸ë¦¬ê³ , \"ë§Œì•½ ì—¬ëŸ¬ featureë“¤ì´ ì„œë¡œ ì—°ê´€ì´ ìˆë‹¤ë©´ ì´ë¥¼ ì—°ì‚°ì˜ ìµœì í™”ë¥¼ ìœ„í•´ ì´ìš©í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?\" ë¼ëŠ” ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë‹¤. ì—¬ê¸°ì„œ Graphical Modelì€ ì´ëŸ¬í•œ ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ì„œ ì—°ì‚° ìµœì í™”ì— ëŒ€í•œ insightë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n## Relation\n\nê° featureë“¤ ì¦‰, Random Variableë“¤ ê°„ì˜ ê´€ê³„ëŠ” í¬ê²Œ ì„¸ ê°€ì§€ ì¢…ë¥˜ê°€ ìˆë‹¤.\n\n1. **Correlation(ìƒê´€ê´€ê³„)**  \n   ì‰½ê²Œ ìƒê°í•˜ë©´ ë‘ Random Variableì´ ìˆì„ ë•Œ, ì„œë¡œê°€ ê°’ ì¶”ì •ì— ì˜í–¥ì„ ì¤€ë‹¤ëŠ” ê²ƒì´ë‹¤. ì¦‰, íŠ¹ì • Random Variableì˜ ê°’ì´ ê´€ì¸¡ë˜ì—ˆì„ ë•Œ, Random Variableì´ ê°€ì§€ëŠ” ê°’ì˜ ë²”ìœ„ê°€ ì œí•œë˜ê³ , í™•ë¥ ì´ ë³€í™”í•œë‹¤.  \n   ì¦‰, $X$ì™€$Y$ê°€ ì„œë¡œ Correlationì´ ì¡´ì¬í•œë‹¤ë©´, $P(X) \\neq P(X|Y)$  \n   ê·¸ë ‡ê¸°ì— ë‘ Random Variableì´ ì„œë¡œ ë…ë¦½(independence)ì´ë¼ë©´, Correlationì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²ƒì´ë‹¤.  \n2. **Causality(ì¸ê³¼ê´€ê³„)**  \n   ì‰½ê²Œ Correlationê³¼ í—·ê°ˆë¦´ ìˆ˜ ìˆì§€ë§Œ, CausalityëŠ” ì›ì¸ê³¼ ê²°ê³¼ê°€ ë‚˜íƒ€ë‚˜ëŠ” ê´€ê³„ë¥¼ ì˜ë¯¸í•œë‹¤. ì‰¬ìš´ ì˜ˆì‹œë¡œ Xë¼ëŠ” ì‚¬ê±´ê³¼ Yë¼ëŠ” ì‚¬ê±´ì´ ë¹ˆë²ˆí•˜ê²Œ ê°™ì´ ë°œìƒí•œë‹¤ê³ , ì‰½ê²Œ Xë¼ëŠ” ì‚¬ê±´ì´ Yì˜ ì›ì¸ì´ë¼ê³  ë§í•  ìˆ˜ëŠ” ì—†ëŠ” ê²ƒê³¼ ê°™ì€ ì›ë¦¬ì´ë‹¤. ë˜í•œ, ì¤‘ìš”í•œ íŠ¹ì§• ì¤‘ì— í•˜ë‚˜ëŠ” ë°©í–¥ì´ ë¶„ëª…í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì›ì¸ê³¼ ê²°ê³¼ëŠ” ëŒ€ê²Œ ë¶„ë¦¬ë˜ê¸° ë•Œë¬¸ì— ì›ì¸ì´ ë˜ëŠ” ì‚¬ê±´ê³¼ ê²°ê³¼ê°€ ë˜ëŠ” ì‚¬ê±´ì´ ë¶„ëª…ì´ êµ¬ë¶„ëœë‹¤. ê²°ë¡ ì ìœ¼ë¡œ, Causalityë¥¼ ê°€ì§€ëŠ” ë‘ ì‚¬ê±´ì€ ì„œë¡œ Correlationì´ ìˆëŠ” ê²ƒì€ ìëª…í•˜ì§€ë§Œ, Correlationì´ ì¡´ì¬í•œë‹¤ê³  Causalityë¥¼ ë‹¨ì •í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ì¦‰, Correlationì´ Causalityë¥¼ í¬í•¨í•˜ëŠ” ê°œë…ì´ë‹¤. ê·¸ë ‡ê¸°ì— ì„œë¡œ ë…ë¦½ì´ë¼ë©´, Causalityë„ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²ƒì´ë‹¤.\n3. **Independence(ë…ë¦½)**  \n   ìœ„ì— ì œì‹œëœ ë‘ ê°€ì§€ëŠ” dependence ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì´ëŠ” ë‘ Random Variableì˜ ê°’ì´ ì„œë¡œì˜ ê°’ì— ì˜í–¥ì„ ì „í˜€ ì£¼ì§€ ì•ŠìŒì„ ì˜ë¯¸í•œë‹¤.  \n   ì¦‰, $X$ì™€ $Y$ê°€ ì„œë¡œ ë…ë¦½í•˜ë‹¤ë©´, $P(X) = P(X|Y), P(Y) = P(Y|X)$ì´ë‹¤.  \n   (ê²°ê³¼ì ìœ¼ë¡œ Independenceê°€ ì•„ë‹ˆë¼ë©´ ìµœì†Œí•œì˜ Correlationì´ ì¡´ì¬í•œë‹¤.)\n\nì´ëŸ¬í•œ ê´€ê³„ë¥¼ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆì„ì§€ë¥¼ ê³ ë¯¼í•´ë³´ì. ìš°ë¦¬ê°€ ì§‘ì¤‘ì ìœ¼ë¡œ ì‚´í´ë³¼ ê²ƒì€ **Independence**ì´ë‹¤. ë§Œì•½, ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” ê²°ê³¼ê°’($Y$)ê°€ ì¡´ì¬í•  ë•Œ, íŠ¹ì • feature($X_{1}$)ê°€ ì„œë¡œ ë…ë¦½í•œë‹¤ê³  í•˜ì. $P(Y|X_{1})=P(Y)$ì— ì˜í•´ì„œ $X_{1}$ëŠ” ì „í˜€ ì“¸ëª¨ê°€ ì—†ëŠ” ì •ë³´ì„ì„ ì•Œ ìˆ˜ê°€ ìˆë‹¤. ì´ë ‡ê²Œ ëª…í™•í•œ independenceë¥¼ ì•ˆë‹¤ë©´ í•´ë‹¹ featureë¥¼ Learning ë° Estimationì—ì„œ ì œê±°í•˜ëŠ” ê²ƒì€ ì‰¬ìš¸ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ê´€ê³„ë¥¼ ëª…í™•í•˜ê²Œ ë°íˆê¸° ì–´ë ¤ìš¸ ë•Œê°€ ë§ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê²°êµ­ ìš°ë¦¬ê°€ Machine Learningì„ í†µí•´ì„œ êµ¬í•˜ê³ ì í•˜ëŠ” ì‹ì¸ ì•„ë˜ ì‹ì„ ì–´ë–»ê²Œ í•˜ë©´ ì¢€ ë” ìµœì í™”í•  ìˆ˜ ìˆì„ê¹Œ?\n\n$$\nP(Y|X_{1}, X_{2}, \\cdots, X_{N}) = \\frac{P(Y, X_{1}, X_{2}, \\cdots, X_{N})}{P(X_{1}, X_{2}, \\cdots, X_{N})}\n$$\n\nì—¬ê¸°ì„œì˜ í•µì‹¬ì€ ë°”ë¡œ **Joint Probability**ì— ìˆë‹¤. ìš°ë¦¬ëŠ” ê²°êµ­ ì¢‹ë“  ì‹«ë“  **Joint Probability**ë¥¼ êµ¬í•´ì•¼ í•œë‹¤.\n\n$$\n\\begin{align*}\nP(X_{1}, X_{2}, \\cdots, X_{N}) &= P(X_{1} | X_{2}, X_{3}, \\cdots, X_{N}) \\times P(X_{2}, X_{3}, \\cdots, X_{N})\\\\\n&= P(X_{1} | X_{2}, X_{3}, \\cdots, X_{N}) \\times P(X_{2} |, X_{3}, X_{4}, \\cdots, X_{N}) \\times P(X_{3}, X_{4}, \\cdots, X_{N}) \\\\\n&= \\prod_{i=1}^{N} P(X_{i} | X_{i+1}, X_{i+2}, \\cdots, X_{N})\n\\end{align*}\n$$\n\nìœ„ì— ì œì‹œí•œ **Probability Chain Rule**ì— ì˜í•´ì„œ ìš°ë¦¬ëŠ” Joint ProbabilityëŠ” ê°ê°ì˜ Random Variable ì˜ Conditional Probabilityë¼ê³  í•  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” Random Variableì´ Nê°œ ìˆê³ , ê° Random Variableì˜ dimensionì´ Lì´ë¼ê³  í•  ë•Œ, ë‹¤ìŒê³¼ ê°™ì•„ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n$$\nL^{N} \\times L^{N-1} \\times \\cdots \\times L^{1} = O(L^{N})\n$$\n\nì´ëŸ¬í•œ ì—°ì‚°ì„ ì–´ë–»ê²Œ í•˜ë©´ ì¢€ ë” ìµœì í™”í•  ìˆ˜ ìˆì„ê¹Œ? HintëŠ” Conditional Probability ê° ê°ì˜ ë³€ìˆ˜ì˜ ì–‘ì„ ì¤„ì´ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ê°€ ì–´ë–¤ ê´€ê³„ê°€ ìˆì„ ë•Œ, ì´ Random Variableì˜ ê°¯ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì„ê¹Œ? ë°”ë¡œ ë³€ìˆ˜ ê°„ Conditional Independenceê°€ ì´ì— ëŒ€í•œ í•´ë‹µì„ ì œì‹œí•œë‹¤.\n\n### Conditional Independence\n\nConditional IndependenceëŠ” Conditional Probabilityì²˜ëŸ¼ íŠ¹ì • ì •ë³´(ë‹¤ë¥¸ Random Variableì˜ ê°’)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë‘ Random Variableì´ ì„œë¡œ ë…ë¦½ì´ë¼ëŠ” ê²ƒì´ë‹¤.\n\nì‰½ê²Œ ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•œë‹¤ë©´, \"ê³¼ìŒ\"ê³¼ \"ë¹¨ê°„ ì–¼êµ´\" ì‚¬ì´ì˜ ê´€ê³„ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” \"ë¹¨ê°„ ì–¼êµ´\"ì¸ ì‚¬ëŒì´ \"ê³¼ìŒ\"ì„ í–ˆì„ ê²ƒì´ë¼ê³  íŒë‹¨í•  ê²ƒì´ë‹¤. ì¦‰, \"ë¹¨ê°„ ì–¼êµ´\"ê³¼ \"ê³¼ìŒ\" ì‚¬ì´ì—ëŠ” ê´€ê³„ê°€ ì¡´ì¬í•œë‹¤(dependency). í•˜ì§€ë§Œ, \"í˜ˆì¤‘ ì•Œì½”ì˜¬ ë†ë„\"ë¼ëŠ” ì •ë³´ê°€ ì£¼ì–´ì§„ë‹¤ë©´ ì–´ë–¨ê¹Œ? \"í˜ˆì¤‘ ì•Œì½”ì˜¬ ë†ë„\"ê°€ ì£¼ì–´ì§„ë‹¤ë©´, ì‚¬ì‹¤ \"ë¹¨ê°„ ì–¼êµ´\"ì€ ë” ì´ìƒ \"ê³¼ìŒ\" ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ì— ì˜í–¥ì„ 1ë„ ì£¼ì§€ ì•Šì„ ê²ƒì´ë‹¤. ì´ë•Œì—ëŠ” \"ê³¼ìŒ\"ê³¼ \"ë¹¨ê°„ ì–¼êµ´\"ì€ independenceí•˜ë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ° ê²½ìš°ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\text{ê³¼ìŒ} \\not\\!\\perp\\!\\!\\!\\perp \\text{ë¹¨ê°„ ì–¼êµ´}\n$$\n$$\n\\text{ê³¼ìŒ} \\perp\\!\\!\\!\\!\\perp \\text{ë¹¨ê°„ ì–¼êµ´} |\\ \\text{í˜ˆì¤‘ ì•Œì½”ì˜¬ ë†ë„}\n$$\n\nì¦‰, í™•ë¥ ì— ì ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\nP(\\text{ê³¼ìŒ} | \\text{ë¹¨ê°„ ì–¼êµ´, í˜ˆì¤‘ ì•Œì½”ì˜¬ ë†ë„}) = P(\\text{ê³¼ìŒ} | \\text{í˜ˆì¤‘ ì•Œì½”ì˜¬ ë†ë„})\n$$\n\nì—¬ê¸°ì„œ ìš°ë¦¬ê°€ í•˜ê³  ì‹¶ì—ˆë˜ ê²ƒì´ ë‚˜ì™”ë‹¤. ë°”ë¡œ \"ë¹¨ê°„ ì–¼êµ´\"ì´ë¼ëŠ” Random Variableì´ ì—†ì–´ì¡Œë‹¤. ì¦‰, \"ê³¼ìŒ\"ê³¼ \"ë¹¨ê°„ ì–¼êµ´\" ì‚¬ì´ì˜ ê´€ê³„ ê°™ì€ ê²ƒì„ ì°¾ì„ ìˆ˜ ìˆë‹¤ë©´, ìš°ë¦¬ëŠ” ê³„ì‚° ê³¼ì •ì„ ë‹¨ìˆœí™”í•  ìˆ˜ ìˆë‹¤.\n\nì¦‰, ì´ê²ƒì´ ìš°ë¦¬ê°€ **Graph**ë¥¼ í†µí•´ì„œ ì°¾ê³ ì í•˜ëŠ” ê²ƒì´ë‹¤.\n\n## Graphical Model\n\n**Graphical Model**ì€ **Graph**ë¥¼ ì´ìš©í•´ì„œ Random Variableë“¤ì˜ ê´€ê³„ë¥¼ í‘œí˜„í•˜ê³ , ì´ë¥¼ í†µí•´ì„œ **Joint Probability**ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ë‹¤. **Graph**ë¥¼ ê·¸ë¦¬ëŠ” ë°©ë²•ì€ ê¸°ë³¸ì ìœ¼ë¡œ Random Variable í•˜ë‚˜ í•˜ë‚˜ê°€ Graphì˜ Nodeê°€ ë˜ê³ , ê° Nodeê°„ì˜ ê´€ê³„ê°€ Edgeê°€ ëœë‹¤. ê·¸ëŸ°ë°, ì´ ê´€ê³„ê°€ Correlationì´ëƒ, Causalityëƒì— ë”°ë¼ì„œ ë‘ ê°€ì§€ ì¢…ë¥˜ë¡œ ë‚˜ë‰˜ê²Œ ëœë‹¤. <mark>**Correlation**ì€ ì¼ë°˜ì ìœ¼ë¡œ ê´€ê³„ì˜ ë°©í–¥ì´ ì—†ê¸°ì— **Undirected Graph**</mark>ë¡œ í‘œí˜„í•˜ê³ , <mark>**Causality**ëŠ” ê´€ê³„ì˜ ë°©í–¥ì´ ìˆê¸°ì— **Directed Graph**</mark>ë¡œ í‘œí˜„í•œë‹¤. ì´ëŠ” ì•„ë˜ì—ì„œ ë” ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.\n\n### Markov Random Field(Undirected Graphical Model, Correlation)\n\n**Markov Random Field**(MRF)ë¼ê³  ë¶ˆë¦¬ë©°, **Correlation**ë¥¼ í‘œí˜„í•œ Graphì´ë‹¤. ê° NodeëŠ” Random Variableì„ ì˜ë¯¸í•˜ë©°, EdgeëŠ” Correlationë¥¼ ì˜ë¯¸í•œë‹¤. ì¦‰, ë‘ Nodeê°€ Edgeë¡œ ì—°ê²°ë˜ì–´ ìˆë‹¤ë©´, ë‘ Random Variableì€ Independenceí•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n![ml-undirected-graph-1](/images/ml-undirected-graph-1.jpg)\n\nì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê²ƒì€ Random Variableì„ ëŒ€í‘œí•˜ëŠ” Nodeì™€ Correlationì„ ëŒ€í‘œí•˜ëŠ” Edgeì´ê¸° ë•Œë¬¸ì—, Graph $G=(V, E)$ì—ì„œ Random Variableì˜ ì§‘í•© $X = \\{X_{1}, X_{2}, \\cdots, X_{|V|}\\}$ì´ê³ , $\\{1,2, \\cdots, |V|\\}$ê°€ ì£¼ì–´ì§ˆ ë•Œ ë°˜ë“œì‹œ ì•„ë˜ì— ì œì‹œëœ **Markov Propertyë“¤**ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤.\n\n1. <mark>**Pairwise Markov Property**</mark>  \n   ì¸ì ‘í•˜ì§€ ì•Šì€ Node ë‘ ê°œëŠ” ë‹¤ë¥¸ ëª¨ë“  Nodeê°€ ì£¼ì–´ì§ˆ ë•Œ conditionally independentí•˜ë‹¤.  \n   (ì•„ë˜ì—ì„œ \\ëŠ” í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.)\n   $$\n   X_{i} \\perp\\!\\!\\!\\!\\perp X_{j} | X_{S\\backslash\\{i, j\\}}\n   $$\n2. <mark>**Local Markov Property**</mark>  \n   í•œ Nodeì— ì¸ì ‘í•œ ëª¨ë“  Node(Neighbors)ê°€ ì£¼ì–´ì§ˆ ë•Œ, í•´ë‹¹ NodeëŠ” ë‹¤ë¥¸ ëª¨ë“  Nodeì™€ conditionally independentí•˜ë‹¤.  \n   (ì•„ë˜ì—ì„œ $\\mathcal{N}_{i}$ëŠ” Node iì™€ ì¸ì ‘í•œ ëª¨ë“  Nodeë¥¼ ì˜ë¯¸í•œë‹¤.)\n   $$\n   X_{i} \\perp\\!\\!\\!\\!\\perp X_{S\\backslash \\mathcal{N}_{i}} | X_{\\mathcal{N}_{i}}\n   $$\n3. <mark>**Global Markov Property**</mark>  \n   ë§Œì•½, Nodeë“¤ì˜ Subsetìœ¼ë¡œ ì´ë£¨ì–´ì§„ $A, B$ê°€ íŠ¹ì • subset $C$ê°€ ì£¼ì–´ì§ˆ ë•Œ, ì„œë¡œ conditionally independentí•˜ë‹¤ë©´, $A, B$ì— ì†í•˜ëŠ” ì–´ë–¤ subsetì´ë¼ë„ ì„œë¡œ independentí•˜ë‹¤.  \n   (subsetê°„ì˜ conditionally independentë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œëŠ” íŠ¹ì • Subsetë“¤ê°„ì— ì´ì–´ì§€ëŠ” ëª¨ë“  ê²½ë¡œë¥¼ ì°¨ë‹¨í•  ìˆ˜ ìˆëŠ” subsetì´ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•œë‹¤.)  \n   $$\n   \\begin{align*}\n   X_{A} &\\perp\\!\\!\\!\\!\\perp X_{B} | X_{C} \\\\\n   X_{\\text{subset of }A} &\\perp\\!\\!\\!\\!\\perp X_{\\text{subset of }B} | X_{C} \\\\\n   \\end{align*}\n   $$  \n   ![ml-global-markov-property](/images/ml-global-markov-property.jpg)\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ì „ ê·¸ë¦¼ì—ì„œ Conditional Independenceë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤. $X_{1}, X_{4}$ì˜ ê²½ìš° ë‹¤ë¥¸ ëª¨ë“  Random Variableê³¼ correlationì´ ì¡´ì¬í•˜ì§€ë§Œ, $X_{2}, X_{3}$ì˜ ê²½ìš° $X_{1}, X_{4}$ë§Œ ì•Œë©´ ëœë‹¤. ì¦‰, $X_{2} \\perp\\!\\!\\!\\!\\perp X_{3} | X_{1}, X_{4}$ì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ ê´€ê³„ë¥¼ í™•ë¥  ì‹ì—ì„œ ë…¹ì—¬ë‚¼ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\nP(X_{1}, X_{2}, X_{3}, X_{4}) &= P(X_{2}|X_{1},\\cancel{X_{3}},X_{4})P(X_{1}, X_{3}, X_{4}) (\\because X_{2} \\perp\\!\\!\\!\\!\\perp X_{3} | X_{1}, X_{4}) \\\\\n&= P(X_{2}|X_{1},X_{4})P(X_{1}, X_{3}, X_{4})\n\\end{align*}\n$$\n\në˜í•œ, ìš°ë¦¬ëŠ” Graphë¥¼ í†µí•´ì„œ Joint Probabilityë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n$$\nP(\\cap_{i=1}^{N}X_{i}) = \\frac{1}{Z} \\prod_{C \\in \\mathcal{C}} \\psi_{C}(\\cap_{X_{j}\\in C}X_{j})\n$$\n\nì‹ì´ ë‹¤ì†Œ ë‚œí•´í•˜ë‹¤. í•˜ë‚˜ í•˜ë‚˜ í•´ì„ì„ í•´ë³´ë„ë¡ í•˜ì. ë¨¼ì €, $P(\\cap_{i=1}^{N}X_{i})$ì´ë‹¤. ì´ëŠ” Joint Probabilityë¥¼ í‘œí˜„í•˜ëŠ” ë°©ë²• ì¤‘ì˜ í•˜ë‚˜ë¡œ ë‹¨ìˆœíˆ ì´ë¥¼ ì •ë¦¬í•˜ë©´, $P(\\cap_{i=1}^{N}X_{i})=P(X_{1} \\cap X_{2} \\cap \\cdots \\cap X_{N})=P(X_{1}, X_{2}, \\cdots, X_{N})$ì´ë‹¤. ë‹¤ìŒì€ $C$ì™€ $\\mathcal{C}$ì´ë‹¤. ë‘˜ ë‹¤ ì•„ë§ˆ ì§‘í•©ì¼ ê²ƒì´ë¼ëŠ” ê²ƒì€ $\\in$ ê¸°í˜¸ ë•ë¶„ì— ì•Œ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì–´ë–¤ ë°ì´í„°ë¥¼ ë‹´ê³  ìˆëŠ” ì§‘í•©ì¼ê¹Œ? ì´ëŠ” Random Variableë“¤ë¡œ ì´ë£¨ì–´ì§„ ë¶€ë¶„ ì§‘í•©ì´ë‹¤. ì´ë¥¼ <mark>**Clique($C$)**</mark>ë¼ê³  í•œë‹¤. CliqueëŠ” Graphì—ì„œ Nodeë“¤ì˜ ë¶€ë¶„ ì§‘í•©ìœ¼ë¡œ, Graphì—ì„œ **Fully Connected Node**ì˜ ì§‘í•©ì„ ì˜ë¯¸í•œë‹¤. ì´ê²ƒì´ ê°€ì§€ëŠ” ì˜ë¯¸ëŠ” ì‚¬ì‹¤ìƒ í•˜ë‚˜ì˜ Nodeë¡œ í•©ì¹  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.(ì´ë¥¼ Graph ìƒì—ì„œì˜ ì¸ìˆ˜ë¶„í•´(**factorization**)ë¼ê³ ë„ í•œë‹¤.) Cliqueì— ì†í•˜ëŠ” Nodeë¼ë¦¬ëŠ” ì„œë¡œ ì™„ë²½í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì´ ì¤‘ì— í•˜ë‚˜ì˜ Nodeë¼ë„ ë‹¤ë¥¸ Nodeì™€ ì—°ê²°ì„ ê°€ì§„ë‹¤ë©´, ì´ì— ì†í•˜ëŠ” ëª¨ë“  Nodeê°€ ì´ ê´€ê³„ë¡œ ì—°ê²°ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ì¶”ê°€ì ìœ¼ë¡œ Cliqueë“¤ ì¤‘ì—ì„œ ë‹¤ë¥¸ Cliqueì— ì†í•˜ì§€ ì•ŠëŠ” Cliqueë“¤ì„ <mark>**Maximal Clique($\\mathcal{C}$)**</mark>ë¼ê³  í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” Maximal Cliqueë¥¼ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œê¸°í•œ ê²ƒì´ë‹¤.\n\n![ml-max-clique](/images/ml-max-clique.jpg)\n\në§ˆì§€ë§‰ìœ¼ë¡œ $\\psi$ì´ë‹¤. ì´ëŠ” <mark>**Clique Potential Function**</mark>ë¡œ, ê° Cliqueì˜ Node(Random Variable)ë¥¼ parameterë¡œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ë¡œ í™•ë¥ ê³¼ ë¹„ìŠ·í•œ ì„±ì§ˆì„ ê°€ì§€ì§€ë§Œ í™•ë¥ ì²˜ëŸ¼ í•©ì´ 1ì´ ì•„ë‹ ìˆ˜ë„ ìˆê³ , ê°’ ìì²´ê°€ ìŒìˆ˜ì¼ ìˆ˜ë„ ìˆë‹¤. ì¦‰, ì´ë¥¼ êµ¬í•  ë•Œì—ëŠ” ê° Random Variableì˜ ê²½ìš°ì˜ ìˆ˜ì™€ í•´ë‹¹ ê²½ìš°ì˜ ìƒëŒ€ì  í™•ë¥ ë¡œ ì´ë£¨ì–´ì§„ tableì„ ì‘ì„±í•˜ê³ , ì´ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ ì°¾ì•„ë‚¸ ê²ƒì´ $\\psi$ì´ë‹¤. ëŒ€ê²Œì˜ ê²½ìš° $\\psi$ëŠ” í•´ë‹¹ Parameterë¡œ ì´ë£¨ì–´ì§„ Condition Probability ë˜ëŠ” Joint Probabilityê°€ ë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. í•˜ì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ì—ë„ $\\psi$ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.(ì´ì— ëŒ€í•œ ì—„ë°€í•œ ì¦ëª…ì€ ì—¬ê¸°ì„œ ë‹¤ë£¨ì§€ ì•Šì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤.) ì—¬ê¸°ì„œ <mark>$Z$</mark>ì˜ ì˜ë¯¸ë¥¼ ë§ˆì§€ë§‰ìœ¼ë¡œ ì§šì–´ë³´ìë©´, ë‹¨ìˆœí•œ normalizationì´ë‹¤. $\\psi$ê°€ ìš´ì¢‹ê²Œë„ Joint Probability, Conditional Probabilityë¡œ ì‰½ê²Œ êµ¬í•´ì§„ë‹¤ë©´ $Z=1$ì´ë‹¤. í•˜ì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš°ì—ëŠ” ì´ë“¤ì˜ í•©ì´ 1ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— Normalizationì´ í•„ìš”í•œ ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\nZ &= \\sum_{X_{1}}\\sum_{X_{2}} \\cdots \\sum_{X_{N}}{\\prod_{C \\in \\mathcal{C}} \\psi_{C}(\\cap_{X_{j}\\in C}X_{j})} \\\\\n&= \\sum_{X_{1}, X_{2}, \\cdots, X_{N}}{\\prod_{C \\in \\mathcal{C}} \\psi_{C}(\\cap_{X_{j}\\in C}X_{j})}\n\\end{align*}\n$$\n\nê²°ë¡ ì ìœ¼ë¡œ ì˜ë¯¸ë¥¼ ë”°ì§€ìë©´, ìœ„ì—ì„œ êµ¬í•œ **Maximal Clique**ì— íŠ¹ì • í•¨ìˆ˜ë¥¼ ì·¨í•œ $\\psi$ê°€ ì¸ìˆ˜ë¶„í•´(**factorization**)ì—ì„œ í•˜ë‚˜ì˜ ì¸ì(**factor**)ê°€ ë˜ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ **factor function**ì´ë¼ê³ ë„ ë¶€ë¥¸ë‹¤.\n\nì, ë§ˆì§€ë§‰ìœ¼ë¡œ ìš°ë¦¬ê°€ 4ê°œì˜ Random Variable 4ê°œ($A, B, C, D$)ê°€ ìˆì„ ë•Œ, Graphë¡œ ê·¸ë¦´ ìˆ˜ ìˆëŠ” í˜•íƒœë¥¼ ë„¤ ê°œ ì •ë„ ê°€ì •í•˜ì—¬ ì˜ˆì‹œë“¤ì„ ì‚´í´ë³¼ ê²ƒì´ë‹¤.\n\n![ml-undirected-graph-2](/images/ml-undirected-graph-2.jpg)\n\n1. $A, B, C, D$ê°€ ì„ í˜•ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤.  \n   ì—¬ê¸°ì„œëŠ” **Maximal Clique**ê°€ 3ê°œì´ë‹¤($\\{\\{A, B\\}, \\{ B, C\\}, \\{ C, D\\}\\}$). ë”°ë¼ì„œ, ì´ë¥¼ í†µí•´ì„œ Joint Probabilityë¥¼ ì¶”ì •í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n   $$\n   P(A, B, C, D) = \\frac{1}{Z} \\times \\psi_{1}(A, B) \\times \\psi_{2}(B, C) \\times \\psi_{3}(C, D)\n   $$  \n   ì—¬ê¸°ì„œ ì§ì ‘ì ìœ¼ë¡œ í•œ ë²ˆ $P(A, B, C, D)$ë¥¼ ì¶”ì •í•´ë³´ì.  \n   $$\n   \\begin{align*}\n   P(A, B, C, D) &= P(A| B, C, D) \\times P(B | C, D) \\times P(C, D) \\\\\n   &= P(A|B) \\times P(B|C) \\times P(C, D)\n   \\end{align*}\n   $$  \n   ì¦‰, ì´ë ‡ê²Œ ì¼ë ¬ë¡œ ëœ Graphì—ì„œëŠ” ë§ˆì§€ë§‰ $\\psi$ë¥¼ ì œì™¸í•˜ê³ ëŠ” ëª¨ë‘ Conditional Probabilityì´ê³ , ë§ˆì§€ë§‰ $\\psi$ëŠ” Joint Probabilityì´ë‹¤. ê·¸ë¦¬ê³ , $Z$ëŠ” 1ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n2. $A, B, C, D$ê°€ ëª¨ë‘ ì™„ë²½í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆë‹¤.  \n   ì´ ê²½ìš°ì—ëŠ”  **Maximal Clique**ê°€ 1ê°œì´ë‹¤($\\{\\{A, B, C, D\\}\\}$). ë”°ë¼ì„œ, Joint Probabilityë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.  \n   $$\n   P(A, B, C, D) = \\frac{1}{Z} \\times \\psi(A, B, C, D)\n   $$  \n   ê²°ë¡ ì ìœ¼ë¡œ Cliqueê°€ í•˜ë‚˜ê¸° ë•Œë¬¸ì— ì¤„ì¼ ìˆ˜ ìˆëŠ” ë³€ìˆ˜ê°€ ì—†ë‹¤. ì¦‰, $\\psi$ê°€ Joint Probabilityì´ê³ , $Z$ëŠ” 1ì´ë‹¤.\n3. **Maximal Clique**ê°€ 2ê°œì´ë‹¤($\\{\\{A, B, D\\}\\, \\{A, C, D\\}\\}$). ë”°ë¼ì„œ, ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬ëœë‹¤.  \n   $$\n   \\begin{align*}\n   P(A, B, C, D) &= P(B|A, C, D) \\times P(A, C, D) \\\\\n   &= P(B|A,D) \\times P(A, C, D) \\\\\n   &= \\frac{1}{Z} \\times \\psi_{1}(A, B, D) \\times \\psi_{2}(A, C, D) \\\\\n   \\end{align*}\n   $$\n4. **Maximal Clique**ê°€ 4ê°œì´ë‹¤($\\{\\{A, B\\}, \\{ A, C\\}, \\{ B, D\\}, \\{ C, D\\}\\ \\}$).  \n   $$\n   \\begin{align*}\n   P(A, B, C, D) &= P(A|B, C, D) \\times P(B|C, D) \\times P(C, D) \\\\\n   &= P(A|B, C) \\times P(B|D) \\times P(C, D) \\\\\n   &= \\frac{P(A, B, C)}{P(B, C)} \\times P(B|D) \\times P(C, D) \\\\\n   &= \\frac{P(B, C| A)}{P(A)P(B, C)} \\times P(B|D) \\times P(C, D) \\\\\n   &= \\frac{P(B|A)P(C|A)}{P(A)P(B, C)} \\times P(B|D) \\times P(C, D) \\\\\n   &= \\frac{1}{P(B,C)} \\times P(A, B) \\times P(C|A) \\times P(B|D) \\times P(C, D) \\\\\n   &\\neq \\frac{1}{Z} \\times \\psi_{1}(A, B) \\times \\psi_{2}(A, C) \\times \\psi_{3}(B, D) \\times \\psi_{4}(C, D) \\\\\n   \\end{align*}\n   $$  \n   ì´ê²ƒì´ ë°”ë¡œ $\\psi$ë¥¼ í™•ë¥  í•¨ìˆ˜ë¼ê³  ë¶€ë¥´ì§€ ì•ŠëŠ” ì´ìœ ì´ë‹¤.  \n   ìš°ë¦¬ê°€ $\\psi$ë¥¼ í™•ë¥  í•¨ìˆ˜ í˜•íƒœë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œëŠ” **Chordal graph**(4ê°œ ì´ìƒì˜ Nodeë¡œ ì´ë£¨ì–´ì§„ Cycleì—ì„œëŠ” ì¤‘ê°„ì— ë°˜ë“œì‹œ Cycleì„ ì´ë£¨ëŠ” Edgeê°€ ì•„ë‹Œ Edgeê°€ ì¡´ì¬í•˜ëŠ” Graph) í˜•íƒœë¥¼ ê°€ì ¸ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. $\\psi$ê°€ í™•ë¥  í•¨ìˆ˜ë¡œ í‘œí˜„ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ìš°ë¦¬ê°€ í•˜ê³ ì í•˜ëŠ” ì¼ì— ì˜í–¥ì„ ì£¼ì§€ëŠ” ì•Šìœ¼ë‹ˆ ê·¸ëŸ°ê°€ë³´ë‹¤ í•˜ê³  ë„˜ì–´ê°€ë„ ë¬´ë°©í•˜ë‹¤.\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” **factorization**ì´ë¼ëŠ” ê°œë…ì„ ìµí˜”ê³ , ì´ê²ƒì´ ê°€ëŠ¥í•˜ê¸° ìœ„í•´ì„œëŠ” Chordal graphê°€ ì£¼ì–´ì§„ ìƒí™©ì—ì„œ Markov Propertyë¥¼ ë§Œì¡±í•´ì•¼ í•¨ì„ í™•ì¸í–ˆë‹¤. ê·¸ë¦¬ê³ , ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ **factorization** í˜•íƒœë¥¼ ì¢€ ë” ëª…í™•í•˜ê²Œ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ í‘œí˜„í•˜ê³ , ì´ë¥¼ <mark>**factor graph**</mark>ë¼ê³  ì •ì˜í•œë‹¤. ë”°ë¼ì„œ, ê° **factor**(ì¸ìˆ˜)ëŠ” **Maximal Clique** ë‹¨ìœ„ë¡œ ìƒì„±ëœë‹¤.\n\n![ml-factor-graph-1](/images/ml-factor-graph-1.jpg)\n\n### Bayesian Network(Directed Graphical Model, Causality)\n\n**Bayesian Network**ë¼ê³  ë¶ˆë¦¬ë©°, **Causality**ë¥¼ í‘œí˜„í•œ Graphì´ë‹¤. ê° NodeëŠ” Random Variableì„ ì˜ë¯¸í•˜ë©°, EdgeëŠ” Causality(ì›ì¸($C$) -> ê²°ê³¼($R$))ë¥¼ ì˜ë¯¸í•œë‹¤. ê·¸ë ‡ê¸°ì— êµ‰ì¥íˆ ëª…í™•í•˜ê²Œ í‘œí˜„ì´ ë  ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´, $P(R, C) = P(C|R)P(R)$ì„ì„ ëª…ë°±í•˜ê²Œ ë“œëŸ¬ë‚¸ë‹¤. ê·¸ë ‡ê¸°ì— ìš°ë¦¬ëŠ” í•´ë‹¹ Graphê°€ ì£¼ì–´ì§€ëŠ” ìˆœê°„ Joint Probabilityë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìœ ì¶”í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n![ml-bayesian-network](/images/ml-bayesian-network.jpg)\n\nì¦‰, ì´ê²ƒì„ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\nP(\\cap_{i=1}^{N}X_{i}) = \\prod_{i \\in \\{1, 2, \\cdots, N\\}} P(X_{i}| \\cap_{j \\in \\text{Parents}(X_{i})} X_{j})\n$$\n\nì´ëŸ¬í•œ ì  ë•Œë¬¸ì— Bayesian Networkì—ì„œëŠ” Cycleì´ ì¡´ì¬í•  ìˆ˜ ì—†ë‹¤. ì™œëƒí•˜ë©´, Cycleì´ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì€ ê° Random Varaibleì´ ì„œë¡œ ì›ì¸ê³¼ ê²°ê³¼ê°€ ë˜ëŠ” ê²ƒì´ ë•Œë¬¸ì— ì‚¬ì‹¤ìƒ í•˜ë‚˜ì˜ ì‚¬ê±´ì´ë¼ëŠ” ì˜ë¯¸ë¥¼ ë‚´í¬í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê¸°ì— ì´ëŠ” ì‚¬ì‹¤ìƒ ì¡´ì¬í•  ìˆ˜ ì—†ë‹¤.\n\nì—¬ê¸°ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ Conditional Independenceë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼ Marginal Independenceì— ëŒ€í•œ íŒíŠ¸ë„ ì–»ì„ ìˆ˜ ìˆë‹¤. ì´ë•Œ ìš°ë¦¬ëŠ” <mark>**D-Seperation**</mark>ì´ë¼ëŠ” ë°©ë²•ì„ í™œìš©í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ìì‹ ê³¼ ì£¼ë³€ 2ê°œì˜ Nodeê°€ ì´ë£° ìˆ˜ ìˆëŠ” ê´€ê³„ 3ê°€ì§€ë¥¼ ì •ì˜í•´ì•¼ í•œë‹¤.\n\n![ml-bayesian-network-2](/images/ml-bayesian-network-2.jpg)\n\n1. **head-to-tail**  \n   ì´ ê²½ìš°ì—ëŠ” $X \\rightarrow Y \\rightarrow Z$ì˜ ê´€ê³„ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ, $X$ì™€ $Z$ëŠ” $Y$ê°€ ì£¼ì–´ì§ˆ ë•Œ, ì„œë¡œ Independentí•˜ë‹¤.  \n   $$\n   \\begin{align*}\n   P(X, Z | Y) &= \\frac{P(X,Y,Z)}{P(Y)}\\\\\n   &= \\frac{P(X)P(Y|X)P(Z|Y)}{P(Y)}\\\\\n   &= \\frac{P(X, Y)}{P(Y)} \\times P(Z|Y) \\\\\n   &= P(X | Y)P(Z | Y)\n   \\end{align*}\n   $$\n2. **tail-to-tail**  \n   ì´ ê²½ìš°ì—ëŠ” $X \\leftarrow Y \\rightarrow Z$ì˜ ê´€ê³„ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ, $X$ì™€ $Z$ëŠ” $Y$ê°€ ì£¼ì–´ì§ˆ ë•Œ, ì„œë¡œ Independentí•˜ë‹¤.  \n   $$\n   \\begin{align*}\n   P(X, Z | Y) &= \\frac{P(X, Y, Z)}{P(Y)} \\\\\n   &= \\frac{P(X|Y)P(Y)P(Z|Y)}{P(Y)} \\\\\n   &= P(X | Y)P(Z | Y)\n   \\end{align*}\n   $$\n3. **head-to-head**  \n   ì´ ê²½ìš°ì—ëŠ” $X \\rightarrow Y \\leftarrow Z$ì˜ ê´€ê³„ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ, $X$ì™€ $Z$ëŠ” ì„œë¡œ Independentí•˜ë‹¤.  \n   ì¦‰, Conditional Independenceê°€ ì•„ë‹ˆë¼ Marginal Independenceì´ë‹¤.  \n   $$\n   \\begin{align*}\n   P(X, Z) &= \\sum_{Y} P(X, Y, Z) \\\\\n   &= \\sum_{Y} P(X)P(Y|X,Z)P(Z) \\\\\n   &= P(X)P(Z)\\sum_{Y} P(Y|X,Z) \\\\\n   &= P(X)P(Z)\n   \\end{align*}\n   $$\n\nì´ ê´€ê³„ì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ $X,Z$ê°„ edgeê°€ ì¡´ì¬í•´ì„œëŠ” ì•ˆëœë‹¤ëŠ” ì ì´ë‹¤. ìœ„ì˜ ê´€ê³„ë¥¼ í™œìš©í•˜ë©´, ì¸ì ‘í•œ ê´€ê³„ì—ì„œì˜ Conditional IndependenceëŠ” íŒë³„ì´ ê°€ëŠ¥í•˜ë‹¤.í•˜ì§€ë§Œ, <mark>**D-Seperation**</mark>ì„ í†µí•´ì„œ ì´ë¥¼ ë” ë„“ì€ ë²”ìœ„ë¡œ í™•ì¥í•  ìˆ˜ ìˆë‹¤. ì„¸ Nodeì˜ ì§‘í•© $A, B, C$ê°€ ì£¼ì–´ì§ˆ ë•Œ, $A \\perp\\!\\!\\!\\!\\perp B | C$ì´ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤.\n\n1. Aì—ì„œ Bë¡œ ê°€ëŠ” ê²½ë¡œê°€ í•˜ë‚˜ ì´ìƒ ì¡´ì¬í•œë‹¤.(ì—¬ê¸°ì„œ ê²½ë¡œëŠ” ë°©í–¥ì„ ì‹ ê²½ì“°ì§€ ì•Šê³  ì—°ê²° ì—¬ë¶€ì— ë”°ë¼ ê²°ì •í•œë‹¤.)\n2. ëª¨ë“  ê²½ë¡œì— ëŒ€í•´ì„œ, Cì— ì†í•˜ëŠ” Nodeê°€ ì ì–´ë„ í•˜ë‚˜ head-to-tail ë˜ëŠ” tail-to-tail ê´€ê³„ë¥¼ ì¤‘ê³„í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.\n3. ëª¨ë“  ê²½ë¡œì— ëŒ€í•´ì„œ, Cì— ì†í•˜ëŠ” NodeëŠ” head-to-head ê´€ê³„ë¥¼ ì¤‘ê³„í•˜ë©´ ì•ˆë˜ë©°, head-to-head ê´€ê³„ë¥¼ ì¤‘ê³„í•˜ëŠ” Nodeì˜ ìì†ì´ì—¬ë„ ì•ˆëœë‹¤.\n\nì¦‰, $A, B, C$ê°€ ì´ëŸ¬í•œ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•  ë•Œ, ìš°ë¦¬ëŠ” $C$ê°€ $A, B$ë¥¼ Blockí–ˆë‹¤ê³  í•˜ë©°, $A \\perp\\!\\!\\!\\!\\perp B | C$ì´ë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì™€ ê°™ì€ ë‘ ê²½ìš°ë¥¼ ì˜ˆë¥¼ ë“¤ì–´ë³¼ ìˆ˜ ìˆë‹¤.\n\n![ml-d-seperation](/images/ml-d-seperation.jpg)\n\nì™¼ìª½ì˜ ê²½ìš° Cì˜ parentê°€ Aì—ì„œ Bë¡œ ê°€ëŠ” ê²½ë¡œì—ì„œ head-to-headë¥¼ ì¤‘ê³„í•˜ê³  ìˆë‹¤. ë”°ë¼ì„œ, Aì™€ BëŠ” Conditionally Independenceë¥¼ ë§Œì¡±í•˜ì§€ ì•ŠëŠ”ë‹¤. ë°˜ë©´, ì˜¤ë¥¸ìª½ì˜ ê²½ìš° Cê°€ Aì—ì„œ Bë¡œ ê°€ëŠ” ê²½ë¡œì—ì„œ head-to-tail ê´€ê³„ë¥¼ ì¤‘ê³„í•˜ê³  ìˆìœ¼ë¯€ë¡œ, Aì™€ BëŠ” Conditionally Independenceë¥¼ ë§Œì¡±í•œë‹¤. ì—¬ê¸°ì„œ ì¬ë°ŒëŠ” ì ì€ Aì™€ BëŠ” ë‘ ê²½ìš° ëª¨ë‘ Marginal Independenceë¥¼ ë§Œì¡±í•œë‹¤ëŠ” ì ì´ë‹¤. ì™œëƒí•˜ë©´, Aì—ì„œ Bë¡œ ê°€ëŠ” ê²½ë¡œê°€ ìˆœë°©í–¥ë§Œìœ¼ë¡œëŠ” ì´ë£¨ì–´ì§€ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.\n\në§ˆì§€ë§‰ìœ¼ë¡œ, Bayesian Networkë„ **factorization**ì´ ê°€ëŠ¥í•˜ë‹¤ A, Bì˜ **Causality**ê°€ $P(A|B)P(B)$ë¥¼ ì˜ë¯¸í•œë‹¤ëŠ” ì ì„ í™œìš©í•´ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ ì •ì˜í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.\nì¦‰, ì´ˆê¸° ì‹œì‘ ì ì€ ìì‹ ë§Œì„ ê°€ì§€ëŠ” factorë¥¼ ê°€ì§€ê³ , head-to-head ê´€ê³„ëŠ” í•˜ë‚˜ë¡œ í†µì¼í•˜ë©°, ë‚˜ë¨¸ì§€ ê´€ê³„(head-to-head, ë“±)ëŠ” ë³„ë„ë¡œ factorë¥¼ ë¶„ë¦¬í•œë‹¤. ì¦‰, ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ê°€ì§„ë‹¤.\n\n![ml-factor-graph-2](/images/ml-factor-graph-2.jpg)\n\n```plaintext\n ğŸ¤” Markov Blankets\n\n Markov Blanketì€ íŠ¹ì • Nodeì— ëŒ€í•œ ì •ë³´(ê´€ê³„)ê°€ ìˆëŠ” ëª¨ë“  Nodeë¥¼ ì˜ë¯¸í•œë‹¤. \n ì¦‰, íŠ¹ì • Random Variableì˜ í™•ë¥ ì´ ê¶ê¸ˆí•˜ë‹¤ë©´, ì´ Markov Blanketë§Œ ê°€ì§€ë©´ ëœë‹¤. \n ê·¸ ì¤‘ì—ì„œë„ ê°€ì¥ ì‘ì€ í¬ê¸°ë¡œ ëª¨ë“  í•„ìš”í•œ ì •ë³´ë¥¼ ë‹´ì€ subsetì„ Markov Boundaryë¼ê³  í•œë‹¤. \n Markov BoundaryëŠ” Markov Random Fieldì—ì„œëŠ” Neighborì´ê³ ,\n Bayesian Networkì—ì„œëŠ” Parent, Child, Co-Parentì´ë‹¤.\n```\n\n![ml-markov-boundary](/images/ml-markov-boundary.jpg)\n\n### Factor Graph\n\nì• ì„œ ë³¸ ë‘ ê°€ì§€ Graph í‘œí˜„ ë°©ë²•ì€ ê° ê° ì¥ë‹¨ì ì„ ê°€ì§€ê³  ìˆë‹¤.\n\n1. Markov Random FieldëŠ” Joint Probabilityë¥¼ Potentialì´ë¼ëŠ” ì„ì˜ì˜ ë³€ìˆ˜ë¥¼ í†µí•´ì„œ ì¶”ì •í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ, ëª…í™•ì„±ì´ ë–¨ì´ì§€ì§€ë§Œ, Conditional Independenceë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì€ ë” ë¶„ëª…í•˜ê³  ì‰½ë‹¤.\n2. Bayesian NetworkëŠ” Joint Probabilityë¥¼ ëª…í™•í•˜ê²Œ íŒë³„í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, Conditional Independenceë¥¼ íŒë³„í•˜ëŠ” ê²ƒì´ ë” ì–´ë µê³  ë³µì¡í•˜ë‹¤.\n\nì´ëŸ¬í•œ ì¥ë‹¨ì ì„ ëª¨ë‘ ì‚´ë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•ìœ¼ë¡œ ì œì‹œëœ ê²ƒì´ Factor Graphì´ë‹¤. ìœ„ì—ì„œ ê° ê° Factor Graphë¥¼ í‘œí˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œëŠ” ì œì‹œí•˜ì˜€ìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤. Factor GraphëŠ” ê·¼ë³¸ì ìœ¼ë¡œ Graphì˜ ìš”ì†Œë“¤ì„ ì¸ìˆ˜ë¶„í•´(Factorization)í•˜ì—¬ ì¸ìˆ˜(Factor)ë¡œ ë¶„ë¦¬í•´ë‚¸ ê²ƒì´ë‹¤. ê·¸ë ‡ê¸°ì— ë” ëª…í™•í•œ êµ¬ë¶„ì´ ê°€ëŠ¥í•˜ë‹¤. ê° NodeëŠ” Factorì™€ ê¸°ì¡´ Nodeì— í•´ë‹¹í•˜ëŠ” ê°’ì´ ë‘ ê°œ ë‹¤ ì¡´ì¬í•˜ê³ , FactorëŠ” ê½‰ ì°¬ ë„¤ëª¨, ê¸°ì¡´ Node(Variable)ëŠ” ë¹„ì–´ìˆëŠ” ë™ê·¸ë¼ë¯¸ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\n\nê·¸ë¦¬ê³ , ì—¬ê¸°ì„œëŠ” Joint Probabilityë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤.\n\nVariable NodeëŠ” $\\{X_{1},X_{2}, \\cdots, X_{N}\\}$ì´ê³ , Factor Nodeê°€ $\\{f_{1},f_{2}, \\cdots, f_{M}\\}$ì¼ ë•Œ, $f_{j}$ì™€ ì´ì›ƒí•œ Variable Nodeì˜ ì§‘í•©ì„ $\\mathcal{N}_{j}$ë¼ê³  í•˜ì.\n\n$$\nP(X_{1}, X_{2}, \\cdots, X_{N}) = \\prod_{j=1}^{M}{f_{j}(\\cap_{X \\in \\mathcal{N}_{j}} X)}\n$$\n\nì´ë ‡ê²Œ í‘œí˜„í•˜ëŠ” ê²ƒì€ í™•ì‹¤íˆ Markov Random Fieldì—ì„œëŠ” ëª…í™•í•˜ë‹¤. í•˜ì§€ë§Œ, Bayesian Networkì—ì„œëŠ” í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì–´ëŠì •ë„ ìƒì—ˆë‹¤ê³  ë³¼ ìˆ˜ë„ ìˆë‹¤. ì–´ì°¨í”¼ Conditional Probabilityì¸ë°, ë‹¤ë¥´ê²Œ í‘œí˜„í•œ ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ, ì´ë¥¼ ì´ìš©í•˜ê²Œ ë˜ë©´ ê¸°ì¡´ì— ë¬¸ì œì˜€ë˜ Conditional Independenceë¥¼ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´ Factor Graphì—ì„œëŠ” Conditional Independenceë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ í•´ë‹¹ ì§‘í•©ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ëª¨ë“  ê²½ë¡œì—ì„œ ì¤‘ê°„ì— í•˜ë‚˜ë¼ë„ Variable Nodeê°€ ê»´ìˆëŠ”ì§€ë§Œ í™•ì¸í•´ë„ ì¶©ë¶„í•˜ë‹¤.\n\n![ml-factor-graph-3](/images/ml-factor-graph-3.jpg)\n\në”°ë¼ì„œ, ì•ìœ¼ë¡œì˜ ê³¼ì •ì—ì„œëŠ” Factor Graphë¥¼ Mainìœ¼ë¡œ í•˜ì—¬ ì„¤ëª…ì„ ì§„í–‰í•˜ë„ë¡ í•˜ê² ë‹¤.\n\n## Message Passing\n\nìš°ë¦¬ëŠ” ì•ì˜ Graph í‘œí˜„ì„ í†µí•´ì„œ Featureë¥¼ Factorë¡œ ì••ì¶•í•˜ëŠ” ê³¼ì •ì„ ìµí˜”ë‹¤. ì´ ì—­ì‹œ ì—„ì²­ë‚œ ê³„ì‚° íš¨ìœ¨ì„ ê°€ì ¸ì˜¨ë‹¤. í•˜ì§€ë§Œ, ì´ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‹¤. ê·¸ê²ƒì€ Message Passing ë°©ë²•ì´ë‹¤. ìš°ì„  ìš°ë¦¬ê°€ í•´ê²°í•˜ê³ ìí•˜ëŠ” ë¬¸ì œë¥¼ ì •ì˜í•´ë³´ì. ìš°ë¦¬ëŠ” Joint Probability($P(X_{1}, X_{2}, \\cdots, X_{N})$)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¤ìŒ ê°’ì„ êµ¬í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆë‹¤.\n\n1. <mark>**Marginalization**</mark>  \n   Marginal ProbabilityëŠ” Joint Probabilityì—ì„œ êµ¬í•˜ê³ ì í•˜ëŠ” Random Variableì„ ì œì™¸í•œ ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ ë”í•œ ê²ƒì´ë‹¤.\n   $$\n   \\begin{align*}\n   P(X_{i}) &= \\sum_{X_{j}}P(X_{i}, X_{j}) \\\\\n   &= \\sum_{X_{j}, X_{k}}P(X_{i}, X_{j}, X_{k}) \\\\\n   &= \\cdots \\\\\n   &= \\sum_{X_{-i}}P(X_{1}, X_{2}, \\cdots, X_{N})\n   \\end{align*}\n   $$  \n   ì¦‰, ì´ë¥¼ ì¼ë°˜ì ì¸ ë°©ë²•ìœ¼ë¡œ í’€ê³ ìí•˜ë©´ Random Variable($X_{i}$)ì´ ê° ê° $\\mathbb{R}^{L}$ë¡œ ì •ì˜ëœë‹¤ê³  í•  ë•Œ, $L^{N-1}$ë²ˆì˜ í•©ì—°ì‚°ì´ í•„ìš”í•˜ë‹¤.\n2. <mark>**Maximization**</mark>  \n   Joint Probabilityì˜ ìµœëŒ“ê°’ì„ ê°–ê²Œ í•˜ëŠ” ê²½ìš°ì˜ ìˆ˜($\\hat{X}$)ë¥¼ êµ¬í•˜ê³ ì í•œë‹¤ë©´ ë‹¤ìŒì„ êµ¬í•´ì•¼ í•œë‹¤.  \n   $$\n   \\hat{X} = \\argmax_{X_{1}, X_{2}, \\cdots, X_{N}} P(X_{1}, X_{2}, \\cdots, X_{N})\n   $$  \n   ì´ ë˜í•œ ë¬´ì‹í•˜ê²Œ í’€ê³ ìí•˜ë©´, $L^{N-1}$ë²ˆì˜ max ì—°ì‚°ì´ í•„ìš”í•˜ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ì´ë¥¼ í•œ ë²ˆ ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœì¸ ì¼ìí˜• Factor Graphë¡œ í‘œí˜„í•´ë³´ì.\n\n![ml-bp-1](/images/ml-bp-1.jpg)\n\nì—¬ê¸°ì„œ $P(X_{2})$ë¥¼ ì•Œê³  ì‹¶ë‹¤ê³  í•´ë³´ì. ê·¸ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ ì‹ì´ ì •ë¦¬ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\nP(X_{2}) &= \\sum_{X_{1}}P(X_{1},X_{2}) \\\\\n&= \\sum_{X_{1}, X_{3}, X_{4}, X_{5}}P(X_{1}, X_{2}, X_{3}, X_{4}, X_{5}) \\\\\n&= \\sum_{X_{1}}\\sum_{X_{3}}\\sum_{X_{4}}\\sum_{X_{5}}P(X_{1}, X_{2}, X_{3}, X_{4}, X_{5}) \\\\\n&= \\sum_{X_{1}}\\sum_{X_{3}}\\sum_{X_{4}}\\sum_{X_{5}}f_{a}(X_{1}, X_{2})f_{b}(X_{2}, X_{3})f_{c}(X_{3}, X_{4})f_{d}(X_{4}, X_{5}) \\\\\n&= (\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{3}}\\sum_{X_{4}}\\sum_{X_{5}}f_{b}(X_{2}, X_{3})f_{c}(X_{3}, X_{4})f_{d}(X_{4}, X_{5})) \\\\\n&= (\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{3}}\\sum_{X_{4}}f_{b}(X_{2}, X_{3})f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\\\\n&= (\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{3}}f_{b}(X_{2}, X_{3})\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5}))\n\\end{align*}\n$$\n\nì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ” ë¬´ì—‡ì¼ê¹Œ? ì´ëŠ” ë‹¨ìˆœí•˜ê²Œ ìˆœì„œë¥¼ ë°”ê¾¸ì–´ ì¬ì¡°í•©í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ Computingì„ ì¤„ì¼ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤¬ë‹¤. ë¨¼ì €, ì•ì˜ $\\sum$ì—°ì‚°ë§Œ ë‹¨ë…ìœ¼ë¡œ í•  ë•Œ, $L$ë²ˆì˜ ì—°ì‚°ì´ í•„ìš”í•˜ê³ , ë’¤ì— ì—°ì†í•´ì„œ ë‚˜ì˜¤ëŠ” 3ë²ˆì˜ $\\sum$ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ê²°êµ­ $L^{3}$ì˜ ì—°ì‚°ì´ í•„ìš”í•˜ë‹¤. ì¦‰, $L + L^{3}$ì˜ í•©ì—°ì‚°ìœ¼ë¡œ marginalization ê²°ê³¼ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê¸°ì— ë” íš¨ìœ¨ì ì¸ ì—°ì‚°ì´ ê°€ëŠ¥í•œ ê²ƒì´ë‹¤. ì´ëŠ” íŠ¹íˆ Graphì˜ ì¤‘ì•™ì— ìˆëŠ” ê°’ì„ êµ¬í•  ë•Œ ë” ë„ë“œë¼ì§€ê²Œ ë‚˜íƒ€ë‚œë‹¤. ì „ì²´ marginalization ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\nP(X_{1}) &= \\sum_{X_{2}}f_{a}(X_{1}, X_{2})\\sum_{X_{3}}f_{b}(X_{2}, X_{3})\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5}) \\rightarrow L^{4} \\\\\nP(X_{2}) &= (\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{3}}f_{b}(X_{2}, X_{3})\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\rightarrow L^{3} + L \\\\\nP(X_{3}) &= (\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\rightarrow 2L^{2} \\\\\nP(X_{4}) &= (\\sum_{X_{3}}f_{c}(X_{3}, X_{4})\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))(\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\rightarrow L^{3} + L \\\\\nP(X_{5}) &= \\sum_{X_{4}}f_{d}(X_{4}, X_{5})\\sum_{X_{3}}f_{c}(X_{3}, X_{4})\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\sum_{X_{1}}f_{a}(X_{1}, X_{2}) \\rightarrow L^{4}\n\\end{align*}\n$$\n\nì´ê²ƒì´ ëì´ ì•„ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¤‘ë³µëœ ì—°ì‚°ì„ ë³„ë„ë¡œ ì €ì¥í•´ë‘ì–´ì„œ ë” ë¹ ë¥¸ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\nP(X_{2}) &= \\underbrace{(\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))}_{\\red{\\mu_{a\\rightarrow2}(X_{2})}}(\\sum_{X_{3}}f_{b}(X_{2}, X_{3})\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\\\\nP(X_{3}) &= \\underbrace{(\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\red{\\mu_{a\\rightarrow2}(X_{2})})}_{\\red{\\mu_{b\\rightarrow3}(X_{3})}}(\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\\\\nP(X_{4}) &= \\underbrace{(\\sum_{X_{3}}f_{c}(X_{3}, X_{4})\\red{\\mu_{b\\rightarrow3}(X_{3})})}_{\\red{\\mu_{c\\rightarrow4}(X_{4})}}(\\sum_{X_{5}}f_{d}(X_{4}, X_{5})) \\\\\nP(X_{5}) &= \\underbrace{\\sum_{X_{4}}f_{d}(X_{4}, X_{5})\\red{\\mu_{c\\rightarrow4}(X_{4})}}_{\\red{\\mu_{d\\rightarrow5}(X_{5})}}\n\\end{align*}\n$$\n\n![ml-bp-2](/images/ml-bp-2.jpg)\n\nì¦‰, ì´ì „ Marginalizationì—ì„œ ê³„ì‚°í–ˆë˜ $\\mu_{\\text{factor}\\rightarrow\\text{variable}}(X_{\\text{variable}})$ë¥¼ ì €ì¥í•´ì„œ, ë‹¤ìŒ Marginalization ì—°ì‚° ì‹œì— ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì „ì²´ Marginalizationì„ êµ¬í•˜ëŠ”ë°ì—ë„ ë” ë¹ ë¥¸ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ ë°©ì‹ì€ ì—­ìœ¼ë¡œ ì§„í–‰í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•œë° ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\nP(X_{4}) &= (\\sum_{X_{3}}f_{c}(X_{3}, X_{4})\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))\\underbrace{(\\sum_{X_{5}}f_{d}(X_{4}, X_{5}))}_{\\blue{\\mu_{d\\rightarrow4}(X_{4})}} \\\\\nP(X_{3}) &= (\\sum_{X_{2}}f_{b}(X_{2}, X_{3})\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))\\underbrace{(\\sum_{X_{4}}f_{c}(X_{3}, X_{4})\\blue{\\mu_{d\\rightarrow4}(X_{4})})}_{\\blue{\\mu_{c\\rightarrow3}(X_{3})}} \\\\\nP(X_{2}) &= (\\sum_{X_{1}}f_{a}(X_{1}, X_{2}))\\underbrace{(\\sum_{X_{3}}f_{b}(X_{2}, X_{3})\\blue{\\mu_{c\\rightarrow3}(X_{3})})}_{\\blue{\\mu_{b\\rightarrow2}(X_{2})}} \\\\\nP(X_{1}) &= \\underbrace{\\sum_{X_{2}}f_{a}(X_{1}, X_{2})\\blue{\\mu_{b\\rightarrow2}(X_{2})}}_{\\blue{\\mu_{a\\rightarrow1}(X_{1})}}\n\\end{align*}\n$$\n\n![ml-bp-3](/images/ml-bp-3.jpg)\n\nì´ë¥¼ í•©ì³ì„œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ê°€ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\nP(X_{1}) &= \\blue{\\mu_{a\\rightarrow1}(X_{1})} &= \\red{\\mu^{-}(X_{1})}\\blue{\\mu^{+}(X_{1})} \\\\\nP(X_{2}) &= \\red{\\mu_{a\\rightarrow2}(X_{2})}\\blue{\\mu_{b\\rightarrow2}(X_{2})}&= \\red{\\mu^{-}(X_{2})}\\blue{\\mu^{+}(X_{2})} \\\\\nP(X_{3}) &= \\red{\\mu_{b\\rightarrow3}(X_{3})}\\blue{\\mu_{c\\rightarrow3}(X_{3})}&= \\red{\\mu^{-}(X_{3})}\\blue{\\mu^{+}(X_{3})} \\\\\nP(X_{4}) &= \\red{\\mu_{c\\rightarrow4}(X_{4})}\\blue{\\mu_{d\\rightarrow4}(X_{4})}&= \\red{\\mu^{-}(X_{4})}\\blue{\\mu^{+}(X_{4})} \\\\\nP(X_{5}) &= \\red{\\mu_{d\\rightarrow5}(X_{5})}&= \\red{\\mu^{-}(X_{5})}\\blue{\\mu^{+}(X_{5})} \\\\\n&\\therefore P(X_{i}) = \\red{\\mu^{-}(X_{i})}\\blue{\\mu^{+}(X_{i})} \\\\\n\\end{align*}\n$$\n\n![ml-bp-4](/images/ml-bp-4.jpg)\n\n$\\mu^{+}$ì™€ $\\mu^{-}$ì˜ ë°©í–¥ì´ í—·ê°ˆë¦´ ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ìì‹ ($X_{i}$)ì„ ê¸°ì¤€ìœ¼ë¡œ í° ìª½ì—ì„œ ì™”ëŠ”ì§€ ì‘ì€ ìª½ì—ì„œ ì™”ëŠ”ì§€ë¥¼ í‘œì‹œí•œë‹¤ê³  ìƒê°í•˜ë©´ ì‰½ë‹¤. ë”°ë¼ì„œ, $\\mu$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë˜ì–´ì§ˆ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\mu^{-}(X_{1}) &= 1,\\, \\mu^{+}(X_{N}) = 1 \\text{ì´ê³ ,}\\\\\n\\mu^{-}(X_{i}) &= \\sum_{X_{i-1}}f_{i}(i-1, i)\\mu^{-}(X_{i-1}) \\\\\n\\mu^{+}(X_{i}) &= \\sum_{X_{i+1}}f_{i}(i, i+1)\\mu^{+}(X_{i+1}) \\\\\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ $\\mu$ê°€ ë°”ë¡œ <mark>**Message**</mark>ë¥¼ ì˜ë¯¸í•œë‹¤. ì¦‰, ìš°ë¦¬ê°€ ë§ˆì¹˜ ìš´ë™ì¥ì—ì„œ ì‚¬ëŒ ìˆ˜ë¥¼ ì„¸ê¸° ìœ„í•´ì„œ ì• ì‚¬ëŒì´ ë§í•œ ìˆ˜ + 1ì„ ë°˜ë³µí•˜ë©´ì„œ ì§„í–‰í•˜ëŠ” ê²ƒì²˜ëŸ¼ Messageë¥¼ ì „ë‹¬í•˜ë©° ì „ì²´ í™•ë¥ ì„ êµ¬í•´ë‚˜ê°€ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì„ **Message Passing**ì´ë¼ê³  í•˜ë©°, ì´ ë°©ë²•ì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” Marginal Probabilityë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´, $\\mu^{-}(X_{i})$ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ì—°ì‚°ëŸ‰ì´ $(i-1) \\times L$ì´ë¼ëŠ” ê²ƒê³¼, $mu^{+}(X_{i})$ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ì—°ì‚°ëŸ‰ì´ $(N-i) \\times L$ì´ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤. ë”°ë¼ì„œ, ê° ê°ì˜ Marginalizationì„ êµ¬í•˜ê¸° ìœ„í•œ ì—°ì‚°ëŸ‰ì´ $L^{N-1}$ì—ì„œ $(N-1)L$ë¡œ ì¤„ì–´ë“¤ì—ˆë‹¤.\n\nì—¬ê¸°ê¹Œì§€ ìš°ë¦¬ëŠ” Lineìœ¼ë¡œ ë˜ì–´ìˆëŠ” ê°€ì¥ ê°„ë‹¨í•œ Factor Graphì—ì„œì˜ <mark>**Sum-Product Belief Propagation**</mark>ì„ ì•Œì•„ë³¸ ê²ƒì´ë‹¤. ì´ì œë¶€í„° ìš°ë¦¬ëŠ” ë” ë³µì¡í•œ ìƒí™©ì—ì„œì˜ Belief Propagation(BP)ì„ ì‚´í´ë³¼ ê²ƒì´ë‹¤. Belief Propagationê³¼ Message Passingì€ ëŒ€ê²Œ ë¹„ìŠ·í•œ ì˜ë¯¸ë¡œ ì‚¬ìš©ë˜ì–´ ì§„ë‹¤(ì¼ë¶€ëŠ” Message Passing í›„ì— ë°ì´í„°ë¥¼ ê°€ê³µí•˜ëŠ” ì‘ì—…ì„ ë¶„ë¦¬í•˜ê³  ì´ë¥¼ í†µí•©í•˜ì—¬ Belief Propagationì´ë¼ê³  í•˜ê¸°ë„ í•œë‹¤.)\n\n### Sum-Product Belief Propagation\n\ní•©ì˜ ê³±ì„ í†µí•´ì„œ Marginal Probabilityë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ì• ì„œ ë³´ì•˜ë˜ Linear Factor Graph ë¿ë§Œ ì•„ë‹ˆë¼ Treeí˜•íƒœì˜ Factor Graphì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë¬¼ë¡  Cycleì´ ì¡´ì¬í•˜ëŠ” Factor Graphê°€ ì¡´ì¬í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì´ ê²½ìš°ì— ëŒ€í•´ì„œëŠ” íŠ¹ë³„í•œ ì•Œê³ ë¦¬ì¦˜ì„ ë³„ë„ë¡œ ì ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ë”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” Treeí˜•íƒœì˜ Factor Graphì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤.\n\nìš°ì„  ì•„ë˜ ê·¸ë¦¼ì„ í†µí•´ì„œ ëŒ€ëµì ì¸ ì´í•´ë¥¼ í•´ë³´ë„ë¡ í•˜ì.\n\n![ml-sum-product-bp-1](/images/ml-sum-product-bp-1.jpg)\n\nìš°ë¦¬ëŠ” ìœ„ì—ì„œ Line Factor Graphì—ì„œ ì–´ë–»ê²Œ Marginal Probabilityë¥¼ ì–´ë–»ê²Œ êµ¬í•˜ëŠ”ì§€ë¥¼ ë³´ì•˜ë‹¤. Tree êµ¬ì¡°ì—ì„œë„ ë™ì¼í•˜ê²Œ ê²°êµ­ Marginal Probabilityë¥¼ ìì‹ ê³¼ ì´ì›ƒí•œ Factor Nodeë“¤ë¡œ ë¶€í„° ì „ë‹¬ëœ Messageì˜ ê³±ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë‹¨ì§€ ë‹¤ë¥¸ ì ì€ ì´ì›ƒí•œ factorê°€ ë³µìˆ˜ ê°œë¼ëŠ” ê²ƒì´ë‹¤.  \n(factor ë˜ëŠ” variableì— í•´ë‹¹í•˜ëŠ” Node ì¤‘ì—ì„œ indexê°€ iì¸ Nodeì™€ ì¸ì ‘í•œ Node(Node iê°€ factorë¼ë©´ variable, variableì´ë¼ë©´ factorì´ë‹¤.)ë“¤ì˜ index ì§‘í•©ì„ $\\mathcal{N}_{i}$ ë¼ê³ í•˜ê³ , ê°’ì€ ì¢…ë¥˜ì˜ Nodeì˜ indexë¥¼ ëª¨ì•„ë‘” ì§‘í•© Iê°€ ìˆì„ ë•Œ $X_{I} = \\{X_{i}\\}_{i \\in I}$ë¼ê³  í•˜ì.)\n\n$$\nP(X_{i}) = \\prod_{p \\in \\mathcal{N}_{i}}\\mu_{p \\rightarrow i}(X_{i})\n$$\n\nê·¸ë ‡ë‹¤ë©´, ì—¬ê¸°ì„œ $\\mu_{p \\rightarrow i}(X_{i})$ë¥¼ ê° ê° ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? ê·¸ëŸ¬ê¸° ìœ„í•´ì„œ ë¹¨ê°„ìƒ‰ ë¶€ë¶„ì„ ìì„¸íˆ ë´ë³´ì.\n\n![ml-sum-product-bp-2](/images/ml-sum-product-bp-2.jpg)\n\nì—¬ê¸°ì„œë„ ê¸°ì¡´ Linear Factor Graphì™€ ë‹¤ë¥¸ ì ì€ Factor Nodeì—­ì‹œ ì—¬ëŸ¬ ê°œì˜ Variable Nodeì™€ ì—°ê²°ëœë‹¤ëŠ” ì ì´ë‹¤. ì´ ë¶€ë¶„ë§Œ ë–¼ì–´ì„œ ìì„¸íˆ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n![ml-sum-product-bp-3](/images/ml-sum-product-bp-3.jpg)\n\nê·¸ë ‡ê¸°ì— ì´ì „ Variable Nodeë¡œ ë¶€í„° ì˜¤ëŠ” Messageë“¤ê³¼ factor ê°’ì„ í•¨ê»˜ ê³±í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. ì—¬ê¸°ì„œ, Varaible Nodeì—ì„œ factor Nodeë¡œ ì˜¤ëŠ” Messageë¥¼ $\\nu$ë¼ê³  ì •ì˜í•œë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì€ Factor Nodeì™€ ì´ì›ƒí•œ Variable Node ì¤‘ì—ì„œ Messageë¥¼ ì „ë‹¬í•  Variable NodeëŠ” ì—°ì‚°ì—ì„œ ì œì™¸í•´ì•¼ í•œë‹¤ëŠ” ì ì´ë‹¤.\n\n$$\n\\mu_{u \\rightarrow i}(X_{i}) = \\sum_{X_{\\mathcal{N}_{u}\\backslash\\{i\\}}}f_{u}(X_{\\mathcal{N}})\\prod_{j \\in \\mathcal{N}\\backslash\\{i\\}}{\\nu_{j \\rightarrow u}(x_{j})}\n$$\n\nê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ $\\nu$ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n![ml-sum-product-bp-4](/images/ml-sum-product-bp-4.jpg)\n\n$$\n\\nu_{j \\rightarrow u}(X_{j}) = \\prod_{v \\in \\mathcal{N}_{j}\\backslash\\{u\\}}\\mu_{v \\rightarrow j}(X_{j})\n$$\n\në”°ë¼ì„œ, Marginal Probability($P(X_{i})$)ë¥¼ êµ¬í•˜ê³ ì í•  ë•Œ ìš°ë¦¬ëŠ” Leaf Nodeì—ì„œ ë¶€í„° ì‹œì‘í•´ì„œ ì°¨ë¡€ì°¨ë¡€ ê°’ì„ êµ¬í•˜ë©´ì„œ, $\\mu_{\\mathcal{N}_{i} \\rightarrow i}$ë¥¼ ëª¨ë‘ êµ¬í•  ë•Œê¹Œì§€ ì—°ì‚°ì„ ìˆ˜í–‰í•´ì•¼ í•œë‹¤.\n\n```plaintext\n ğŸ¤” Loopy Sum-Product BP\n\n ìš°ë¦¬ëŠ” Sum-Product BPë¥¼ Treeì—ì„œë§Œ ì“¸ ìˆ˜ ìˆë‹¤ê³  ì œí•œí•˜ì˜€ì§€ë§Œ, \n ì‚¬ì‹¤ Cycleì´ ì¡´ì¬í•˜ëŠ” Factor Graphì—ì„œë„ ë™ì¼í•œ BPë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n í•˜ì§€ë§Œ, ì´ ê²½ìš°ì—ëŠ” ë¹„ë¡€ ê´€ê³„ë¥¼ í†µí•´ì„œ ë‚˜íƒ€ë‚¼ ìˆ˜ ë°–ì— ì—†ê¸° ë•Œë¬¸ì—\n ê²°ê³¼ê°’ì— ëŒ€í•´ì„œ 100% í™•ì‹ í•  ìˆ˜ëŠ” ì—†ë‹¤.\n```\n\n### Max Product Belief Propagation\n\nBelief Propagationì€ ì• ì„œ ì‚´í´ë³´ì•˜ë˜ Marginal Probabilityë¥¼ êµ¬í•  ë•Œì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ, Maximization ë¬¸ì œë¥¼ í’€ ë•Œì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ê°€ì¥ ê°„ë‹¨í•œ ì˜ˆì‹œì¸ Linear Factor Graphë¥¼ ê°€ì •í•´ë³´ì.\n\n![ml-bp-1](/images/ml-bp-1.jpg)\n\n$$\nP(X_{1}, X_{2}, X_{3}, X_{4}, X_{5}) = f_{a}(X_{1}, X_{2})f_{b}(X_{2}, X_{3})f_{c}(X_{3}, X_{4})f_{d}(X_{4}, X_{5})\n$$\n\nì´ ê²½ìš°ì— $P(X_{1}, X_{2}, X_{3}, X_{4}, X_{5})$ë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” $\\hat{x_{1}}, \\hat{x_{2}}, \\hat{x_{3}}, \\hat{x_{4}}, \\hat{x_{5}}$ë¥¼ ì°¾ì•„ë³´ì.  \n\n$$\n\\begin{align*}\n&\\max_{X_{1}, X_{2}, X_{3}, X_{4}, X_{5}} f_{a}(X_{1}, X_{2})f_{b}(X_{2}, X_{3})f_{c}(X_{3}, X_{4})f_{d}(X_{4}, X_{5})\\\\\n&= \\max_{X_{1}}\\{f_{a}(X_{1}, X_{2})\\max_{X_{2}}\\{\\max_{X_{3}}\\{f_{b}(X_{2}, X_{3}) \\max_{X_{4}}\\{f_{c}(X_{3}, X_{4})\\max_{X_{5}}\\{f_{d}(X_{4}, X_{5})\\}\\}\\}\\}\\}\\\\\n&= \\max_{X_{2}}\\{\\max_{X_{1}}\\{f_{a}(X_{1}, X_{2})\\} \\max_{X_{3}}\\{f_{b}(X_{2}, X_{3}) \\max_{X_{4}}\\{f_{c}(X_{3}, X_{4})\\max_{X_{5}}\\{f_{d}(X_{4}, X_{5})\\}\\}\\}\\}\\\\\n&= \\max_{X_{3}}\\{\\max_{X_{2}}\\{f_{b}(X_{2}, X_{3})\\max_{X_{1}}\\{f_{a}(X_{1}, X_{2})\\}\\} \\max_{X_{4}}\\{f_{c}(X_{3}, X_{4})\\max_{X_{5}}\\{f_{d}(X_{4}, X_{5})\\}\\}\\}\\\\\n&= \\max_{X_{4}}\\{\\max_{X_{3}}\\{f_{c}(X_{3}, X_{4})\\max_{X_{2}}\\{f_{b}(X_{2}, X_{3})\\max_{X_{1}}\\{f_{a}(X_{1}, X_{2})\\}\\}\\}\\max_{X_{5}}\\{f_{d}(X_{4}, X_{5})\\}\\}\\\\\n&= \\max_{X_{5}}\\{f_{d}(X_{4}, X_{5})\\max_{X_{4}}\\{\\max_{X_{3}}\\{f_{c}(X_{3}, X_{4})\\max_{X_{2}}\\{f_{b}(X_{2}, X_{3})\\max_{X_{1}}\\{f_{a}(X_{1}, X_{2})\\}\\}\\}\\}\\}\\\\\n\\end{align*}\n$$\n\nMarginalizationê³¼ êµ‰ì¥íˆ ìœ ì‚¬í•˜ë‹¤ê³  í•˜ë‹¤. ì´ë¥¼ ì´ì „ì— ì‚¬ìš©í•œ Tree êµ¬ì¡°ì— ë°˜ì˜í•´ë„ ë™ì¼í•˜ë‹¤.\n\n![ml-sum-product-bp-1](/images/ml-sum-product-bp-1.jpg)\n\në‹¨, ì—¬ê¸°ì„œ Messageì™€ ìµœì¢… ê²°ê³¼ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì¬ì •ì˜í•˜ë©´ ëë‚œë‹¤.\n\n$$\n\\begin{align*}\n\\max P(X_{1}, X_{2}, \\cdots, X_{N}) &= \\max_{X_{i}}\\{\\prod_{p\\in\\mathcal{N}_{i}}\\mu_{p \\rightarrow i}(X_{i})\\} \\\\\n\\mu_{u \\rightarrow i}(X_{i}) &= \\max_{\\mathcal{N}_{u}\\backslash\\{i\\}}\\{f_{u}(\\mathcal{N}_{i})\\prod_{j \\in \\mathcal{N}_{u}\\backslash\\{i\\}}\\nu_{j \\rightarrow u}(X_{j})\\} \\\\\n\\nu_{j \\rightarrow u}(X_{j}) &= \\prod_{v\\in\\mathcal{N}_{j}\\backslash\\{u\\}}\\mu_{v \\rightarrow j}(X_{j})\n\\end{align*}\n$$\n\nì¶”ê°€ì ìœ¼ë¡œ Max Sum Belief Propagationì„ ì†Œê°œí•˜ê² ë‹¤. ì´ëŠ” Maximization ë¬¸ì œë¥¼ í’€ ë•Œ, $\\log$ë¥¼ ì·¨í•œ ê²°ê³¼ë„ ë™ì¼í•˜ë‹¤ëŠ” ì ì„ í™œìš©í•˜ì—¬ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒê³¼ ê°™ì´ ì‹ì´ ì¡°ê¸ˆ ë³€í™”í•œë‹¤. ì´ ë°©ì‹ì„ ì“°ë©´, ë„ˆë¬´ ì‘ì€ probabilityë¡œ ì¸í•œ ë¬¸ì œë¥¼ í”¼í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\max \\red{\\log} P(X_{1}, X_{2}, \\cdots, X_{N}) &= \\max_{X_{i}}\\{\\red{\\sum_{p\\in\\mathcal{N}_{i}}}\\mu_{p \\rightarrow i}(X_{i})\\} \\\\\n\\mu_{u \\rightarrow i}(X_{i}) &= \\max_{\\mathcal{N}_{u}\\backslash\\{i\\}}\\{\\red{\\log} f_{u}(\\mathcal{N}_{i}) + \\red{\\sum_{j \\in \\mathcal{N}_{u}\\backslash\\{i\\}}}\\nu_{j \\rightarrow u}(X_{j})\\} \\\\\n\\nu_{j \\rightarrow u}(X_{j}) &= \\red{\\sum_{v\\in\\mathcal{N}_{j}\\backslash\\{u\\}}}\\mu_{v \\rightarrow j}(X_{j})\n\\end{align*}\n$$\n\n## Construction from Data\n\nì•ì—ì„œëŠ” Graphë¥¼ í†µí•´ì„œ ì—°ì‚° ê³¼ì •ì„ Optimizationí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì•˜ë‹¤ë©´, ì—¬ê¸°ì„œëŠ” ì‹¤ì œ ê´€ì¸¡ dataë¥¼ ì´ìš©í•´ì„œ ì–´ë–»ê²Œ Graphë¥¼ êµ¬ì¡°í™”í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ì„œ ì•Œì•„ë³¼ ê²ƒì´ë‹¤. ì´ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ë§ì€ Algorithmì´ ì¡´ì¬í•˜ì§€ë§Œ ê°€ì¥ ê¸°ë³¸ì´ ë  ìˆ˜ ìˆëŠ” Algorithmì¸ **Chow-Liu Algorithm**ë§Œ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n### Chow-Liu Algorithm\n\nì œì¼ ë¨¼ì € êµ¬í•´ì•¼í•  ê²ƒì€ **Joint Probability**ì´ë‹¤. ì´ëŠ” Empirical distributionì„ ì´ìš©í•˜ì—¬ êµ¬í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ëŠ” featureê°€ Nê°œì¸ ì´ Kê°œì˜ dataê°€ ìˆì„ ë•Œ, ë‹¤ìŒê³¼ ê°™ì´ **Joint Probability**ë¥¼ êµ¬í•œ ê²ƒì´ë‹¤.\n\n$$\np(X_{1}=x_{1}, X_{2}=x_{2}, \\cdots, X_{N}=x_{n}) = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{1}[x^{(k)}=(x_{1}, x_{2}, \\cdots, x_{n})]\n$$\n\nìœ„ì™€ ê°™ì´ **Joint Probability**ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, **Chow-Liu Algorithm**ì—ì„œëŠ” **Second Order Conditional Probability**(ì´ ë‘ê°œì˜ Random Variableë¡œ êµ¬ì„±ëœ Conditional Probability. ì¦‰, Condtionë„ í•˜ë‚˜ì´ê³ , í™•ë¥ ì„ êµ¬í•˜ê³ ì í•˜ëŠ” ë³€ìˆ˜ë„ í•˜ë‚˜ì´ë‹¤.)ì™€ **Marginal Probability**ë¡œ Graphë¥¼ ê°€ì •í•˜ê³  **Bayesian Network**ë¥¼ êµ¬ì„±í•œë‹¤. ì´ ê²½ìš°ì—ëŠ” í˜•íƒœê°€ Tree í˜•íƒœë¡œ ë§Œë“¤ì–´ì§€ê¸° ë•Œë¬¸ì— ê²°ë¡ ì ìœ¼ë¡œ head-to-head ê´€ê³„ê°€ ë§Œë“¤ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤.(ê° ê°ì˜ nodeëŠ” í•˜ë‚˜ì˜ parentë§Œ ê°–ê¸° ë•Œë¬¸ì´ë‹¤.)\n\n![ml-chow-liu-1](/images/ml-chow-liu-1.jpg)\n\në”°ë¼ì„œ, ìœ„ì™€ ê°™ì€ Graphë¡œ ì¶”ì •í–ˆë‹¤ë©´, í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤.\n\n$$\np(x_{1}, x_{2}, \\cdots, x_{n}) = p(x_{6}|x_{5})p(x_{5}|x_{2})p(x_{4}|x_{2})p(x_{3}|x_{2})p(x_{2}|x_{1})p(x_{1})\n$$\n\nì—¬ê¸°ì„œ ì´ì œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë§Œ í’€ë©´ ëì´ë‹¤. Empirical distributionìœ¼ë¡œ êµ¬í•œ Joint Probability($p$)ì™€ ìš°ë¦¬ê°€ ì¶”ì •í•œ Graphì—ì„œì˜ Joint Probability($p_{\\intercal}$)ì‚¬ì´ì˜ ì°¨ì´ê°€ ìµœì†Œê°€ ë˜ë„ë¡ í•˜ë©´ ëœë‹¤. ì´ë¥¼ ìœ„í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ **KL Divergence**ì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ê°€ êµ¬í•˜ê³  ì‹¶ì€ **Bayesian Network**ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\argmin_{\\intercal\\text{:tree}} KL(p||p_{\\intercal})) = \\argmin_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}p(x^{(k)})\\log \\frac{p(x^{(k)})}{p_{\\intercal}(x^{(k)})}\n$$\n\nê·¸ë ‡ë‹¤ë©´, ì¢€ ë” ë©´ë°€í•˜ê²Œ $p_{\\intercal}$ì„ ì •ì˜í•´ë³´ì.\n\n$$\n\\begin{align*}\np_{\\intercal}(x_{1}, x_{2}, \\cdots, x_{N}) &= \\prod_{i=1}^{N}p(x_{i}|x_{\\text{parent}(i)})\\, (\\because \\text{Bayesian Network Definition})\\\\\n&= p(x_{root})\\prod_{(i,j) \\in E}p(x_{j}|x_{i})\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ VëŠ” nodeì˜ ì§‘í•©ì„ ì˜ë¯¸í•˜ê³ , EëŠ” edgeë¥¼ ì €ì¥í•˜ë©° ê° tuple(i,j)ëŠ” (parent, child)ë¥¼ ì˜ë¯¸í•œë‹¤. ê·¸ë¦¬ê³ , Treeì—ì„œëŠ” ë‹¨ í•˜ë‚˜ì˜ Nodeë§Œ Rootì´ê³  parentê°€ ì—†ê¸° ë•Œë¬¸ì— í•´ë‹¹ Rootë§Œ marginal Probabilityë¥¼ ê°€ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\np_{\\intercal}(x_{1}, x_{2}, \\cdots, x_{N}) &= p(x_{root})\\prod_{(i,j) \\in E}p(x_{j}|x_{i})\\\\\n&= p(x_{root})\\prod_{(i,j) \\in E} \\frac{p(x_{j},x_{i})}{p(x_{i})}\\\\\n&= \\red{p(x_{root})}\\prod_{(i,j) \\in E} \\frac{p(x_{j},x_{i})\\red{p(x_{j})}}{p(x_{i})p(x_{j})}\\\\\n&= \\prod_{i\\in V}p(x_{i}) \\prod_{(i,j) \\in E} \\frac{p(x_{j},x_{i})}{p(x_{i})p(x_{j})}\\\\\n\\end{align*}\n$$\n\në§ˆì§€ë§‰ì´ ì¢€ ì• ë§¤í•  ìˆ˜ ìˆëŠ” treeì´ê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•œ ê²ƒì´ë‹¤. íŠ¹ì • nodeë¡œ ê°€ëŠ” pathëŠ” ë‹¨ í•˜ë‚˜ì´ê¸° ë•Œë¬¸ì— $j$ë¡œ ëë‚˜ëŠ” edgeë„ í•˜ë‚˜ì¼ ìˆ˜ ë°–ì— ì—†ë‹¤. ë”°ë¼ì„œ, $p(x_{root})\\prod_{(i,j) \\in E} p(x_{j}) = \\prod_{i=V}p(x_{i})$ì¼ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\nì´ê²ƒì´ ì •ì˜ë˜ë©´, ìš°ë¦¬ëŠ” ì´ì œ ìµœì ì˜ Treeì¸ $\\intercal_{*}$ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\intercal_{*} &= \\argmin_{\\intercal\\text{:tree}} KL(p||p_{\\intercal})\\\\\n&= \\argmin_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}p(x^{(k)})\\log \\frac{p(x^{(k)})}{p_{\\intercal}(x^{(k)})}\\\\\n&= \\argmin_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}\\cancel{p(x^{(k)})\\log{p(x^{(k)})}} -p(x^{(k)})\\log{p_{\\intercal}(x^{(k)})}\\\\\n&= \\argmax_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}p(x^{(k)})\\log{p_{\\intercal}(x^{(k)})}\\\\\n&= \\argmax_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}p(x^{(k)})\\log({\\prod_{i\\in V}p(x_{i}^{(k)}) \\prod_{(i,j) \\in E} \\frac{p(x_{j}^{(k)},x_{i}^{(k)})}{p(x_{i}^{(k)})p(x_{j}^{(k)})}})\\\\\n&= \\argmax_{\\intercal\\text{:tree}} \\sum_{x^{(k)} \\in \\mathcal{D}}\\sum_{i\\in V}p(x^{(k)})\\log(p(x_{i}^{(k)})) + \\sum_{x^{(k)} \\in \\mathcal{D}}\\sum_{(i,j) \\in E} p(x^{(k)})\\log({\\frac{p(x_{j}^{(k)},x_{i}^{(k)})}{p(x_{i}^{(k)})p(x_{j}^{(k)})}})\\\\\n&= \\argmax_{\\intercal\\text{:tree}} \\sum_{i\\in V}\\sum_{x^{(k)} \\in \\mathcal{D}}p(x^{(k)})\\log(p(x_{i}^{(k)})) + \\sum_{(i,j) \\in E}\\sum_{x^{(k)}_{i}, x^{(k)}_{j}} p(x^{(k)}_{i}, x^{(k)}_{j})\\log({\\frac{p(x_{j}^{(k)},x_{i}^{(k)})}{p(x_{i}^{(k)})p(x_{j}^{(k)})}})\\, (\\because \\text{marginalization})\\\\\n&= \\argmax_{\\intercal\\text{:tree}} \\cancel{\\sum_{i\\in V}-H(X_{i})} + \\sum_{(i,j) \\in E}I(X_{i}, X_{j})\\, (\\because H(X_{i})\\text{ëŠ” constantì´ë‹¤.})\\\\\n&= \\argmax_{\\intercal\\text{:tree}}\\sum_{(i,j) \\in E}I(X_{i}, X_{j})\\\\\n\\end{align*}\n$$\n\në§ˆì§€ë§‰ marginalizationì€ í—·ê°ˆë¦°ë‹¤ë©´, í•´ë‹¹ Postingì˜ Sum-Product BP ë¶€ë¶„ì„ ë‹¤ì‹œ ë³´ê³ ì˜¤ë„ë¡ í•˜ì.\n\nì, ì´ì œ ìš°ë¦¬ê°€ ì–»ì€ ê²°ë¡ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ê²°êµ­, ìµœì ì˜ TreeëŠ” $I(X_{i}, X_{j})$ê°€ ìµœëŒ€ê°€ ë˜ëŠ” Treeì´ë‹¤. I(Mutual Information)ì´ í—·ê°ˆë¦°ë‹¤ë©´, [Information Theory](/posts/ml-base-knowledge#Information-Theory) ì •ë¦¬í•´ë†“ì€ Postingì„ ë‹¤ì‹œ ë³´ê³  ì˜¤ì. ê²°êµ­, $X_{i}, X_{j}$ê°„ì˜ ëª¨ë“  Mutual Informationì„ êµ¬í•´ì„œ weighted graphë¥¼ êµ¬ì¶•í•œë‹¤ìŒì— Kruskal Algorithmì„ í†µí•´ì„œ ìµœì  Treeë¥¼ ì°¾ìœ¼ë©´ ë˜ëŠ” ê²ƒì´ë‹¤.\n\në”°ë¼ì„œ, ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. ê°€ëŠ¥í•œ ëª¨ë“  (i,j) ìŒì— ëŒ€í•˜ì—¬ $I(X_{i}, X_{j})$ë¥¼ êµ¬í•˜ì—¬, Weighted Graphë¥¼ êµ¬ì„±í•œë‹¤.\n2. Kruskal Algorithmì„ ìˆ˜í–‰í•œë‹¤.\n   1. weightì˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ Edgeë¥¼ ì •ë ¬í•œë‹¤.\n   2. í•˜ë‚˜ì”© Edgeë¥¼ ë½‘ìœ¼ë©´ì„œ, Cycleì´ ìƒê¸°ëŠ”ì§€ í™•ì¸í•˜ì—¬ ìƒê¸°ë©´ ë²„ë¦¬ê³ , Cycleì´ ìƒê¸°ì§€ ì•Šìœ¼ë©´ Treeì— ì¶”ê°€í•œë‹¤.(Cycle ì—¬ë¶€ëŠ” ë™ì¼í•œ Nodeê°€ ë‘ ê°œ ë‹¤ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸)\n   3. ëª¨ë“  Nodeë¥¼ ë½‘ì•˜ë‹¤ë©´ ì¢…ë£Œí•˜ê³ , ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 2ë²ˆì„ ë°˜ë³µ ì‹œí–‰í•œë‹¤.\n\nì´ë ‡ê²Œ Graphë¥¼ ë§Œë“¤ê²Œ ë˜ë©´, ìš°ë¦¬ëŠ” Joint Probabilityë¥¼ ì´ì „ì— ë°°ìš´ Optimization ë°©ë²•ì„ í†µí•´ì„œ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , ì´ë¥¼ Modelì— ì§ì ‘ ì ìš©í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ìš°ë¦¬ê°€ Classificationì„ ìˆ˜í–‰í•  ë•Œì´ë‹¤.\n\n$$\n\\argmax_{\\mathcal{l} \\in \\{0,1,2,\\cdots, 9\\}} p_{\\mathcal{l}} \\times p(x^{\\text{new}}|\\mathcal{l}^{\\text{new}}=\\mathcal{l}) \\propto \\argmax_{\\mathcal{l} \\in \\{0,1,2,\\cdots, 9\\}} p_{\\mathcal{l}} \\times p_{\\intercal}(x^{\\text{new}})\n$$\n\nìœ„ì™€ ê°™ì´ ì¶”ì •í•˜ì—¬ ê³„ì‚°ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Medium[Chullin], Graphical Modelì´ë€ ë¬´ì—‡ì¸ê°€ìš”?, <https://medium.com/@chullino/graphical-model%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80%EC%9A%94-2d34980e6d1f>\n- Wiki, Markov Random Field, <https://en.wikipedia.org/wiki/Markov_random_field>\n- Adaptive Computation and Machine Learning, Thomas G. Dietterich\n- <https://cedar.buffalo.edu/~srihari/CSE574/Chap8/Ch8-PGM-Inference/Ch8.3.2-FactorGraphs.pdf>\n","slug":"ml-graphical-model","date":"2022-11-14 13:08","title":"[ML] 8. Graphical Model","category":"AI","tags":["ML","GraphicalModel","ConditionalIndependence","MarkovRandomField","BayesianNetwork","FactorGraph","D-Seperation","Factorization","MarkovProperty","MessagePassing","BeliefPropagation","Chow-LiuAlgorithm"],"desc":"Machine Learningì€ ì£¼ì–´ì§„ dataë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” pattern(Model)ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œë¼ê³  í•˜ì˜€ë‹¤. ê·¸ë ‡ë‹¤ë©´, \"dataê°€ ê°€ì§€ëŠ” ì—¬ëŸ¬ê°€ì§€ ì •ë³´(feature)ë“¤ ì¤‘ì—ì„œ ì–´ë–¤ featureë¥¼ ì¤‘ì ì ìœ¼ë¡œ ë³´ê³  ì´ìš©í•  ìˆ˜ ìˆì„ê¹Œ?\" ê·¸ë¦¬ê³ , \"ë§Œì•½ ì—¬ëŸ¬ featureë“¤ì´ ì„œë¡œ ì—°ê´€ì´ ìˆë‹¤ë©´ ì´ë¥¼ ì—°ì‚°ì˜ ìµœì í™”ë¥¼ ìœ„í•´ ì´ìš©í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?\" ë¼ëŠ” ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë‹¤. ì—¬ê¸°ì„œ Graphical Modelì€ ì´ëŸ¬í•œ ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ì„œ ì—°ì‚° ìµœì í™”ì— ëŒ€í•œ insightë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nì—¬íƒœê¹Œì§€ MLì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ë¡ ì„ ì‚´í´ë³´ì•˜ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì–´ë–¤ Modelì„ ì„ íƒí•˜ê³ , í•™ìŠµê³¼ ì¶”ì •ì„ í•´ì•¼í• ì§€ ê²°ì •í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” ì–´ë–¤ ê²ƒì´ ì¢‹ì€ Modelì´ê³ , ê° Model ê°„ì— ì–´ë–»ê²Œ ë¹„êµë¥¼ ìˆ˜í–‰í•  ê²ƒì¸ì§€ ê·¸ë¦¬ê³  ë” ë‚˜ì•„ê°€ Modelì„ í˜¼í•©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë³¼ ê²ƒì´ë‹¤.\n\n## What is Good Model?\n\nìš°ë¦¬ê°€ ì‚¬ëŒ imageë¥¼ ì…ë ¥ë°›ì•„ì„œ ê¸´ ë¨¸ë¦¬ë¥¼ ê°€ì§„ ì‚¬ëŒì¸ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” classifierë¥¼ ë§Œë“ ë‹¤ê³  í•˜ì. ì´ë•Œ ì–´ë–¤ Modelì´ ì¢‹ì€ Modelì´ ë  ìˆ˜ ìˆì„ê¹Œ?\n\nê°€ì¥ ì‰½ê²Œ ìƒê°í•  ìˆ˜ ìˆëŠ” Modelì€ Fully Connected Neural Network(FCNN)ë¥¼ êµ¬ì„±í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•´ì„œ Imageì˜ ê° pixelì„ ì¼ë ¬ë¡œ ì¤„ ì„¸ì›Œ ì…ë ¥í•  ìˆ˜ ë°–ì— ì—†ë‹¤. í•˜ì§€ë§Œ, ì´ëŠ” pixelë“¤ ê°„ì˜ ì¸ì ‘ ê´€ê³„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ê²Œ í•œë‹¤ëŠ” ë‹¨ì  ë•Œë¬¸ì— ë†’ì€ ì„±ëŠ¥ì„ ë‚´ê¸°ê°€ ì–´ë ¤ì› ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í—¤ì„œ ì œì‹œëœ ë°©ë²•ì´ Convolutional Neural Network(CNN)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” FCNNì„ ì ìš©í•˜ê¸° ì´ì „ì— Imageì— Filterë¥¼ ì ìš©í•˜ì—¬ íŠ¹ì • êµ¬ê°„ì„ ëŒ€í‘œí•˜ëŠ” ê°’ì„ ë½‘ì•„ë‚´ì„œ ë” íš¨ìœ¨ì ì¸ í•™ìŠµì„ í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.(ë¬¼ë¡  ë” ìì„¸íˆ ë‹¤ë£¨ë©´ Pooling Layer ë“± ë” ìì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤. í•´ë‹¹ ê¸€ì„ ì°¸ê³ í•˜ë„ë¡ í•˜ì. [ğŸ”— CNN(Convolutional Neural Networks) ì‰½ê²Œ ì´í•´í•˜ê¸°](https://halfundecided.medium.com/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-cnn-convolutional-neural-networks-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-836869f88375))\n\nìš°ë¦¬ì˜ ë‡Œì—ì„œë„ Imageë¥¼ ì¸ì‹í•˜ê³  ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œ, colorì™€ motion ê·¸ë¦¬ê³  ìœ¤ê³½ ë“±ì„ ë”°ë¡œ ë”°ë¡œ ì²˜ë¦¬í•œë‹¤ê³  í•œë‹¤. ì¦‰, CNNì€ ì´ëŸ¬í•œ Domain Knowledgeë¥¼ í™œìš©í•œ í›Œë¥­í•œ ì˜ˆì‹œ ì¤‘ í•˜ë‚˜ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì¦‰, ì—¬ê¸°ì„œ ë§í•˜ê³ ì í•˜ëŠ” ë°”ëŠ” ê²°êµ­ ëª¨ë“  í™˜ê²½ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” Modelì€ ì—†ë‹¤ëŠ” ê²ƒì´ë©°, Good Modelì€ ìš°ë¦¬ê°€ í•˜ê³ ì í•˜ëŠ” ì¼ì— ë”°ë¼ì„œ Domain Knowledgeë¥¼ ì¶©ì‹¤í•˜ê²Œ í™œìš©í•˜ì—¬ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ” Modelì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.\n\n```plaintext\n ğŸ¤” Data Augmentation\n \n Domain Knowledgeë¥¼ í™œìš©í•˜ì—¬ Modelì˜ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ \n ë‹¨ìˆœíˆ Model ìì²´ë¥¼ ë°”ê¾¸ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼ Domain Knowledgeë¥¼ ë°”íƒ•ìœ¼ë¡œ \n Dataë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë” ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ì´ ìˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì„ \n Data Augmentationì´ë¼ê³  í•œë‹¤.\n\n Image data ê°™ì€ ê²½ìš°ì—ëŠ” ì›ë³¸ Imageë¥¼ ì•½ê°„ íšŒì „ì‹œí‚¤ê±°ë‚˜ í™•ëŒ€í•˜ê±°ë‚˜ \n Noiseë¥¼ ì£¼ëŠ” ë“±ì˜ ì‘ì—…ì„ í•˜ì—¬ ì „ì²´ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ ëŠ˜ë¦´ ìˆ˜ ìˆë‹¤.\n\nText ê°™ì€ ê²½ìš°ì—ëŠ” ë™ì˜ì–´ë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì¥ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ\nëŠ˜ë¦¬ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤.\n```\n\n![ml-data-augmentation](/images/ml-data-augmentation.png)\n\n## Comparison between Models\n\nì—¬ê¸°ì„œ ë§Œì•½ ìš°ë¦¬ê°€ ì–»ì„ ìˆ˜ ìˆëŠ” Modelì˜ ì¢…ë¥˜ê°€ ë‹¤ì–‘í•˜ë‹¤ë©´ ì´ë“¤ì„ ì–´ë–»ê²Œ ë¹„êµí•˜ì—¬ í•˜ë‚˜ì˜ Modelì„ ì„ íƒí•  ìˆ˜ ìˆì„ê¹Œ? ì´ ì—­ì‹œ ì¤‘ìš”í•œ ë¬¸ì œì´ë‹¤.\n\nì‚¬ì‹¤ ìš°ë¦¬ê°€ í•™ìŠµí–ˆë˜ dataë¥¼ ê·¸ëŒ€ë¡œ í‰ê°€í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ êµ‰ì¥íˆ ë¶ˆê³µí‰í•˜ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ìš°ë¦¬ê°€ ë§Œë“¤ê³ ì í•˜ëŠ” Modelì€ ì¼ë°˜ì ìœ¼ë¡œ ì–´ëŠ ìƒí™©ì— ë‘ì–´ë„ ê·¸ë¦¬ê³  ì•ˆë³¸ dataì¼ì§€ë¼ë„ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í•˜ê¸°ë¥¼ ì›í•œë‹¤. ì¦‰, ìš°ë¦¬ì˜ Modelì´ **Generalization**ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê¸°ë¥¼ ë°”ë€ë‹¤.\n\nì´ëŸ¬í•œ Modelì˜ **Generalization** ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ Datasetì„ Trainê³¼ Test setìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì´ê²ƒë„ ë¶€ì¡±í•  ë•Œê°€ ìˆë‹¤. íŠ¹ì • Modelì´ íŠ¹ì • Train setì—ì„œë§Œ ì„±ëŠ¥ì´ ë†’ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” **Cross Validation**ì´ë¼ëŠ” ë°©ì‹ì„ ë„ì…í•œë‹¤. ì´ëŠ” ìš°ë¦¬ê°€ ê°€ì§„ datasetì„ ê³¨ê³ ë£¨ testì™€ train setìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰, ì—¬ëŸ¬ ë²ˆì˜ trainingì„ ìˆ˜í–‰í•˜ë©°, testë¥¼ ìˆ˜í–‰í•˜ê¸°ë¥¼ ë°˜ë³µí•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³ , ì´ë¥¼ í‰ê· ì„ ë‚´ì„œ ì „ì²´ì ì¸ Model ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\n![ml-k-fold-cross-validation](/images/ml-k-fold-cross-validation.png)\n\nìœ„ì™€ ê°™ì´ ê³µí‰í•˜ê²Œ kê°œë¡œ ë‚˜ëˆ„ëŠ” ë°©ì‹ì„ k fold cross validationì´ë¼ê³  í•˜ë©°, í•´ë‹¹ ì˜ˆì‹œëŠ” $k=4$ì¸ ê²½ìš°ì´ë‹¤. ì¦‰, ìœ„ì™€ ê°™ì´ Validationì„ í•˜ê¸° ìœ„í•´ì„œëŠ” Modelì˜ ìˆ˜ê°€ $N$ê°œë¼ê³  í•  ë•Œ, ì´ $N \\times k$ë²ˆì˜ Trainingê³¼ Evaluationì´ í•„ìš”í•˜ë‹¤.\n\ní•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ë˜ ê°„ê³¼í•œ ì‚¬ì‹¤ì€ hyperparameterê°€ ê° modelë§ˆë‹¤ í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì‚¬ì‹¤ì´ë‹¤. ì¦‰, Hyper Parameterë¥¼ ì •í•˜ëŠ” ê³¼ì • ì—­ì‹œ í•„ìš”í•œë°, ì´ëŠ” ê° ê°ì˜ Model ë‚´ë¶€ì—ì„œ ì–´ë–¤ Hyper Parameterë¥¼ ì‚¬ìš©í• ì§€ì— ëŒ€í•œ í•©ì˜ê°€ í•„ìš”í•œ ê²ƒì´ë‹¤. ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ ì–´ì©” ìˆ˜ ì—†ì´ ìš°ë¦¬ëŠ” Trainingê³¼ Evaluationì„ ìˆ˜í–‰í•´ì•¼ í•˜ë©°, ì´ë¥¼ ìœ„í•œ dataë¥¼ ë³„ë„ë¡œ ë¶„ë¦¬í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ê°€ ê°€ì§€ëŠ” datasetì„ ë‹¤ìŒê³¼ ê°™ì´ ì„¸ê°œë¡œ ë‚˜ëˆ„ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n![ml-dataset](/images/ml-dataset.png)\n\nì—¬ê¸°ì„œ ë” ì •ë‹¹í•˜ê²Œ í•˜ê³  ì‹¶ë‹¤ë©´, ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ì„ ë°˜ë³µí•´ì•¼ í•œë‹¤.\n\n![ml-nested-cross-validation](/images/ml-nested-cross-validation.png)\n\ní•˜ì§€ë§Œ, ì´ëŠ” êµ‰ì¥íˆ ë¹„ìš©ì´ ì»¤ì§ˆ ìˆ˜ ìˆë‹¤. validation setì„ ê³ ë¥¼ ë•Œ, $k^{\\prime}$ê°œê°€ í•„ìš”í•˜ë‹¤ê³  í•œë‹¤ë©´, ìš°ë¦¬ëŠ” $N \\times k^{\\prime} \\times k$ë²ˆì˜ Trainingê³¼ Evaluationì´ í•„ìš”í•œ ê²ƒì´ë‹¤. êµ‰ì¥íˆ ë¹„ìš©ì´ ì»¤ì§€ê¸° ë•Œë¬¸ì— ëŒ€ê²Œ validation setê¹Œì§€ cross validationí•˜ëŠ” nested cross validationì€ ìƒí™©ì— ë”°ë¼ ì‚¬ìš©ë˜ê¸°ë„ í•˜ê³ , ì‚¬ìš©ë˜ì§€ ì•Šê¸°ë„ í•œë‹¤.\n\n## Combining Simple Models\n\nì¢‹ì€ Modelì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë°©ë²• ì¤‘ì—ì„œ ê°€ì¥ ì‰½ê²Œ ìƒê°í•  ìˆ˜ ìˆëŠ” ê²ƒ ì¤‘ì— í•˜ë‚˜ê°€ ì—¬ëŸ¬ ê°œì˜ Modelì„ í™œìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì‰½ê²Œ ì§‘ë‹¨ ì§€ì„±ì„ í™œìš©í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ì„ **Ensemble**(ì•™ìƒë¸”)ì´ë¼ê³  ë¶€ë¥´ê³ , ì´ë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆë‹¤.\n\n1. ì„œë¡œ ë‹¤ë¥¸ ì—¬ëŸ¬ ê°œì˜ Model, ë˜ëŠ” Hyperparameterë§Œì„ ë³€ê²½í•˜ê±°ë‚˜ ë˜ëŠ” featureë¥¼ ë‹¤ë¥´ê²Œ ë³€í˜•í•˜ì—¬ Modelì„ ì—¬ëŸ¬ ê°œ ìƒì„±í•˜ê³  í‰ê·  ë˜ëŠ” ìµœëŒ“ê°’ì„ ì·¨í•˜ëŠ” ë°©ë²• (**Voting**)\n2. ì—¬ëŸ¬ ê°œì˜ Modelì„ í˜¼í•©í•˜ì§€ë§Œ, ê° ë‹¨ê³„ì— ë”°ë¼ì„œ Modelì„ ì„ íƒí•˜ëŠ” ë°©ë²• (**Stacking**)\n3. datasetì„ ì—¬ëŸ¬ ë²ˆ samplingí•˜ì—¬ ê° ê°ì˜ Modelì„ ë§Œë“¤ê³ , ê° Modelì˜ ê²°ê³¼ë¥¼ í‰ê·  ë˜ëŠ” ìµœëŒ“ê°’ì„ ì·¨í•˜ëŠ” ë°©ë²• (**Bagging**, **Pasting**)\n4. ì´ì „ê³¼ëŠ” ë‹¬ë¦¬ ì• ì„œ ì§„í–‰í•œ Modelì˜ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ë‹¤ìŒ Modelì— ì ìš©í•˜ê¸°ë¥¼ ë°˜ë³µí•˜ë©°, ì—¬ëŸ¬ Modelì„ ì œì‘í•˜ê³  ì·¨í•©í•˜ëŠ” ë°©ë²• (**Boosting**)\n\ní¬ê²ŒëŠ” ì´ë ‡ê²Œ 3ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ê°ê°ì„ ìì„¸íˆ ë‹¤ë£¨ì§€ëŠ” ì•Šê³ , **Boosting** ë°©ì‹ ì¤‘ì—ì„œë„ ë§ì´ ì‚¬ìš©ë˜ëŠ” ë°©ë²• ì¤‘ì— í•˜ë‚˜ì¸ **AdaBoost**ì— ëŒ€í•´ì„œ ì¢€ ë” ìì„¸íˆ ë‹¤ë¤„ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n### AdaBoost\n\nAdaptive Boostingì˜ ì•½ìì¸ AdaBoostëŠ” ì´ë¦„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ë°˜ë³µì ì¸ ì‘ì—…ì„ í†µí•´ì„œ ìµœì¢… Modelì˜ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ìš°ì„  Boosting ë°©ë²• ìì²´ê°€ ë™ì‹œì— Modelì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ë©´ì„œ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°©ë²•ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ì´ì „ Modelë“¤ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ë‹¤ìŒ Modelì—ê²Œ ë„˜ê²¨ì¤„ ìˆ˜ ìˆëŠ” íŠ¹ë³„í•œ ì •ë³´ëŠ” ë¬´ì—‡ì¼ê¹Œ? ì´ëŠ” ë°”ë¡œ ìì‹ ë“¤ì´ ì˜ëª» ë¶„ë¥˜í•œ ë°ì´í„°ì— ëŒ€í•œ ì •ë³´ì´ë‹¤. ìì‹ ë“¤ì´ ì˜ëª» ë¶„ë¥˜í•œ dataë“¤ì—ê²Œ ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ë„ë¡ í•˜ì—¬ ë‹¤ìŒ Modelì—ì„œëŠ” ì´ë¥¼ ì¤‘ì‹¬ì ìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìµœì¢… Modelì˜ ì„±ëŠ¥ì„ ë†’ì—¬ë³´ìëŠ” ê²ƒì´ Ideaì´ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ì´ê²ƒì´ ì–´ë–»ê²Œ ê°€ëŠ¥í• ê¹Œ? ë§¤ìš° ê°„ë‹¨í•œ ì´ì§„ ë¶„ë¥˜ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¥¼ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ë‹¤. ìš°ë¦¬ê°€ ë§Œì•½ íŠ¹ì • ì„ê³„ê°’($\\theta_{t}$)ë³´ë‹¤ ì‘ìœ¼ë©´ -1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 1ì´ë¼ê³  ë¶„ë¥˜í•˜ëŠ” ì•„ì£¼ ê°„ë‹¨í•œ ë¶„ë¥˜ê¸°(weak classifier, decision stump)ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•˜ì.\n\n$$\nf_{t}(x) = \\begin{cases} -1 & \\text{if } x < \\theta_{t} \\\\ 1 & \\text{otherwise} \\end{cases}\n$$\n\nì´ì œ ìš°ë¦¬ëŠ” ì´ ê°„ë‹¨í•œ ë¶„ë¥˜ê¸° Tê°œë¥¼ í•©ì³ì„œ ë³µì¡í•œ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•  ë¶„ë¥˜ê¸°ë¥¼ ì œì‘í•  ê²ƒì´ë‹¤. ì´ ë•Œ, ê° ë¶„ë¥˜ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê°€ì¤‘ì¹˜($\\alpha_{t}$)ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.\n\n$$\n\\begin{align*}\n\\text{output} = \\text{sign}(F_{T}(x)) \\\\\nF_{T}(x) = \\sum_{t=1}^{T} \\alpha_{t} f_{t}(x)\n\\end{align*}\n$$\n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ìœ„ ì‹ì—ì„œ ì–´ë–»ê²Œ í•˜ë©´, í˜„ëª…í•˜ê²Œ $\\theta_{t}, \\alpha_{t}$ë¥¼ ê²°ì •í•  ìˆ˜ ìˆì„ê¹Œ? ì´ì— ëŒ€í•œ í•´ë‹µìœ¼ë¡œ **AdaBoost**ëŠ” ì´ì „ $F_{t-1}$ì— ì˜í•´ ë°œìƒí•œ **error**ì— ì§‘ì¤‘í•œë‹¤.\n\nìš°ì„  $F_{t}$ì˜ Error($E(F_{t})$)ë¥¼ ì•„ë˜ì™€ ê°™ë‹¤ê³  í•˜ì.\n\n$$\nE(F_{t}) = \\sum_{i=1}^{N} \\exp(-y^{(i)}F_{t}(x^{(i)}))\n$$\n\nì¦‰, ì˜ˆì¸¡ì´ ë§ë‹¤ë©´ errorëŠ” $1 \\over e$, í‹€ë¦¬ë‹¤ë©´ $e$ë§Œí¼ errorê°€ ì¦ê°€í•œë‹¤.  \nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” í˜„ì¬ í•™ìŠµí•  Model ì´ì „ê¹Œì§€ì˜ Modelì˜ í•˜ë‚˜ì˜ ë°ì´í„°ì— ëŒ€í•œ Errorë¥¼ $\\gamma_{t}^{(i)}$ë¼ê³  ì •ì˜í•´ë³´ì.\n\n$$\n\\gamma_{t}^{(i)} = \\exp(-y^{(i)}F_{t-1}(x^{(i)})),\\quad \\gamma_{1}^{(i)} = 1\n$$\n\në‹¤ì‹œ í•œ ë²ˆ $\\gamma_{t}^{(i)}$ì˜ ì˜ë¯¸ë¥¼ ì •ì˜í•˜ë©´, ê°„ë‹¨í•˜ê²Œ ì´ì „ê¹Œì§€ì˜ Modelì˜ í•©ìœ¼ë¡œ ë§Œë“  Modelì´ ì˜ ë¶„ë¥˜í–ˆë‹¤ë©´, $e$ ê·¸ë ‡ì§€ ì•Šë‹¤ë©´, $1 \\over e$ê°€ ëœë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ê³„ì†í•´ì„œ Error ì‹ì„ ì •ë¦¬í•´ë³´ì.\n\n$$\n\\begin{align*}\nE(F_{t}) &= \\sum_{i=1}^{N}\\{\\exp(-y^{(i)}F_{t-1}(x^{(i)})) \\times \\exp(-y^{(i)}\\alpha_{t}f_{t}(x^{(i)}))\\} \\\\\n&= \\sum_{i=1}^{N} \\gamma_{t}^{(i)} \\exp(-y^{(i)}\\alpha_{t}f_{t}(x^{(i)})) \\\\\n&= \\sum_{i:y^{(i)}=f_{t}(x^{(i)})}\\gamma_{t}^{(i)}\\exp(-\\alpha_{t}) + \\sum_{i:y^{(i)}\\neq f_{t}(x^{(i)})}\\gamma_{t}^{(i)}\\exp(\\alpha_{t}) \\\\\n&= \\sum_{i=1}^{N}\\gamma_{t}^{(i)}\\exp(-\\alpha_{t}) + \\sum_{i:y^{(i)}\\neq f_{t}(x^{(i)})}\\gamma_{t}^{(i)}(\\exp(\\alpha_{t})-\\exp(-\\alpha_{t})) \\\\\n&= \\exp(-\\alpha_{t})\\sum_{i=1}^{N}\\gamma_{t}^{(i)} + (\\exp(\\alpha_{t})-\\exp(-\\alpha_{t}))\\sum_{i:y^{(i)}\\neq f_{t}(x^{(i)})}\\gamma_{t}^{(i)}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ Errorë¥¼ ê°€ì¥ ì‘ê²Œ í•  ìˆ˜ ìˆëŠ” $\\theta_{t}, \\alpha_{t}$ë¥¼ ì°¾ê¸° ìœ„í•œ ë°©ë²•ì€ ê° ê° ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. ì‹ì—ì„œ $\\theta_{t}$ê°€ ë°”ê¿€ ìˆ˜ ìˆëŠ” ê²ƒì€ $f_{t}$ë°–ì— ì—†ë‹¤. ì¦‰ $\\sum_{i:y^{(i)}\\neq f_{t}(x^{(i)})}\\gamma_{t}^{(i)}$ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒì´ë‹¤.  \n   ì¦‰, $\\gamma_{t}^{(i)}$ëŠ” ì´ì „ ë¶„ë¥˜ê¸°($F_{t-1}$)ê°€ ì˜ ë¶„ë¥˜í–ˆë‹¤ë©´ $e$, ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ $1 \\over e$ê°€ ë˜ëŠ”ë°, ì´ë“¤ì˜ í•©ì´ ìµœì†Œê°€ ë˜ë„ë¡ í•˜ëŠ” ì„ê³„ê°’ $\\theta_{t}$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.  \n   ì¦‰, ê¸°ì¡´ ë¶„ë¥˜ê¸°ê°€ ì˜ëª» ë¶„ë¥˜í•œ dataì— ëŒ€í•´ì„œ ë” ì¤‘ì ì ìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë„ë¡ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ë‹¤ì‹œ ë¶„ë¥˜í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n2. Errorë¥¼ $\\alpha_{t}$ì— ëŒ€í•œ ë¯¸ë¶„ì„ í•˜ì—¬, 0ì´ ë˜ë„ë¡ í•˜ëŠ” $\\alpha_{t}$ë¥¼ ì°¾ìœ¼ë©´ ëœë‹¤. ì´ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\alpha_{t} = \\frac{1}{2}\\ln\\frac{1-\\varepsilon_{t}}{\\varepsilon_{t}},\\quad \\varepsilon_{t} = \\frac{\\sum_{i:y^{(i)}\\neq f_{t}(x^{(i)})}\\gamma_{t}^{(i)}}{\\sum_{i}^{N}\\gamma_{t}^{(i)}}\n$$\n\nì—¬ê¸°ì„œ $\\varepsilon_{t}$ë¥¼ ìì„¸íˆ ë³´ë©´, ë¶„ëª¨ëŠ” decision stumpì˜ ìµœëŒ€ Errorì´ê³  ë¶„ìëŠ” í˜„ì¬ decision stumpì˜ Errorë¥¼ ì˜ë¯¸í•œë‹¤. ì´ê²ƒì´ ì§ì ‘ì ìœ¼ë¡œ $\\alpha_{t}$ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒì´ë‹¤.\n\në”°ë¼ì„œ ì´ë¥´ ì¡°ê¸ˆ ë” ì •ë¦¬í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. $\\varepsilon_{t} \\gt \\frac{1}{2} \\rArr \\alpha_{t} \\lt 0$  \n   $\\varepsilon_{t} \\gt \\frac{1}{2}$ë¼ëŠ” ê²ƒì€ ì‚¬ì‹¤ $f_{t}$ì˜ ì„±ëŠ¥ì´ ì„ íƒì§€ ë‘ ê°œì§€ í•˜ë‚˜ë¥¼ Randomí•˜ê²Œ ê³ ë¥´ëŠ” ê²½ìš°ì˜ í™•ë¥  $\\frac{1}{2}$ë³´ë‹¤ ëª»í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ ê²½ìš°ì— $\\alpha_{t}$ë¥¼ ìŒìˆ˜ë¡œ ì„¤ì •í•˜ì—¬ ì ìš©í•˜ëŠ” ê²ƒì´ ë°˜ëŒ€ë¡œ í™•ë¥ ì„ ì ìš©í•˜ëŠ” ê²ƒì´ê³ , ì´ê²ƒì´ ì „ì²´ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆê¸°ì— íƒ€ë‹¹í•˜ë‹¤.\n2. $\\varepsilon_{t} = \\frac{1}{2} \\rArr \\alpha_{t} = 0$  \n   ë§Œì•½, ì„±ëŠ¥ì´ ë”± $\\frac{1}{2}$ë¼ë©´, ë” ì´ìƒ ê°œì„ ì˜ ì—¬ì§€ê°€ ì—†ì–´ì§„ë‹¤. ì¦‰, $\\alpha_{t}$ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì ìš©í•˜ê²Œ ë˜ë©´, $F_{t}=F_{t-1}$ì´ ëœë‹¤. ì¦‰, ë” ì´ìƒì˜ Model ì¤‘ì²©ì€ ë¬´ì˜ë¯¸í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë¯€ë¡œ í•´ë‹¹ ë‹¨ê³„ì— ë„ë‹¬í•˜ë©´ í•™ìŠµì„ ì¤‘ë‹¨í•œë‹¤.\n3. $0 \\lt \\varepsilon_{t} \\lt \\frac{1}{2} \\rArr \\alpha_{t} \\gt 0$  \n   ì¼ë°˜ì ì¸ ê²½ìš°ë¡œ, ìƒˆë¡­ê²Œ ë§Œë“  ë¶„ë¥˜ê¸°ê°€ ê¸°ì¡´ ë¶„ë¥˜ê¸°($F_{t-1}$)ë¥¼ ë³´ì™„í•  ë§Œí¼ ì˜ ì˜ˆì¸¡ì„ í•˜ê³  ìˆê¸°ì— $\\alpha_{t}$ë¥¼ ì–‘ìˆ˜ë¡œ ì„¤ì •í•˜ì—¬ ì ìš©í•œë‹¤.\n4. $\\varepsilon_{t} \\rarr 0 \\rArr \\alpha \\rarr \\infin$  \n   $\\varepsilon_{t}$ê°€ 0ì— ê°€ê¹Œì›Œì§€ë©´, ì¦‰, $f_{t}$ê°€ ëª¨ë“  dataë¥¼ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•œë‹¤ë©´, ì‚¬ì‹¤ìƒ ê¸°ì¡´ ë¶„ë¥˜ê¸°ë“¤ì€ ë” ì´ìƒ ì˜ë¯¸ê°€ ì—†ë‹¤. í•˜ë‚˜ì˜ $decision stump$ë¡œ ì™„ë²½í•˜ê²Œ ë¶„ë¥˜ë˜ëŠ” ë¬¸ì œì˜€ê¸° ë•Œë¬¸ì´ë‹¤. ì¦‰, $F_{t} = f_{t}$ê°€ ëœë‹¤.\n\n### Decision Tree\n\nì• ì„  **AdaBoost**ì—ì„œëŠ” Decision Stumpë¥¼ ë‹¤ë£¨ì—ˆì§€ë§Œ, ë” ë‹¤ì–‘í•œ ë¶„ë¥˜ê¸°ë¥¼ ì´ìš©í•´ì„œ Decision Treeë¥¼ êµ¬ì„±í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤. ì‹¤ì œ Stacking ë˜ëŠ” Bagging ë“±ì˜ ì‘ì—…ì„ í•  ë•Œì—ëŠ” ë‹¨ìˆœí•œ Decision Stumpì˜ í•© ê°™ì€ í˜•íƒœê°€ ì•„ë‹ˆë¼ Treeí˜•íƒœë¡œ êµ¬ì„±ë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤(Decisionì„ í•  ë•Œë§ˆë‹¤ ê°€ì§€ì¹˜ê¸°ë¥¼ í•˜ë©° ë‚˜ë‰˜ëŠ” í˜•íƒœ). ê·¸ë¦¬ê³  ì‹¤ì œë¡œë„ ì´ í˜•íƒœê°€ ì¸ê°„ì˜ ì‚¬ê³  ê³¼ì •ë„ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤. ë”°ë¼ì„œ, ëŒ€ê²Œì˜ ê²½ìš° ì„±ëŠ¥ë„ ì¢‹ì€ ë¿ë§Œ ì•„ë‹ˆë¼ ì§ê´€ì ì´ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ë°©ì‹ì„ ì‚¬ìš©í•´ì„œ ì—¬ëŸ¬ Modelì„ í˜¼í•©í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤. ì´ ì•ˆì—ì„œ Decisionì„ ìˆ˜í–‰í•  ë•Œ ë³µì¡í•œ Deep Learningì„ ìˆ˜í–‰í•  ìˆ˜ë„ ìˆê³ , ë‹¨ìˆœí•˜ê²Œ Decision Stumpë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆëŠ” ê²ƒì´ë‹¤.\n\n![ml-decision-tree](/images/ml-decision-tree.png)\n\nê·¸ë ‡ë‹¤ë©´, ì´ëŸ¬í•œ Decision Treeë¥¼ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ” ê²Œ ì¢‹ì„ì§€ë¥¼ ì¡°ê¸ˆë§Œ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤. ê°€ì •ì„ í•˜ë‚˜ í•´ë³´ì. ìš°ë¦¬ê°€ ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” Categoryê°€ 10ê°œì´ê³ , featureê°€ 100ê°œì´ë‹¤. ì´ë•Œ, ì–´ë–¤ Featureë¥¼ ì´ìš©í•œ ì–´ë–¤ Modelì„ ì‚¬ìš©í•œ ê²ƒì„ ìš°ì„ ìœ¼ë¡œ ì ìš©í•´ì•¼í• ê¹Œ? ì´ê²ƒì´ ì‚¬ì‹¤ ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì œì´ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜(ID3, CART, ë“±)ì´ ì œì‹œë˜ì—ˆë‹¤. í•˜ì§€ë§Œ, ê²°êµ­ í•µì‹¬ì€ ê° ê°ì˜ ë‹¨ê³„ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì¥ ì ì ˆí•˜ê²Œ ë‚˜ëˆ„ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, Model(f)ì— ëŒ€í•´ì„œ <mark>**ì–»ì„ ìˆ˜ ìˆëŠ” ì •ë³´ì˜ ì–‘**(**IG**, Information Gain)</mark>ì´ ë§ì„ ìˆ˜ë¡ ì¢‹ì€ Modelì´ë¼ê³  ì¹­í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\nIG(\\mathcal{D}, f) = I(\\mathcal{D}) - \\sum_{j=1}^{J} \\frac{D_{j}}{D}I(\\mathcal{D}_{j})\n$$\n\nì—¬ê¸°ì„œ, ë˜ ê·¸ë ‡ë‹¤ë©´, IëŠ” ë¬´ì—‡ì¸ì§€ ê¶ê¸ˆí•  ìˆ˜ ìˆë‹¤. ì´ëŠ” Impurity(ì •ë³´ì˜ í˜¼íƒë„)ë¥¼ ì˜ë¯¸í•˜ë©°, ì´ë¥¼ í‘œí˜„í•˜ëŠ” ì§€í‘œëŠ” ì•„ë˜ì™€ ê°™ì€ ê²ƒë“¤ì´ ìˆë‹¤.\n\n1. Gini Impurity\n2. Entropy\n3. Classification Error\n\nìœ„ ì¤‘ì—ì„œ ìš°ë¦¬ê°€ [ğŸ”— ML Base Knowledge(Information Theory)](/posts/ml-base-knowledge#Information-Theory)ì—ì„œ ë‹¤ë£¨ì—ˆë˜ **Entropy**ì— ê¸°ë°˜í•œ ë°©ë²•ì´ ê°€ì¥ ì¦ê²¨ì„œ ì‚¬ìš©ë˜ì–´ì§„ë‹¤.\n\nì¦‰, Entropyì— ê¸°ë°˜í•œ ì„¤ëª…ì„ í•˜ìë©´, ìš°ë¦¬ëŠ” IG(ì •ë³´ íšë“ëŸ‰)ë¥¼ ìµœëŒ€í™”í•˜ê¸° ìœ„í•œ ì„ íƒì„ í•˜ê²Œ ë˜ë©´, í•´ë‹¹ ê²°ì •ì˜ Childë“¤ì€ ì ì€ Entropyë¥¼ ê°€ì§€ê²Œ ë˜ê³  ì´ ê³¼ì •ì„ ë°˜ë³µí•´ ë‚˜ê°€ë©´ì„œ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤.\n\nì¦‰, Decision Treeë¥¼ ìƒì„±í•  ë•Œì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ featureì™€ Modelì„ ì ìš©í•˜ë©° ê° Modelì´ ê°€ì§€ëŠ” IGë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ Treeì˜ Rootì—ì„œë¶€í„° Modelì„ ì„ íƒí•˜ë©° ë‚´ë ¤ì˜¤ëŠ” ê²ƒì´ë‹¤.\n\n## Cutting down a Compex Model\n\në˜í•œ, ì¢‹ì€ Modelì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ ì•„ì´ëŸ¬ë‹ˆí•˜ê²Œë„ ì¼ë¶€ ì •ë³´ë¥¼ ì‚­ì œí•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ë•Œê°€ ìˆë‹¤. ëŒ€ê²Œ Deep Learning í™˜ê²½ì—ì„œ ë§ì´ ë°œìƒí•˜ëŠ” ê²½ìš°ì¸ë°, **over fitting**ìœ¼ë¡œ ì¸í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì¼ë¶€ edgeë¥¼ ì œê±°í•˜ëŠ” **dropout**ì„ ìˆ˜í–‰í•œë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì€ **over fitting**ì„ ë°©ì§€í•  ë¿ë§Œ ì•„ë‹ˆë¼ í•™ìŠµì˜ ì†ë„ ì—­ì‹œ ê°œì„ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ìì£¼ ì‚¬ìš©ë˜ì–´ì§„ë‹¤. ì‹¤ì œë¡œ modelì˜ ì„±ëŠ¥ì´ ì¦ê°€í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ ë‹¤ë£¬ ë…¼ë¬¸ì´ ë³„ë„ë¡œ ìˆìœ¼ë‹ˆ ì°¸ê³ í•  ìˆ˜ ìˆë‹¤ë©´ í•´ë³´ë„ë¡ í•˜ì. ë§Œì•½ ì‹œê°„ì´ ëœë‹¤ë©´ ì´ì— ëŒ€í•´ì„œë„ ë‹¤ë£° ìˆ˜ ìˆë„ë¡ í•˜ê² ë‹¤.\n\n- Frankle, Jonathan, and Michael Carbin. \"The lottery ticket hypothesis: Finding sparse, trainable neural networks.\" ICRL 2019\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- [ğŸ”— CNN(Convolutional Neural Networks) ì‰½ê²Œ ì´í•´í•˜ê¸°](https://halfundecided.medium.com/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-cnn-convolutional-neural-networks-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-836869f88375)\n","slug":"ml-model-selection","date":"2022-11-08 16:07","title":"[ML] 7. Model Selection","category":"AI","tags":["ML","ModelSelection","CrossValidation","Boosting","AdaBoost","DecisionTree","NetworkPruning"],"desc":"ì—¬íƒœê¹Œì§€ MLì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ë¡ ì„ ì‚´í´ë³´ì•˜ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì–´ë–¤ Modelì„ ì„ íƒí•˜ê³ , í•™ìŠµê³¼ ì¶”ì •ì„ í•´ì•¼í• ì§€ ê²°ì •í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” ì–´ë–¤ ê²ƒì´ ì¢‹ì€ Modelì´ê³ , ê° Model ê°„ì— ì–´ë–»ê²Œ ë¹„êµë¥¼ ìˆ˜í–‰í•  ê²ƒì¸ì§€ ê·¸ë¦¬ê³  ë” ë‚˜ì•„ê°€ Modelì„ í˜¼í•©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë³¼ ê²ƒì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nìš°ë¦¬ê°€ NLì„ ì œëŒ€ë¡œ ë¶„ì„í•˜ê¸° ìœ„í•´ì„œ ê° ë‹¨ì–´ê°€ ê°€ì§„ ì˜ë¯¸ë¥¼ ì•Œì•„ì•¼í•˜ë©°, ì´ë¥¼ ë„˜ì–´ì„œ ë¬¸ì¥ì´ ê°€ì§€ëŠ” ì˜ë¯¸ë¥¼ íŒŒì•…í•´ì•¼ í•œë‹¤. ê²°ë¡ ì ìœ¼ë¡œ ì´ ê³¼ì •ì´ ê³ ë„í™”ëœ NLPë¥¼ ìœ„í•œ í•µì‹¬ ë‹¨ê³„ì´ë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” Rawí•œ í˜•íƒœë¡œ ì£¼ì–´ì§„ textë¥¼ ì²˜ë¦¬í•´ì„œ ë” ë‚˜ì€ í˜•íƒœì˜ êµ¬ì¡°ë¥¼ ë§Œë“¤ í•„ìš”ê°€ ìˆë‹¤. ë”°ì§€ê³  ë³´ë©´ í•˜ë‚˜ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ì¹˜ë§Œ ì´ì „ text processing chapterê³¼ ë‹¤ë¥¸ ì ì€ ë¬¸ì¥ êµ¬ë¶„ê³¼ ê°™ì€ ê°„ë‹¨í•œ ê³¼ì •ì´ ì•„ë‹Œ Linguistic ë‹¨ê³„ì— ë”°ë¥¸ ì²˜ë¦¬ ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë˜í•œ, ê° ë‹¨ê³„ ì—­ì‹œ NLP ì¤‘ì— í•˜ë‚˜ë¼ê³  í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ ë˜í•œ MLê³¼ DLì„ í†µí•´ì„œ ê³ ë„í™”í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤. Morphology ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ì—¬ Syntax, Semanticê¹Œì§€ ì–´ë–»ê²Œ ë‹¤ë£¨ê²Œ ë˜ëŠ”ì§€ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## POS tagging\n\nMorphology ë‹¨ê³„ì—ì„œ ê°€ì¥ ê¸°ë³¸ì´ë˜ëŠ” ìš”ì†Œì´ê¸° ë•Œë¬¸ì— ì´ë¥¼ ë¨¼ì € ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤. Part of Speechë¼ëŠ” ë‹¨ì–´ì˜ ëœ» ìì²´ê°€ \"í’ˆì‚¬\"ì´ë‹¤. ì´ëŠ” ë‹¨ì–´ì˜ ë¬¸ë²•ì ì¸ ê¸°ëŠ¥ì´ë‚˜ í˜•íƒœ ë“±ì„ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ ì œì‹œë˜ì—ˆë‹¤. ì´ë¥¼ êµ¬ë¶„í•˜ë ¤ëŠ” ì‹œë„ëŠ” ë””ì˜¤ë‹ˆì†ŒìŠ¤ ì´ì „ë¶€í„° ìˆì—ˆì§€ë§Œ ê·¼ë³¸ì ì¸ í˜•íƒœë¥¼ ì œì‹œí•œ ê²ƒì€ ë””ì˜¤ë‹ˆì†ŒìŠ¤ê°€ ì²« ë²ˆì§¸ì´ë‹¤. ê·¸ëŠ” ê¸°ì›ì „ 100ë…„ì— ì§€ê¸ˆê³¼ êµ‰ì¥íˆ ìœ ì‚¬í•œ í˜•íƒœì˜ 8ê°œì˜ í’ˆì‚¬ë¥¼ ì œì‹œí•˜ì˜€ë‹¤. ì§€ê¸ˆë„ 8ê°œì§€ë§Œ, ê°íƒ„ì‚¬ì™€ í˜•ìš©ì‚¬ ë“±ì´ ì¶”ê°€ë˜ê³  ëª‡ëª‡ ìš”ì†Œê°€ ë¹ ì¡Œë‹¤. ì´ë¥¼ NLP ê³¼ì •ì—ì„œ inputìœ¼ë¡œ í™œìš©í•˜ê²Œ ë˜ë©´ ì–¸ì–´ì˜ ëª¨í˜¸ì„±ì„ í•´ê²°í•˜ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤. í’ˆì‚¬ë¥¼ í†µí•´ì„œ ë‹¨ì–´ê°€ ê°€ì§€ëŠ” ëœ»ì˜ ë²”ìœ„ê°€ ë” ì¤„ì–´ë“¤ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ê° ë‹¨ì–´ë§ˆë‹¤ í‘œì‹œí•˜ëŠ” ì ˆì°¨ë¥¼ preprocessingìœ¼ë¡œ ì§„í–‰í•˜ëŠ” ê²½ìš°ë„ ë§ë‹¤.\n\nìš°ì„  POSì˜ ì¼ë°˜ì ì¸ ì¢…ë¥˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n- Noun(ëª…ì‚¬)\n- Verb(ë™ì‚¬)\n- Adjective(í˜•ìš©ì‚¬)\n- Adverb(ë¶€ì‚¬)\n- Preposition(ì „ì¹˜ì‚¬)\n- Conjunction(ì ‘ì†ì‚¬)\n- Pronoun(ëŒ€ëª…ì‚¬)\n- Interjection(ê°íƒ„ì‚¬)\n\ní•˜ì§€ë§Œ, Computer Scienceì—ì„œëŠ” ì´ë¥¼ ì¢€ ë” ëª…í™•í•˜ê²Œ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ ë” ë§ì€ ë¶„ë¥˜(tag)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ëŒ€í‘œì ì¸ ì˜ˆì‹œê°€ [ğŸ”— Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)ì´ë‹¤. ì—¬ê¸°ì„œëŠ” 36ê°œì˜ ì¢…ë¥˜ë¥¼ í™œìš©í•˜ì—¬ í‘œê¸°í•œë‹¤. ì´ì™¸ì—ë„ Brown Corpus ë“± ë‹¤ì–‘í•œ tagging ë°©ë²•ì´ ìˆë‹¤. ë˜í•œ, ì–¸ì–´ì— ë”°ë¼ì„œëŠ” ë³„ë„ì˜ í’ˆì‚¬ë¥¼ ì •ì˜í•˜ëŠ” ê²½ìš°ë„ ë§ê¸° ë•Œë¬¸ì— ì–¸ì–´ë§ˆë‹¤ ì ì ˆí•œ ë°©ì‹ì„ ì‚¬ìš©í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n\ntagë¥¼ ì •í•  ë•Œ ì¼ë°˜ì ì¸ ê·œì¹™ì€ ìš°ë¦¬ê°€ ì¤‘/ê³ ë“±í•™êµ ì‹œê°„ì— ë°°ì› ì„ ë¬¸ë²• ìš”ì†Œë¥¼ ì ìš©í•œ ê²ƒì´ ë§ë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ë©´ ëœë‹¤. NNS ê°™ì€ ê²½ìš°ëŠ” ë³µìˆ˜ëª…ì‚¬ ë’¤ì— ë¶™ì€ së¥¼ í¬í•¨í•˜ëŠ” tagë¥¼ ì˜ë¯¸í•˜ê³ , VBDëŠ” ë™ì‚¬ ê³¼ê±°í˜•ì„ ì˜ë¯¸í•œë‹¤. ì´ì™€ ê°™ì€ í˜•íƒœë¡œ í’ˆì‚¬ë¥¼ ì¢€ ë” ì„¸ë¶„í™”í•œ ê²ƒ ì™¸ì—ëŠ” ì°¨ì´ê°€ ì—†ë‹¤.\n\n### How can I get?\n\nê·¸ë ‡ë‹¤ë©´, ì–´ë–»ê²Œ í•˜ë©´ POS taggingëœ ë°ì´í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ì§€ê°€ ê¶ê¸ˆí•  ê²ƒì´ë‹¤. ì‹ ê¸°í•˜ê²Œë„ ê°€ì¥ ì‰¬ìš´ ì¶”ë¡ ì„ í•˜ë”ë¼ë„ 90%ì˜ ì •í™•ë„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì´ë‹¤.\n\n1. ë‹¨ì–´ê°€ ê°€ì§€ëŠ” í’ˆì‚¬ ì¤‘ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ê²ƒì„ í‘œê¸°í•œë‹¤.\n2. ëª» ë³¸ ë‹¨ì–´ì¸ ê²½ìš° Noun(ëª…ì‚¬)ë¡œ í‘œê¸°í•œë‹¤.\n\nì´ê²ƒì´ ê°€ëŠ¥í•œ ì´ìœ ëŠ” ì‚¬ì‹¤ìƒ ëŒ€ë¶€ë¶„ì˜ wordëŠ” ëª¨í˜¸í•˜ì§€ ì•Šë‹¤ëŠ” ì ì´ë‹¤. ëŒ€ë¶€ë¶„ì˜ wordëŠ” í’ˆì‚¬ ì•ì—ì„œëŠ” ê·¸ë ‡ê²Œ ë³€í™”ë¬´ìŒí•˜ì§€ ì•Šë‹¤. **í•˜ì§€ë§Œ,** íŠ¹ì • wordëŠ” ì‚¬ëŒ ì¡°ì°¨ë„ í—·ê°ˆë¦¬ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ëŒ€ê²Œ í†µê³„ì ìœ¼ë¡œ 11%ì •ë„ëŠ” ì‚¬ëŒ ì¡°ì°¨ë„ í—·ê°ˆë¦´ ìˆ˜ ìˆëŠ” í˜•íƒœì˜ í’ˆì‚¬ê°€ ì£¼ì–´ì§„ë‹¤ê³  í•œë‹¤. ê·¸ë˜ì„œ, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ Statistic Inferenceë¥¼ í™œìš©í•˜ëŠ” ê²½ìš°ê°€ ìˆê³ , ìš°ë¦¬ê°€ ì• ì„œ ë°°ì› ë˜ HMMì„ í™œìš©í•˜ë©´ 97%, MaxEntë¥¼ í™œìš©í•˜ë©´ 99% ì •í™•ë„ë¥¼ ê°€ì§€ëŠ” taggerë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ë¬¼ë¡  ë” ë³µì¡í•œ Deep Learningì„ í™œìš©í•œë‹¤ë©´ ë” ë†’ì€ ì„±ëŠ¥ë„ ê°€ëŠ¥ì€ í•  ê²ƒì´ë‹¤.\n\n## Morphology\n\nMorphology ë‹¨ê³„ì—ì„œ POS taggingì´ ì¤‘ìš”í•˜ê¸´ í•˜ì§€ë§Œ ë” ë‚˜ì•„ê°ˆ í•„ìš”ê°€ ìˆë‹¤. ê²°êµ­ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë” ì™„ë²½í•˜ê²Œ ì°¾ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ëŒ€ê²Œì˜ ê²½ìš° POS taggingì„ í¬í•¨í•˜ëŠ” Morphology taggingì„ ìˆ˜í–‰í•œë‹¤. íŠ¹ì • ë‹¨ì–´ë¥¼ ì‚¬ì „í˜• ê¸°ë³¸í˜•(lemma) ë˜ëŠ” ë” ë‚˜ì•„ê°€ ê°€ì¥ ë¿Œë¦¬ê°€ ë˜ëŠ” ìš”ì†Œ rootì™€ stemìœ¼ë¡œ ë‚˜ëˆ„ê³  ì—¬ê¸°ì— í’ˆì‚¬ë¥¼ ë§ë¶™ì´ëŠ” í˜•íƒœì´ë‹¤. ìš°ë¦¬ê°€ ì–»ì€ í’ˆì‚¬(tag)ì™€ lemmaë§Œ ê°–ê³ ë„ ìš°ë¦¬ëŠ” ì›ë˜ ë‹¨ì–´ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ê³ , ëœ»ì˜ ë²”ìœ„ë¥¼ ë” í•œì •í•  ìˆ˜ ìˆë‹¤. ë” ë‚˜ì•„ê°€ rootì™€ stemìœ¼ë¡œ ë‚˜ëˆ„ê²Œ ë˜ë©´ ë³´ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ë” ë©´ë°€í•œ ì˜ë¯¸ íŒŒì•…ì´ ê°€ëŠ¥í•´ì§„ë‹¤. ì´ë¥¼ êµ¬í˜„í•  ë•Œì—ëŠ” ëŒ€ê²Œ 4ê°€ì§€ ë°©ë²• ì¤‘ì— í•˜ë‚˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\n\n1. Word form list  \n   ê°„ë‹¨í•˜ê²Œ ìƒê°í•˜ë©´, word listì—ì„œ ë‹¨ì–´ë¥¼ ì¡°íšŒí•˜ëŠ” ë°©ì‹ì´ë‹¤. ëŒ€ê²Œ key, valueë³´ë‹¤ëŠ” Trie í˜•íƒœë¡œ ë‹´ëŠ” ê²ƒì„ ì„ í˜¸í•œë‹¤. TrieëŠ” ê° nodeê°€ sequence ë°ì´í„°ì˜ ìš”ì†Œ í•˜ë‚˜í•˜ë‚˜ê°€ ë˜ëŠ” treeë¥¼ ì˜ë¯¸í•˜ë©°, sequence ë°ì´í„°ì˜ ì¡°íšŒë¥¼ ìœ„í•´ ì‚¬ìš©ëœë‹¤.\n2. Direct coding  \n   rootì™€ stemì„ ì°¾ëŠ” ê³¼ì •ì€ ì‚¬ì‹¤ ì˜ì–´ì—ì„œëŠ” ê°„ë‹¨í•˜ë‹¤. ì• ë’¤ì—ì„œ ë¶€í„° ì§„í–‰í•˜ë©´ì„œ ëŒ€í‘œì ì¸ stemì„ ì œê±°í•´ ë‚˜ê°€ë©´, rootë§Œ ë‚¨ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ, ì¼ë¶€ ì¼ë³¸ì–´ì™€ ê°™ì€ ê²½ìš°ì—ëŠ” ì´ê²ƒì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ë„ ìˆë‹¤. ì´ ê²½ìš°ì—ëŠ” ë‹¤ë¥¸ ë°©ì‹ì„ ì ìš©í•´ì•¼ í•œë‹¤.\n3. Finite state machinery  \n   ê° ë‹¨ì–´ì˜ í˜•íƒœë¥¼ FSMìœ¼ë¡œ ì •ì˜í•˜ì—¬ ë³€í•  ìˆ˜ ìˆëŠ” í˜•íƒœì™€ ì´ì— ë”°ë¥¸ í’ˆì‚¬ ë“±ì„ ë¯¸ë¦¬ í‘œí˜„í•˜ì—¬ ì •ì˜í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n4. CFG, DATR, Unification  \n   ì–¸ì–´í•™ì— ê¸°ë°˜í•œ ë¶„ì„ë²•ì´ë‹¤.\n\nì‚¬ì‹¤ ì´ëŸ¬í•œ ë°©ë²•ì„ ì§ì ‘ êµ¬í˜„í•˜ëŠ” ê²ƒì€ í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìˆëŠ” POS taggerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í˜„ëª…í•  ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” POS taggerëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì´ ìˆë‹¤.\n\n| Library | Language | ProgrammingLanguage |\n| :------ | :------- | :------------------ |\n| NLTK    | English  | Python              |\n| spaCy   | English  | Python              |\n| KoNLPy  | í•œê¸€     | Python              |\n\n## Syntactic Analysis\n\nMorphology ë‹¨ê³„ì—ì„œëŠ” ê° wordì˜ ëœ»ì„ ë‹¤ë£¨ì—ˆë‹¤ë©´, ì´ ë‹¨ê³„ëŠ” wordì˜ ê²°í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§€ëŠ” ë¬¸ì¥ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ëŠ” ë‹¨ê³„ì´ë‹¤. ë¬¸ì¥ êµ¬ì¡°ë¥¼ ë¶„ì„(êµ¬ë¬¸ ë¶„ì„)í•˜ëŠ” ë°©ë²•ì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰˜ì–´ì§„ë‹¤.\n\n1. <mark>**Phrase Structure**</mark>  \n   ë¬¸ì¥ì„ Phrase(êµ¬) ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ êµ¬ì¡°í™” ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. ë‹¨ì–´ ê° ê°ì˜ í’ˆì‚¬ì—ì„œ ë¶€í„° ì‹œì‘í•˜ì—¬ ì´ë“¤ì„ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ë¬¸ì¥ ìš”ì†Œ(ëŒ€ê²Œ phrase)ë¥¼ ë§Œë“¤ì–´ í•˜ë‚˜ì˜ ë¬¸ì¥ì„ ë§Œë“œëŠ” êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.\n2. <mark>**Dependency Structure**</mark>  \n   ë¬¸ì¥ì—ì„œ ê° ë‹¨ì–´ê°€ ê°€ì§€ëŠ” ì˜ì¡´ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¸ êµ¬ì¡°ì´ë‹¤.\n\nê° êµ¬ì¡°ëŠ” ë‘˜ë‹¤ Tree í˜•íƒœë¡œ ì´ë£¨ì–´ì§€ë©°, ë¶„ì„í•˜ëŠ” ë°©ë²•ë„ ì„œë¡œ ë§¤ìš° ë‹¤ë¥´ë‹¤. ê° ë°©ë²•ì€ ë°‘ì—ì„œë¶€í„° ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.\n\n### Phrase Structure\n\në¬¸ì¥ì„ ì´ë£¨ëŠ” ìš”ì†Œë“¤ê³¼ ìš”ì†Œë“¤ì˜ êµ¬ì¡°í™” ê·œì¹™ì„ ì •ì˜í•´ì•¼ ìš°ë¦¬ëŠ” ì´ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ì •ì˜í•œ ê²ƒì„ Grammarë¼ê³  í•œë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ ìœ„í•´ì„œ ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ **CFG**ì´ë‹¤. **CFG**ëŠ” Context Free Grammarì˜ ì•½ìë¡œ, ëª¨ë“  ì˜ êµ¬ì¡°í™”ëœ ë¬¸ì¥ë“¤ì„ ì •ì˜í•  ìˆ˜ ìˆëŠ” ê·œì¹™ë“¤ì„ ì˜ë¯¸í•œë‹¤. ê° ê°ì˜ Ruleì€ ì™¼ìª½ì—ëŠ” ë¬¸ë²•ì  typeì´ ì£¼ì–´ì§€ê³ , ì˜¤ë¥¸ìª½ì—ëŠ” ì´ë¥¼ ì´ë£¨ëŠ” ìš”ì†Œë“¤ì´ ì •ì˜ë˜ì–´ì§„ë‹¤. ê° ìš”ì†ŒëŠ” í•˜ìœ„ ë¬¸ë²•ì  type ë˜ëŠ” ì´ì „ì— ì œì‹œí•œ POSê°€ ë  ìˆ˜ ìˆë‹¤.\n\nê°€ì¥ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì–´ì§€ëŠ” ë¬¸ë²•ì  typeë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ì´ì™¸ì—ë„ ê¸°ìˆ í•˜ì§€ ì•Šì€ POSë„ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤.\n\n| Symbol  | Mean               | Korean   |\n| :------ | :----------------- | :------- |\n| NP      | Noun Phrase        | ëª…ì‚¬ êµ¬  |\n| VP      | Verb Phrase        | ë™ì‚¬ êµ¬  |\n| S       | Sentence           | ë¬¸ì¥     |\n| DET(DT) | Determiner         | ê´€ì‚¬     |\n| N       | NOUN               | ëª…ì‚¬     |\n| V       | Verb               | ë™ì‚¬     |\n| PREP    | Preposition        | ì „ì¹˜ì‚¬   |\n| PP      | Preposition Phrase | ì „ì¹˜ì‚¬êµ¬ |\n\nì´ì— ë”°ë¼ ëŒ€í‘œì ì¸ Ruleì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n- S -> NP VP\n- NP -> (DT) N\n- NP -> N\n- VP -> V (NP)\n\nìœ„ì— ì œì‹œëœ Ruleì€ ê°€ì¥ ê¸°ë³¸ì ì¸ ê·œì¹™ìœ¼ë¡œ ì—¬ê¸°ì„œ ë” í™•ì¥ëœ ê·œì¹™ì„ ë§Œë“¤ì–´ì„œ Parsingì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ì´ë ‡ê²Œ ê·œì¹™ì„ ë§Œë“¤ì–´ì„œ ìˆ˜í–‰ì„ í•˜ê²Œ ë˜ë©´ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ë°”ë¡œ ì—¬ëŸ¬ ê°œì˜ Parsing Resultê°€ ë§Œë“¤ì–´ì¡Œì„ ë•Œ ì´ ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ ê°€ì¥ ì ì ˆí•œì§€ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ì¦‰, ë„ˆë¬´ êµ¬ì²´ì ì¸ Ruleì„ ë§Œë“¤ê¸°ì—ëŠ” Parsingì´ í•˜ë‚˜ë„ ë˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì´ ë§Œë“¤ì–´ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ê³ , ê·¸ë ‡ë‹¤ê³  ë„ˆë¬´ ì ì€ Ruleì„ ì ìš©í•˜ê²Œ ë˜ë©´ Parsingì´ ë„ˆë¬´ ë§ì´ ë§Œë“¤ì–´ì§€ê²Œ ëœë‹¤.\n\në”°ë¼ì„œ, ê²°ë¡ ì ìœ¼ë¡œ ë§í•˜ìë©´ ìœ„ì™€ ê°™ì€ í˜•íƒœì˜ CFGë¡œëŠ” phrase structureë¥¼ êµ¬ì¡°í™”í•˜ëŠ”ë° í•œê³„ê°€ ìˆë‹¤ëŠ” ê²°ë¡ ì„ ë‚´ë¦¬ê²Œ ëœë‹¤. ê²°êµ­ ì•„ë˜ì™€ ê°™ì€ ë‘ ê°œì˜ ë¬¸ì œì ì— ì§ë©´í•˜ê²Œ ë˜ê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ ê° ê° ì œì‹œëœë‹¤.\n\n1. Repeated work  \n   ë¬¸ì¥ êµ¬ì¡°ê°€ ë™ì¼í•œ ê²½ìš° ê²°êµ­ ë™ì¼í•œ ì‘ì—…ì„ ë°˜ë³µí•˜ê²Œ ëœë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ Treebankë¼ëŠ” êµ¬ì¡°ë¥¼ ë„ì…í•˜ê³ , ì´ê²ƒì˜ ì¼ë¶€ë¥¼ Dynamic Programmingì˜ Memoizationì²˜ëŸ¼ ì €ì¥í•´ë‘ì—ˆë‹¤ê°€ ì“°ëŠ” ë°©ì‹ì„ ì ìš©í•œë‹¤. ì¦‰, ê¸°ì¡´ì—ëŠ” Ruleë§Œì„ ì €ì¥í•˜ê³ , ë•Œì— ë”°ë¼ ì´ë¥¼ ì ìš©í•˜ì˜€ë‹¤ë©´, ì´ì œëŠ” ëª¨ë“  ë‹¨ì–´ì˜ í’ˆì‚¬ì™€ êµ¬ì¡°ë¥¼ ê¸°ë¡í•´ë‘ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ í†µí•´ì„œ ì´ë¯¸ ë‚˜ì™”ë˜ ì‘ì—…ì˜ ê²½ìš° ë¹ ë¥¸ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•´ì§„ë‹¤.\n2. Choosing the correct parse  \n   ìœ„ì—ì„œ ë§í–ˆë˜ ê²ƒì²˜ëŸ¼ ìš°ë¦¬ëŠ” ê²°êµ­ <mark>ê°€ì¥ ì ì ˆí•  ê±° ê°™ì€ parsing resultë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤.</mark> Ruleì— ê¸°ë°˜í•œ ë°©ì‹ìœ¼ë¡œëŠ” í•œê³„ê°€ ìˆì§€ë§Œ ìš°ë¦¬ê°€ Statisticí•œ ë°©ì‹ì„ í™œìš©í•œë‹¤ë©´ ì´ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” CFGì—ì„œ ë‚˜ì•„ê°€ PCFG(Probabilistic CFG)ë¥¼ ì ìš©í•˜ì—¬ ì´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ Statisticalí•œ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ì„œë„ Treebank êµ¬ì¡°ê°€ í•„ìš”í•˜ë‹¤.\n\n![nlp-cfg-treebank](/images/nlp-cfg-treebank.jpg)\n\nì´ì œë¶€í„°ëŠ” ì‹¤ì œë¡œ PCFGë¥¼ ì–´ë–»ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ìì„¸íˆ ë‹¤ë¤„ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n#### PCFG\n\nì• ì„œ ì–˜ê¸°í•œ ê²ƒì²˜ëŸ¼ Probabilistic CFGë¡œ, ê° Ruleë§ˆë‹¤ Probabilityë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ìœ ì˜í•  ê²ƒì€ ë‹¤ìŒ ë‚´ìš©ì´ë‹¤.\n\n1. ê¸°ì¡´ ì •ì˜í•œ ë¬¸ë²•ì  typeì„ ë§Œë“œëŠ” Ruleì— ê° ê°ì˜ í™•ë¥ ì„ ì •ì˜í•œë‹¤.\n2. ì´ë•Œ ê° ë¬¸ë²•ì  typeì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” Ruleì˜ í™•ë¥ ì˜ í•©ì€ ë°˜ë“œì‹œ 1ì´ë‹¤.\n3. ë˜í•œ, ê° ë‹¨ì–´ê°€ íŠ¹ì • POSì¼ í™•ë¥ ë„ ê°™ì´ êµ¬í•´ì•¼ í•œë‹¤.  \n   ex. N -> fish (0.5), V -> fish (0.1)  \n   (ì‹¤ì œë¡œ ì´ë ‡ê²Œ í¬ê²Œ ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤. N ë˜ëŠ” V ì¼ ë•Œ, Fishì¼ í™•ë¥ ì´ë¯€ë¡œ êµ‰ì¥íˆ ì‘ì€ ê°’ì´ ë‚˜ì˜¤ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.)\n\në”°ë¼ì„œ, ì–´ë–¤ treebankê°€ ë” ì ì ˆí•œ ì§€ëŠ” ê° ê°ì˜ treebankì˜ ëª¨ë“  Ruleì˜ í™•ë¥ ì˜ ê³±ì„ êµ¬í•´ì„œ ë¹„êµí•˜ë©´ ëœë‹¤. êµ‰ì¥íˆ ì‰½ê²Œ ì´ ê³¼ì •ì´ ê°€ëŠ¥í•œ ê²ƒì´ë‹¤. ì•„ë˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œì´ë‹¤.\n\n![nlp-pcfg](/images/nlp-pcfg.jpg)\n\nì´ë ‡ê²Œ ì£¼ì–´ì¡Œì„ ë•Œ, $p(t_1)$ê³¼ $p(t_2)$ëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\np(t_{1}) &= 1.0 \\\\\n&\\times 0.3 \\times 0.6 \\\\\n&\\times 0.4 \\\\\n&\\times 0.5 \\times 0.1 \\times 0.4 \\times 1.0 \\times 0.4  \\\\\n&= 0.000576 \\\\\n\\\\\np(t_{2}) &= 1.0 \\\\\n&\\times 0.3 \\times 0.4 \\\\\n&\\times 1.0 \\\\\n&\\times 0.4 \\\\\n&\\times 0.5 \\times 0.6 \\times 1.0 \\times 1.0 \\times 0.4 \\\\\n&= 0.00576 \\\\\n\\\\\n\\therefore p(t_{1}) &\\lt p(t_{2})\n\\end{align*}\n$$\n\në”°ë¼ì„œ, $t_{2}$ í˜•íƒœê°€ ë” ì ì ˆí•˜ë‹¤ê³  íŒë³„í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n```plaintext\n ğŸ¤” Chomsky Normal Form\n\n ê¸°ì¡´ CFGì˜ í˜•íƒœì˜ ëª¨í˜¸í•¨ì„ ì œê±°í•˜ê³ , ì¢€ ë” ëª…í™•í•œ í˜•íƒœë¡œ ì •ì˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n ëŒ€í‘œì ìœ¼ë¡œ ëª¨í˜¸í•œ ë‚´ìš©ì´ Sentenceì•ˆì— Sentenceë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°(n-ary)ë¼ë“ ì§€,\n ëª…ë ¹ë¬¸ê³¼ ê°™ì€ ë¬¸ì¥ì„ ìœ„í•œ ì£¼ì–´ ì‚­ì œ(unary/empty) ë“±ì´ ì¡´ì¬í•œë‹¤.\n ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ recursive í˜•íƒœë‚˜ empty í˜•íƒœ ë“±ì„ ì œê±°í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n\n ê²°ë¡ ìƒ PCFGì—ì„œëŠ” í™•ë¥  í‘œê¸°ì‹œì— ëª¨í˜¸í•œ í‘œê¸°ë¥¼ ì œê±°í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.\n```\n\n#### CKY Parsing\n\nì• ì„œ ìš°ë¦¬ê°€ treebank ì¤‘ì—ì„œ ë” í° í™•ë¥ ê³± ê°’ì„ ê°€ì§€ëŠ” ê²ƒì´ ìµœì ê°’ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. í•˜ì§€ë§Œ, ì‚¬ì‹¤ ì´ ê³¼ì •ì´ ê·¸ë ‡ê²Œ ì‰½ì§€ëŠ” ì•Šë‹¤. ì™œëƒí•˜ë©´, ìš°ë¦¬ê°€ ê°€ì§€ëŠ” Parsing ResultëŠ” êµ‰ì¥íˆ ë§ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ë¥¼ ì—°ì‚°í•˜ëŠ” ë¹„ìš©ì´ êµ‰ì¥íˆ ë¹„ì‹¸ì§„ë‹¤. ì´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì—°ì‚°í•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì œì‹œëœ ê²ƒì´ CKY Parsingì´ë‹¤.\n\nPseudo codeëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n```javascript\nfunction CKY(words, grammar) returns [scores, backpointers]\n  // score[i][j] = \n  // ëª¨ë“  Symbol(ë¬¸ë²•ì  type, ex. S, NP, VP)ì— ëŒ€í•˜ì—¬ \n  // ië¶€í„° jê¹Œì§€ wordë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ìµœëŒ“ê°’ì„ ì €ì¥\n  score = new double[#(words) + 1][#(words)+1][#(Symbol)]\n  // back[i][j] = \n  // ëª¨ë“  Symbol(ë¬¸ë²•ì  type, ex. S, NP, VP)ì— ëŒ€í•˜ì—¬ \n  // ië¶€í„° jê¹Œì§€ wordë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ìµœëŒ“ê°’ì„ ë§Œë“œëŠ” ìš”ì†Œì˜ ìœ„ì¹˜ë¥¼ ì €ì¥\n  // (=back pointer)\n  back = new Pair[#(words)+1][#(words)+1][#(Symbol)]\n  for (i=0; i < #(words); i++)\n    // ì´ˆê¸°í™” ë‹¨ê³„ë¡œ ê° ë‹¨ì–´ê°€ Symbolì¼ í™•ë¥ ì„ ì…ë ¥\n    for (A in Symbol)\n      if A -> words[i] in grammar\n        score[i][i+1][A] = P(A -> words[i])\n    // unary ì¦‰ ìƒëµë˜ì–´ì„œ í‘œí˜„ë˜ëŠ” ê²½ìš°ë¥¼ ìœ„í•´ì„œ í™•ë¥  ì¬ê³„ì‚°\n    // ex. Stop!! (S->VP,VP->V)\n    boolean added = true\n    while (added)\n      added = false\n      for A, B in Symbol\n        if score[i][i+1][B] > 0 && A -> B in grammar\n          prob = p(A -> B) * score[i][i+1][B]\n          if prob > score[i][i+1][A]\n            score[i][i+1][A] = prob\n            back[i][i+1][A] = B\n            added = true\n  for (span = 2 to #(words))\n    for (begin = 0 to #(words) - span)\n      // ì¼ë°˜ì ì¸ ë‘ í•­ì˜ í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§€ëŠ” ê²½ìš°ë¥¼ ê³„ì‚°\n      end = begin + span\n      for (split = begin + 1 to end-1)\n        for (A, B, C in Symbol)\n          prob = score[begin][split][B] * score[split][end][C]*P(A -> B C)\n          if prob > score[begin][end][A]\n            score[begin][end][A] = prob\n            back[begin][end][A] = new Triple(split, B, C)\n      // unaryì¸ ê²½ìš°ë¥¼ ê³ ë ¤í•´ì„œ ì¬ê³„ì‚°\n      boolean added = true\n      while (added)\n        added = false\n        for (A, B in Symbol)\n          prob = P(A -> B) * score[begin][end][B]\n          if prob > score[begin][end][A]\n            score[begin][end][A] = prob\n            back[begin][end][A] = B\n            added = true\n  return score, back\n```\n\nì „ì²´ì ì¸ ë™ì‘ê³¼ì •ì€ ê·¸ë¦¼ì„ í†µí•´ì„œ ì´í•´í•  ìˆ˜ ìˆë‹¤. ë¨¼ì €ì´ˆê¸° score í• ë‹¹ë¶€í„° ì²« ë‹¨ê³„ì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ê¸°ê¹Œì§€ëŠ” ì•„ë˜ ê·¸ë¦¼ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.\nê° ê·¸ë¦¼ì„ ë‹¤ìŒì„ ì˜ë¯¸í•œë‹¤.\n\n1. scoreë¥¼ ìœ„í•œ ê³µê°„ í• ë‹¹\n2. scoreì— ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” Symbol -> word í™•ë¥  ì…ë ¥\n3. unari caseë¥¼ í™•ì¸í•´ì„œ í™•ë¥  ì…ë ¥\n\n![nlp-cky-1](/images/nlp-cky-1.png)\n\nê·¸ ë‹¤ìŒ ë‹¨ê³„ë¡œëŠ” ë‹¨ê³„ì ìœ¼ë¡œ ê´€ê³„ë¥¼ ì ë¦½í•œë‹¤.\n\n1. ê°™ì€ í˜•ê´‘íœìœ¼ë¡œ ì¹ í•´ì§„ ë°ì´í„°ê°„ ê´€ê³„ê°€ ìµœëŒ“ê°’ì„ ê°€ì§„ë‹¤.\n2. unari caseë„ í™•ì¸í•œ ê²°ê³¼ S->VPê°€ ì´ˆê¸°í™” ëœë‹¤.\n\n![nlp-cky-2](/images/nlp-cky-2.png)\n\në§ˆì§€ë§‰ì—ì„œ ë‹¤ì‹œ ê´€ê³„ë¥¼ ì •ë¦¬í•  ë•Œ, ìœ ì˜í•  ì ì´ ìˆë‹¤. ë°”ë¡œ score\\[0\\]\\[3\\]ì™€ score\\[1\\]\\[4\\]ë„ ì¤‘ìš”í•˜ì§€ë§Œ score\\[0\\]\\[2\\]ì™€ score\\[2\\]\\[4\\]ì— ì˜í•œ ê´€ê³„ë„ ë°˜ë“œì‹œ ìœ ì˜í•´ì„œ ë³´ì•„ì•¼ í•œë‹¤.\n\n![nlp-cky-3](/images/nlp-cky-3.png)\n\n#### modeling\n\nì›ë˜ë¼ë©´, modeling ë‹¨ê³„ë„ ë‹¤ë£¨ì–´ì•¼í•˜ì§€ë§Œ, í•´ë‹¹ ë‹¨ê³„ì—ì„œëŠ” ë„˜ì–´ê°€ë„ë¡ í•œë‹¤. ì´ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ê°„ë‹¨í•˜ê²ŒëŠ” ë‹¨ìˆœíˆ ë¹ˆë„ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒë¶€í„° EM algorithmì„ í™œìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì´ ìˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•Šê² ë‹¤.\n\n### Dependency Structure\n\në¬¸ì¥ì—ì„œ ê° ë‹¨ì–´ì˜ ì˜ì¡´ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” Dependency StructureëŠ” ì¤‘ì‹¬ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” wordë¡œ ë¶€í„° ì´ì— ì˜ì¡´í•˜ëŠ” wordë“¤ì˜ ê´€ê³„ë¡œ í™•ì¥ë˜ë©° í‘œê¸°ëœë‹¤. ë”°ë¼ì„œ, ë¬¸ì¥ì—ì„œëŠ” ëŒ€ê²Œ ë™ì‚¬ê°€ ì¤‘ì‹¬ì´ ë˜ê³ , ê·¸ë¦¬ê³  ê·¸ ë‹¤ìŒìœ¼ë¡œëŠ” ì „ì¹˜ì‚¬, ëª…ì‚¬ ë“±ì´ ë’¤ë¥¼ ì‡ê²Œ ëœë‹¤. ì´ë¥¼ íŒŒì•…í•˜ê²Œ ë˜ë©´, ë‹¨ì–´ê°€ ì—°ê´€ì„±ê³¼ ì „ì²´ì ì¸ êµ¬ì¡°ì˜ ì•ˆì •ì„± ë“±ì„ íŒŒì•…í•˜ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤.\n\nì´ í˜•íƒœë¥¼ ì–»ê¸° ìœ„í•´ì„œ í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì´ ìˆë‹¤.\n\n1. Dynamic Programming  \n   ì•„ì£¼ ì‰½ê²Œ ìƒê°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ìœ¼ë¡œ **PCFGë¥¼ í™œìš©**í•˜ëŠ” ê²ƒì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ PCFGë¥¼ í™œìš©í•˜ì—¬ tree í˜•íƒœë¥¼ êµ¬ì¶•í•˜ë©´ ì´ë¥¼ ì´ìš©í•˜ì—¬ Dependency Structureë¥¼ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤. ë‹¨ìˆœíˆ treeì˜ ì•„ë˜ì„œ ë¶€í„° ì˜ì¡´ ê´€ê³„ë¥¼ ê°€ì§„ ë‹¨ì–´ë¥¼ ê³ ë¥´ë©´ì„œ rootê¹Œì§€ ì˜¬ë¼ì˜¤ë©´ ì´ê²ƒìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤. í•˜ì§€ë§Œ, ì´ ê³¼ì •ì€ ì‹œê°„ì  ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.\n2. Graph Algorithm  \n   **ê°€ì¥ ì •í™•ë„ê°€ ë†’ì€ ë°©ì‹**ìœ¼ë¡œ Sentenceì— ëŒ€í•œ Minimum Spanning Treeë¥¼ êµ¬ì„±í•˜ê³ , ì´ë¥¼ í™œìš©í•˜ì—¬ ML classifierë¥¼ ì œì‘í•˜ì—¬ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. ê°€ì¥ ë†’ì€ ì •í™•ë„ë¥¼ ì›í•œë‹¤ë©´ í•´ë‹¹ ë°©ì‹ì„ í™œìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.\n3. Constraint Satisfaction  \n   ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ ë§Œë“¤ê³  ê±°ê¸°ì„œ ì œí•œì‚¬í•­ì„ ë§Œì¡±í•˜ì§€ ì•ŠëŠ” êµ¬ì¡°ë¥¼ ì œê±°í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ ë˜í•œ ë§ì´ ì‚¬ìš©ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.\n4. Deterministic Parsing  \n   Greedy algorithmì— ê¸°ë°˜í•˜ì—¬ êµ¬í˜„ëœ ë°©ì‹ìœ¼ë¡œ ë§¤ìš° ë†’ì§€ëŠ” ì•Šì§€ë§Œ ì ì ˆí•œ ì •í™•ë„ì— **ë¹ ë¥¸ ì†ë„**ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì— ë§ì´ ì‚¬ìš©ë˜ì–´ì§„ë‹¤.\n\n#### Malt Parser\n\nì—¬ê¸°ì„œëŠ” Deterministic Parsing ì¤‘ì—ì„œ ê°€ì¥ ì‰¬ìš´ ë°©ë²• ì¤‘ì— í•˜ë‚˜ì¸ Malt Parserë¥¼ ì¢€ ë” ë‹¤ë¤„ë³´ë„ë¡ í•˜ê² ë‹¤.\n\nì´ëŠ” 3ê°œì˜ ìë£Œ êµ¬ì¡°ì™€ 4ê°œì˜ actionì„ í†µí•´ì„œ ì •ì˜ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.  \në¨¼ì € ìë£Œêµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. stack($\\sigma$)  \n   dependency treeì˜ ìƒìœ„ ìš”ì†Œë¥¼ ì €ì¥í•´ë‘ëŠ” ê³µê°„ìœ¼ë¡œ, ì²˜ìŒì—ëŠ” ROOTë¼ëŠ” ìš”ì†Œë¥¼ ê°–ê³  ì‹œì‘í•œë‹¤.\n2. buffer($\\beta$)  \n   input sequenceë¥¼ ì €ì¥í•˜ëŠ” ê³µê°„ìœ¼ë¡œ, ì²˜ìŒì—ëŠ” input sequenceë¥¼ ì „ì²´ë¥¼ ì €ì¥í•˜ê³  ìˆë‹¤.\n3. arcs($A$)  \n   ìµœì¢…ìœ¼ë¡œ ë§Œë“¤ê³ ì í•˜ëŠ” dependency treeë¥¼ ì˜ë¯¸í•œë‹¤. ì²˜ìŒì—ëŠ” ë¹„ì–´ ìˆëŠ” ìƒíƒœë¡œ ì‹œì‘í•œë‹¤.\n\nactionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. Reduce  \n   stack($\\sigma$)ì—ì„œ wordë¥¼ popí•œë‹¤.\n2. Shift  \n   buffer($\\beta$)ì—ì„œ stack($\\sigma$)ìœ¼ë¡œ wordë¥¼ pushí•œë‹¤. ì´ë•Œ ë¬¸ì¥ì˜ ì•ì˜ ë‹¨ì–´ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ì „ë‹¬í•œë‹¤.\n3. Left-Arc  \n   stack($\\sigma$)ì˜ í˜„ì¬ wordê°€ buffer($\\beta$)ì˜ ë‹¤ìŒ wordì— ì˜ì¡´í•˜ëŠ” ê²½ìš°, ì´ ê´€ê³„ë¥¼ ì—°ê²°í•˜ì—¬ arcs($A$)ì— ì €ì¥í•œë‹¤.  \n   ê²°ë¡ ìƒ stack($\\sigma$)ì—ì„œëŠ” popì´ ë˜ê³ , buffer($\\beta$)ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ë˜ë©°, arcs($A$)ì—ëŠ” depdendencyê°€ í•˜ë‚˜ ì¶”ê°€ëœë‹¤.\n4. Right-Arc  \n   buffer($\\beta$)ì—ì„œ ë‹¤ìŒ wordë¥¼ stack($\\sigma$)ì— pushí•˜ê³ , ê¸°ì¡´ stack($\\sigma$)ì˜ ì´ì „ wordì— ì˜ì¡´í•˜ëŠ” ê´€ê³„ë¥¼ arcs($A$)ì— ì¶”ê°€í•œë‹¤.\n5. Finish  \n   buffer($\\beta$)ì— ë” ì´ìƒ wordê°€ ì—†ë‹¤ë©´, ëª¨ë“  ì—°ì‚°ì„ ë§ˆë¬´ë¦¬í•  ìˆ˜ ìˆë‹¤.\n\nì´ ë˜í•œ ì˜ˆì‹œë¥¼ í†µí•´ì„œ ì•Œì•„ë³´ëŠ” ê²ƒì´ ëª…í™•í•˜ë‹¤.\n\nìš°ë¦¬ê°€ `Happy children like to play with their friends.`ë¥¼ ë¶„ì„í•˜ê³  ì‹¶ë‹¤ê³  í•˜ì. ê·¸ë ‡ë‹¤ë©´, ì ˆì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ëœë‹¤.\n\n| Index | Action    | Stack($\\sigma$)                   | Buffer($\\beta$)        | Arcs($A$)                                      |\n| :---- | :-------- | :-------------------------------- | :--------------------- | :--------------------------------------------- |\n| 0     |           | [ROOT]                            | [Happy, children, ...] | $\\empty$                                       |\n| 1     | Shift     | [ROOT, Happy]                     | [children, like, ...]  | $\\empty$                                       |\n| 2     | LA(amod)  | [ROOT]                            | [children, like, ...]  | {amod(children, happy) = $A_{1}$}              |\n| 3     | Shift     | [ROOT, children]                  | [like, to, ...]        | $A_{1}$                                        |\n| 4     | LA(nsubj) | [ROOT]                            | [like, to, ...]        | $A_{1} \\cup ${nsubj(like, children)} = $A_{2}$ |\n| 5     | RA(root)  | [ROOT, like]                      | [to, play, ...]        | $A_{2} \\cup ${root(ROOT, like)} = $A_{3}$      |\n| 6     | Shift     | [ROOT, like, to]                  | [play, with, ...]      | $A_{3}$                                        |\n| 7     | LA(aux)   | [ROOT, like]                      | [play, with, ...]      | $A_{3} \\cup ${aux(play, to)} = $A_{4}$         |\n| 8     | RA(xcomp) | [ROOT, like, play]                | [with, their,...]      | $A_{4} \\cup ${xcomp(like, play)} = $A_{5}$     |\n| 9     | RA(prep)  | [ROOT, like, play, with]          | [their, friends, .]    | $A_{5} \\cup ${prep(play, with)} = $A_{6}$      |\n| 10    | Shift     | [ROOT, like, play, with, their]   | [friends, .]           | $A_{6}$                                        |\n| 11    | LA(poss)  | [ROOT, like, play, with]          | [friends, .]           | $A_{6} \\cup ${poss(friends, their)} = $A_{7}$  |\n| 12    | RA(pobj)  | [ROOT, like, play, with, friends] | [.]                    | $A_{7} \\cup ${pobj(with, friends)} = $A_{8}$   |\n| 13    | Reduce    | [ROOT, like, play, with]          | [.]                    | $A_{8}$                                        |\n| 14    | Reduce    | [ROOT, like, play]                | [.]                    | $A_{8}$                                        |\n| 15    | Reduce    | [ROOT, like]                      | [.]                    | $A_{8}$                                        |\n| 16    | RA(punc)  | [ROOT, like, .]                   | []                     | $A_{8} \\cup${punc(like, .)} = $A_{9}$          |\n| 17    | Finish    | [ROOT, like, .]                   | []                     | $A_{9}$                                        |\n\nì ì´ëŸ° ì˜ˆì‹œë¥¼ ë³´ì•˜ë‹¤ë©´, ë‹¹ì—°íˆ ê¶ê¸ˆí•´í•  ê²ƒì€ ì–´ë–»ê²Œ Actionì„ ê³ ë¥¼ ê²ƒì¸ê°€ì´ë‹¤. ì´ëŠ” Discriminative classifier ì¦‰, Maxentë‚˜ ì—¬íƒ€ Machine Learning ë°©ë²•ì„ ë™ì›í•˜ì—¬ ê²°ì •í•œë‹¤. PCFGë¥¼ í™œìš©í•˜ëŠ” ë°©ì‹ë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ì•½ê°„ ë‚®ì„ì§€ë¼ë„ ì´ë¥¼ í™œìš©í•˜ë©´ ë§¤ìš° ë¹ ë¥´ê²Œ parsingì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.\n\n```plaintext\n ğŸ¤” Projectivity\n \n ì‚¬ì‹¤ ì—¬íƒœê¹Œì§€ ìš°ë¦¬ëŠ” ì—°ì†ë˜ì–´ ìˆëŠ” wordê°„ì˜ ì˜ì¡´ì„±ì„ íŒŒì•…í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ì•˜ë‹¤.(íŠ¹íˆ PCFG)\n í•˜ì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ë„ ë¶„ëª…íˆ ì¡´ì¬í•œë‹¤. ëŒ€í‘œì ì¸ ì˜ˆì‹œê°€ ì•„ë˜ì´ë‹¤.\n Who did Bill buy the coffee from yesterday?\n ì—¬ê¸°ì„œ fromì€ Whoì™€ ê´€ê³„ê°€ ìˆì§€ë§Œ, ìš°ë¦¬ê°€ ì—¬íƒœê¹Œì§€ ì‚´í´ë³¸ PCFGì™€ Malt Parserë¡œ \n ì´ ê´€ê³„ë¥¼ ë°íˆëŠ”ë°ì—ëŠ” í•œê³„ê°€ ìˆë‹¤.\n ë”°ë¼ì„œ, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ í›„ì²˜ë¦¬ë‚˜ ì¶”ê°€ì ì¸ actionì„ Malt Parserì— ë”í•˜ê±°ë‚˜ \n ì•„ë‹ˆë©´ ì•„ì˜ˆ ë‹¤ë¥¸ ë°©ì‹ì„ ì•™ìƒë¸”í•˜ì—¬ í•´ê²°í•˜ê¸°ë„ í•œë‹¤.\n```\n\n## Semantics\n\nìì„¸íˆ ì—¬ê¸°ì„œ ë‹¤ë£¨ì§€ ì•Šì§€ë§Œ, êµ¬ë¬¸ ë¶„ì„ì„ í†µí•´ ì–»ì€ Treeë¥¼ í†µí•´ì„œ ì–´ë–»ê²Œ ì˜ë¯¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì•Œì•„ë³´ê² ë‹¤. ë¨¼ì €, ìš°ë¦¬ëŠ” ì „ì²´ ìš”ì†Œë¥¼ ë‹¤ì‹œ í•œ ë²ˆ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆˆë‹¤.\n\n1. Entities  \n   íŠ¹ì • ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” í•˜ë‚˜ì˜ ì£¼ì²´ì´ë‹¤. ì£¼ë¡œ NPê°€ ëª¨ë‘ ì—¬ê¸°ì— ì†í•œë‹¤.\n2. Functions  \n   Entity ë˜ëŠ” ë‹¤ë¥¸ Functionì—ê²Œ ë™ì‘, íŠ¹ì„±, ë“±ì„ ì ìš©í•œë‹¤. í˜•ìš©ì‚¬, ë™ì‚¬ ë“±ì´ ì—¬ê¸°ì— ì†í•œë‹¤.\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ `Every nation wants George to love Laura.`ë¼ëŠ” ë¬¸ì¥ì„ ê°–ê³  ìˆë‹¤ë©´, ìš°ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì´ Treeë¥¼ ê·¸ë¦´ ìˆ˜ ìˆê³ , ì´ë¥¼ ì´ìš©í•´ì„œ ì˜ë¯¸ ë¶„ì„ì´ ê°€ëŠ¥í•˜ë‹¤.\n\n![nlp-semantic](/images/nlp-semantic.jpg)\n\nìœ„ Treeë¥¼ ì•„ë˜ì—ì„œë¶€í„° ì—°ê²°í•´ì„œ ë‚˜ê°€ë©´ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì¡°í™”ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n| Index | Expression                                                |\n| :---- | :-------------------------------------------------------- |\n| 1     | love(x, Laura)                                            |\n| 2     | love(x, Laura)                                            |\n| 3     | love(George, Laura)                                       |\n| 4     | want(x, love(George, Laura))                              |\n| 5     | present(want(x, love(George, Laura)))                     |\n| 6     | Every(nation)                                             |\n| 7     | present(want(Every(nation), love(George, Laura)))         |\n| 8     | assert(present(want(Every(nation), love(George, Laura)))) |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Penn Treebank POS tagging, <https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html>\n- spaCy, <https://spacy.io/>\n- NLTK, <https://www.nltk.org/>\n- KoNLPy, <https://konlpy.org/ko/latest/index.html>\n- NLP CFG, <https://tildesites.bowdoin.edu/~allen/nlp/nlp1.html>\n","slug":"nlp-language-parsing","date":"2022-11-07 15:05","title":"[NLP] 8. Language Parsing","category":"AI","tags":["NLP","POS","PCFG","Morphology","Syntax","Semantics"],"desc":"ìš°ë¦¬ê°€ NLì„ ì œëŒ€ë¡œ ë¶„ì„í•˜ê¸° ìœ„í•´ì„œ ê° ë‹¨ì–´ê°€ ê°€ì§„ ì˜ë¯¸ë¥¼ ì•Œì•„ì•¼í•˜ë©°, ì´ë¥¼ ë„˜ì–´ì„œ ë¬¸ì¥ì´ ê°€ì§€ëŠ” ì˜ë¯¸ë¥¼ íŒŒì•…í•´ì•¼ í•œë‹¤. ê²°ë¡ ì ìœ¼ë¡œ ì´ ê³¼ì •ì´ ê³ ë„í™”ëœ NLPë¥¼ ìœ„í•œ í•µì‹¬ ë‹¨ê³„ì´ë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” Rawí•œ í˜•íƒœë¡œ ì£¼ì–´ì§„ textë¥¼ ì²˜ë¦¬í•´ì„œ ë” ë‚˜ì€ í˜•íƒœì˜ êµ¬ì¡°ë¥¼ ë§Œë“¤ í•„ìš”ê°€ ìˆë‹¤. ë”°ì§€ê³  ë³´ë©´ í•˜ë‚˜ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ì¹˜ë§Œ ì´ì „ text processing chapterê³¼ ë‹¤ë¥¸ ì ì€ ë¬¸ì¥ êµ¬ë¶„ê³¼ ê°™ì€ ê°„ë‹¨í•œ ê³¼ì •ì´ ì•„ë‹Œ Linguistic ë‹¨ê³„ì— ë”°ë¥¸ ì²˜ë¦¬ ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë˜í•œ, ê° ë‹¨ê³„ ì—­ì‹œ NLP ì¤‘ì— í•˜ë‚˜ë¼ê³  í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ ë˜í•œ MLê³¼ DLì„ í†µí•´ì„œ ê³ ë„í™”í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤. Morphology ë‹¨ê³„ë¶€í„° ì‹œì‘í•˜ì—¬ Syntax, Semanticê¹Œì§€ ì–´ë–»ê²Œ ë‹¤ë£¨ê²Œ ë˜ëŠ”ì§€ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\ní•´ë‹¹ Postingì—ì„œëŠ” Maximum Entropyë¥¼ ì´ìš©í•˜ì—¬ ìµœì ì˜ parameterë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” Machine Learning ì ‘ê·¼ë²•ì— ê¸°ë°˜í•œ NLP ë°©ì‹ì„ ì œì•ˆí•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ NLë¥¼ ìˆ˜í•™ì ì¸ í˜•íƒœë¡œ ë³€í˜•í•˜ê¸° ìœ„í•œ ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì¸ word2vecì— ëŒ€í•œ ì„¤ëª…ë„ ê°™ì´ ì§„í–‰í•œë‹¤.\n\n## MaxEnt Model\n\nMaximum Entropy Model(MEM)ì˜ ì•½ìë¡œ, ì´ê²ƒì˜ ì˜ë¯¸ëŠ” ì£¼ì–´ì§„ datasetì„ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì ì ˆí•œ ë¶„í¬ëŠ” Prior Knowledgeë¥¼ ë§Œì¡±í•˜ëŠ” ë¶„í¬ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ë†’ì€ Entropyë¥¼ ê°€ì§€ëŠ” ë¶„í¬ë¼ëŠ” ê²ƒì´ë‹¤.  \n\nì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê´€ì¸¡ì— ì˜í•´ì„œ ì •ì˜ëœ ê²ƒì´ë‹¤.\n\n1. ë‹¤ì–‘í•œ ë¬¼ë¦¬í˜„ìƒë“¤ì€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ Entropyë¥¼ ìµœëŒ€í™”í•˜ë ¤ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ë ¤ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n2. ë” ì ì€ ìˆ˜ì˜ ë…¼ë¦¬ë¡œ ì„¤ëª…ì´ ê°€ëŠ¥í•œ ê²½ìš°, ë§ì€ ìˆ˜ì˜ ë…¼ë¦¬ë¥¼ ì„¸ìš°ì§€ ë§ë¼ (ì˜¤ì»´ì˜ ë©´ë„ë‚ )\n\në‹¤ì†Œ ì–µì§€ê°™ì•„ ë³´ì´ëŠ” ë…¼ë¦¬ì¼ì§€ë¼ë„ í›„ì— ê°€ì„œ ì‚´í´ë³´ë©´, Machine Learningì˜ Logistic Regressionì— ì—°ê²°ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ìš°ì„ ì€ ì´ ì •ë„ ë…¼ë¦¬ë¡œ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ì •ë„ë¡œ ì´í•´í•´ë³´ì.\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ í’€ì–´ì•¼í•  ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & H(p) = - \\sum_{i=1}^{N}p_i\\log p_i &\\\\\n  \\text{subject to} \\quad & p_i \\geq 0, & i = 1, ..., N \\\\\n                          & \\sum_{i=1}^{N}p_i = 1 &\\\\\n                          & \\text{Other Prior Knowledge}\n\\end{align*}\n$$\n\nì´ë¥¼ ì´ìš©í•´ì„œ ë¬¸ì œë¥¼ ì„¸ ê°œ ì •ë„ í’€ì–´ë³´ë©´ ê°ì´ ì¡ì„ ìˆ˜ ìˆëŠ”ë° í•œ ë²ˆ ë”°ë¼ì™€ë³´ë„ë¡ í•˜ì.\n\n### Example\n\n> <mark>**1. ì£¼ì‚¬ìœ„ ë˜ì§€ê¸°**</mark>\n\n1ë¶€í„° 6ê¹Œì§€ì˜ ëˆˆì´ ìˆëŠ” ì£¼ì‚¬ìœ„ê°€ ìˆë‹¤ê³  í•  ë•Œ, ì£¼ì‚¬ìœ„ì˜ ê° ëˆˆì´ ë‚˜ì˜¬ í™•ë¥ ì„ ì•Œê³  ì‹¶ë‹¤ê³  í•˜ì. ì´ë•Œ ìš°ë¦¬ëŠ” ê°„ë‹¨í•˜ê²Œ $1\\over6$ì´ë¼ê³  ë§í•  ê²ƒì´ë‹¤. ì´ê²ƒë„ Maximum Entropyì— ê¸°ë°˜í•œ ì¶”ë¡  ë°©ë²• ì¤‘ì— í•˜ë‚˜ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒ ì‹ì„ ë³´ì.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & H(p) = - \\sum_{i=1}^{N}p_i\\log p_i &\\\\\n  \\text{subject to} \\quad & p_i \\geq 0, & i = 1, ..., N \\\\\n                          & \\sum_{i=1}^{N}p_i = 1 &\n\\end{align*}\n$$\n\nì •ë§ ì•„ë¬´ëŸ° ì •ë³´ê°€ ì—†ì„ ë•Œì—ëŠ” ìœ„ì˜ ì‹ì„ Lagrangianì„ ì“°ì§€ ì•Šê³ ë„ uniform distributionì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ëŠ” [ğŸ”— [ML] Base Information Theory](/posts/ml-base-knowledge#Information-Theory)ì—ì„œ ì‚´í´ë³´ì•˜ì—ˆë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ì¢€ ë” ë³µì¡í•œ ê²½ìš°ë¥¼ ê³ ë ¤í•´ë³´ì. ì•„ë˜ëŠ” Duke University ECE587 ìˆ˜ì—… PPTì˜ ì˜ˆì œì´ë‹¤.\n\n> <mark>**2. í‰ê· ì´ ì£¼ì–´ì¡Œì„ ë•Œì˜ ì¶”ë¡ **</mark>\n\nìš°ë¦¬ê°€ ë§Œì•½ í‰ê·  ë°ì´í„°ë¥¼ ì•Œê³  ìˆë‹¤ë©´, ì´ë¥¼ Maximum Entropyë¡œ ì–´ë–»ê²Œ ì¶”ì •í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ì•„ë˜ëŠ” ì–´ëŠ fastfoodì ì˜ ë©”ë‰´ë¼ê³  í•˜ì.\n\n| Item    | Price | Calories |\n| :------ | :---- | :------- |\n| Burger  | $1    | 1000     |\n| Chicken | $2    | 600      |\n| Fish    | $3    | 400      |\n| Tofu    | $8    | 200      |\n\nê·¸ë¦¬ê³  íŠ¹ì • í•™ìƒì´ ì´ ê°€ê²Œì—ì„œ í•˜ë£¨ì— í•˜ë‚˜ì”© ë¨¹ëŠ”ë‹¤ê³  í•  ë•Œ, í‰ê·  ì†Œë¹„ ê°€ê²©ì´ $2.5ë¼ê³  í•˜ì. ê·¸ë ‡ë‹¤ë©´, ì´ í•™ìƒì´ ê°€ì¥ ë§ì´ ë¨¹ëŠ” ë©”ë‰´ëŠ” ë¬´ì—‡ì¼ì§€ë¥¼ ì¶”ë¡ í•´ë³´ëŠ” ê²ƒì´ë‹¤.  \nì¦‰, ì´ë¥¼ ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & H(p) = - \\sum_{i=1}^{N}p_i\\log p_i &\\\\\n  \\text{subject to} \\quad & p_i \\geq 0, & i = 1, ..., N \\\\\n                          & \\sum_{i=1}^{N}p_i = 1 &\\\\\n                          & E[\\text{price}] = 2.5 &\n\\end{align*}\n$$\n\nì´ë¥¼ Lagrangian ë°©ì‹ì„ ì´ìš©í•´ì„œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = - \\sum_{i}^{N}p_{i}\\log{p_{i}} + \\lambda_{0}(\\sum_{i=1}^{N}p_{i} - 1) + \\lambda_{1}(\\sum_{i=1}^{N}\\text{price}_{i}\\times{p_{i}} -2.5)\n$$\n\nìœ„ ì‹ì„ ê° ê°ì˜ $p_{i}$ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n{\\partial \\mathcal{L}\\over\\partial p_{i}} = -\\log{p_{i}} -1 + \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i}\n$$\n\në”°ë¼ì„œ, $p_{i}$ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n0 &= -\\log{p_{i}} -1 + \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} \\\\\n\\log{p_{i}} &= \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} - 1 \\\\\np_{i} &= e^{\\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} - 1}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ë‚˜ì˜¤ëŠ” ëª¨ë“  ì‹ê³¼ ì œí•œ ì¡°ê±´ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n- $p(Burger) = e^{\\lambda_{0} + \\lambda_{1} - 1}$, $p(Chicken) = e^{\\lambda_{0} + 2\\lambda_{1} - 1}$, $p(Fish) = e^{\\lambda_{0} + 3\\lambda_{1} - 1}$, $p(Tofu) = e^{\\lambda_{0} + 8\\lambda_{1} - 1}$\n- $p(Burger) + p(Chicken) + p(Fish) + p(Tofu) = 1$\n- $p(Burger) + 2p(Chicken) + 3p(Fish) + 8p(Tofu) = 2.5$\n\nìœ„ì˜ ì‹ì„ ì—°ë¦½í•´ì„œ í’€ë©´, $\\lambda_{0} = 1.2371$, $\\lambda_{1}=0.2586$ì´ê³ , ì „ì²´ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n| Item    | p      |\n| :------ | :----- |\n| Burger  | 0.3546 |\n| Chicken | 0.2964 |\n| Fish    | 0.2478 |\n| Tofu    | 0.1011 |\n\n> <mark>**3. ì£¼ì‚¬ìœ„ì˜ ëˆˆì˜ í•©**</mark>\n\n1ë²ˆì—ì„œ ë³´ì•˜ë˜ ì£¼ì‚¬ìœ„ë¥¼ nê°œ ë˜ì ¸ì„œ ë‚˜ì˜¨ ëˆˆì˜ í•©ì„ ì•Œ ë•Œ, ì£¼ì‚¬ìœ„ì˜ ë¹„ìœ¨ì„ ì¶”ì •í•œë‹¤ê³  í•´ë³´ì.\n\nì´ë•Œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë³€ìˆ˜ë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n- ì£¼ì‚¬ìœ„ì˜ ê°¯ìˆ˜ : $n$\n- iê°œì˜ ëˆˆì„ ê°€ì§„ ì£¼ì‚¬ìœ„ì˜ ê°¯ìˆ˜ : $n_{i}$\n- ì „ì²´ ëˆˆì˜ ìˆ˜ì˜ í•© : $n\\alpha$\n- ì¶”ê°€ë˜ëŠ” Prior Knowledge : $\\sum_{i=1}^{6}{i n_{i}} = n\\alpha$\n\nì´ë¥¼ Maximum Entropyë¥¼ ì´ìš©í•´ì„œ í’€ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ë¡ ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\np_{i} = {e^{\\lambda_{i}}\\over{\\sum_{i=1}^{6}{e^{\\lambda_{i}}}}}\n$$\n\n## Generalization\n\nMaximum Entropyë¥¼ ìœ„ì˜ ì‹ì„ í†µí•´ì„œ êµ¬í•˜ëŠ” ê²ƒë„ ë¬¸ì œëŠ” ì—†ì§€ë§Œ ìš°ë¦¬ëŠ” ì¢€ ë” ì¼ë°˜í™”ëœ ì‹ì„ ì›í•œë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì„ ê³ ë ¤í•´ë³´ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ê°€ ë§ˆì§€ë§‰ ë³´ì•˜ë˜ ì˜ˆì‹œê°€ ì‚¬ì‹¤ì€ ìš°ë¦¬ê°€ í•˜ê³ ì í•˜ëŠ” ê³¼ì •ì„ ëŒ€í‘œí•˜ëŠ” í•˜ë‚˜ì˜ ì˜ˆì‹œì´ë‹¤. ìš°ë¦¬ê°€ ê°€ì§„ ì‚¬ì „ ì§€ì‹ì€ ì´ì „ì— ê´€ì¸¡í•œ ë°ì´í„°ì™€ ì´ê²ƒì˜ classì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê´€ì¸¡ ê²°ê³¼ì˜ ê°€ì§“ìˆ˜(class)ê°€ $K$ê°œì´ê³ , ë°ì´í„°ì˜ inputê³¼ ê²°ê³¼ë¥¼ $(X, Y)$ ìŒì´ ë¼ê³  í•  ë•Œ, íŠ¹ì • input data($X_{i}$)ê°€ class kì¼ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n$$\np(Y_{i} = k) = {e^{w^{\\top}_{k}X_{i}}\\over{\\sum_{k^{\\prime}=1}^{K}{e^{w_{k^{\\prime}}^{\\top}X_{i}}}}}\n$$\n\nì—¬ê¸°ì„œ ê°€ì¥ ì¤‘ìš”í•œ Pointê°€ ë°œê²¬ëœë‹¤. ë°”ë¡œ ì´ ì‹ì´ **softmax** í•¨ìˆ˜ë¼ëŠ” ê²ƒì´ë‹¤. <mark>ì¦‰, Maximum Entropyë¥¼ í†µí•œ classificationì˜ ì˜ë¯¸ëŠ” ì‚¬ì‹¤ìƒ multinomial logistic regressionì˜ ë‹¤ë¥¸ ì´ë¦„ì¼ ë¿ì´ë‹¤.</mark> (logistic regressionì— ëŒ€í•œ ë‚´ìš©ì€ [ğŸ”— [ML] 3. Logistic Regression](/posts/ml-logistic-regression)ì—ì„œ ë‹¤ë£¨ì—ˆë‹¤.)\n\në”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” ë³„ë„ë¡œ Modeling, Estimation, Smoothing ì ˆì°¨ë¥¼ ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤. machine learningì˜ ë°©ë²•ê³¼ ë™ì¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n\n## Features\n\nNLì˜ ê°€ì¥ í° íŠ¹ì§•ì€ dataê°€ sparseí•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. domainë§ˆë‹¤ ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ì™€ ë¹ˆë„ê°€ ë„ˆë¬´ë‚˜ ì²œì°¨ë§Œë³„ì´ê¸° ë•Œë¬¸ì— sparse í˜„ìƒì´ í•„ì—°ì ìœ¼ë¡œ ë°œìƒí•œë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œ ëŒ€ê²Œì˜ dataëŠ” domain ë³„ë¡œ ë”°ë¡œë”°ë¡œ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ë˜í•œ, dataì—ì„œ ì˜¬ë°”ë¥¸ featureë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ êµ‰ì¥íˆ ì¤‘ìš”í•˜ë‹¤.  \nì´ë¥¼ ìœ„í•´ NLì—ì„œ ì „í†µì ìœ¼ë¡œ ì“°ë˜ ë°©ì‹ì€ ëŒ€ì†Œë¬¸ì ì—¬ë¶€, ì–µì–‘ í‘œê¸°, í’ˆì‚¬, ë¬¸ì¥êµ¬ì¡°, ëœ» ë“±ì„ ë‹¨ì–´ì— ë¯¸ë¦¬ ì ìš©í•˜ê¸°ë„ í•˜ì—¬ ì´ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤. ê·¸ëŸ°ë° ì´ëŸ¬í•œ í’ˆì‚¬, ëœ» ë“±ì„ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ë„ Statistical Inferenceê°€ í•„ìš”í•˜ë‹¤. ë”°ë¼ì„œ, ì•ìœ¼ë¡œ chapterì—ì„œëŠ” í’ˆì‚¬ì™€ ë¬¸ì¥êµ¬ì¡° ëœ»ì„ ì •ì˜í•˜ê¸° ìœ„í•œ ê¸°ìˆ ë“¤ê³¼ ì´ë¥¼ ì–´ë–»ê²Œ ì°¾ì„ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì•Œì•„ë³¼ ê²ƒì´ë‹¤.\n\në˜í•œ, Wordìì²´ë¥¼ Vectorë¡œ ì¹˜í™˜í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” Word2Vecë°©ì‹ì— ëŒ€í•´ì„œë„ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- Maximum Entropy ìë£Œ ì°¸ê³ , <https://www2.isye.gatech.edu/~yxie77/ece587/Lecture11.pdf>\n","slug":"nlp-maxent","date":"2022-11-07 10:02","title":"[NLP] 7. MaxEnt","category":"AI","tags":["NLP","MaximumEntropyModel","softmax"],"desc":"í•´ë‹¹ Postingì—ì„œëŠ” Maximum Entropyë¥¼ ì´ìš©í•˜ì—¬ ìµœì ì˜ parameterë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” Machine Learning ì ‘ê·¼ë²•ì— ê¸°ë°˜í•œ NLP ë°©ì‹ì„ ì œì•ˆí•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ NLë¥¼ ìˆ˜í•™ì ì¸ í˜•íƒœë¡œ ë³€í˜•í•˜ê¸° ìœ„í•œ ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì¸ word2vecì— ëŒ€í•œ ì„¤ëª…ë„ ê°™ì´ ì§„í–‰í•œë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nì´ì „ê¹Œì§€ íŠ¹ì • wordë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ modelingì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ íŠ¹ì • wordì˜ sequenceë¥¼ í†µí•´ì„œ ê° wordì— ëŒ€í•œ classificationì„ í•œ ë²ˆì— í•˜ê³  ì‹¶ì€ ê²½ìš°ëŠ” ì–´ë–»ê²Œ í• ê¹Œ?(ì˜ˆë¥¼ ë“¤ì–´, ê° ë‹¨ì–´ì˜ í’ˆì‚¬ë¥¼ ì§€ì •í•˜ëŠ” ì¼) ì¼ë°˜ì ìœ¼ë¡œ ê° ë‹¨ì–´ê°€ íŠ¹ì • í•´ë‹¹ classì¼ í™•ë¥ ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì´ ì¼ë°˜ì ì¼ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ í™•ë¥ ì„ êµ¬í•  ë°©ë²•ì€ ì—†ì„ê¹Œ? ê·¸ ë°©ë²•ì€ ë°”ë¡œ bigramì„ ì´ìš©í•˜ë©´ ë  ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì‚¬ì‹¤ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ë¬¸ë§¥ì´ ë‹¨ì–´ ìì²´ë³´ë‹¤ëŠ” ì´ì „ classê°€ ë” ì˜í–¥ì´ í¬ë‹¤ë©´, ì´ëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ? ì´ë¥¼ ìœ„í•œ í•´ê²°ì±…ì´ HMMì´ë‹¤. NLP ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ ë„“ê²Œ ì‚¬ìš©ë˜ê³  ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” NLP ë¶„ì•¼ì—ì„œ ì–´ë–»ê²Œ ì´ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³¼ ê²ƒì´ë‹¤.\n\n## Markov Model\n\nHMMì„ ì•Œì•„ë³´ê¸°ì „ì— Markov Modelì„ ì•Œì•„ì•¼ í•œë‹¤. ì´ëŠ” íŠ¹ì • sequenceì˜ í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰ ìš°ë¦¬ì—ê²Œ state sequence ($S= {s_{0}, s_{1}, ..., s_{N}}$)ê°€ ì£¼ì–´ì§ˆ ë•Œ, ê° stateì—ì„œ ë‹¤ìŒ stateë¡œ ì „ì´(ì´ë™)í•  í™•ë¥ ì„ ì´ìš©í•´ì„œ state sequenceì˜ í™•ë¥ ì„ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\n![nlp-markov-model-1](/images/nlp-markov-model-1.jpg)\n\nìœ„ì˜ ê·¸ë¦¼ì´ state ê° ê°ì—ì„œ ë‹¤ìŒ stateë¡œ ì „ì´í•  í™•ë¥ ì„ ë‚˜íƒ€ë‚¸ ê²ƒì´ë¼ë©´, ìš°ë¦¬ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì€ ê·¸ë¦¼ìœ¼ë¡œ sequenceì˜ í™•ë¥ ì„ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n![nlp-markov-model-2](/images/nlp-markov-model-2.jpg)\n\në”°ë¼ì„œ, ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ìš°ë¦¬ê°€ ë§Œì•½ $(s_{0}, s_{1}, s_{0}, s_{2})$ìœ¼ë¡œ ì´ë£¨ì–´ì§„ sequenceì˜ í™•ë¥ ì„ ì–»ê¸°ë¥¼ ë°”ë€ë‹¤ë©´, ê·¸ í™•ë¥ ì€ ì•„ë˜ì™€ ê°™ì•„ì§„ë‹¤.\n$$\n\\begin{align*}\np(s_{0}, s_{1}, s_{0}, s_{2}) &= p(s_{0}| \\text{start}) \\times p(s_{1}|s_{0}) \\times p(s_{0}|s_{1}) \\times p(s_{2}|s_{1}) \\times p(end|s_{2}) \\\\\n&= \\pi_{0} \\times p_{01} \\times p_{10} \\times p_{12} \\times 1\n\\end{align*}\n$$\n\nì´ë¥¼ ì˜ ì‚´í´ë³´ë‹ˆ bigramì—ì„œì˜ Likelihoodë¥¼ êµ¬í•˜ëŠ” ê³µì‹ê³¼ ë˜‘ê°™ë‹¤. ì¦‰, state ê° ê°ì„ wordë¼ê³  ë³¸ë‹¤ë©´, Markov Modelì„ í†µí•´ì„œ êµ¬í•  ìˆ˜ ìˆëŠ” í™•ë¥ ì€ bigramì˜ Likelihoodì¸ ê²ƒì´ë‹¤.\n\nê·¸ë¦¬ê³  ì´ë¥¼ ì¼ë°˜í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\np(seq) = \\prod_{i=1}^{N}p(seq_{i}|seq_{i-1})\n$$\n\nê·¸ëŸ°ë°, ì—¬ê¸°ì„œ nì´ 3 ì´ìƒì¸ ngramì„ ì ìš©í•˜ê³  ì‹¶ë‹¤ë©´, ê° stateë¥¼ n-1 gramìœ¼ë¡œ ì„¤ì •í•˜ë©´ ëœë‹¤.\n\n$$\n\\begin{align*}\nX_{i} &= (Q_{i-1}, Q_{i}) \\text{ë¼ë©´, }\\\\\nP(X_{i} | X_{i-1}) &= P(Q_{i-1}, Q_{i} | Q_{i-2}, Q_{i-1}) \\\\\n&= P(Q_{i} | Q_{i-2}, Q_{i-1})\n\\end{align*}\n$$\n\në”°ë¼ì„œ, trigramì„ ì ìš©í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\n$$\n\\begin{align*}\np((start, w_{0}), (w_{0}, w_{1}), (w_{1}, w_{0}), (w_{0}, w_{2})) &= p(w_{0}| \\text{start}, \\text{start}) \\times p(w_{1}|\\text{start}, w_{0}) \\times p(w_{0}|w_{0}, w_{1}) \\times p(w_{2}|w_{1}, w_{0}) \\times p(end|w_{0}, w_{2}) \\\\\n&= \\pi_{0} \\times p_{01} \\times p_{10} \\times p_{12} \\times 1\n\\end{align*}\n$$\n\n## Hidden Markov Model\n\nHidden Markov Modelì€ stateë¥¼ í•˜ë‚˜ ë” ë§Œë“ ë‹¤ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤. ê·¸ë˜ì„œ, ìš°ë¦¬ê°€ ì§ì ‘ ê´€ì¸¡í•˜ëŠ” state(**observed state**)ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ì¸¡í•˜ì§€ ì•Šì§€ë§Œ, ê´€ì¸¡í•œ stateë“¤ì— ì˜ì¡´í•˜ëŠ” state(**hidden state**) ì´ ë‘ ê°œì˜ stateë¥¼ ì‚¬ìš©í•œë‹¤. ì¼ë°˜ì ì¸ ì˜ˆì‹œê°€ textê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ ìš°ë¦¬ëŠ” ê° ë‹¨ì–´ë¥¼ observed stateë¼ê³  í•œë‹¤ë©´, ê° ë‹¨ì–´ì˜ í’ˆì‚¬ë¥¼ hidden stateë¼ê³  ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n![nlp-markov-model-3](/images/nlp-markov-model-3.jpg)\n\nìœ„ì˜ ì˜ˆì‹œëŠ” ìš°ë¦¬ê°€ ê´€ì¸¡í•˜ëŠ” ë°ì´í„°($O$)ê°€ 3ê°œì˜ stateë¥¼ ê°€ì§€ê³ , ì´ ì‚¬ê±´ì— ì˜ì¡´ì ì¸ ë˜ ë‹¤ë¥¸ ì‚¬ê±´($H$)ì´ 3ê°œì˜ stateë¥¼ ê°€ì§€ëŠ” ê²½ìš°ì´ë‹¤. ì´ë¥¼ ì´ìš©í•´ì„œ ê¸°ì¡´ Markov Modelë³´ë‹¤ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.\n\n### Estimation\n\nìš°ë¦¬ê°€ í•  ìˆ˜ ìˆëŠ” ì‘ì—…ì€ í¬ê²Œ ë‘ ê°€ì§€ì´ë‹¤. ì¼ë°˜ì ì¸ Markov Modelì—ì„œ í•  ìˆ˜ ìˆë˜ ë°©ì‹ì´ **Trellis** ë°©ì‹ì´ê³ , ë˜ ë‹¤ë¥¸ ë°©ì‹ì´ **Viterbi** ë°©ì‹ì´ë‹¤.\n\n1. $(o_{0}, o_{1}, o_{0}, o_{2})$ì˜ í™•ë¥ ì´ ê¶ê¸ˆí•  ë•Œ(**Trellis**)\n2. $(o_{0}, o_{1}, o_{0}, o_{2})$ê°€ ì£¼ì–´ì§ˆ ë•Œ, ì´ê²ƒì˜ hidden stateì˜ sequence ì¤‘ ê°€ì¥ ìœ ë ¥í•œ sequenceë¥¼ ì°¾ê³ ìí•  ë•Œ(**Viterbi**)\n\nìœ„ì˜ ê²½ìš°ë¥¼ ê°ê° í’€ì–´ë³´ë„ë¡ í•˜ì.\n\n> <mark>**1. Trellis**</mark>\n\nìš°ë¦¬ê°€ ì§ì ‘ ê´€ì¸¡í•œ ë°ì´í„°ì˜ sequence ìì²´ì˜ í™•ë¥ ì´ ê¶ê¸ˆí•  ë•Œì´ë‹¤. ë”°ë¼ì„œ, ì´ì— ëŒ€í•œ ë¶„ì„ì€ $(o_{0}, o_{1}, o_{0}, o_{2})$ì˜ í™•ë¥ ì„ ë¶„ì„í•´ë³´ë©´ì„œ ì„¤ëª…í•˜ê² ë‹¤.\n\n$$\n\\begin{align*}\np(o_{0}, o_{1}, o_{0}, o_{2}) &= p(o_{0}, o_{1}, o_{0}) \\times p(o_{2} | o_{0}, o_{1}, o_{0}) \\\\\n&= p(o_{0}, o_{1}, o_{0}) \\times \\{p(o_{2} | h_{0})p(h_{0} | o_{0}, o_{1}, o_{0}) + p(o_{2} | h_{1})p(h_{1} | o_{0}, o_{1}, o_{0}) + p(o_{2} | h_{2})p(h_{2} | o_{0}, o_{1}, o_{0})\\} \\\\\n&= p(o_{0}, o_{1}, o_{0}) \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n&= p(o_{0}, o_{1}) \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n&= p(o_{0}) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n&= \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) }\n\\end{align*}\n$$\n\nì´ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n![nlp-hidden-markov-model-1](/images/nlp-hidden-markov-model-1.jpg)\n\në˜í•œ, ì´ ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ì¶•ì†Œê°€ ê°€ëŠ¥í•˜ë‹¤.\n\n$$\n\\begin{align*}\n  &\\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}\\alpha_{0 i} \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}{\\alpha_{1 i} } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}{\\alpha_{2 i} } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}{\\alpha_{3 i} }\n\\end{align*}\n$$\n\nìš°ë¦¬ëŠ” ì´ë¥¼ í†µí•´ì„œ, Markov Modelì˜ íŠ¹ì§•ì„ í•˜ë‚˜ ë°°ìš¸ ìˆ˜ ìˆë‹¤. ê·¸ê²ƒì€ ë°”ë¡œ ë³µì¡í•œ sequence ì „ì²´ì˜ í™•ë¥ ì—ì„œ ë²—ì–´ë‚˜ì„œ ë°”ë¡œ ì§ì „ì˜ í™•ë¥ ê°’ë§Œ ìœ¼ë¡œ ë‹¤ìŒ í™•ë¥ ì„ ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì´ Markov Chainì´ë¼ëŠ” ì´ë¡ ì´ê³ , ì´ë¥¼ ì´ìš©í–ˆê¸° ë•Œë¬¸ì— Markov Modelë¼ê³  ë¶€ë¥´ëŠ” ê²ƒì´ê¸°ë„ í•˜ë‹¤.\n\në”°ë¼ì„œ, $\\alpha$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\alpha(t, i) = \\sum_{k=1}^{N}{\\alpha(t-1, k)p(h_{i}|h_{k})p(o = s_{t}|h_{i})} \\quad (s_{t} = \\text{inputìœ¼ë¡œ ë“¤ì–´ì˜¨ sequenceì˜ të²ˆì§¸ ê°’})\n$$\n\në˜, ì´ë¥¼ ë°˜ëŒ€ë¡œ í•  ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n![nlp-hidden-markov-model-2](/images/nlp-hidden-markov-model-2.jpg)\n\n$$\n\\begin{align*}\n  &\\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =& \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{\\beta_{3i}} \\\\\n  =& \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{\\beta_{2i}} \\\\\n  =& \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{\\beta_{1i}} \\\\\n  =& \\sum_{i=0}^{2}{\\beta_{0i}} \\\\\n\\end{align*}\n$$\n\n$$\n\\beta(t, i) = \\sum_{k=1}^{N}{\\beta(t+1, k)p(h_{k}|h_{i})p(o = s_{t}|h_{i})} \\quad (s_{t} = \\text{inputìœ¼ë¡œ ë“¤ì–´ì˜¨ sequenceì˜ të²ˆì§¸ ê°’})\n$$\n\nìœ„ì˜ ì²˜ëŸ¼ ì•ì—ì„œë¶€í„° í’€ì´ë¥¼ í•´ë‚˜ê°€ë©´ì„œ, $\\alpha$ì˜ í•©ìœ¼ë¡œ ëì´ ë‚˜ë„ë¡ í‘¸ëŠ” ë°©ë²•ì„ forwarding ë°©ì‹ì´ë¼í•˜ê³ , ë°˜ëŒ€ë¡œ ë’¤ì—ì„œë¶€í„° í’€ì´í•˜ë©´ì„œ $\\beta$ì˜ í•©ìœ¼ë¡œ í‘¸ëŠ” ë°©ë²•ì„ backwarding ë°©ì‹ì´ë¼ê³  í•œë‹¤. ì‚¬ì‹¤ ì´ ê²½ìš°ëŠ” HMMì´ êµ³ì´ ì•„ë‹ˆë”ë¼ë„, MMìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆìœ¼ë‹ˆ êµ³ì´ í•„ìš”ëŠ” ì—†ë‹¤. í•˜ì§€ë§Œ, ì´ê²ƒì€ í›„ì— modeling ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì•Œì•„ë‘ì–´ì•¼ í•œë‹¤.\n\n> <mark>**2. Viterbi**</mark>\n\nì´ëŠ” observed stateì˜ sequenceì— ì˜í•´ì„œ íŒŒìƒë˜ëŠ” ê°€ì¥ ì ì ˆí•œ hidden sequenceë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. ì´ë¥¼ í†µí•´ì„œ í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ ê²ƒì´ sequence classificationì´ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´ ê°€ì¥ ìœ ë ¥í•œ hidden stateì˜ sequenceë¥¼ $\\hat{s}^{(H)}$ë¼ê³  í•˜ì. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n\\hat{s}^{(H)} &= \\argmax_{s^{(H)} \\in S^{(H)}}P(s^{(H)}|s^{(O)}) \\\\\n&= \\argmax_{s^{(H)} \\in S^{(H)}}P(s^{(O)}|s^{(H)})P(s^{(H)}) \\\\\n&= \\argmax_{{h_{1}, h_{2}, ..., h_{N}} \\in S^{(H)}}\\underbrace{P(o_{1}, o_{2}, ... , o_{N}|h_{1}, h_{2}, ... , h_{N})}_{\\text{Markov Model}}\\underbrace{P(h_{1}, h_{2}, ... , h_{N})}_{\\text{Markov Model}} \\\\\n&= \\argmax_{{h_{1}, h_{2}, ..., h_{N}} \\in S^{(H)}}\\prod_{i=1}^{N}p(o_{i}|h_{i})p(h_{i}|h_{i-1})\n\\end{align*}\n$$\n\n![nlp-hidden-markov-model-3](/images/nlp-hidden-markov-model-3.jpg)\n\nì¦‰, ê° layerì—ì„œ ë‹¨ í•˜ë‚˜ì˜ ê°€ì¥ í° outputë§Œ ì‚´ì•„ë‚¨ì„ ìˆ˜ ìˆê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. ì´ ê³¼ì •ì´ ì‚¬ì‹¤ìƒ HMMì˜ ë³¸ì§ˆì ì¸ ëª©í‘œì´ë‹¤. sequenceë¥¼ ì…ë ¥í•´ì„œ sequence í˜•íƒœì˜ classification ê²°ê³¼ë¥¼ ì–»ëŠ” ê²ƒì´ë‹¤.\n\n### Modeling\n\nì—¬íƒœê¹Œì§€ HMMì„ í™œìš©í•˜ì—¬ sequential classë¥¼ ì–´ë–»ê²Œ estimation í•˜ëŠ”ì§€ ì•Œì•„ë³´ì•˜ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ì œëŠ” ì´ë¥¼ ìœ„í•´ì„œ ì‚¬ìš©ë˜ëŠ” í™•ë¥ ê°’ì„ êµ¬í•´ì•¼í•œë‹¤. í•„ìš”í•œ í™•ë¥ ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n- $p(h_{i}|h_{i-1})$ : Hidden Stateì—ì„œ Hidden Stateë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•œ í™•ë¥ ì´ë‹¤.\n- $p(o_{i}|h_{i})$ : ë°©ì¶œ í™•ë¥ ë¡œ íŠ¹ì • Hidden Stateì—ì„œ ë‹¤ìŒ Stateì˜ Observed Stateë¡œ ë„˜ì–´ê°€ëŠ” ë°©ë²•ì´ë‹¤.\n- $\\pi_{i}$\n\nTrelli ë°©ì‹ì—ì„œ ë§Œë“¤ì—ˆë˜, $\\alpha$ì™€ $\\beta$ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•´ì•¼ í•œë‹¤. ê° ê°ì€ í•´ë‹¹ ê³¼ì •ê¹Œì§€ ì˜¤ë©´ì„œ ëˆ„ì í•´ì˜¨ í™•ë¥ ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì˜ ë°˜ì˜í•  ìˆ˜ ìˆëŠ” í™•ë¥  ê°’ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ìƒê°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ í‰ê· ì„ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ë¨¼ì € ì‚´í´ë³´ì.\n\n$$\n\\begin{align*}\n  c(i, j, k) &= h_{i}\\text{ì—ì„œ } h_{j}\\text{ë¡œ ë„˜ì–´ê°€ê³ , } o_{k}\\text{ê°€ ê´€ì¸¡ë  í™•ë¥ ì˜ í•©} \\\\\n  &= \\sum_{t=2}^{T} \\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j) \\\\\n  \\\\\n  c(i,j) &= h_{i}\\text{ì—ì„œ } h_{j}\\text{ë¡œ ë„˜ì–´ê°ˆ í™•ë¥ ì˜ í•©} \\\\\n  &= \\sum_{k=1}^{K}\\sum_{t=2}^{T}{\\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j)} \\\\\n  \\\\\n  c(i) &= h_{i}\\text{ì—ì„œ ìƒíƒœë¥¼ ë³€ê²½í•˜ëŠ” í™•ë¥ ì˜ í•©} \\\\\n  &= \\sum_{j=1}^{N}\\sum_{k=1}^{K}\\sum_{t=2}^{T}{\\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j)} \\\\\n\\end{align*}\n$$\n\nìœ„ì˜ ê°’ì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆë˜ í™•ë¥ ì„ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\np(h_{j}|h_{i}) &= {c(i,j)\\over c(i)} \\\\\np(o_{k}|h_{i}) &= {c(i,j,k)\\over c(i,j)}\n\\end{align*}\n$$\n\nì¦‰, ìš°ë¦¬ëŠ” ë‹¤ìŒ ê³¼ì •ì„ ìˆ˜í–‰í•˜ì—¬ Modelingì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n1. ì´ˆê¸°ê°’ ($p(h_{i}|h_{i-1})$, $p(o_{i}|h_{i})$, $\\pi_{i}$)ì„ ì´ˆê¸°í™” í•œë‹¤.  \n2. Trellië¥¼ í†µí•´ì„œ $\\alpha$, $\\beta$ë¥¼ ê³„ì‚°í•œë‹¤.\n3. $p(h_{i}|h_{i-1})$, $p(o_{i}|h_{i})$ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤.  \n   ($pi_{i}$ê°™ì€ ê²½ìš°ëŠ” ë°œìƒ ë¹ˆë„ë¡œ ì—…ë°ì´íŠ¸ í•œë‹¤.)\n4. ì„ê³„ì¹˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ 2,3ë²ˆì„ ë°˜ë³µí•œë‹¤.\n\nì´ ê³¼ì •ì„ ëŒ€ê²Œ 10ë²ˆ ì •ë„ë§Œ í•˜ë©´ ìˆ˜ë ´í•˜ê²Œ ë˜ê³ , ì´ë¥¼ í™•ë¥ ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-hmm","date":"2022-10-21 21:55","title":"[NLP] 6. Hidden Markov Model","category":"AI","tags":["NLP","MarkovModel","HMM","HiddenMarkovModel"],"desc":"ì´ì „ê¹Œì§€ íŠ¹ì • wordë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ modelingì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ íŠ¹ì • wordì˜ sequenceë¥¼ í†µí•´ì„œ ê° wordì— ëŒ€í•œ classificationì„ í•œ ë²ˆì— í•˜ê³  ì‹¶ì€ ê²½ìš°ëŠ” ì–´ë–»ê²Œ í• ê¹Œ?(ì˜ˆë¥¼ ë“¤ì–´, ê° ë‹¨ì–´ì˜ í’ˆì‚¬ë¥¼ ì§€ì •í•˜ëŠ” ì¼) ì¼ë°˜ì ìœ¼ë¡œ ê° ë‹¨ì–´ê°€ íŠ¹ì • í•´ë‹¹ classì¼ í™•ë¥ ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì´ ì¼ë°˜ì ì¼ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ í™•ë¥ ì„ êµ¬í•  ë°©ë²•ì€ ì—†ì„ê¹Œ? ê·¸ ë°©ë²•ì€ ë°”ë¡œ bigramì„ ì´ìš©í•˜ë©´ ë  ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì‚¬ì‹¤ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ë¬¸ë§¥ì´ ë‹¨ì–´ ìì²´ë³´ë‹¤ëŠ” ì´ì „ classê°€ ë” ì˜í–¥ì´ í¬ë‹¤ë©´, ì´ëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ? ì´ë¥¼ ìœ„í•œ í•´ê²°ì±…ì´ HMMì´ë‹¤. NLP ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ ë„“ê²Œ ì‚¬ìš©ë˜ê³  ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” NLP ë¶„ì•¼ì—ì„œ ì–´ë–»ê²Œ ì´ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³¼ ê²ƒì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nNaive Bayes Modelì€ ê°€ì¥ ì‰½ê²Œ Classificationì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” Modelì´ì§€ë§Œ, ì„±ëŠ¥ì´ ë‹¤ë¥¸ Modelì— ë¹„í•´ ë›°ì–´ë‚˜ì§€ëŠ” ì•Šë‹¤. ê·¸ëŸ¼ì—ë„ Naive BayesëŠ” ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” Modelì´ê¸°ì— ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ê³ , Classificationì˜ insightë¥¼ í‚¤ìš°ëŠ”ë° ë§ì€ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œëŠ”, ì „ë°˜ì ì¸ ê°œë…ê³¼ ì´ë¥¼ ì§ì ‘ Spam Filteringì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì‚´í´ë³¸ë‹¤.\n\n## Naive Bayes Model\n\níŠ¹ì • classì—ì„œ í•´ë‹¹ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë°œìƒë˜ëŠ”ì§€ì™€ ì‹¤ì œë¡œ í•´ë‹¹ classì˜ ë¹ˆë„ë¥¼ í™œìš©í•˜ì—¬, classificationì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ìš°ì„  ì´ë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ë¨¼ì € ì •ì˜í•´ë³´ì.\n\n- **documents($D$)**: ì—¬ëŸ¬ ê°œì˜ Documentë¥¼ ì˜ë¯¸í•˜ë©°, í•˜ë‚˜ì˜ DocumentëŠ” ëŒ€ê²Œ ì—¬ëŸ¬ ê°œì˜ wordsë¥¼ í¬í•¨í•œë‹¤. ê° documentëŠ” $d_{i} \\in D$ì˜ í˜•íƒœë¡œ í‘œí˜„í•œë‹¤.\n- **classes($C$)**: classëŠ” ë‘ ê°œ ì´ìƒì„ ê°€ì§„ë‹¤. ê° í´ë˜ìŠ¤ëŠ” $c_{i} \\in C$ì˜ í˜•íƒœë¡œ í‘œí˜„ëœë‹¤.\n- **labeled dataset**: ì´ëŠ” (document($d_{i}$), class($c_{i}$))ê°€ í•˜ë‚˜ì”© mappingëœ í˜•íƒœë¡œ ì¡´ì¬í•œë‹¤. ìš°ë¦¬ê°€ ê°€ì§€ëŠ” datasetìœ¼ë¡œ í•™ìŠµ, í‰ê°€ ì‹œì— ì‚¬ìš©í•œë‹¤. ëŒ€ê²Œ í‰ê°€ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ëŠ” í•™ìŠµ ì‹œì— ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¸ˆì§€í•˜ê¸° ë•Œë¬¸ì— ë³„ë„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì‚¬ìš©í•œë‹¤.\n- **word($w$)**: í•˜ë‚˜ì˜ wordë¥¼ ì˜ë¯¸í•˜ë©° NLP í•™ìŠµ ì‹œì— ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ì‘ì€ ë‹¨ìœ„ì´ë‹¤. ëŒ€ê²Œ document í•˜ë‚˜ì— ìˆëŠ” ë‹¨ì–´ì˜ ìˆ˜ëŠ” Nìœ¼ë¡œ í‘œê¸°í•˜ê³ , uniqueí•œ ë‹¨ì–´ì˜ ìˆ˜ëŠ” V(size of vocabulary)ë¡œ í‘œì‹œí•œë‹¤.\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ ì°¾ê³ ì í•˜ëŠ” ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ classëŠ” ë‹¤ìŒì„ í†µí•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\nc_{MAP} &= \\argmax_{c \\in C}{P(c|d)} \\\\\n&= \\argmax_{c \\in C}{p(d|c)p(c)\\over p(d)} \\\\\n&= \\argmax_{c \\in C}{p(d|c)p(c)} \\\\\n&= \\argmax_{c \\in C}{p(w_{1}, w_{2}, ... , w_{N} | c)p(c)} \\\\\n&= \\argmax_{c \\in C}{\\prod_{i=1}^{N}p(w_{i}|c)p(c)} \\\\\n&= \\argmax_{c \\in C}{\\log(\\prod_{i=1}^{N}p(w_{i}|c)p(c))} \\\\\n&= \\argmax_{c \\in C}{\\sum_{i=1}^{N}\\log p(w_{i}|c) + \\log{p(c)}} \\\\\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ìš°ë¦¬ê°€ language modelì„ ë¬´ì—‡ìœ¼ë¡œ ì •í–ˆëŠ”ì§€ê°€ ì¤‘ìš”í•˜ë‹¤. ìœ„ì—ì„œëŠ” uni-gramì´ë¼ê³  ê°€ì •í•´ì„œ í’€ì´í–ˆì§€ë§Œ, bi-gramì¸ ê²½ìš° documentì˜ í˜•íƒœê°€ $d={(w_{1}, w_{2}), (w_{2}, w_{3}), ... , (w_{N-1}, w_{N})}$ì´ë‹¤. ë”°ë¼ì„œ, ì „ì²´ì ì¸ í¬ê¸°ì™€ vocabularyìì²´ë„ ë°”ë€Œê²Œ ëœë‹¤.\n\nì¦‰, ìš°ë¦¬ëŠ” train setì„ í†µí•´ì„œ vocabularyë¥¼ ì™„ì„±í•œë‹¤. ê·¸ë¦¬ê³ , ê° wordì˜ count ë° í•„ìš”ì— ë”°ë¼ í•„ìš”í•œ word sequenceì˜ countë¥¼ ìˆ˜ì§‘í•˜ì—¬ $p(w_i)$ë¥¼ êµ¬í•œ í›„ ìœ„ì— ë°©ë²•ì„ í†µí•´ì„œ íŠ¹ì • classë¥¼ ì¶”ì¸¡í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\nì´ì œ êµ¬ì²´ì ì¸ Naive Bayesì˜ ë™ì‘ ì ˆì°¨ëŠ” Spam Filteringì´ë¼ëŠ” Case Studyë¥¼ í†µí•´ì„œ ìì„¸íˆ ì‚´í´ë³´ë„ë¡ í•˜ì.\n\n## Case Study. Spam Filtering\n\nì´ˆê¸° NLPê°€ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ì—ˆë˜ ì˜ˆì‹œ ì¤‘ì— í•˜ë‚˜ì´ë‹¤. ì—¬ëŸ¬ ê°œì˜ ë©”ì¼ì— spamì¸ì§€ hamì¸ì§€ë¥¼ labelingí•œ ë°ì´í„°ë¥¼ ê°–ê³  í›„ì— inputìœ¼ë¡œ mail ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ filteringí•˜ëŠ” ê²ƒì´ë‹¤. ìœ„ì—ì„œ ì‚´í´ë³´ì•˜ë˜ í™•ë¥ ì„ ê·¸ëŒ€ë¡œ ì ìš©í•˜ë©´ ëœë‹¤. ì˜ˆì¸¡ì— í•„ìš”í•œ í™•ë¥ ì„ ìŠµë“í•˜ê³ , ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ê³¼ ì´ë¥¼ í‰ê°€í•˜ëŠ” ë°©ë²•ì˜ ìˆœìœ¼ë¡œ ì„¤ëª…í•˜ê² ë‹¤.\n\n### 0. Preprocessing\n\nì‚¬ì‹¤ mail dataì˜ í˜•íƒœê°€ ì´ìƒí•  ìˆ˜ë„ ìˆë‹¤. Subjectë¶€í„° ì‹œì‘í•˜ì—¬ ë‚ ì§œ ë°ì´í„° ê·¸ë¦¬ê³  íŠ¹ìˆ˜ ë¬¸ì ë“±ì´ ì¡´ì¬í•  ìˆ˜ ìˆëŠ”ë°, ì´ë¥¼ ë¨¼ì € ì²˜ë¦¬í•´ì„œ í›„ì— ìˆì„ Modeling ë‹¨ê³„ì—ì„œ ì˜ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í˜•íƒœë¥¼ ë³€í˜•í•´ì£¼ì–´ì•¼ í•œë‹¤.\n\n[ğŸ”— ì´ì „ Posting(Text Processing)](/posts/nlp-text-processing)ì—ì„œ ë°°ì› ë˜ ê¸°ìˆ ë“¤ì„ í™œìš©í•˜ì—¬ ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n\nëŒ€í‘œì ìœ¼ë¡œ í•´ì¤„ ìˆ˜ ìˆëŠ” ì‘ì—…ë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. ëŒ€ì†Œë¬¸ì í†µì¼\n2. alphabetì´ í•˜ë‚˜ë¼ë„ ë“¤ì–´ìˆì§€ ì•Šì€ ë°ì´í„°ëŠ” ì‚­ì œ\n3. date, ì°¸ì¡° ë“±ì„ ì˜ë¯¸í•˜ëŠ” ë°ì´í„° ì‚­ì œ\n\n### 1. Modeling\n\nParameter Estimation / Learning / Modeling ë“±ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” ë‹¨ê³„ì´ë‹¤. ì¼ë‹¨ ìš°ë¦¬ëŠ” train setìœ¼ë¡œë¶€í„° ìš°ë¦¬ê°€ ì›í•˜ëŠ” í™•ë¥ ì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. ê·¸ ì „ì— ìš°ë¦¬ê°€ ì–´ë–¤ language modelì„ ì´ìš©í• ì§€ ì„ íƒí•´ì•¼ í•œë‹¤. ë¨¼ì € uni-gramì¸ ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ train setì´ ì •ì˜ëœë‹¤.\n$$\n\\text{TrainSet} = {(d_{1}, c_{1}),  (d_{2}, c_{2}), ..., (d_{N}, c_{N})}\n$$\n$$\nd_{i} = \\begin{cases}\n  {w_{1}, w_{2}, ... , w_{M_{i}}} \\quad&\\text{unigram} \\\\\n  {(<s>, w_{1}), (w_{1}, w_{2}), ... , (w_{M_{i}}, </s>)} \\qquad&\\text{bigram}\n\\end{cases}\n$$\n\nì´ì œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” parameter, ì¦‰ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ì´ë‹¤.\n\n> **unigram**\n\n$$\n\\begin{align*}\np(w_{i}|c_{j}) &= {\\text{count}(w_{i}, c_{j}) \\over \\sum_{w \\in V} \\text{count}(w, c_{j})} \\\\\np(c_{j}) &= {\\sum_{i = 1}^{N}{1[c_{i} = c_{j}]} \\over N}\n\\end{align*}\n$$\n\n> **bigram**\n\n$$\n\\begin{align*}\np(w_{i}|w_{i-1},c_{j}) &= {\\text{count}((w_{i-1}, w_{i}), c_{j}) \\over \\sum_{(w^{(1)}, w^{(2)}) \\in V} \\text{count}((w^{(1)}, w^{(2)}), c_{j})} \\\\\np(c_{j}) &= {\\sum_{i = 1}^{N}{1[c_{i} = c_{j}]} \\over N}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ë°˜ë“œì‹œ Smoothingì„ í•´ì£¼ì–´ì•¼ í•œë‹¤. ì™œëƒí•˜ë©´, spam mailì—ì„œ ì•ˆ ë³¸ ë‹¨ì–´ê°€ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë„ˆë¬´ë‚˜ ë†’ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, ì‹¤ì œ $p(w_{i}|c_{j})$ëŠ” ì•„ë˜ì™€ ê°™ì´ ë³€ê²½ëœë‹¤. (ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ë“¤ê¸° ìœ„í•´ì„œ Add-1 ë°©ì‹ì„ ì‚¬ìš©í–ˆë‹¤. - í•´ë‹¹ ë‚´ìš©ì´ ê¸°ì–µì´ ë‚˜ì§€ ì•ŠëŠ”ë‹¤ë©´, [ğŸ”— ì´ì „ í¬ìŠ¤íŒ…](/posts/nlp-language-modeling)ì„ ë‹¤ì‹œ ë³´ê³  ì˜¤ì.)\n\n$$\np(w_{i}|c_{j}) = {\\text{count}(w_{i}, c_{j}) + 1 \\over \\sum_{w \\in V} \\text{count}(w, c_{j}) + |V|}\n$$\n\nì£¼ì˜í•  ì ì€ ë‹¤ì‹œ í•œ ë²ˆ ê°•ì¡°í•˜ì§€ë§Œ, $V$ëŠ” í›„ì— Estimationì—ì„œ inputìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë‹¨ì¼ documentê¹Œì§€ í¬í•¨í•œ Vocabularyì´ë‹¤.\n\n### 2. Estimation\n\nì´ì œ ìš°ë¦¬ê°€ ì–»ì€ parameterë¥¼ ì´ìš©í•´ì„œ ì‹¤ì œ input dataì— ëŒ€í•œ estimationì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\nì´ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\hat{c} = \\argmax_{c \\in C} p(c)\\prod_{w \\in d_{\\text{input}}}p(w|c)\n$$\n\në¬¼ë¡  ì–´ë–¤ n-gramì„ ì“°ëƒì— ë”°ë¼ $d_{\\text{input}}$ë„ í˜•íƒœê°€ ë‹¬ë¼ì§ˆ ê²ƒì´ë‹¤.\n\n### 3. Evaluation\n\nì´ì œ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ê²ƒì´ë‹¤. í‰ê°€ëŠ” ìš°ë¦¬ê°€ ì•Œì•„ë´¤ë˜ Accuracyì™€ F1 Scoreë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤. Binary Classificationì´ê¸° ë•Œë¬¸ì— ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n| prediction\\answer | True                                                                       | False                                                                     |\n| :---------------- | :------------------------------------------------------------------------- | :------------------------------------------------------------------------ |\n| Positive          | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} = c, c = \\text{spam}]$    | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} \\neq c, c = \\text{ham}]$ |\n| Negative          | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} \\neq c, c = \\text{spam}]$ | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} = c, c = \\text{ham}]$    |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-naive-bayes","date":"2022-10-21 15:37","title":"[NLP] 5. Naive Bayes","category":"AI","tags":["NLP"],"desc":"Naive Bayes Modelì€ ê°€ì¥ ì‰½ê²Œ Classificationì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” Modelì´ì§€ë§Œ, ì„±ëŠ¥ì´ ë‹¤ë¥¸ Modelì— ë¹„í•´ ë›°ì–´ë‚˜ì§€ëŠ” ì•Šë‹¤. ê·¸ëŸ¼ì—ë„ Naive BayesëŠ” ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” Modelì´ê¸°ì— ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ê³ , Classificationì˜ insightë¥¼ í‚¤ìš°ëŠ”ë° ë§ì€ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œëŠ”, ì „ë°˜ì ì¸ ê°œë…ê³¼ ì´ë¥¼ ì§ì ‘ Spam Filteringì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì‚´í´ë³¸ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nì´ì „ Postingì—ì„œëŠ” sentenceì˜ ì ì ˆì„±ì„ í™•ì¸í•œë‹¤ë“ ì§€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìœ ì¶”í•œë‹¤ë“ ì§€ ì˜¤íƒ€ë¥¼ ì •ì •í•˜ëŠ” ë“±ì— í•„ìš”í•œ ê¸°ë³¸ì ì¸ Language Modeling ë°©ì‹ì„ ì‚´í´ë³´ì•˜ë‹¤. ì´ë²ˆì—ëŠ” ì‹¤ì œë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì˜ˆì œì¸ Classificationì„ Language Modelì„ ì´ìš©í•˜ì—¬ ì–´ë–»ê²Œ êµ¬í˜„í•˜ëŠ”ì§€ë¥¼ ë‹¤ë£¬ë‹¤.\n\n## Classification\n\nClassificationì€ inputì´ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ ì•Œë§ì€ ë¶„ë¥˜ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. ë‹¨ìˆœíˆ Ruleì— ê¸°ë°˜í•˜ì—¬ ì´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ë„ ìˆì§€ë§Œ, Statisticí•œ Language Modelingì„ ì´ìš©í•˜ë©´, ë” ì •í™•ë„ê°€ ë†’ì€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ê²°êµ­ Statistic Predictionì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” 3ê°œ(Estimation, Modeling, Evaluation)ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ë´ì•¼ í•˜ëŠ” ê²ƒì€ Classificationë„ ë™ì¼í•˜ë‹¤. ë”°ë¼ì„œ, ì´ì— ëŒ€í•´ì„œ ì‚´í´ë³¼ ê²ƒì´ê³ , ê·¸ ì „ì— ë¨¼ì € Classification Model ì˜ ì¢…ë¥˜ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## Generative Model vs Discriminative Model\n\nClassificationì—ì„œ ì´ìš©ë˜ëŠ” Modelì„ í¬ê²Œ ë‘ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ”ë° ì´ì— ëŒ€í•´ì„œ ë¨¼ì € ì•Œì•„ë³´ë„ë¡ í•˜ì.\n\n1. **Generative Model(ìƒì„± Model)**\n   1. Naive Bayes\n   2. Hidden Markov Model(HMM)\n2. **Discriminative Model(íŒë³„ Model)**\n   1. Logistic Regression\n   2. K Nearest Neighbors\n   3. Support Vector Machine\n   4. Maximum Entropy Model(MaxEnt)\n   5. Neural Network(Deep Learning)\n\në‘ Modelì˜ ê°€ì¥ í° ì°¨ì´ì ì€ ì¶”ë¡ ì˜ ê³¼ì •ì´ë‹¤. ìš°ë¦¬ê°€ ì›í•˜ëŠ” ë°ì´í„° $P(\\text{class}=c | \\text{input} = \\text{data})$(íŠ¹ì • dataê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê° classì˜ ì†í•  í™•ë¥ )ë¥¼ ì–»ëŠ” ê³¼ì •ì´ ì„œë¡œ ë‹¤ë¥´ë‹¤.\n\n**ì²« ë²ˆì§¸**ë¡œ, $P(\\text{class}=c, \\text{input} = \\text{data})$ì¼ í™•ë¥ ì„ êµ¬í•˜ì—¬ **ê°„ì ‘ì **ìœ¼ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\n$$\n\\begin{align*}\nP(\\text{class}=c | \\text{input} = \\text{data}) &= {{P(\\text{class}=c, \\text{input} = \\text{data})}\\over{P(\\text{input} = \\text{data})}} \\\\\n&\\propto {P(\\text{class}=c, \\text{input} = \\text{data})}\n\\end{align*}\n$$\n\nì´ëŸ° ì‹ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ì¶”ë¡ í•˜ëŠ” ë°©ì‹ì„ <mark>Generative Model</mark>ì´ë¼ê³  í•œë‹¤. ì´ ë°©ì‹ì€ ê²°êµ­ Conditional Probabilityë¥¼ ì¶”ë¡ í•˜ê¸° ìœ„í•´ì„œ Joint Probabilityë¥¼ ì´ìš©í•˜ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì— ì–´ëŠì •ë„ í•œê³„ê°€ ì¡´ì¬í•œë‹¤ëŠ” ì ì„ ìœ ì˜í•˜ì.\n\n**ë‘ ë²ˆì§¸**ë¡œëŠ”, $P(\\text{class}=c | \\text{input} = \\text{data})$ë¥¼ **ì§ì ‘ì **ìœ¼ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œ, ë§ˆì¹œ Conditional Probabilityë¥¼ êµ¬í•œ ê²ƒê³¼ ìœ ì‚¬í•œ íš¨ê³¼ë¥¼ ë‚´ëŠ” **Discriminant Function(íŒë³„ í•¨ìˆ˜)**ì´ë¼ëŠ” íŠ¹ë³„í•œ í•¨ìˆ˜ë¥¼ inputì— ì ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ í•¨ìˆ˜ ì¤‘ì—ì„œ ê°€ì¥ ëŒ€í‘œì ì¸ ê²ƒì´ Softmax functionì´ë‹¤. ìš°ë¦¬ê°€ ë§Œì•½ inputì„ softmax functionì— ì…ë ¥í•˜ê²Œ ë˜ë©´, ì´ ê°’ì€ [0, 1] ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ í‘œí˜„ëœë‹¤. ì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” í•´ë‹¹ inputì´ classì¸ ê²½ìš° 1ì— ê°€ê¹ê²Œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° 0ì— ê°€ê¹ê²Œ í‘œí˜„í•˜ì—¬ ì—¬ëŸ¬ ë°ì´í„°ì— ì ìš©í•˜ë©´, classì˜ inpuutì— ë”°ë¥¸ ë¶„í¬ ì–‘ìƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , ì´ ë¶„í¬ ì–‘ìƒì„ í™•ë¥ ë¡œ ì¦‰ê°ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— softmax functionì„ ì·¨í•œ ê²°ê³¼ê°€ $P(\\text{class}=c | \\text{input} = \\text{data})$ê³¼ ë¹„ë¡€í•œë‹¤ëŠ” ê²°ë¡ ì„ ë‚¼ ìˆ˜ ìˆë‹¤. ìì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•˜ë‹¤ë©´, [ğŸ”— Logistic Regression](/posts/ml-logistic-regression#Logistic-Regression)ì„ ì°¸ê³ í•˜ë„ë¡ í•˜ì. ì´ëŸ¬í•œ ë°©ì‹ì„ ìš°ë¦¬ëŠ” <mark>Discriminative Model</mark>ì´ë¼ê³  í•œë‹¤.\n\nìœ„ì—ì„œ ì œì‹œí•œ ë°©ë²•ë“¤ ì¤‘ ëŒ€í‘œì ì¸ ë°©ë²•ë“¤ì€ ë³„ë„ì˜ Postingì„ í†µí•´ì„œ ì •ë¦¬í•˜ì˜€ë‹¤. í•´ë‹¹ ë§í¬ë¥¼ ì°¸ì¡°í•˜ì—¬ í™•ì¸í•´ë³´ë„ë¡ í•˜ì.\n\n- **Generative Model(ìƒì„± Model)**\n  - [ğŸ”— Naive Bayes](/posts/nlp-naive-bayes)\n  - [ğŸ”— Hidden Markov Model(HMM)](/posts/nlp-hmm)\n- **Discriminative Model(íŒë³„ Model)**\n  - [ğŸ”— Maximum Entropy Model(MaxEnt)](/posts/nlp-maxent)\n  - [ğŸ”— Logistic Regression](/posts/ml-logistic-regression)\n\n## Estimation\n\nì–´ë–¤ Modelì„ ì„ íƒí–ˆë‹¤ê³  í•˜ë”ë¼ë„ ê²°êµ­ ìš°ë¦¬ê°€ Classë¥¼ ê²°ì •í•˜ëŠ” ê³¼ì •ì„ ë™ì¼í•˜ë‹¤. ìœ„ì˜ ê³¼ì •ì„ í†µí•´ì„œ ì–´ì°Œë˜ì—ˆë“  ë‹¤ìŒ ê°’ì„ ì°¾ìœ¼ë©´ ëœë‹¤.\n\n$$\n\\begin{align*}\nc^{\\prime} &= \\argmax_{c \\in C}{P(\\text{class}=c | \\text{input} = \\text{data})}\n\\end{align*}\n$$\n\n## Modeling\n\nModelì„ ë§Œë“œëŠ” ê³¼ì •, ì¦‰ í•™ìŠµí•˜ëŠ” ê³¼ì •ì€ ê²°êµ­ Modelì˜ êµ¬í˜„ë§ˆë‹¤ ì²œì°¨ ë§Œë³„ì´ë‹¤. Naive BayesëŠ” ë‹¨ìˆœí•˜ê²Œ dataì˜ wordì™€ countë¥¼ í™œìš©í•˜ê³ , HMMì€ EM algorithmì„ í™œìš©í•˜ë©°, Linear Regressionì€ Gradient Descentë¥¼ í™œìš©í•œë‹¤. ë”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•Šê³  ìœ„ì—ì„œ ì œì‹œí•œ ë§í¬ë¥¼ ë”°ë¼ê°€ì„œ ê° Modelë§ˆë‹¤ì˜ í•™ìŠµë²•ì„ í™•ì¸í•´ë³´ë„ë¡ í•˜ì.\n\n## Evaluation\n\nClassificationì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ê²ƒ ì—­ì‹œ ì¤‘ìš”í•œ ì¼ì´ë‹¤. ê°€ì¥ ì‰¬ìš´ Binary Classificationë¶€í„° ì•Œì•„ë³´ì.\n\nbinary classificaitonì˜ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì´ 4ê°œ ì¤‘ í•˜ë‚˜ë¡œ ê²°ì •ëœë‹¤.\n\n| prediction\\answer | True           | False          |\n| :---------------- | :------------- | :------------- |\n| Positive          | true positive  | false positive |\n| Negative          | false negative | true negative  |\n\nì´ë¥¼ ì‰½ê²Œ ì´í•´í• ë ¤ë©´, ë³‘(ì½”ë¡œë‚˜)ì˜ ì–‘ì„±/ìŒì„± íŒì •ì´ rowì— í•´ë‹¹í•˜ê³ , ì‹¤ì œ ë³‘ì˜ ì—¬ë¶€ë¥¼ columnìœ¼ë¡œ ìƒê°í•˜ë©´ ì‰½ë‹¤. ë˜í•œ, ê° cellì˜ ê°’ì´ í—·ê°ˆë¦´ ìˆ˜ ìˆëŠ”ë°, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì´ ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ì˜ˆì¸¡ ê²°ê³¼ëŠ” ê·¸ëŒ€ë¡œ ë³´ì—¬ì£¼ë©´ì„œ, ì´ê²ƒì´ í‹€ë ¸ëŠ”ì§€ ë§ì•˜ëŠ”ì§€ë¥¼ ì•ì— true/falseë¡œ í‘œí˜„í–ˆë‹¤ê³  ìƒê°í•˜ë©´ ì‰½ë‹¤.\n\nclassificationì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œëŠ” ëŒ€í‘œì ìœ¼ë¡œ 4 ê°€ì§€ê°€ ìˆë‹¤.\n\n1. **Accuracy(ì •í™•ë„)**  \n   ê°€ì¥ ì‰½ê²Œ ê·¸ë¦¬ê³  ì¼ë°˜ì ìœ¼ë¡œ ìƒê°í•˜ëŠ” ì§€í‘œë‹¤. ìœ„ì˜ í‘œì—ì„œëŠ” ì „ì²´ ê²½ìš°ì˜ ìˆ˜ë¥¼ ë”í•˜ì—¬ ì˜³ê²Œ ì˜ˆì¸¡í•œ ê²ƒ(true postive, true negative)ì˜ í•©ì„ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.\n   $tp + fn \\over tp + fp + fn + tn$  \n   í•˜ì§€ë§Œ, ì´ ë°©ì‹ì€ í•œê³„ê°€ ìˆë‹¤. ë°”ë¡œ, ë°ì´í„°ê°€ í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì³ì ¸ìˆì„ ë•Œì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìš°ë¦¬ê°€ ì§„ì§œë¥¼ ì§„ì§œë¼ê³  ë§ì¶œí™•ë¥ ì€ ë†’ì§€ë§Œ, ê°€ì§œë¥¼ ê°€ì§œë¼ê³  ë§ì¶œ í™•ë¥ ì´ ë‚®ë‹¤ê³  í•  ë•Œ, ì´ë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ê¸°ê°€ ì–´ë µë‹¤. ê·¸ëŸ°ë° ë°ì´í„°ì—ì„œ ì§„ì§œê°€ ê°€ì§œë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ì„ ê²½ìš° ì •í™•ë„ëŠ” ì¢‹ì€ ì§€í‘œë¡œ ì“°ê¸° ì–´ë µë‹¤ëŠ” ê²ƒì´ë‹¤.\n2. **Precision(ì •ë°€ë„, ì •ë‹µë¥ )**  \n   ì‰½ê²Œ ì •ë‹µ ìì²´ë¥¼ ë§í í™•ë¥ ì…ë‹ˆë‹¤.  \n   $tp \\over tp + fn$\n3. **Recall(ì¬í˜„ìœ¨)**  \n   ì˜ˆì¸¡ì´ ë§ì„ í™•ë¥ ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  \n   $tp \\over tp + fp$\n4. **F1 Score**  \n   ì¢€ ë” ì„¸ë¶„í™”ëœ í‰ê°€ì§€í‘œì´ë‹¤. ì¡°í™” í‰ê· ì— ê¸°ë°˜í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì •í™•í•˜ê²Œ í‰ê°€í•  ë•Œ ì‚¬ìš©í•œë‹¤.  \n   ${2\\over{{1\\over\\text{Precision}} + {1\\over\\text{Recall}}}} = 2 \\times {\\text{Precision} \\times \\text{Recall} \\over \\text{Precision} + \\text{Recall}}$\n\nì—¬ê¸°ê¹Œì§€ ë´¤ìœ¼ë©´, ìŠ¬ìŠ¬ multi classì˜ ê²½ìš°ì—ëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ì§€ ê¶ê¸ˆí•  ê²ƒì´ë‹¤. ëŒ€ê²Œ ë‘ ê°€ì§€ ë°©ë²•ì„ í†µí•´ì„œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\n> **1. Micro Average**\n\nì „ì²´ classë¥¼ í•˜ë‚˜ì˜ binary tableë¡œ í•©ì¹˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, í´ë˜ìŠ¤ê°€ A, B, C 3ê°œê°€ ìˆë‹¤ë©´, ê° í´ë˜ìŠ¤ ë³„ë¡œ ì˜ˆì¸¡ ì„±ê³µë„ë¥¼ binaryë¡œ í‘œì‹œí•˜ê³ , ì´ë¥¼ í•˜ë‚˜ì˜ í…Œì´ë¸”ë¡œ í•©ì¹˜ëŠ” ê²ƒì´ë‹¤. ê·¸ í›„ì—ëŠ” binaryì—ì„œ ê³„ì‚°í•˜ëŠ” ì‹ì„ ê·¸ëŒ€ë¡œì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  \n\n> **2. Macro Average**\n\nmulti classì˜ ê²½ìš°ì—ë„ ë³„ë¡œ ë‹¤ë¥¼ ê²ƒì€ ì—†ë‹¤. ë‹¨ì§€ Precisionê³¼ Recall ê·¸ë¦¬ê³  Accuracyê°€ ì–´ë–»ê²Œ ë°”ë€ŒëŠ”ì§€ë§Œ ì•Œë©´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.  \n\n| prediction\\answer | c1            | c2            | c3            | c4            |\n| :---------------- | :------------ | :------------ | :------------ | :------------ |\n| c1                | true positive | x             | x             | x             |\n| c2                | x             | true positive | x             | x             |\n| c3                | x             | x             | true positive | x             |\n| c4                | x             | x             | x             | true positive |\n\n- Precision: $c_{ii} \\over \\sum_{j}c_{ij}$\n- Recall: $c_{ii} \\over \\sum_{j}c_{ji}$\n- Accuracy: $c_{ii} \\over \\sum_{i}\\sum_{j}c_{ij}$\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-classification","date":"2022-10-21 13:37","title":"[NLP] 4. Classification","category":"AI","tags":["NLP","Classification","Generative","Discriminative","ModelEvaluation"],"desc":"ì´ì „ Postingì—ì„œëŠ” sentenceì˜ ì ì ˆì„±ì„ í™•ì¸í•œë‹¤ë“ ì§€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìœ ì¶”í•œë‹¤ë“ ì§€ ì˜¤íƒ€ë¥¼ ì •ì •í•˜ëŠ” ë“±ì— í•„ìš”í•œ ê¸°ë³¸ì ì¸ Language Modeling ë°©ì‹ì„ ì‚´í´ë³´ì•˜ë‹¤. ì´ë²ˆì—ëŠ” ì‹¤ì œë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì˜ˆì œì¸ Classificationì„ Language Modelì„ ì´ìš©í•˜ì—¬ ì–´ë–»ê²Œ êµ¬í˜„í•˜ëŠ”ì§€ë¥¼ ë‹¤ë£¬ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nì´ì œ í†µê³„ì ì¸ ê´€ì ì—ì„œ NLì„ inputìœ¼ë¡œ í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ë°©ë²•ì„ ì°¾ì„ ê²ƒì´ë‹¤. ì´ë¥¼ Language Modelingì´ë¼ê³  í•˜ë©°, ì´ë¥¼ ìœ„í•´ì„œ ë˜ëŠ” ì´ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•œ ë°©ë²•ë“¤ì„ ì†Œê°œí•  ê²ƒì´ë‹¤.\n\n## Noisy Channel\n\nì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ì›í•˜ëŠ” ê²°ê³¼ê°€ ìˆë‹¤. ê·¸ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” ë§ì„ í•˜ê±°ë‚˜ í–‰ë™ì„ í•˜ê±°ë‚˜ ê¸€ì„ ì“´ë‹¤. ê·¸ ê³¼ì •ì€ ìš°ë¦¬ê°€ ê°–ê³  ì‹¶ì€ Aë¼ëŠ” ê²ƒì„ ì–»ê¸° ìœ„í•´ì„œ Bë¼ëŠ” í–‰ë™ì„ ëŒ€ì‹ í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. ì¦‰, ìš°ë¦¬ëŠ” ì´ë¥¼ Aì— noiseê°€ ê»´ì„œ Bë¼ëŠ” ê²ƒì´ ìƒì„±ë˜ì—ˆë‹¤ê³  ìƒê°í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ìš°ë¦¬ê°€ ê´€ì¸¡í•  ìˆ˜ ìˆëŠ” ê²ƒì€ Bë°–ì— ì—†ëŠ” ê²ƒì´ë‹¤.\n\nì´ëŠ” ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” NLì—ì„œë„ ë™ì¼í•˜ë‹¤. ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²°ê³¼ê°’ Aë¥¼ ì–»ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” Bë¼ëŠ” ë¬¸ì¥, ìŒì„±ì„ ì œì‹œí•œë‹¤. ê·¸ ê²°ê³¼ê°€ ì›í•˜ëŠ” ê²°ê³¼ë¡œ ë  ìˆ˜ ìˆëŠ” í™•ë¥ ì„ ì–»ì–´ì„œ ìµœì¢… ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ ëª©í‘œì¸ ê²ƒì´ë‹¤.\n\nì´ ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤.\n\n$$\nP(A|B) = {P(B|A)P(A)\\over{P(B)}}\\quad(\\text{Bayes Rule})\n$$\n\nìš°ë¦¬ê°€ ì–»ê³  ì‹¶ì€ $P(A|B)$ ë¥¼ ì–»ê¸° ìœ„í•´ì„œ, $P(A)$ì™€ $P(B|A)$ ë¥¼ í†µí•´ì„œ êµ¬í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. ì´ì— ëŒ€í•œ ë” ìì„¸í•œ ë‚´ìš©ì€ MLì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì´ë‹¤. ì¶”ì²œí•˜ëŠ” Postingì€ [ğŸ”— [ML] Parametric Estimation](/posts/ml-parametric-estimation)ì´ë‹¤.\n\n## Language Modeling\n\nê²°êµ­ ìš°ë¦¬ê°€ ì–»ê³  ì‹¶ì€ ê²ƒì€ íŠ¹ì • ë¬¸ì¥ì˜ í• ë‹¹ëœ í™•ë¥ ì¸ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, Language Modelì€ inputìœ¼ë¡œ word sequenceê³¼ ë“¤ì–´ì™”ì„ ë•Œ, í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë‹¤.\nê·¸ë¦¬ê³ , ì´ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ parameterë¥¼ ì°¾ëŠ” ê³¼ì •ì„ Language Modelingì´ë¼ê³  í•œë‹¤.\n\n## Input(N-gram)\n\nëŒ€ê²Œ ì´ëŸ¬í•œ ëª¨ë¸ì€ ë¬¸ì¥ ë˜ëŠ” wordì˜ ë°°ì—´ì´ ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì§ˆ ë•Œ, $W = w_{1}\\ w_{2}\\ w_{3}\\ ...\\ w_{n}$ ì•„ë˜ì™€ ê°™ì€ í˜•íƒœë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\n\n- **Single word probability**  \n  í•˜ë‚˜ì˜ ë‹¨ì–´ì˜ í™•ë¥ ì„ ë‚˜íƒ€ë‚¼ ë•Œ ë‹¨ìˆœí•˜ê²Œ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•œë‹¤.  \n  $P(w_{i})\\quad(w_{i} \\in W)$\n- **Sequence of Words probability**  \n  ì¼ë°˜ì ìœ¼ë¡œ sentenceì˜ í™•ë¥ ì„ ë‚˜íƒ€ë‚¼ ë•Œ, ì—¬ëŸ¬ ë¬¸ì¥ì„ í•œêº¼ë²ˆì— ê°€ì§€ëŠ” í™•ë¥ ì´ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.  \n  $P(W) = P(w_{1}, w_{2}, w_{3}, ..., w_{n})$\n- **single word probability with context**  \n  ì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ì´ì „ì— ì‚¬ìš©í•œ ë‹¨ì–´ê°€ ë¬¸ë§¥ì´ë¼ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, êµ¬ì²´ì ì¸ ë‹¨ì–´ë“¤ ì´í›„ì— íŠ¹ì • ë‹¨ì–´ê°€ ë‚˜ì˜¤ëŠ” ê²ƒì€ ë¬¸ë§¥ì„ ë°˜ì˜í•œ í™•ë¥ ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.  \n  $P(W) = P(w_{5}| w_{1}, w_{2}, w_{3}, w_{4})$\n\nìœ„ì˜ ì‹ì„ ë³´ê²Œ ë˜ë©´, ìš°ë¦¬ëŠ” ë‹¤ì‹œ í•œë²ˆ sentenceì˜ í™•ë¥ ì„ ë‹¤ì‹œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\nP(W) = P(w_{1}) \\times P(w_{2}|w_{1}) \\times P(w_{3}|w_{1},w_{2}) \\times ... \\times P(w_{n}| w_{1},w_{2},..., w_{n-1})\n$$\n\nìœ„ì˜ ì‹ì„ ë³´ê²Œë˜ë©´, Wê°€ ì§§ë‹¤ê³  í•˜ë”ë¼ë„ êµ‰ì¥íˆ ë§ì€ ì²˜ë¦¬ê°€ í•„ìš”í•˜ê³ , ì €ì¥ì„ ìœ„í•´ ë§ì€ ê³µê°„ì´ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” í˜„ì¬ ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë„ˆë¬´ ì˜¤ë˜ëœ ë‹¨ì–´ì— ëŒ€í•´ì„œëŠ” ë¬´ì‹œë¥¼ í•˜ë„ë¡ í•˜ëŠ” ë°©ë²•ì„ ì·¨í•˜ëŠ” ê²ƒì´ë‹¤.(**Markov Chain**) ì´ë¥¼ \"n ë²ˆì§¸ê¹Œì§€ í—ˆë½\"í–ˆì„ ë•Œ, ì´ë¥¼ n-gram ì´ë¼ê³  ë¶€ë¥¸ë‹¤.\n\n$$\np(W) = \\prod_{i=1}^{n}{p(w_{i}|w_{i-n+1},w_{i-n+2},...,w_{i-1})}\n$$\n\nê·¸ë ‡ë‹¤ë©´, n-gramì—ì„œ ì ì ˆí•œ nì´ë€ ë¬´ì—‡ì¼ê¹Œ? ì¼ë°˜ì ìœ¼ë¡œëŠ” nì´ í¬ë‹¤ëŠ” ê²ƒì€ contextë¥¼ ë§ì´ ë°›ì•„ë“¤ì¼ ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ë¡œ ë°›ì•„ë“¤ì—¬ì§ˆ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, nì´ í´ìˆ˜ë¡ ì„±ëŠ¥ì˜ ìµœì í™” ê°€ëŠ¥ì„±ì´ ë” ë†’ë‹¤. í•˜ì§€ë§Œ, Vocabularyì˜ ì‚¬ì´ì¦ˆê°€ ì»¤ì§€ëŠ” ê²½ìš°ë¥¼ ì˜ˆë¥¼ ë“¤ì–´ë³´ì. ì—¬ê¸°ì„œëŠ” $|V| = 60$k ë¼ê³  í•´ë³´ì.\n\n| n-gram          | p(w_{i})                         | # of parameters   |\n| :-------------- | :------------------------------- | :---------------- |\n| 0-gram(uniform) | ${1\\over\\vert V\\vert}$           | 1                 |\n| 1-gram(unigram) | $p(w_{i})$                       | $6\\times10^4$     |\n| 2-gram(bigram)  | $p(w_{i}\\vert w_{i-1})$          | $3.6\\times10^9$   |\n| 3-gram(trigram) | $p(w_{i}\\vert w_{i-2}, w_{i-1})$ | $2.16\\times10^14$ |\n\nnì´ ì»¤ì§ˆ ìˆ˜ë¡ ê°€ëŠ¥í•œ ì¡°í•©ì˜ ìˆ˜ëŠ” êµ‰ì¥íˆ ì»¤ì§€ê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ ë³´ì§€ ëª»í•˜ëŠ” ê²½ìš°ì˜ ìˆ˜ë„ êµ‰ì¥íˆ ì¦ê°€í•˜ê²Œ ë˜ì–´ dataìì²´ì˜ ë¹ˆë„ê°€ ì ì–´ì§€ëŠ” í˜„ìƒ(sparse)ì´ ë°œìƒí•œë‹¤. ë”°ë¼ì„œ, ëŒ€ê²Œì˜ ê²½ìš° ìµœëŒ€ nì˜ í¬ê¸°ëŠ” 3ì •ë„ë¡œ í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\n\n```plaintext\n ğŸ¤” ì£¼ì˜\n\n ì‹¤ì œ ë°ì´í„°ë¥¼ ê°€ê³µí•  ë•Œì—ëŠ” bigramë¶€í„°ëŠ” ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ í‘œì‹œí•´ì£¼ì–´ì•¼ í•œë‹¤. \n ê·¸ë ‡ì§€ ì•Šìœ¼ë©´, ì²«ë²ˆì§¸ ë¬¸ìì˜ í™•ë¥ ì„ êµ¬í•  ë•Œ, ì´ì „ ë‹¨ì–´ì˜ ì˜í–¥ì„ ë°›ì„ ìˆ˜ ì—†ë‹¤.\n ì •í•´ì§„ ê·œì¹™ì€ ì—†ì§€ë§Œ, ëŒ€ê²Œ <s></s>ë¥¼ ì´ìš©í•œë‹¤.\n ex.  bigram : <s> w1 w2 w3 w4 </s>\n     trigram : <s> <s> w1 w2 w3 w4 </s> </s>\n```\n\n```plaintext\n ğŸ¤” Google N-gram\n\n êµ¬ê¸€ì—ì„œ 2006ë…„ì— N-gramì„ ì§ì ‘ êµ¬ì„±í•œ ê²ƒì´ ìˆë‹¤. \n ì´ 1,024,908,257,229ê°œì˜ ë‹¨ì–´ê°€ ì¡´ì¬í•˜ê³ , 40íšŒ ì´ìƒ ë“±ì¥í•˜ëŠ” 5-gramì´ 1,176,470,663ê°œ ì¡´ì¬í•œë‹¤. \n ì´ Vocabularyì˜ sizeëŠ” 200ë²ˆ ì´í•˜ë¡œ ë“±ì¥í•˜ëŠ” ê²ƒì€ ì œì™¸í•˜ë©´, 13,588,391ê°œì´ë‹¤. \n```\n\n## Estimation\n\nMLì—ì„œëŠ” Estimationì„ ìˆ˜í–‰í•  ë•Œ, continuousí•˜ê²Œ ì¶”ì •í•˜ì˜€ë‹¤. ì¦‰, ë³´ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ continuousí•œ ë¶„í¬ì˜ parameterë§Œ ì¶”ì •í•˜ë©´ ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ, NLPì—ì„œëŠ” ë‹¤ë¥´ë‹¤. NLë¥¼ continuousí•˜ê²Œ í‘œí˜„í•  ë§ˆë•…í•œ ë°©ë²•ì´ ì—†ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê²°êµ­ ëª¨ë“  í™•ë¥ ì„ discreteí•˜ê²Œ êµ¬í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” íŠ¹ì • ë‹¨ì–´ì˜ í™•ë¥ ì„ êµ¬í•˜ëŠ” ë°©ë²•ì€ ë‹¨ í•˜ë‚˜ê°€ ëœë‹¤.\n\n$$\n|T| = \\text{count of observed tokens}\n$$\n$$\nc(w_{i}) = \\text{count of observed } w_{i}\n$$\n$$\n\\begin{align*}\n&P(w_{i}) = {{c(w_{i})}\\over{|T|}} \\\\\n&P(w_{i}| w_{i-1}) = {{c(w_{i-1}, w_{i})}\\over{c(w_{i-1})}} \\\\\n&P(w_{i}| w_{i-2}, w_{i-1}) = {{c(w_{i-2}, w_{i-1}, w_{i})}\\over{c(w_{i-2}, w_{i-1})}} \\\\\n\\end{align*}\n$$\n\nì´ë•Œ, ë°˜ë“œì‹œ sequenceì˜ ìˆœì„œë¥¼ ìœ ì˜í•˜ë„ë¡ í•˜ì. ìˆœì„œê°€ ë°”ë€Œë©´ ë‹¤ë¥¸ ì¢…ë¥˜ì´ë‹¤.\n\n> **Small Example**\n\në°ì´í„°ê°€ ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì§„ë‹¤ê³  í•˜ì.\n\n```plaintext\n He can buy the can of soda.\n```\n\nì´ë•Œ ê° n-gramì„ ì´ìš©í•œ modelì˜ í™•ë¥ ë“¤ì„ ì‚´í´ë³´ì.\n\n| model    | probability                                                                                                                                                     |\n| :------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| uni-gram | $p(He)=p(buy)=p(the)=p(of)=p(soda)=p(.) = 0.125$<br /> $p(can)=0.25$                                                                                            |\n| bi-gram  | $p(He\\vert <s>)=p(can\\vert He)=p(the\\vert buy)=p(can\\vert the)=p(soda\\vert of)=p(.\\vert sode) =p(</s> \\vert .) = 1$<br /> $p(buy\\vert can)=p(of\\vert can)= 0.5$ |\n| tri-gram | $p(He\\vert <s>, <s>)=p(can\\vert <s>, He)=p(the\\vert He, buy)=...=p(</s>\\vert ., </s>) =1$                                                                       |\n\n## Evaluation\n\ní‰ê°€í•  ë•ŒëŠ” MLê³¼ ê²°êµ­ì€ ë™ì¼í•˜ë‹¤. ìš°ë¦¬ê°€ í™•ë¥ ë¶„í¬ë¥¼ êµ¬í•  ë•Œ, ì‚¬ìš©í•œ ë°ì´í„° ì™¸ì— ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ ì˜ ì ìš©ì´ ë˜ì—ˆëŠ”ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, wordì˜ ê°¯ìˆ˜ì™€ ë°ì´í„°ì˜ ìˆ˜ê°€ êµ‰ì¥íˆ ë§ì€ NLì˜ íŠ¹ì„±ìƒ ì´ Evaluation ë‹¨ê³„ì—ë§Œ êµ‰ì¥íˆ ë§ì€ ì‹œê°„ì„ ì†Œëª¨í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì¦‰ê°ì ì¸ í‰ê°€ë¥¼ ìœ„í•´ì„œ ì‚¬ìš©í•˜ëŠ” ì²™ë„ê°€ ìˆë‹¤.\n\n> **Perplexity**\n\ntrain setì„ í†µí•´ í•™ìŠµì„ í•˜ê³ , test setì„ í†µí•´ì„œ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ë•Œ, train setì„ í†µí•´ êµ¬í•œ í™•ë¥ ì´ ì‹¤ì œ test setì—ì„œ ì–´ëŠì •ë„ì˜ Entropyë¥¼ ë°œìƒì‹œí‚¤ëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ë‹¤. ì›ë˜ì˜ ì‹ì€\n$PP = 2^{H}$ì´ì§€ë§Œ, ì´ë¥¼ ë³€í˜•í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\nPP(W) &= \\sqrt[N]{1 \\over P(w_1, w_2, ..., w_N)} \\\\\n&= \\sqrt[N]{\\prod_{i=1}^{N}{P(w_i|w_{i-n+1}, w_{i-n+2}, ..., w_{i-1})}}\n\\end{align*}\n$$\n\nì´ë¥¼ í†µí•´ì„œ, ì‹¤ì œë¡œ í•´ë‹¹ ë¬¸ì œê°€ ë„ˆë¬´ ì–´ë µì§€ëŠ” ì•Šì€ì§€, ì„ íƒí•œ modelì´ ì˜ëª»ë˜ì§€ëŠ” ì•Šì•˜ëŠ”ì§€ë¥¼ íŒë‹¨í•œë‹¤.\n\n| model    | probability                                                                                                                                                     | entropy |\n| :------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------ |\n| uni-gram | $p(He)=p(buy)=p(the)=p(of)=p(soda)=p(.) = 0.125$<br /> $p(can)=0.25$                                                                                            | 2.75    |\n| bi-gram  | $p(He\\vert <s>)=p(can\\vert He)=p(the\\vert buy)=p(can\\vert the)=p(soda\\vert of)=p(.\\vert sode) =p(</s> \\vert .) = 1$<br /> $p(buy\\vert can)=p(of\\vert can)= 0.5$ | 0.25    |\n| tri-gram | $p(He\\vert <s>, <s>)=p(can\\vert <s>, He)=p(the\\vert He, buy)=...=p(</s>\\vert ., </s>) =1$                                                                       | 0       |\n\nìœ„ì˜ ì˜ˆì‹œë¥¼ ê°€ì ¸ì™€ì„œ ë´ë³´ì. ë¬¼ë¡  ë™ì¼í•œ datasetì—ì„œ perplexityë¥¼ ì¸¡ì •í•˜ê¸°ëŠ” í–ˆì§€ë§Œ, nì´ ì»¤ì§ˆ ìˆ˜ë¡ ì ì  entropyê°€ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ëŸ° dataê°€ ì¢‹ì€ ê±¸ê¹Œ? ì´ëŠ” ì¢‹ì€ ê²Œ ì•„ë‹ˆë‹¤. ì™œëƒí•˜ë©´, í•´ë‹¹ datasetì—ì„œë§Œ ì˜ ì‘ë™í•˜ë„ë¡ ë˜ì–´ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì¼ëª… **overfitting**ì´ë‹¤.\n\n## Smooting\n\nìœ„ì—ì„œ ë§í•œ overfittingì„ ì–´ëŠì •ë„ í•´ì†Œí•  ë¿ë§Œ ì•„ë‹ˆë¼ ì •ë§ í° ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” probabilityê°€ 0ì´ ë˜ëŠ” ë¬¸ì œ(ìš°ë¦¬ê°€ trainsetì„ í†µí•´ í•™ìŠµí•œ í™•ë¥  ë¶„í¬ì—ì„œ testsetì— ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” í™•ë¥ ê°’ì´ ì—†ì„ ë•Œ, ì¦‰ í•´ë‹¹ í™•ë¥ ì´ 0ì¼ë•Œ)ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” smoothingì´ í•„ìˆ˜ì ì´ë‹¤. probabilityê°€ 0ì´ ëœë‹¤ëŠ” ê²ƒì€ í›„ì— ìœ„ì—ì„œ êµ¬í•œ í™•ë¥ ë¡œ Predictionì„ í•  ë•Œ ëª¨ë“  ì˜ˆì¸¡ì„ ë§ì¹˜ëŠ” ìš”ì¸ì´ ëœë‹¤. ì™œëƒí•˜ë©´, ì¶”ì •í™•ë¥ ì˜ ìµœì  Entropyë¥¼ ì˜ë¯¸í•˜ëŠ” Cross Entropyë¥¼ $\\infin$ë¡œ ë§Œë“¤ê¸° ë•Œë¬¸ì´ë‹¤. (ìµœì  Entropyê°€ ë¬´í•œëŒ€ë¼ëŠ” ê²ƒì€ ì¶”ì •ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.)\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” probabilityê°€ 0ì´ ë˜ì§€ ì•Šê²Œ í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ê¸°ì¡´ì˜ í™•ë¥ ì˜ ì¼ë¶€ë¥¼ ë‚˜ëˆ„ì–´ì£¼ë„ë¡ í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤. ì´ê²ƒì´ smoothingì´ë‹¤.\n\nì´ë•Œ ë°˜ë“œì‹œ ìœ ì˜í•  ì ì€ smoothingì„ í•˜ê±´ ì•ˆí•˜ê±´ ê° í™•ë¥ ì˜ ì´í•©ì€ 1ì´ ë˜ë„ë¡ ë³´ì¥í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\sum_{w \\in \\Omega}p(w) = 1\n$$\n\nëŒ€ëµ 6ê°€ì§€ì˜ ëŒ€í‘œì ì¸ smoothing ë°©ì‹ë“¤ì„ ì†Œê°œí•˜ê² ë‹¤.\n\n> <mark>**1. Add-1(Laplace)**</mark>\n\nê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì˜ smoothing ë°©ë²•ì´ì§€ë§Œ, ì‹¤ìš©ì ì¸ë©´ì€ ë‹¤ì†Œ ë–¨ì–´ì§€ëŠ” ë°©ë²•ì´ë‹¤. ì•„ì´ë””ì–´ëŠ” ê°„ë‹¨í•˜ë‹¤. predictionì„ ìˆ˜í–‰í•  ë•Œ, í˜„ì¬ ë“¤ì–´ì˜¨ inputê¹Œì§€ í¬í•¨í•˜ì—¬ ë§Œë“  $|V|$ë¥¼ ë¶„ëª¨ì— ë”í•˜ê³ , ë¶„ìì— 1ì„ ë”í•´ì£¼ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´, ì„¤ì‚¬ countê°€ 0ì´ ë”ë¼ë„ í™•ë¥ ì´ 0ì´ ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.\n\n$$\np(w) = {X \\over Y} \\rArr p^{\\prime}(w) = {X + 1 \\over Y + |V|}\n$$\n\nì—¬ê¸°ì„œ, $|V|$ ê°’ì´ ì •ë§ í—·ê°ˆë ¸ëŠ”ë°, ì•„ë¬´ë„ ì˜ ì„¤ëª…ì„ ì•ˆí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ì§šê³  ë„˜ì–´ê°€ë©´, ìš°ë¦¬ê°€ í™•ë¥ ê°’ì„ ì–»ê¸° ìœ„í•´ì„œ ì‚¬ìš©í–ˆë˜ datasetê³¼ í˜„ì¬ predictionì„ í•˜ê¸° ìœ„í•´ì„œ ë“¤ì–´ì˜¨ input ë‘˜ì—ì„œ ë°œìƒí•œ ëª¨ë“  uniqueí•œ ë‹¨ì–´ì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. (# of vocabulary)\n\nê·¸ë ‡ê²Œ í•´ì•¼ë§Œ $\\sum_{w \\in \\Omega}p(w) = 1$ì„ ë§Œì¡±í•˜ëŠ” ê°’ì´ ë‚˜ì˜¨ë‹¤.\n\në”°ë¼ì„œ, ì´ë¥¼ ê° ê°ì˜ n-gramì— ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n| n    | $p(w_{i})$                                                 | $p^{\\prime}(w_{i})$                                                            |\n| :--- | :--------------------------------------------------------- | :----------------------------------------------------------------------------- |\n| $1$  | $c(w_{i}) \\over \\vert T \\vert$                             | $c(w_{i}) + 1 \\over \\vert T \\vert + \\vert V \\vert$                             |\n| $2$  | ${{c(w_{i-1}, w_{i})}\\over{c(w_{i-1})}}$                   | ${{c(w_{i-1}, w_{i}) + 1}\\over{c(w_{i-1})} + \\vert V \\vert} $                  |\n| $3$  | ${{c(w_{i-2}, w_{i-1}, w_{i})}\\over{c(w_{i-2}, w_{i-1})}}$ | ${{c(w_{i-2}, w_{i-1}, w_{i}) + 1}\\over{c(w_{i-2}, w_{i-1})} + \\vert V \\vert}$ |\n\n> <mark>**2. Add-k**</mark>\n\n1ì´ë¼ëŠ” ìˆ«ìê°€ ê²½ìš°ì— ë”°ë¼ì„œëŠ” êµ‰ì¥íˆ í° ì˜í–¥ì„ ì¤„ ë•Œê°€ ìˆë‹¤. íŠ¹íˆ ê¸°ì¡´ ë°ì´í„°ì˜ ìˆ˜ê°€ ì ì€ ê²½ìš°ì— ë”ìš± ê·¸ë ‡ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ 1ë³´ë‹¤ ì‘ì€ ì„ì˜ì˜ ê°’(k)ì„ ì“°ëŠ” ë°©ë²•ì´ë‹¤.\n\n$$\np(w) = {X \\over Y} \\rArr p^{\\prime}(w) = {X + 1 \\over Y + k|V|}\n$$\n\ní•˜ì§€ë§Œ, ìœ„ì™€ ê°™ì€ ë°©ì‹ì€ ê²°êµ­ ì–´ë–¤ í™•ë¥  ê°’ì´ë“ ì§€ ë¶„ìì— 1ì„ ë”í•˜ê¸° ë•Œë¬¸ì— ë¶ˆí‰ë“±í•˜ê²Œ ê°’ì„ ë‚˜ëˆ ì¤€ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´, ì• ì´ˆì— count(ë¶„ì)ê°€ í° ë°ì´í„°ì—ê²Œ 1ì€ ë³„ë¡œ ì˜í–¥ì„ ì•ˆì£¼ê² ì§€ë§Œ, ë¶„ìê°€ ì²˜ìŒë¶€í„° ì‘ì•˜ë˜ ê²½ìš°ì—ëŠ” ì´ë¡œ ì¸í•´ì„œ ë°›ëŠ” ì˜í–¥ì´ êµ‰ì¥íˆ í¬ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, ì´ëŸ¬í•œ í•œê³„ì ì„ ê·¹ë³µí•  ìˆ˜ ìˆëŠ” ë°©ë²•ë“¤ì´ ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ë“¤ì´ë‹¤.\n\n> <mark>**3.Good Turing**</mark>\n\nì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ìš°ë¦¬ëŠ” ìƒˆë¡œìš´ featureì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì•¼ í•œë‹¤. ë°”ë¡œ wordì˜ frequencyì˜ frequencyì´ë‹¤.\n\n$$\nN_{k} = \\sum_{i=1}^{n}1[c(w_{i}) = k]\n$$\n\nì•„ë§ˆ ì˜ˆì‹œë¥¼ ë´ì•¼ ì´í•´ê°€ ë¹ ë¥¼í…Œë‹ˆ í•˜ë‚˜ì˜ ì˜ˆì‹œë¥¼ ë³´ì.\n\n```plaintext\n sam I am I am sam I do not eat\n```\n\nì´ ê²½ìš° ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ countë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n  &c(I) &=\\ 3 \\\\\n  &c(sam) &=\\ 2\\\\\n  &c(am) &=\\ 2\\\\\n  &c(do) &=\\ 1\\\\\n  &c(not) &=\\ 1\\\\\n  &c(eat) &=\\ 1\\\\\n\\end{align*}\n\\quad\\rArr\\quad\n\\begin{align*}\n  & N_{1} = 3 \\\\\n  & N_{2} = 2 \\\\\n  & N_{3} = 1\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ Good Turingì€ í•œ ë²ˆë„ ì•ˆë³¸ ë°ì´í„°ì—ê²ŒëŠ” í•œ ë²ˆë§Œ ë³´ëŠ” ê²½ìš°ì˜ ìˆ˜ë¥¼ ì „ì²´ ê²½ìš°ì˜ ìˆ˜ë¡œ ë‚˜ëˆˆê°’ë§Œí¼ì˜ í™•ë¥ ì„ ë‚˜ëˆ„ì–´ì£¼ê³ , ê¸°ì¡´ ë°ì´í„°ë“¤ì—ê²ŒëŠ” laplaceì²˜ëŸ¼ 1ì„ ë”í•´ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë¹„ë¡€í•˜ëŠ” ë§Œí¼ì„ ê³±í•´ì£¼ì–´ ì ì ˆí•œ í™•ë¥ ì„ ê°€ì ¸ê°ˆ ìˆ˜ ìˆê²Œí•˜ì˜€ë‹¤.\n\në”°ë¼ì„œ, ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n$$\np(w_{i}) = {(c(w_{i})+1)N_{c(w_{i})+1}\\over{N_{c(w_{i})}}} \\times {1\\over |T|},\\quad (N_{0} = 1)\n$$\n\nëŒ€ê²Œ ${(c(w_{i})+1)N_{c(w_{i})+1}\\over{N_{c(w_{i})}}}$ì´ ë¶€ë¶„ì„ $c^{*}$ë¼ê³ ë„ ë¶€ë¥¸ë‹¤.\n\n> <mark>**4. Kneser-Ney**</mark>\n\nê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” Smoothing ë°©ì‹ìœ¼ë¡œ ê¸°ì–µí•´ë‘ëŠ” ê²ƒì´ ì¢‹ë‹¤. ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì €, Absolute Discountingì„ ë¨¼ì € ì´í•´í•´ì•¼ í•œë‹¤.  \nGood-turing ë°©ì‹ì„ ì‚¬ìš©í–ˆì„ ë•Œ $c$ì™€ $c^{*}$ì‚¬ì´ì— ì°¨ì´ê°€ ê²½í—˜ì ìœ¼ë¡œ íŠ¹ì • ìƒìˆ˜ë§Œí¼ì”© ì°¨ì´ê°€ ë‚œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ì—¬,\n\n$$\nc^{*} = c - d\n$$\n\nChurchê³¼ Galeì€ ì´ë¥¼ Absolute Discounting í™•ë¥ ì´ë¼ë©° ë‹¤ìŒ ì‹ì„ ì œì‹œí•œë‹¤.\n\n$$\nP(w_{i}|w_{i-1}) = {c(w_{i-1}, w_{i}) -d \\over c(w_{i-1})} + \\lambda(w_{i-1})P(w)\n$$\n\nì—¬ê¸°ì„œ ë’·ì— ë¶€ë¶„ $\\lambda(w_{i-1})P(w)$ì€ discountingìœ¼ë¡œ ë°œìƒí•œ ì˜¤ì°¨ë¥¼ ë§¤ê¾¸ê¸° ìœ„í•œ ê°’ì´ë‹¤.\n\nì—¬ê¸°ì„œ Kneser-Ney problemì€ ë” ë„“ì€ ë²”ìœ„ë¡œ í™•ì¥ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë²”ìœ„ë¡œ í™•ì¥ì‹œí‚¨ ê²ƒì´ë‹¤. ê¸°ì¡´ì—ëŠ” bigramìœ¼ë¡œ ì œí•œë˜ì–´ ìˆë˜ Absolute Discountingì˜ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•ëœë‹¤.\n\n$$\nP_{KN}(w_{i}|w_{i-n+1}^{i-1}) = {\\max(c(w_{i-1}, w_{i}) -d, 0) \\over c(w_{i-n+1}^{i-1})} + \\lambda(w_{i-n+1}^{i-1})P_{KN}(w_{i}|w_{i-n+2}^{i-1})\n$$\n\n(ìœ„ì˜ ì‹ì— ëŒ€í•´ì„œ ì •í™•í•˜ê²Œ ì´í•´ë¥¼ í•˜ì§€ ì•Šì•˜ì§€ë§Œ, ê·¸ë ‡êµ¬ë‚˜ í•˜ê³  ë„˜ì–´ê°€ë„ ì¶©ë¶„í•  ê²ƒ ê°™ë‹¤.)\n\n> <mark>**5. Backoff & Interpolation**</mark>\n\nìƒí™©ì— ë”°ë¼ì„œ unigram, bigram, trigramì„ ê°€ì¤‘ì¹˜ë§Œí¼ ë”í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤. ê²°êµ­ n-gramì—ì„œ nì´ ì‘ì•„ì§ˆ ìˆ˜ë¡ detailì„ ì‹ ê²½ì“¸ ìˆ˜ ì—†ì§€ë§Œ, ì‹ ë¢°ë„ ìì²´ëŠ” ëŠ˜ì–´ë‚  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ì ì ˆíˆ ì„ì–´ì“°ë©´ ì¢‹ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ì´ë¡ ì´ë‹¤. í•˜ì§€ë§Œ, ì–´ë–¤ ê²ƒì„ ë” ì¤‘ì ìœ¼ë¡œ ì§ì ‘ ì •í•´ì£¼ì–´ì•¼ í•œë‹¤.\n\n$$\np^{\\prime}(w_{i}|w_{i-2}, w_{i-1}) = \\lambda_{3}p(w_{i}|w_{i-2}, w_{i-1}) + \\lambda_{2}p(w_{i}|w_{i-1}) + \\lambda_{1}p(w_{i}) + {\\lambda_{0}\\over|V|}\n$$\n\nì´ë¥¼ ì •í•  ë•ŒëŠ” ëŒ€ê²Œ held-out dataë¥¼ í™œìš©í•´ì„œ êµ¬í•œë‹¤.(validation setì´ë¼ê³  ë¶€ë¥¸ë‹¤.) ì¦‰, ì „ì²´ corpusë¥¼ (train, validation, test)ë¡œ ì ì ˆíˆ ë‚˜ëˆ„ì–´ ì“°ë¼ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ì„±ëŠ¥ì„ ì¸¡ì •í•  ë•ŒëŠ” testsetì„ ì“°ê³ , $\\lambda$ë¥¼ ì¶”ì •í•  ë•Œì—ëŠ” validation(heldout)setì„ ì‚¬ìš©í•˜ë¼ëŠ” ê²ƒì´ë‹¤.\n\n## Word Class\n\nSmoothing ë°©ì‹ì„ ì´ìš©í•´ì„œ unseen dataë¥¼ ì²˜ë¦¬í•´ì£¼ì—ˆëŠ”ë° ì¢€ ë” íš¨ê³¼ì ìœ¼ë¡œ ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ê³ ë ¤í•œ ê²ƒì´ë‹¤. classë‹¨ìœ„ë¡œ wordë¥¼ groupingí•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ì˜€ë‹¤ê³  í•˜ë”ë¼ë„ íŠ¹ì • groupì— ì†í•œë‹¤ë©´, ì´ë¥¼ í™œìš©í•´ì„œ ì–´ëŠì •ë„ í™•ë¥ ì„ ë¶€ì—¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ ë°©ì‹ì„ í™œìš©í•˜ë©´, ì‹¤ì œë¡œ ë³´ì§€ ì•Šì€ ë°ì´í„°ì— ëŒ€í•´ì„œë„ í˜„ì‹¤ì ì¸ ì¶”ì •ì´ ê°€ëŠ¥í•˜ì§€ë§Œ detailì— ëŒ€í•œ ì„±ëŠ¥ì€ ê°ì†Œí•  ìˆ˜ ìˆë‹¤.\n\n$$\np(w_{i}|w_{i-1}) \\rArr p(w_{i}|c_{i-1}) ={ c(c_{i}, w_{i}) \\over c(c_{i-1}) }\n$$\n\nìœ„ì˜ ì‹ì„ ë³´ë©´, ì´ì „ ë‹¨ì–´ì˜ contextë¥¼ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì´ì œëŠ” classë¥¼ ë³´ê³  ë‹¤ìŒ ë‹¨ì–´ë¥¼ probabilityë¥¼ ê³„ì‚°í•˜ë„ë¡ ë°”ë€ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ í™•ë¥ ì€ classë‚´ë¶€ì—ì„œ í•´ë‹¹ ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ì´ì „ classì˜ ë¹ˆë„ë¡œ ë‚˜ëˆ„ì—ˆë‹¤ê³  ë³´ë©´ ë˜ê² ë‹¤.\n\n$$\np(w_{i}|c_{i-1}) = p(w_{i}|c_{i}) \\times p(c_{i}|h_{i})\n$$\n\nì¦‰, class ë‹¨ìœ„ë¡œ ë‹¨ì–´ë¥¼ ë¬¶ê³  classì—ì„œ ë‹¨ì–´ì˜ ë°œìƒ í™•ë¥ ì— classì—ì„œì˜ n-gramì„ ê³±í•œ ê°’ì´ ë˜ëŠ” ê²ƒì´ë‹¤. ì¼ë°˜ì ì¸ Bayes Decison Ruleì— ê¸°ë°˜í•˜ì—¬ ì„ íƒí•œë‹¤ê³  ë³´ë©´ ë˜ê² ë‹¤.\n\n## Example. Spelling Correction\n\nì—¬íƒœê¹Œì§€ ë°°ìš´ ë‚´ìš©ì„ í™œìš©í•˜ì—¬ Spelling ì˜¤ë¥˜ë¥¼ ì •ì •í•´ì£¼ëŠ” applicationì„ ì œì‘í•œë‹¤ê³  í•´ë³´ì. ë¨¼ì €, Spelling Errorì˜ ì¢…ë¥˜ë¶€í„° ì•Œì•„ë³´ë„ë¡ í•˜ì.\n\n- **Non word Error**  \n  ì˜ëª»ëœ spellingì— ì˜í•´ì„œ ì „í˜€ ëœ»ì´ ì—†ëŠ” ë‹¨ì–´ê°€ ë§Œë“¤ì–´ì§„ ê²½ìš°ì´ë‹¤.  \n  í•´ê²°ì„ ìœ„í•´ì„œëŠ” ì‚¬ì „ì—ì„œ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì°¾ì•„ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²ƒ ë˜ëŠ” ì´ì „ì— ë°°ì› ë˜ shortest edit distanceë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.\n- **Real word Error**  \n  ì˜ëª»ëœ spelling ë˜ëŠ” ìœ ì‚¬í•œ ë°œìŒ ë•Œë¬¸ì— ëœ»ì´ ìˆëŠ” ë‹¨ì–´ê°€ ë§Œë“¤ì–´ì¡Œì§€ë§Œ, ì˜¤ë¥˜ê°€ ì˜ì‹¬ë˜ëŠ” ê²½ìš°ì´ë‹¤.  \n  í•´ê²°ì±…ì€ ë¹„ìŠ·í•œ ë°œìŒ ë˜ëŠ” spellingì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ì°¾ì•„ì„œ í•´ë‹¹ ë‹¨ì–´ì™€ í•¨ê»˜ language modelì— ë„£ì–´ì„œ ê°€ì¥ ë†’ì€ ê°€ëŠ¥ì„±ì„ ê°€ì§€ëŠ” ê°’ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\në¨¼ì €, **Non Word Error** ê°™ì€ ê²½ìš°ëŠ” ì˜¤íƒ€ ë°ì´í„°ì— ì›ë˜ ì“°ë ¤ê³  í–ˆë˜ ê°’ì„ labelingí•´ì„œ ëª¨ì•„ë‘ê³  ë‹¤ìŒ ê°’ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.\n(*x=ì˜¤íƒ€ë°ì´í„°, w=ì‚¬ì „ì—ìˆëŠ”ë‹¨ì–´)\n\n- $P(x|w)$ = xê°€ wì¼ ê°€ëŠ¥ì„±\n- $p(w)$ = wì˜ í™•ë¥ \n\nê·¸ë¦¬ê³  ë‚˜ì„œ, ë‹¤ìŒì„ ì‹¤í–‰ì‹œì¼œì„œ ê°€ì¥ ì ì ˆí•œ $\\hat{w}$ë¥¼ ì°¾ìœ¼ë©´ ëœë‹¤.\n$$\n\\begin{align*}\n\\hat{w} &= \\argmax_{w \\in V}P(w|x) \\\\\n&= \\argmax_{w \\in V}{P(x|w)P(w)}\n\\end{align*}\n$$\n\n**Real Word Error**ì˜ ê²½ìš°ì—ëŠ” ê²°êµ­ ì´ì „ ë‹¨ì–´ sequenceë¥¼ í™œìš©í•´ì•¼ í•œë‹¤. ì „ì²´ corpusë¥¼ í•™ìŠµí•´ì„œ tri-gramì„ ì¶”ì¶œí•´ë†“ê³ , ë²ˆê°ˆì•„ê°€ë©´ì„œ í›„ë³´ ë‹¨ì–´ë“¤ì„ ì§‘ì–´ë„£ì–´ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í›„ë³´ ë‹¨ì–´ê°€ ë‹¤ìŒê³¼ ê°™ì´ ì •í•´ì¡Œë‹¤ê³  í•˜ì. ($\\bold{w}_{3} = {w_3^{(1)}, w_3^{(2)}, w_3^{(3)}, ...}$) ì´ë•Œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” wëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\hat{w}_{3} = \\argmax_{w_{3}^{(i)} \\in \\bold{w}_{3}} P(w_{3}^{(i)}| w_{1}, w_{2})\n$$\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-language-modeling","date":"2022-10-21 12:15","title":"[NLP] 3. Language Modeling","category":"AI","tags":["NLP","NoisyChannel","Ngram","LanguageModeling","Smoothing","WordClass"],"desc":"ì´ì œ í†µê³„ì ì¸ ê´€ì ì—ì„œ NLì„ inputìœ¼ë¡œ í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ë°©ë²•ì„ ì°¾ì„ ê²ƒì´ë‹¤. ì´ë¥¼ Language Modelingì´ë¼ê³  í•˜ë©°, ì´ë¥¼ ìœ„í•´ì„œ ë˜ëŠ” ì´ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•œ ë°©ë²•ë“¤ì„ ì†Œê°œí•  ê²ƒì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nìš°ë¦¬ëŠ” Linear Regression, Logistic Regression, SVMì„ ê±°ì¹˜ë©° dataë¡œ ë¶€í„° ìœ ì˜ë¯¸í•œ patternì„ ë°œê²¬í•˜ëŠ” ê³¼ì •ì„ ì•Œì•„ë³´ì•˜ë‹¤. ì´ ê³¼ì •ì€ ìš°ë¦¬ì—ê²Œ ëª…í™•í•œ ì‹ í•˜ë‚˜ë¥¼ ì œì‹œí•˜ì˜€ê³ , ëª¨ë“  ê³¼ì •ì„ ìš°ë¦¬ê°€ ì œì–´í•  ìˆ˜ ìˆê²Œ í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ë¥¼ ìš°ë¦¬ê°€ ëª¨ë‘ ëª…í™•í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ê²ƒì¸ì§€ëŠ” ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ì´í•´í•˜ì§€ëŠ” ëª»í•˜ì§€ë§Œ, ì•Œì•„ì„œ ìµœì ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê²Œ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ê¹Œ? ì´ëŸ° ë§ˆë²•ê°™ì€ ì¼ì— ëŒ€í•œ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ” ê²ƒì´ Neural Networkì´ë‹¤.\n  ê²Œ ì•Œì§€ ëª»í•˜ì§€ë§Œ inputì´ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ ì²˜ë¦¬í•´ì„œ outputì„ ì „ë‹¬í•˜ëŠ” ì‹œìŠ¤í…œì„ ìš°ë¦¬ì˜ ì‹ ì²´ì—ì„œ ì°¾ê²Œ ëœë‹¤. ë°”ë¡œ ìš°ë¦¬ ëª¸ì„ ì´ë£¨ëŠ” ì‹ ê²½ë§ì´ë‹¤. ì˜ˆì‹œë¡œ ìš°ë¦¬ëŠ” ëˆˆì„ í†µí•´ ë¹›ì´ë¼ëŠ” inputì„ ë°›ìœ¼ë©´, ìš°ë¦¬ ëˆˆê³¼ ë‡Œì—ì„œ ë¬´ìŠ¨ ì¼ì´ ë°œìƒí•˜ëŠ”ì§€ëŠ” ëª¨ë¥´ì§€ë§Œ ê²°ê³¼ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë¬¼ì²´ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. ì´ ê³¼ì •ì„ ì¶”ì¸¡ì˜ ê³¼ì •ì— ë„ì…í•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ?\n\n## Perceptron\n\nPerception(ì¸ì§€ ëŠ¥ë ¥) + Neuron(ì‹ ê²½ ì„¸í¬)ì˜ í•©ì„±ì–´ì´ë‹¤. ì¤‘ê³ ë“±í•™êµ ìƒëª… ìˆ˜ì—…ì„ ë“¤ì—ˆë‹¤ë©´, ìš°ë¦¬ì˜ ëª¨ë“  ì‹ ê²½ì€ ë‰´ëŸ°ì´ë¼ëŠ” ë‹¨ìœ„ ì„¸í¬ë¡œ ì´ë£¨ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ë°°ì› ì„ ê²ƒì´ë‹¤. ì¦‰, ìš°ë¦¬ì˜ ì‹ ê²½ ì„¸í¬ë¥¼ ì»´í“¨í„° ê³µí•™ì—ì„œ í™œìš©í•˜ê¸° ìœ„í•´ì„œ, ìˆ˜í•™ì ìœ¼ë¡œ ë³€í™˜í•œ ê²ƒì´ë‹¤. í˜•íƒœë¥¼ ë¨¼ì € ì‚´í´ë³´ì.\n\n$$\ny = sign(\\bold{w}^{\\top}\\bold{x} + b)\n$$\n\n![nn-perceptron-1](/images/nn-perceptron-1.jpg)\n\nëŒ€ë‹¨í•œ ê²ƒì„ ê¸°ëŒ€í–ˆë‹¤ë©´ ì‹¤ë§í•˜ê² ì§€ë§Œ, simpleí•œ ê²ƒì´ ìµœê³ ë¼ëŠ” ì—°êµ¬ì˜ ì§„ë¦¬ì— ë”°ë¼ì„œ ìœ„ì˜ ì‹ì€ ê½¤ë‚˜ í•©ë¦¬ì ì´ë‹¤. ìš°ë¦¬ê°€ Linear Regressionê³¼ Logistic Regressionì„ ë°°ì› ìœ¼ë‹ˆ ì•Œ ê²ƒì´ë‹¤. ì´ëŠ” ì‚¬ì‹¤ Linear Regressionì„ ì´ìš©í•´ì„œ ìš°ë¦¬ê°€ Classificationì„ ìˆ˜í–‰í•  ë•Œ ì‚¬ìš©í–ˆë˜ ì‹ì´ë‹¤. ì¦‰, perceptron í•˜ë‚˜ëŠ” inputì„ ì„ í˜•ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” decision boundaryë¥¼ ì°¾ëŠ” ê²ƒê³¼ ê°™ë‹¤.\n\n> **Optimization**\n\nê·¸ë ‡ë‹¤ë©´, í•´ë‹¹ perceptronì„ í†µí•´ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒì„ ë§Œì¡±í•˜ëŠ” $\\bold{w}$ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤.\n\n$$\ny_{n} =\n\\begin{cases}\n1  &\\text{ if  } \\bold{x}_{n} \\in \\mathcal{C}_{1} \\\\\n-1 &\\text{ if  } \\bold{x}_{n} \\in \\mathcal{C}_{2} \\\\\n\\end{cases}\n$$\n$$\ny_{n}\\bold{w}^{\\top}\\bold{x}_{n} \\gt 0, \\forall n\n$$\n\nê²°êµ­ Loss í•¨ìˆ˜ëŠ” perceptronì˜ ì˜ëª»ëœ classification ê²°ê³¼ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\mathcal{J}(\\bold{w}) = - \\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{w}^{\\top}\\bold{x}_{n}} \\quad( \\mathcal{M}(\\bold{w}) = \\{ n : y_{n}\\bold{w}^{\\top}\\bold{x}_{n} \\} )\n$$\n$$\n\\nabla_{\\bold{w}}\\mathcal{J}(\\bold{w}) = - \\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{x}_{n}}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Gradient Descentì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} + \\alpha\\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{x}_{n}}\n$$\n\nê°„ë‹¨í•œ ì˜ˆì‹œë¡œ AND, OR Gateë¥¼ percentronì„ í†µí•´ í‘œí˜„í•´ë³´ì.\n\n![nn-and-gate](/images/nn-and-gate.jpg)\n![nn-or-gate](/images/nn-or-gate.jpg)\n\ní•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ë°ì´í„°ëŠ” í•­ìƒ ì™„ë²½í•˜ê²Œ ì„ ìœ¼ë¡œ ë‚˜ë‰˜ì–´ì§€ì§€ëŠ” ì•ŠëŠ”ë‹¤. í•˜ë‚˜ì˜ perceptronìœ¼ë¡œëŠ” ì•„ë˜ì˜ XORì¡°ì°¨ë„ êµ¬ë¶„í•´ë‚¼ ìˆ˜ ì—†ë‹¤.\n\n![nn-multi-line-example](/images/nn-multi-line-example.jpg)\n\n## Multilayer Perceptron\n\nìœ„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ë‚˜ì˜¨ ê²ƒì´ perceptronì„ ë‹¤ì¸µìœ¼ë¡œ ìŒ“ì•„ì„œ í•´ê²°í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ì œëŠ” í•˜ë‚˜ì˜ ì‹ ê²½ì„¸í¬ì˜€ë˜ perceptronì„ ì§„ì§œ ì‹ ê²½ë§ì²˜ëŸ¼ ì—°ê²°í•´ë³´ìëŠ” ê²ƒì´ë‹¤.\n\në¨¼ì € ì¶”ìƒì ì¸ ì˜ˆì‹œë¥¼ ìƒê°í•´ë³´ì. ìš°ë¦¬ê°€ XOR Gateë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ì–´ë–¤ Gateë¥¼ ê²°í•©í•´ì•¼í• ê¹Œ?\n\n$$\na \\oplus b = ab + \\bar{a}\\bar{b}\n$$\n\nìš°ë¦¬ëŠ” AND Gate 2ê°œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³ , í•´ë‹¹ ê²°ê³¼ê°’ì„ ì´ìš©í•´ì„œ OR Gate ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë©´ XOR Gateë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ê° GateëŠ” ìš°ë¦¬ê°€ perceptronìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì—ˆëŠ”ë° ê·¸ëƒ¥ ì´ê²ƒì„ gateë¡œ í‘œí˜„í•˜ë“¯ì´ ë˜‘ê°™ì´ ë‚˜íƒ€ë‚´ë©´ í’€ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?\n\nê·¸ë˜ì„œ ì§ì ‘ ìˆ˜í–‰í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n![nn-xor-gate](/images/nn-xor-gate.jpg)\n\n```plaintext\n ğŸ¤” Insight\n\n ìœ„ì˜ ê³¼ì •ì„ ë³´ë‹¤ë³´ë©´ ë†€ë¼ìš´ ê²ƒì„ í•˜ë‚˜ ë°œê²¬í•  ìˆ˜ ìˆë‹¤. ë°”ë¡œ ì™¼ ìª½ ê·¸ë¦¼ì˜ ë³€í™”ì´ë‹¤. \n ì²«ë²ˆì§¸, ë‘ ê°œì˜ perceptronì„ í†µí•´ì„œ ë§Œë“¤ì–´ì§„ outputì´ ì´ë£¨ëŠ” ê²°ê³¼ê°’ì˜ í˜•íƒœë¡œ featureë¥¼ ë³€í™˜í•˜ë©´, \n í•˜ë‚˜ì˜ perceptronìœ¼ë¡œ decision boundaryë¥¼ ê·¸ë¦´ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. \n ì´ëŠ” ë§ˆì¹˜ ì´ì „ linear regressionì—ì„œ ë°°ì› ë˜ basis function(Ï•)ì´ í–ˆë˜ ì—­í• ì´ë‹¤.\n\n ê·¸ë ‡ë‹¤ë©´, ì´ë¥¼ ë”ìš± í™•ì¥í•´ë³´ì. \n ë§Œì•½ í•´ë‹¹ Layerê°€ ë” ê¹Šì–´ì§„ë‹¤ê³  í•´ë„, ì¶œë ¥ ì§ì „ì˜ layerëŠ” ë‹¨ìˆœíˆ ì´ì „ ëª¨ë“  layerëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ê°€ê³µí•´ì„œ\nfeatureë¥¼ ë³€í™˜í•˜ëŠ” í•˜ë‚˜ì˜ basis function(Ï•)ë¥¼ ì·¨í•œ ê²ƒìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n```\n\nê²°ë¡ ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë” ë³µì¡í•˜ê³ , ì–´ë ¤ìš´ ë¬¸ì œì˜ ê²½ìš°ì—ë„ ë” ê¹Šê²Œ ì‹ ê²½ë§ì„ êµ¬ì„±í•˜ë©´ ê²°êµ­ì€ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n> **Universal Approximation Theorem**\n\nìœ„ì™€ ê°™ì€ ê¹Šì€ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì´ìš©í•˜ìëŠ” ì£¼ì¥ë„ ìˆì§€ë§Œ, ì´ì™€ ìœ ì‚¬í•˜ê²Œ ë„“ì€ ì‹ ê²½ë§ì„ ì“°ìëŠ” ì£¼ì¥ë„ ì¡´ì¬í–ˆë‹¤.  \n\n![nn-universal-approx-theorem-1](/images/nn-universal-approx-theorem-1.jpg)\n\në§Œì•½, ìš°ë¦¬ê°€ í•˜ë‚˜ì˜ Layerì™€ outputì—ì„œ ìµœì¢… output perceptronë§Œ ê°–ê³  ì²˜ë¦¬ë¥¼ í•œë‹¤ë©´, ê²°êµ­ ì—¬ëŸ¬ perceptronì˜ weighted í•©ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ ê²½ìš° ìš°ë¦¬ëŠ” ê³„ë‹¨ í•¨ìˆ˜ì˜ weighted í•©ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆëŠ”ë° perceptronì´ ë§ì•„ì§ˆ ìˆ˜ë¡ ì´˜ì´˜í•´ì§€ë©° ì •ë‹µê³¼ ìœ ì‚¬í•œ ì¶”ë¡ ì´ ê°€ëŠ¥í•´ì§„ë‹¤.(ë§ˆì¹˜ ì ë¶„ì˜ ê°œë…ê³¼ ìœ ì‚¬í•˜ë‹¤. ë¬¼ë¡  ì´ëŠ” ì¶”ìƒì ì¸ ì„¤ëª…ì´ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œëŠ” ê³„ë‹¨í•¨ìˆ˜ì˜ í•©ì´ê¸° ë•Œë¬¸ì— ì¢€ ë‹¤ë¥´ë‹¤.)\n\n![nn-universal-approx-theorem-2](/images/nn-universal-approx-theorem-2.jpg)\n\nìœ„ì˜ ê·¸ë¦¼ì„ ë³´ë©´ ì´í•´í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ì´ ë°©ì‹ì€ ê²°êµ­ ëª¨ë“  í•¨ìˆ˜ í˜•íƒœë¥¼ ê¸°ì–µí•˜ëŠ” ê²ƒì´ë‹¤.(**memorizer**) ì´ê²ƒì€ input dataê°€ ë§ì•„ì§ˆ ìˆ˜ë¡ ë³µì¡ë„ê°€ ê¸‰ê²©í•˜ê²Œ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµê³¼ ì˜ˆì¸¡ê³¼ì •ì— êµ‰ì¥íˆ ë§ì€ ì‹œê°„ì„ ì†Œëª¨í•œë‹¤.\n\n> **Multilayer Optimization(Backpropagation)**\n\nê·¸ë ‡ë‹¤ë©´, ë„“ì€ ì‹ ê²½ë§ì´ í•œê³„ê°€ ìˆìœ¼ë‹ˆ ì„ íƒì§€ëŠ” inputê³¼ output ì‚¬ì´ì˜ layer(**hidden layer**)ì˜ ê°¯ìˆ˜ë¥¼ ëŠ˜ë ¤ì„œ ê¹Šì€ ì‹ ê²½ë§ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ê³  ìˆëŠ” perceptronì€ signí•¨ìˆ˜ë¡œ ê°ì‹¸ì ¸ìˆê¸° ë•Œë¬¸ì— ë¯¸ë¶„ ì‹œì— ê¸°ìš¸ê¸°ê°€ 0ì´ë¼ëŠ” ë¬¸ì œë¥¼ ê°–ëŠ”ë‹¤. ë˜í•œ, ê·¸ë ‡ë‹¤ê³  ì •ë‹µì˜ ê°¯ìˆ˜ë¥¼ ì´ìš©í•˜ê¸°ì—ëŠ” ê° perceptronì˜ ì˜í–¥ì„ ì „ë‹¬í•˜ê¸°ì— ë¶€ì¡±í•˜ë‹¤ëŠ” ê²ƒì´ ëª…í™•í•˜ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” perceptronì— ìˆëŠ” ì •ë‹µì„ íŒë³„í•˜ëŠ” í•¨ìˆ˜ signì„ ë‹¤ë¥¸ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•˜ê¸°ë¡œ í•œë‹¤.\n\n![nn-perceptron-2](/images/nn-perceptron-2.jpg)\n\nì—¬ê¸°ì„œ ì´ í•¨ìˆ˜ë¥¼ ìš°ë¦¬ëŠ” **activation function**ì´ë¼ê³  ë¶€ë¥´ê³  ëŒ€í‘œì ìœ¼ë¡œëŠ” ê°™ì€ ì¢…ë¥˜ê°€ ìˆë‹¤.\n\n- **sigmoid**  \n  ìš°ë¦¬ê°€ ê°€ì¥ ì‰½ê²Œ ìƒê°í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ì´ë‹¤. logistic regressionì—ì„œ ì‚¬ìš©í•´ë³¸ë§Œí¼ ê¸°ìš¸ê¸°ê°’ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.\n- **tanh**  \n  sigmodì™€ êµ‰ì¥íˆ ìœ ì‚¬í•œ í•¨ìˆ˜ì´ë‹¤. ë”°ë¼ì„œ, ë¹„ìŠ·í•œ ìš©ë„ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.\n- **ReLU**  \n  ì¶œë ¥ ì‹œì ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, ê° ê°ì˜ hidden layerì—ì„œ ì´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì™œëƒí•˜ë©´, sigmoid í•¨ìˆ˜ëŠ” ì¶œë ¥ê°’ì˜ í˜•íƒœê°€ [0, 1], tanhëŠ” [-1, 1]ì´ê¸° ë•Œë¬¸ì— ë°˜ë³µí•´ì„œ ì ìš©í•˜ë©´, gradientê°€ ì‚¬ë¼ì§€ëŠ” í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ê¸°ìš¸ê¸°ë¥¼ ìˆëŠ” ê·¸ëŒ€ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì´ëŸ¬í•œ í˜•íƒœë¥¼ ì¶œë ¥ ì´ì „ì—ëŠ” ë§ì´ ì‚¬ìš©í•œë‹¤.\n- **Leaky ReLU**  \n  ReLUê°€ ìŒìˆ˜ê°’ì„ ì™„ì „íˆ ë¬´ì‹œí•˜ëŠ”ë° Leaky ReLUëŠ” ì´ëŸ¬í•œ ë°ì´í„°ê°€ ì¡°ê¸ˆì´ë¼ë„ ì˜ë¯¸ ìˆëŠ” ê²½ìš°ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n- **ELU**  \n  Leaky ReLUì™€ ë¹„ìŠ·í•œ ì´ìœ ì´ë‹¤.\n\n  ![activation-functions](/images/activation-functions.png)\n  \nìë£Œê°€ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤ë©´ [ğŸ”— wikipedia](https://en.wikipedia.org/wiki/Activation_function)ë¥¼ ì°¸ê³ í•˜ì.\n\n---\n\nì ì´ì œ ì‹¤ì œë¡œ ì–´ë–»ê²Œ optimizationì„ ìˆ˜í–‰í• ì§€ë¥¼ ì•Œì•„ë³´ë„ë¡ í•˜ì.\n\në¨¼ì €, LossëŠ” ê°€ì¥ ë§ˆì§€ë§‰ layer(output layer)ì˜ outputê³¼ ì‹¤ì œ ê°’ê³¼ì˜ ì°¨ì´ê°€ ë  ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n(ì•„ë˜ì„œ $\\bold{h}_{L}$ì€ Lë²ˆì§¸ layerì˜ outputì„ ì˜ë¯¸í•œë‹¤.)\n\n$$\n\\begin{align*}\n\\mathcal{L} &= \\sum_{n=1}^{N}{\\ell(y_n, \\bold{h}_L)} \\\\\n&= \\sum_{n=1}^{N}{(y_{n} - \\bold{h}_{L})^2} \\\\\n&= \\sum_{n=1}^{N}{(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))^2} \\\\\n&= \\sum_{n=1}^{N}{(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\sigma(\\bold{w}_{L-1}^{\\top}\\bold{h}_{L-2} + b_{L-2}) + b_{L-1}))^2} \\\\\n&= ...\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê²ƒì€ ìš°ë¦¬ëŠ” ì „ì²´ $\\bold{W}$ë¥¼ í•™ìŠµì‹œì¼œì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ì¶œë ¥ì¸µë§Œ í•™ìŠµí•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ì „ì²´ ëª¨ë“  layerì˜ $\\bold{w}_{i}$ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\nê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ìš°ë¦¬ëŠ” ${{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{i}}}$ë¥¼ ëª¨ë‘ êµ¬í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì•„ë§ˆ ê°€ì¥ ìŠµê´€ì ìœ¼ë¡œ í•˜ëŠ” í–‰ìœ„ëŠ” ìˆ«ìê°€ ì‘ì€ ê°’ë¶€í„° í¸ë¯¸ë¶„í•˜ë©´ì„œ ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ê·¸ë ‡ê²Œ í•˜ì§€ë§ê³  ë°˜ëŒ€ ìˆœì„œë¡œ ë¯¸ë¶„ì„ í•˜ë¼ëŠ” ê²ƒì´ **backpropagation**ì˜ main ideaì´ë‹¤.\n\n$$\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L}}} = \\sum_{n=1}^{N}\\{({\\partial \\bold{h}_L \\over \\partial \\bold{w}_{L}} )\\times\\red{-2(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))}\\}\n$$\n\n$$\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-1}}} = \\sum_{n=1}^{N}\\{({\\partial \\bold{h}_{L} \\over \\partial \\bold{w}_{L-1} })\\times\\red{-2(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))}\\}\n$$\n\nì¦‰, ë‹¤ìŒê³¼ ê°™ì€ chain ruleì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L}}} &= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L}}} \\\\\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-1}}} &= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n&= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{h}_{L-1}}} {{\\partial\\bold{h}_{L-1}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n&= \\blue{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L-1}}} } {{\\partial\\bold{h}_{L-1}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-2}}} &= \\blue{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L-1}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L-2}}} \\\\\n\\end{align*}\n$$\n\nìš°ë¦¬ëŠ” ë¹¨ê°„ìƒ‰ê³¼ íŒŒë€ìƒ‰ ë¶€ë¶„ì˜ ì—°ì‚°ì„ ì¬í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ë˜í•œ, ${{\\partial\\bold{h}_{l}}\\over{\\partial \\bold{w}_{l}}}$ì€ êµ‰ì¥íˆ ì‰¬ìš´ ì—°ì‚°ì´ê¸°ì— ìš°ë¦¬ê°€ ì‹ ê²½ ì¨ì„œ ê³„ì‚°í•´ì•¼ í•  ê°’ì€ ë§¤ë‹¨ê³„ë¥¼ ì—°ê²°í•´ì¤„ $ {{\\partial\\bold{h}_{l}}\\over{\\partial \\bold{h}_{l-1}}}$ì´ë‹¤.\n\n![ml-backpropagation](/images/ml-backpropagation.jpg)\n\n## Loss Function\n\nìš°ì„  KL-Divergence, Entropy, Cross Entropyì— ëŒ€í•œ ì•½ê°„ì˜ ì´í•´ê°€ í•„ìš”í•˜ë‹ˆ ì´ì „ Posting([ğŸ”— Base Knowledge](posts/ml-base-knowledge))ì„ ì‚´í´ë³´ê³  ì˜¤ì.\n\nìœ„ì—ì„œëŠ” ìì—°ìŠ¤ëŸ½ê²Œ Lossë¥¼ ê³„ì‚°í•  ë•Œ, Squared Errorë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ ê²½ìš°ì— ë”°ë¼ì„œëŠ” ë‹¤ì–‘í•œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. multiclass classificationì—ì„œëŠ” **Cross Entropy Loss**ë¥¼ ì‚¬ìš©í•œë‹¤.\n\nìš°ì„  Cross Entropy LossëŠ” ëŒ€ê²Œ L2 Loss(Squared Error)ì™€ ê°™ì´ ë¹„êµë˜ì–´ì§„ë‹¤. ìš°ì„  ìš°ë¦¬ê°€ ì´ì „ [ğŸ”— Parametric Estimation](posts/ml-parametric-estimation)ì—ì„œ MLEë¥¼ ë‹¤ë£° ë•Œ, KL-Divergenceë¥¼ í†µí•´ì„œ MLEê°€ ìµœì  parameterë¥¼ ì°¾ì„ ê²ƒì´ë¼ëŠ” ê±¸ ì¦ëª…í•œ ì ì´ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ [ğŸ”— Logistic Regression](/posts/ml-logistic-regression)ì—ì„œ Squared Errorë¥¼ í†µí•´ì„œ Lossë¥¼ êµ¬í–ˆë˜ ê³µì‹ì„ í™•ì¸í•´ë³´ì.(Gradient Asecent Part)\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê³µì‹ì„ ë´¤ì—ˆë‹¤.\n\n$$\n\\argmax_{\\bold{w}} \\sum_{n=1}^{N}y_{n}\\log{\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}) + (1-y_{n})\\log{(1-\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}))} }\n$$\n\nì´ ê³µì‹ì„ Cross Entropyë¥¼ í†µí•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\nH_{q}(p) &= - \\sum_{x \\in \\Omega}q(x)\\log_{2}p(x) \\\\\n&= \\sum_{n=1}^{N}{[-y_{n}\\log\\hat{y}_{n} - (1- y_{n})\\log(1-\\hat{y}_{n})]}\n\\end{align*}\n$$\n\nì¦‰, ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì–»ì„ ìˆ˜ ìˆëŠ” insightëŠ” Cross EntropyëŠ” sigmoidë¥¼ ì·¨í•œ binary classificationì—ì„œ Squared Errorì™€ ê°™ê³ , ì´ëŸ¬í•œ Cross Entropyë¥¼ Squared Errorê°€ í•  ìˆ˜ ì—†ëŠ” Multiclassì—ëŠ” ì ìš©í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ëŠ” ì ì´ë‹¤. ì™œëƒí•˜ë©´, multiclass classificationì— ì‚¬ìš©ë˜ëŠ” Softmax Functionì„ ì´ìš©í•´ì„œ Sigmoid functionì„ ìœ ë„í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì ì‹œ ê¹Œë¨¹ì—ˆì„ê¹Œë´ Softmax í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ ì ëŠ”ë‹¤.\n\n$$\n\\hat{y}_{k} = {{\\exp(\\bold{w}_{k}^{\\top}\\bold{x})}\\over{\\sum_{i=1}^{K}{\\exp(\\bold{w}_{i}^{\\top}\\bold{x})}}}\n$$\n\në”°ë¼ì„œ, Cross Entropy Lossë¥¼ ëŒ€ì…í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ Lossë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = \\sum_{n=1}^{N}\\sum_{k=1}^{K}[-y_{k,n}\\log\\hat{y}_{k,n}],\\quad y_{k,n} = p(x_{n} \\in C_{k}| x_{n})\n$$\n\nì—¬ê¸°ì„œ $y_{k,n}$ì€ one-hot encodingëœ ë°ì´í„°ë¡œ, ì •ë‹µì¸ classë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ë˜ì–´ ìˆë‹¤. ë”°ë¼ì„œ, multiclass classificationì—ì„œëŠ” ìœ„ì™€ ê°™ì€ Lossë¥¼ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤.\n\nì´ ë‘ê°€ì§€ ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ê°€ì§€ Loss Functionì´ ì´ë¯¸ ì¡´ì¬í•œë‹¤. ì˜ˆì „ì— ì ê¹ ì„¤ëª…í–ˆë˜ L1 Lossë¶€í„° ì‹œì‘í•´ì„œ NLLLoss, KLDivLoss ë“±ë“± ì¡´ì¬í•˜ë©°, dataì˜ íŠ¹ì„±ê³¼ outputì˜ í˜•íƒœì— ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ìŠ¤ìŠ¤ë¡œ Loss Functionì„ ìƒˆë¡œ ì •ì˜í•  ìˆ˜ë„ ìˆë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- activation function, wikipedia, <https://en.wikipedia.org/wiki/Activation_function>\n","slug":"ml-nn","date":"2022-10-20 09:00","title":"[ML] 6. Neural Network","category":"AI","tags":["ML","NeuralNetwork","Perceptron","Backpropagation","CrossEntropyLoss"],"desc":"ìš°ë¦¬ëŠ” Linear Regression, Logistic Regression, SVMì„ ê±°ì¹˜ë©° dataë¡œ ë¶€í„° ìœ ì˜ë¯¸í•œ patternì„ ë°œê²¬í•˜ëŠ” ê³¼ì •ì„ ì•Œì•„ë³´ì•˜ë‹¤. ì´ ê³¼ì •ì€ ìš°ë¦¬ì—ê²Œ ëª…í™•í•œ ì‹ í•˜ë‚˜ë¥¼ ì œì‹œí•˜ì˜€ê³ , ëª¨ë“  ê³¼ì •ì„ ìš°ë¦¬ê°€ ì œì–´í•  ìˆ˜ ìˆê²Œ í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ë¥¼ ìš°ë¦¬ê°€ ëª¨ë‘ ëª…í™•í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ê²ƒì¸ì§€ëŠ” ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ì´í•´í•˜ì§€ëŠ” ëª»í•˜ì§€ë§Œ, ì•Œì•„ì„œ ìµœì ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê²Œ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ê¹Œ? ì´ëŸ° ë§ˆë²•ê°™ì€ ì¼ì— ëŒ€í•œ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ” ê²ƒì´ Neural Networkì´ë‹¤.  ê²Œ ì•Œì§€ ëª»í•˜ì§€ë§Œ inputì´ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ ì²˜ë¦¬í•´ì„œ outputì„ ì „ë‹¬í•˜ëŠ” ì‹œìŠ¤í…œì„ ìš°ë¦¬ì˜ ì‹ ì²´ì—ì„œ ì°¾ê²Œ ëœë‹¤. ë°”ë¡œ ìš°ë¦¬ ëª¸ì„ ì´ë£¨ëŠ” ì‹ ê²½ë§ì´ë‹¤. ì˜ˆì‹œë¡œ ìš°ë¦¬ëŠ” ëˆˆì„ í†µí•´ ë¹›ì´ë¼ëŠ” inputì„ ë°›ìœ¼ë©´, ìš°ë¦¬ ëˆˆê³¼ ë‡Œì—ì„œ ë¬´ìŠ¨ ì¼ì´ ë°œìƒí•˜ëŠ”ì§€ëŠ” ëª¨ë¥´ì§€ë§Œ ê²°ê³¼ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë¬¼ì²´ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. ì´ ê³¼ì •ì„ ì¶”ì¸¡ì˜ ê³¼ì •ì— ë„ì…í•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ?","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nNLPë¥¼ ìˆ˜í–‰í•  ë•Œ, ìš°ë¦¬ëŠ” sequence of characterë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ì œëŒ€ë¡œ ì•Œì•„ì•¼ ì œëŒ€ë¡œ ëœ ì „ì²˜ë¦¬ì™€ í›„ì²˜ë¦¬ ë“±ì´ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ í•´ë‹¹ chapterì—ì„œëŠ” ì–´ë–»ê²Œ ì›ë³¸ ë¬¸ì/ë‹¨ì–´/ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ë‹¤ë£° ê²ƒì´ë‹¤.\n\n## Regular Express\n\nì•„ì£¼ ê¸°ë³¸ì ì¸ ë¬¸ìì—´ ì²˜ë¦¬ ë°©ë²•ì´ë‹¤. ì´ë¥¼ ì•Œê³  ìˆì–´ì•¼ ì‹¤ì§ˆì ì¸ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ë³„ë„ì˜ Postingìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë‹¤ë£¨ì—ˆë‹¤. ([ğŸ”— Regex](/posts/regex))ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ì.\n\n## Text Normalization\n\nìš°ë¦¬ê°€ ì‚¬ìš©í•  NLì€ ì •ì œë˜ì–´ ìˆì§€ ì•Šì•„ì„œ ì—¬ëŸ¬ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. ê·¸ ì¤‘ì—ì„œ ëŒ€ì¤‘ì ìœ¼ë¡œ ì¢‹ë‹¤ê³  ì•Œë ¤ì§„ ë°©ë²•ë“¤ì„ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì•„ë˜ ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\n\n1. Word Tokenization  \n   ë§ ê·¸ëŒ€ë¡œ NL ë°ì´í„°ê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ, ì´ë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìª¼ê°œëŠ” ê²ƒì´ë‹¤.\n2. Word Reformating  \n   ë‹¨ì–´ë¥¼ ë‚˜ëˆ„ì—ˆë‹¤ë©´, ê° ë‹¨ì–´ì˜ í˜•íƒœë¥¼ ì²˜ë¦¬í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ Normalizingí•˜ëŠ” ê²ƒì´ë‹¤.  \n3. Sentence Segmentation  \n   ë¬¸ì¥ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•´ëŠ” ê³¼ì •ì´ë‹¤.\n\nì´ì œ ê° ë‹¨ê³„ë¥¼ ì„¸ë¶€ì ìœ¼ë¡œ ë‹¤ë¤„ë³´ê² ë‹¤.\n\n### 1. Word Tokenization\n\nìš°ì„  ì‰½ê²Œ ìƒê°í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ë‹¨ìˆœíˆ ë„ì–´ì“°ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê²Œ í•˜ë©´, ìš°ë¦¬ëŠ” ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ Corpusì—ì„œ tokenì„ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤.\n\ní•˜ì§€ë§Œ, \"San Francisco\"ì™€ ê°™ì€ ë‹¨ì–´ê°€ ë‘ ê°œì˜ tokenìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•˜ë‚˜ì˜ tokenìœ¼ë¡œ ì²˜ë¦¬ë˜ê¸°ë¥¼ ì›í•  ìˆ˜ ìˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼ ì¼ë¶€ ì–¸ì–´ë“¤(íŠ¹íˆ ì¤‘êµ­ì–´ì™€ ì¼ë³¸ì–´)ì˜ ê²½ìš° ë„ì–´ì“°ê¸° ì—†ì´ ì‘ì„±í•˜ëŠ” ì–¸ì–´ë“¤ì˜ ê²½ìš° ë¬¸ì œëŠ” ë” ì»¤ì§ˆ ìˆ˜ ìˆë‹¤. ì´ ê²½ìš°ì—ëŠ” **Word Segmenting**ì´ë¼ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•  ìˆ˜ë„ ìˆëŠ”ë°, ì›ë¦¬ëŠ” ë§¤ìš° ê°„ë‹¨í•˜ë‹¤. ì–¸ì–´ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ì‚¬ì „ì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì¥ì—ì„œ ì‚¬ì „ì— ì¼ì¹˜í•˜ëŠ” ê°€ì¥ ê¸´ ë¬¸ìì—´ì„ ì°¾ì„ ìˆ˜ ìˆì„ ë•Œê¹Œì§€ tokenì„ ì—°ì¥í•´ì„œ ë§Œë“œëŠ” ë°©ì‹ì´ë‹¤.\n\nê·¸ëŸ¬ë‚˜ ì´ ë°©ì‹ë„ ê²°êµ­ì€ íŠ¹ì • ì–¸ì–´(ì¤‘êµ­ì–´, ë“±)ì—ì„œëŠ” ì˜ ì‘ë™í•˜ì§€ë§Œ, ì¼ë¶€ ì–¸ì–´(ì˜ì–´ ë“±)ì—ì„œëŠ” ì˜ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ë‹¤. ë”°ë¼ì„œ, ìµœê·¼ì—ëŠ” í™•ë¥ ì— ê¸°ë°˜í•˜ì—¬ ê°™ì´ ë“±ì¥í•˜ëŠ” íšŸìˆ˜ê°€ ë§ì„ ê²½ìš° í•˜ë‚˜ì˜ tokenìœ¼ë¡œ ë¬¶ëŠ” í˜•íƒœì˜ tokenizationì„ ë” ì„ í˜¸í•œë‹¤.\n\nì´ ê³¼ì •ì—ì„œ ìš°ë¦¬ê°€ ì¶”ê°€ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ë°”ë¡œ wordì˜ ê°¯ìˆ˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ë‹¤. ëŒ€ê²Œ ìš°ë¦¬ê°€ ê´€ì‹¬ ìˆì–´í•˜ëŠ” ìˆ˜ëŠ” ì´ 3ê°€ì§€ì´ë‹¤.\n\n1. **number of tokens**  \n   ì¦‰, ë„ì–´ì“°ê¸°ë¡œ ë‚˜ë‰˜ì–´ì§€ëŠ” tokenë“¤ì˜ ì´ ê°¯ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.\n2. **number of types(Vocabulary)**  \n   ë„ì–´ì“°ê¸°ë¡œ ë‚˜ë‰˜ì–´ì§„ tokenë“¤ì˜ ì¤‘ë³µì„ ì œê±°í•œ ì¢…ë¥˜ë“¤ì˜ ê°¯ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ëŒ€ê²Œ ì´ëŸ¬í•œ typeë“¤ì˜ ëª¨ìŒì„ Vocabularyë¼ê³  í•œë‹¤.\n3. **number of each type's tokens**  \n   ê° ì¢…ë¥˜ì˜ tokenì´ ì–¼ë§ˆë‚˜ ë§ì´ ë“±ì¥í–ˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.\n\nì—¬ê¸°ì„œ ì´ëŸ¬í•œ tokenì´ë‚˜ typeì´ ì„œë¡œ ê°™ë‚˜ëŠ” ê²ƒì„ ì–´ë–»ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆì„ê¹Œ? ì´ë¥¼ ìœ„í•´ì„œ Word Reformatingì„ ìˆ˜í–‰í•˜ì—¬ ì¢€ ë” ì¼ë°˜ì ì¸ í˜•íƒœë¡œ ë³€í˜•í•˜ì—¬ ìœ„ì˜ ìˆ˜ë“¤ì„ íŒŒì•…í•˜ê¸°ë„ í•œë‹¤.\n\n### 2. Word Normalization and Stemming(Word Reformating)\n\nëŒ€ê²Œì˜ ì–¸ì–´ëŠ” wordì˜ í˜•íƒœê°€ ì—¬ëŸ¬ ê°œë¡œ ì¡´ì¬í•œë‹¤. ì´ ê³¼ì •ì—ì„œ ìš°ë¦¬ê°€ ê³ ë ¤í•´ì•¼ í•  ê²ƒì´ ì •ë§ ë§ë‹¤. ê·¸ ì¤‘ì—ì„œ ê°€ì¥ ê¸°ë³¸ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ì•¼ í•  ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ì˜ì–´ì— ì¤‘ì‹¬ì„ ë‘” ì„¤ëª…ì´ë‹¤.\n\n1. **Uppercase**  \n   ì˜ì–´ì—ì„œ ì²« ê¸€ìëŠ” ëŒ€ë¬¸ìë¡œ ì‹œì‘í•œë‹¤ëŠ” ê·œì¹™ì´ ìˆë‹¤. ë˜ëŠ” ê°•ì¡°í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë¥¼ ëŒ€ë¬¸ìë¡œ í‘œí˜„í•˜ê¸°ë„ í•œë‹¤. ê·¸ ê²°ê³¼ tokenì˜ ì¢…ë¥˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì—ì„œ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ê¸°ë„ í•œë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ëª¨ë‘ lowercaseë¡œ ë°”ê¿”ë²„ë¦¬ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ëª¨ë“  ê²½ìš°ì— ì´ë¥¼ ì ìš©í•  ìˆ˜ ì‡ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ê°€ì¥ ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ USì™€ usì˜ ì˜ë¯¸ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì´ë‹¤. ë˜, ê³ ìœ  ëª…ì‚¬ì¸ General Motorsì™€ ê°™ì€ ê²½ìš°ë„ ë‹¤ë¥´ê²Œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ê³ ë ¤í•´ì„œ ë¨¼ì € ì²˜ë¦¬í•œ ì´í›„ì— ì „ì²´ ë°ì´í„°ë¥¼ lowercaseë¡œ ë³€í™˜í•˜ëŠ” ë°©ì‹ì„ ìˆ˜í–‰í•œë‹¤.  \n2. **Lemmatization**  \n   Lemma(ê¸°ë³¸í˜•, ì‚¬ì „í˜•)ë¡œ ë‹¨ì–´ë¥¼ ë³€í™˜í•˜ëŠ” ê²ƒì´ë‹¤. ê°€ì¥ ê¸°ë³¸ì ì¸ ê²ƒì€ am, are, isì™€ ê°™ì€ beë™ì‚¬ë¥¼ ëª¨ë‘ beë¡œ ë³€í™˜í•˜ê±°ë‚˜ car, cars, car'së¥¼ ëª¨ë‘ ê¸°ë³¸í˜•íƒœì¸ carë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ë‹¤. ëŒ€ê²Œì˜ ê²½ìš°ì—ëŠ” ì´ ê³¼ì •ì—ì„œ ì˜ë¯¸ë¥¼ ì¼ë¶€ ìƒì–´ë²„ë¦¬ê¸° ë•Œë¬¸ì— lemma + tagë¡œ ê¸°ì¡´ tokenì„ ë³µêµ¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” tagë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n3. **Stemming**  \n   morpheme(í˜•íƒœì†Œ)ì€ ì¤‘ì‹¬ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” stemê³¼ í•µì‹¬ ì˜ë¯¸ëŠ” ì•„ë‹ˆì§€ë§Œ stemì— ì¶”ê°€ ì˜ë¯¸ë¥¼ ë”í•´ì£¼ëŠ” affixesë¡œ ë‚˜ëˆ„ì–´ wordë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ê° tokenì„ ê°€ì¥ coreì˜ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” stemìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ì‹ì´ë‹¤. ëŒ€í‘œì ì¸ ì˜ˆì‹œê°€ automate, automatic, automationì„ automatìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” lemmatizationë³´ë‹¤ ë„“ì€ ë²”ìœ„ì˜ wordë¥¼ í•˜ë‚˜ë¡œ ë¬¶ê¸° ë•Œë¬¸ì— ì„¸ë¶€ì˜ë¯¸ê°€ ë” ì†ì‹¤ë  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ê¸°ì¡´ ì˜ë¯¸ë¡œ ë³µêµ¬í•  ìˆ˜ ìˆëŠ” tagë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n\n### 3. Sentence Segmentation\n\në¬¸ì¥ì„ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¡œ ìš°ë¦¬ëŠ” \"?\", \"!\", \".\"ì„ í™œìš©í•œë‹¤. \"?\"ì™€ \"!\" ê°™ì€ ê²½ìš°ëŠ” ë¬¸ì¥ì˜ ëì„ ì˜ë¯¸í•˜ëŠ”ê²ƒì´ ëŒ€ê²Œ ìëª…í•˜ë‹¤. í•˜ì§€ë§Œ, \".\"ì€ ê½¤ë‚˜ ì• ë§¤í•  ìˆ˜ ìˆë‹¤. ì†Œìˆ˜ì , Abbreviation(Mr., Dr., Inc.)ì™€ ê°™ì€ ê²½ìš°ì— ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•´ì„œ Decision Treeë¥¼ ë§Œë“¤ì–´ì„œ ì´ë¥¼ ìˆ˜í–‰í•œë‹¤. ì•„ë˜ì™€ ê°™ì´ ì‚¬ëŒì´ ì§ì ‘ ê·œì¹™ì„ ì •í•  ìˆ˜ë„ ìˆì§€ë§Œ í˜„ì¬ëŠ” ëŒ€ê²Œ í†µê³„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰í•œë‹¤.\n\n![nlp-sentence-segmentation](/images/nlp-sentence-segmentation.jpg)\n\n## Collocation(ì—°ì–´) processing\n\nText Normalizationì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” sentenceë¥¼ êµ¬ë¶„í•˜ê³ , wordë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆì—ˆë‹¤. í•˜ì§€ë§Œ, ë‹¨ìˆœíˆ í•˜ë‚˜ì˜ wordë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì£¼ë³€ ë‹¨ì–´ë¥¼ í™œìš©í•˜ì—¬ ì²˜ë¦¬í•´ì•¼ë§Œ ì–»ì„ ìˆ˜ ìˆëŠ” ì •ë³´ë“¤ì´ ìˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¥¼ Collocation(ì—°ì–´)ë¥¼ í™œìš©í•˜ì—¬ ìˆ˜í–‰í•œë‹¤. ì´ëŠ” íŠ¹ì • ë‹¨ì–´ìŒì´ ë†’ì€ ë¹ˆë„ë¡œ ê°™ì´ ë¶™ì–´ ì‚¬ìš©ë˜ëŠ” í˜„ìƒì„ ë§í•œë‹¤. **\"ëª¨ë“  ë‹¨ì–´ëŠ” ì´ë¥¼ ë™ë°˜í•˜ëŠ” ì£¼ë³€ ë‹¨ì–´ì— ì˜í•´ íŠ¹ì„± ì§€ì–´ì§„ë‹¤.\"** ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ collocationì„ co-ocurrenceë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì„ í•  ìˆ˜ ìˆë‹¤.\n\n1. **lexicography(ì‚¬ì „ í¸ì°¬)** : ê°™ê°€ë‹ˆ ìœ ì‚¬í•œ ëœ»ì„ ê°€ì§€ëŠ” ë‹¨ì–´ëŠ” ë¹ˆë²ˆí•˜ê²Œ ë¶™ì–´ì„œ ì‚¬ìš©ë˜ëŠ”ë° ì´ë¥¼ ì´ìš©í•´ì„œ í•˜ë‚˜ì˜ ë‹¨ì–´ì˜ ëœ»ì„ ì•ˆë‹¤ë©´, ì´ë¥¼ í†µí•´ì„œ ë‹¤ë¥¸ ë‹¨ì–´ì˜ ëœ»ì„ ì¶”ë¡ í•˜ë©° í™•ì¥í•´ë‚˜ê°ˆ ìˆ˜ ìˆë‹¤.\n2. **language modeling** : NLë¥¼ í†µí•´ì„œ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ì„œ íŠ¹ì • parameterë¥¼ ì¶”ì •í•´ë‚´ëŠ” ê²ƒì„ language modelingì´ë¼ê³  í•˜ëŠ”ë° ì´ ê³¼ì •ì—ì„œ collocationì„ í™œìš©í•˜ëŠ” ê²ƒì´ ë‹¨ì¼ ë‹¨ì–´ë¥¼ í™œìš©í•˜ëŠ” ê²ƒë³´ë‹¤ contextë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ì¥ì ì„ ë°œíœ˜í•  ìˆ˜ ìˆë‹¤.\n3. **NL generation** : ìš°ë¦¬ëŠ” ë¬¸ë§¥ìƒ ë§¤ë„ëŸ¬ìš´ ë¬¸ì¥ì„ ì›í•œë‹¤. ì¦‰, \"ê°ì„ ì¡ë‹¤\"ë¥¼ \"ê°ì„ ë¶™ì¡ë‹¤\"ë¼ê³  í–ˆì„ ë•Œ, ëœ»ì„ ì´í•´í•  ìˆ˜ëŠ” ìˆì§€ë§Œ ì–´ìƒ‰í•˜ë‹¤ê³  ëŠë‚€ë‹¤. ë”°ë¼ì„œ, ì´ ê´€ê³„ë¥¼ í™œìš©í•´ì„œ NLì„ ìƒì„±í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— collocationì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ì´ëŸ¬í•œ Collocationì„ ì–´ë–»ê²Œ ì°¾ì„ ìˆ˜ ìˆì„ê¹Œ?\n\n1. **Frequency**  \n   ê°€ì¥ ê°„ë‹¨í•˜ê²Œ ë‹¨ìˆœíˆ ë™ì‹œ ë°œìƒ ë¹ˆë„ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ë‹¤. ì •í™•í•œ íŒŒì•…ì„ ìœ„í•´ì„œëŠ” ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ëŠ” ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ë¥¼ ë¨¼ì € filteringí•  í•„ìš”ê°€ ìˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ a, the, and ë“±ì´ ìˆë‹¤.\n2. **Hypothesis Testing**  \n   ê°€ì„¤ ê²€ì¦ìœ¼ë¡œ ìš°ë¦¬ê°€ ê°€ì •í•œ collocationì„ ì§€ì •í•˜ê³ , ì´ ì‚¬ê±´ì´ ì¼ì–´ë‚  ê°€ëŠ¥ì„±ì„ êµ‰ì¥íˆ ë‚®ê²Œ í•˜ëŠ” ê°€ì„¤ì„ ë°˜ëŒ€ë¡œ ê°€ì •í•œ í›„ì— ì´ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•˜ëŠ” Null Hypothesisë¥¼ ì´ìš©í•œ ì¦ëª…ìœ¼ë¡œ íƒ€ë‹¹ì„±ì„ í™•ë³´í•˜ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ê°€ ë³´ì´ê³ ì í•˜ëŠ” ê²ƒì€ word1, word2ê°€ ìˆì„ ë•Œ, ë‘ ë‹¨ì–´ê°€ ì„œë¡œ ì˜ì¡´ì ì´ë¼ëŠ” ê²ƒì„ ì¦ëª…í•˜ê³  ì‹¶ì€ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, Null Hypothesisë¡œ ë‘ ë‹¨ì–´ëŠ” ë…ë¦½ì´ë‹¤ë¼ê³  ì§€ì •í•˜ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.  \n   $p(w_{1}, w_{2}) = p(w_{1})p(w_{2})$  \n   ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ t-ê²€ì¦ì„ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.  \n   $t = {{p(w_{1}, w_{2}) - p(w_{1})p(w_{2})} \\over \\sqrt{p(w_{1}, w_{2})\\over{N}}} $  \n   tê°’ì´ ì´ì œ ì»¤ì§ˆ ìˆ˜ë¡ ìš°ë¦¬ëŠ” í•´ë‹¹ ê°€ì„¤ì´ í‹€ë ¸ìŒì„ ì¦ëª…í•˜ì—¬ collocationì„ì„ ì£¼ì¥í•  ìˆ˜ ìˆë‹¤.\n\n## Minimum Edit Distance\n\në‹¨ì–´ ë˜ëŠ” ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•  ë•Œ, ì‚¬ì „ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ë„ ìˆì§€ë§Œ ì°¸ê³ í•  corpusê°€ ë§ˆë•…í•˜ì§€ ì•Šê±°ë‚˜ ë” ì¶”ê°€ì ì¸ ìˆ˜ì¹˜ê°€ í•„ìš”í•˜ë‹¤ë©´, Minimum Edit Distanceë¡œ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê¸°ë„ í•œë‹¤. ì¦‰, ë‘ ë¬¸ìì—´ì´ ê°™ì•„ì§€ê¸° ìœ„í•´ì„œ ì–´ëŠì •ë„ì˜ ìˆ˜ì •ì´ í•„ìš”í•œì§€ë¥¼ ìˆ˜ì¹˜í™”í•œ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ì—°ì‚°ì€ ìƒˆë¡œìš´ ë¬¸ì ì¶”ê°€, ì‚­ì œ, ëŒ€ì²´ë§Œ ê°€ëŠ¥í•˜ë‹¤.\n\n```plaintext\nS - N O W Y  | - S N O W - Y\nS U N N - Y  | S U N - - N Y\ndistance : 3 | distance : 5\n```\n\në‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.\n\n1. ë¬¸ìì—´ x, yê°€ ìˆì„ ë•Œ, $E(i, j)$ëŠ” xì˜ 0\\~iê¹Œì§€ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ìì—´ê³¼ yì˜ 0~jê¹Œì§€ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ìì—´ì˜ distanceë¼ê³  í•˜ì.\n2. ì´ë ‡ê²Œ ë˜ë©´, $E(i,j)$ì—ì„œ ìš°ë¦¬ëŠ” ëë¬¸ìì˜ ê·œì¹™ì„ ë³¼ ìˆ˜ ìˆë‹¤.\n\n    ì˜¤ë¥¸ìª½ ë ë¬¸ìê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì¡°í•©ì€ 3ê°€ì§€ ë°–ì— ì—†ë‹¤.\n\n    ```plaintext\n    x[i]        | -           | x[i]\n    -           | y[j]        | y[j]\n    distance: 1 | distance: 1 | distance: 0 or 1\n    ```\n\n3. ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ ì‚¬ì‹¤ì„ ì•Œê²Œ ëœë‹¤.\n\n    $E(i,j)$ëŠ” ë‹¤ìŒ ê²½ìš°ì˜ ìˆ˜ ì¤‘ í•˜ë‚˜ì—¬ì•¼ë§Œ í•œë‹¤.\n\n    - $E(i-1, j) + 1$\n    - $E(i, j-1) + 1$\n    - $E(i-1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )$\n4. ë”°ë¼ì„œ, ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤.\n\n    $E(i, j) =\\min( \\\\\n      \\quad E(i-1, j) + 1, \\\\\n      \\quad E(i, j-1) + 1,  \\\\\n      \\quad E(i - 1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )\\\\\n    )$\n\në§Œì•½, ê° ì—°ì‚°ì˜ ë¹„ìš©ì´ ë‹¤ë¥¼ ê²½ìš°ë¼ë©´, 1 ëŒ€ì‹ ì— ê·¸ ê°’ì„ ë„£ì–´ì£¼ë©´ ì¶©ë¶„íˆ í’€ ìˆ˜ ìˆìœ¼ë©°, ì¶”ê°€ì ìœ¼ë¡œ ìµœì ì˜ ì´ë™í˜•íƒœë¥¼ ì•Œê³  ì‹¶ë‹¤ë©´, back pointer í•˜ë‚˜ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"nlp-text-processing","date":"2022-10-19 21:59","title":"[NLP] 2. Text Processing","category":"AI","tags":["NLP","Regex","Tokenization","Collocation","MinimumEditDistance"],"desc":"NLPë¥¼ ìˆ˜í–‰í•  ë•Œ, ìš°ë¦¬ëŠ” sequence of characterë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ì œëŒ€ë¡œ ì•Œì•„ì•¼ ì œëŒ€ë¡œ ëœ ì „ì²˜ë¦¬ì™€ í›„ì²˜ë¦¬ ë“±ì´ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ í•´ë‹¹ chapterì—ì„œëŠ” ì–´ë–»ê²Œ ì›ë³¸ ë¬¸ì/ë‹¨ì–´/ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ë‹¤ë£° ê²ƒì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nNatural Language(ìì—°ì–´, ì‚¬ëŒì´ ì‚¬ìš©í•˜ëŠ” í†µìƒ ì–¸ì–´)ë¥¼ inputìœ¼ë¡œ í™œìš©í•˜ê³ ì í•˜ëŠ” ë…¸ë ¥ì€ ì»´í“¨í„°ì˜ ë“±ì¥ë¶€í„° ì‹œì‘í•˜ì—¬ ì—¬ëŸ¬ ë²ˆ ì‹œë„ë˜ì–´ ì™”ë‹¤. ì§€ê¸ˆê¹Œì§€ë„ ì™„ë²½í•˜ê²Œ ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ í˜ë“¤ë‹¤. ì™œ Natural Languageë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì€ ì–´ë µê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ NLPì—ì„œëŠ” ì–´ë–¤ ë°©ì‹ì„ í™œìš©í• ì§€ì— ëŒ€í•œ ê°œëµì ì¸ overviewë¥¼ ì œì‹œí•œë‹¤. ë˜í•œ, Natural Languageì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ë‹¨ê³„ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œ Linguistics(ì–¸ì–´í•™)ì„ ê°„ëµí•˜ê²Œ ì •ë¦¬í•œë‹¤.\n\n## NLP\n\nNatural Language Processingì˜ ì•½ìë¡œ ì‚¬ëŒì´ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ë¥¼ inputìœ¼ë¡œ í•˜ì—¬ ì›í•˜ëŠ” ê°’ì„ ì¶”ì¶œí•´ë‚´ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. ì´ë¥¼ ìœ„í•´ì„œ ìš°ë¦¬ëŠ” ì‚¬ëŒì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê±°ë‚˜ ë‹¤ë£° ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì»´í“¨í„°ì—ê²Œ ë¶€ì—¬í•´ì•¼ í•œë‹¤.\n\në¨¼ì €, ì´ëŸ¬í•œ í•„ìš”ê°€ ìˆëŠ” ëŒ€í‘œì ì¸ usecaseë¥¼ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n### Usecase\n\n- **Spam Detection**  \n  ê°€ì¥ ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ mailì—ì„œ spam ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.\n- **POS tagging / NER**  \n  íŠ¹ì • ë‹¨ì–´ ë‹¨ìœ„ì˜ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ê²Œ ë˜ëŠ”ë° ë‹¨ì–´ì˜ í’ˆì‚¬ì™€ ëŒ€ëµì ì¸ ì˜ë¯¸ë¥¼ ê°€ì§„ categoryë¡œ ë¶„ë¥˜ë¡œ taggingí•˜ëŠ” ê³¼ì •ì´ë‹¤. ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ë‹¤ë¥¸ usecaseì—ì„œ í™œìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. í’ˆì‚¬ì™€ categoryëŠ” ë‹¨ì–´ì˜ ëœ»ì„ ì¶”ë¡ í•˜ëŠ”ë° í° ë„ì›€ì„ ì£¼ë©°, ì´ê²ƒì´ ë¬¸ì¥ì˜ ì´í•´ ë“±ì— ë„ì›€ì„ ì£¼ê¸° ë•Œë¬¸ì´ë‹¤.\n- **Sentiment Analysis**  \n  ê°ì •/ì—¬ë¡  ë¶„ì„ ë“±ì˜ ì˜ì—­ì„ ì˜ë¯¸í•˜ë©°, í…ìŠ¤íŠ¸ ë˜ëŠ” ëŒ€í™”ì—ì„œì˜ ê¸ì •/ë¶€ì • ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê±°ë‚˜ í‰ì  ë“±ì„ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.\n- **Conference Resolution**  \n  \"he\", \"she\" ë“± ëŒ€ëª…ì‚¬, ìƒëµ ë‹¨ì–´ ë“±ì„ ì›ë˜ì˜ ë‹¨ì–´ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜ ì±„ìš°ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤. ì´ ì—­ì‹œë„ ì—¬ëŸ¬ ì˜ì—­ì—ì„œ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ì ì¸ ì‘ì—…ì„ í•  ìˆ˜ ìˆë‹¤.  \n- **Word Sense Disambiguation(WSD)**  \n  íŠ¹ì • ë‹¨ì–´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë™ì˜ì–´, ë™ìŒì´ì˜ì–´ ë“±ì—ì„œ ê°€ë¥´í‚¤ëŠ” ì§„ì§œ ì˜ë¯¸ë¥¼ í—·ê°ˆë¦¬ì§€ ì•Šê²Œ ëª…í™•í•˜ê²Œ ë‹¤ì‹œ í•œ ë²ˆ ì²˜ë¦¬í•œë‹¤. ì´ ì—­ì‹œë„ ë‹¤ë¥¸ NLP usecaseì—ì„œ ë‘ë£¨ ì‚¬ìš©ëœë‹¤.\n- **Parsing**  \n  ë¬¸ì¥ì—ì„œ ë‹¨ì–´ë“¤ì„ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ë‹¨ìœ„(êµ¬, ì ˆ, ë¬¸ì¥)ë¡œ ë‹¤ì‹œ groupingí•œë‹¤. ì´ ê³¼ì •ì„ ì˜ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ WSDì™€ Conference Resolution, POS tagging, NERì´ ì´ë£¨ì–´ì§€ë©´ ì¢‹ë‹¤. ì´ ê³¼ì •ì„ í†µí•´ì„œ ë¬¸ì¥ì˜ ê°œëµì ì¸ ì˜ë¯¸ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.\n- **Machine Translation(MT)**  \n  íŠ¹ì • ì–¸ì–´ë¥¼ ë˜ ë‹¤ë¥¸ Natural Languageë¡œ ë³€ê²½í•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.\n- **Information Extraction(IE)**  \n  íŠ¹ì • ë¬¸ì¥ì—ì„œ ì‚¬ìš©ìì—ê²Œ ì˜ë¯¸ìˆì„ë§Œí•œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ë‹¤.\n- **Q&A**  \n  íŠ¹ì • ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ì˜€ì„ ë•Œ, ì´ ëœ»ì„ ì´í•´í•˜ê³ , ì´ì— ì ì ˆí•œ ëŒ€ë‹µì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n- **Paraphrase**  \n  ë¬¸ì¥ì˜ ëœ»ì„ ì´í•´í•˜ê³ , ë” ì‰¬ìš´ í˜•íƒœì˜ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.\n- **Summarization**  \n  ì—¬ëŸ¬ ë¬¸ì¥ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê¸€ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ê³ , ì ì ˆí•œ ë‚´ìš©ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.\n- **Dialog**  \n  Natural Languageë¥¼ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒê³¼ 1:1ë¡œ ë‹´í™”ë¥¼ ì£¼ê³  ë°›ëŠ” ê²ƒì´ë‹¤. ì˜ë¯¸ë¥¼ ì´í•´í•  ë¿ë§Œ ì•„ë‹ˆë¼ ìì‹ ì´ ë‚´ë³´ë‚¼ outputì— ëŒ€í•´ì„œë„ ì ì ˆí•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ í•„ìš”í•˜ë‹¤.\n\nìœ„ì™€ ê°™ì´ ë§ì€ usecaseê°€ ìˆëŠ”ë° ì´ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ì§€ê¸ˆê¹Œì§€ë„ êµ‰ì¥íˆ challengeí•œ ë¶€ë¶„ì´ë‹¤. ê·¸ê²ƒì€ Natural Languageê°€ ê°€ì§€ëŠ” ëª‡ëª‡ íŠ¹ì§• ë•Œë¬¸ì´ë‹¤.\n\n### Why is NLP difficult?\n\nì—¬ê¸°ì„œëŠ” Natural Language ì¤‘ì—ì„œ ì˜ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì„¤ëª…ì´ì§€ë§Œ, í•œêµ­ì–´ë„ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤.\n\n- **non-standard** : Natural Languageë¥¼ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒë“¤ì´ í‘œì¤€ì„ í•­ìƒ ë”°ë¥´ì§€ëŠ” ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ì•½ì–´ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë¬¸ë²•ì— ë§ì§€ ì•ŠëŠ” ë¹„ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ì‚¬ì†Œí†µì„ í•˜ê¸° ë•Œë¬¸ì— ì´ê²ƒì„  ì‹œìŠ¤í…œì´ ì´í•´í•˜ê²Œ í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤.\n- **segmentation** issues : ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¬¶ëŠ” ê²ƒì´ ì–´ë µë‹¤ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ë¬¸ì¥ì˜ ë„ì–´ì“°ê¸°ë¥¼ ì–´ë””ë¡œ ë°›ì•„ë“¤ì´ëƒì— ë”°ë¼ì„œ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒì„ ë³¸ ê²½ìš°ê°€ ìˆì„ ê²ƒì´ë‹¤.\n- **idioms** : ê´€ìš©êµ¬ì˜ ì‚¬ìš©ì€ NLPì—ì„œ ì˜ˆì™¸ì²˜ë¦¬ë¡œ í•´ì£¼ì–´ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤. ë‹¨ì–´ ê·¸ëŒ€ë¡œì˜ ì˜ë¯¸ì™€ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì´ë‹¤.\n- **neologisms** : ì‹ ì¡°ì–´ëŠ” ê³„ì†í•´ì„œ ìƒê²¨ë‚˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ ê³„ì†í•´ì„œ ì—…ë°ì´íŠ¸ í•´ì£¼ëŠ” ê²ƒë„ ë¶€ë‹´ì´ ëœë‹¤.\n- **world knowledge** : ì‚¬ì „ ì§€ì‹ì„ ì•Œê³  ìˆì–´ì•¼ ì´í•´í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´, ë¬¸ì¥ì´ ì¡´ì¬í•œë‹¤. ì¦‰, ì–´ë–¤ ì§€ì‹ì„ ê°€ì§€ê³  ìˆëŠëƒì— ë”°ë¼ì„œ í•´ì„ì´ ë‹¬ë¼ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤.\n- **tricky entity names** : ê³ ìœ  ëª…ì‚¬ ì¤‘ì—ì„œ íŠ¹íˆ contents(ë…¸ë˜, ê·¸ë¦¼, ì†Œì„¤) ë“±ì˜ ì œëª©ì´ í•´ì„ ì‹œì— í—·ê°ˆë¦¬ê²Œ í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´, \"Let it be\"ë¼ëŠ” ë¹„í‹€ì¦ˆì˜ ë…¸ë˜ëŠ” ë¬¸ì¥ ì¤‘ê°„ì— ë“¤ì–´ê°€ë©´, í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë°›ì•„ë“¤ì—¬ì§€ê²Œ ë˜ëŠ”ë° ì´ë¥¼ ì˜ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ í•´ì•¼ í•œë‹¤.\n\nìœ„ì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ìë©´, ë‹¤ì–‘í•œ ë‹¨ì–´ê°€ ë‹¤ì–‘í•œ í˜„ìƒê³¼ ë‹¤ì–‘í•œ ë²•ì¹™(Grammer)ì˜ ì˜í–¥ì„ ë°›ê¸°ì— ì–´ë ¤ìš°ë©°, ë‹¨ì–´ê°€ ê°€ì§€ëŠ” ëª¨í˜¸ì„±ì´ ë¬¸ì œë¥¼ ì•¼ê¸°í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n### Solutions\n\nì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ í¬ê²Œ ë‘ ê°€ì§€ ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n\n- Rule based approach  \n  Gammerì™€ ê°™ì€ ë²•ì¹™ì„ ëª¨ë‘ ì ìš©í•´ì„œ prgorammingì„ í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì´ ë°©ì‹ì€ ë¹„ë¬¸ê³¼ ê°™ì€ ë¬¸ì¥ì„ ì œëŒ€ë¡œ ì²˜ë¦¬í•  ìˆ˜ ì—†ì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì •í™•í•œ í˜•íƒœì˜ ë¬¸ì¥ì´ë¼ë„ ì—¬ëŸ¬ ì˜ë¯¸ë¡œ í•´ì„ë˜ëŠ” ë¬¸ì¥ì—ì„œ ê²½í–¥ì„±ê³¼ ë¬¸ë§¥ì„ ì „í˜€ íŒŒì•…í•  ìˆ˜ ì—†ë‹¤.\n- Statistic based approach  \n  ê·¸ë˜ì„œ ìµœê·¼ì—ëŠ” ê²½í–¥ì„±ê³¼ ë¬¸ë§¥ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ AI ê¸°ìˆ , ML, Deep Learningì„ ì´ìš©í•˜ì—¬ NLPë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ í•˜ë‚˜ì˜ trendë¡œ ìë¦¬ ì¡ì•˜ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì–´ë–»ê²Œ í†µê³„ì ì¸ ì ‘ê·¼ë²•ì´ ê²½í–¥ì„±ê³¼ ë¬¸ë§¥ì„ í¬í•¨í•  ìˆ˜ ìˆì„ê¹Œ? ì´ëŠ” í†µê³„ê°€ ê°€ì§€ëŠ” ê²½í–¥ì„±ì´ë¼ëŠ” íŠ¹ì§•ê³¼ conditional probabilityë¥¼ ì‚¬ìš©í•  ë•Œì˜ ë¬¸ë§¥ì„ í¬í•¨í•œ ê²½í–¥ì„±ì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ í™œìš©í•´ì„œ ê°€ëŠ¥í•˜ë‹¤.\n\n## Linguistics\n\nê²°êµ­ ì•ìœ¼ë¡œ í†µê³„ì ì¸ ë°©ì‹ì„ í™œìš©í•˜ë”ë¼ë„ ìš°ë¦¬ëŠ” ìµœì†Œí•œì˜ ì–¸ì–´í•™ì ì¸ ê¸°ë³¸ì´ í•„ìš”í•˜ë‹¤. ì™œëƒí•˜ë©´, í†µê³„ì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. ìš°ë¦¬ê°€ ì‚¬ìš©í•  ë°ì´í„°ëŠ” text ë˜ëŠ” ìŒì„±ì´ë‹¤. ì´ë¥¼ ì ì ˆí•˜ê²Œ ì²˜ë¦¬í•˜ì—¬ í†µê³„ì— ì‚¬ìš©í•  ìœ ì˜ë¯¸í•œ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. ì´ë¥¼ ìœ„í•´ì„œ ì–¸ì–´í•™ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•œ ê²ƒì´ë‹¤.\n\nì¼ë°˜ì ìœ¼ë¡œ ì–¸ì–´ë¥¼ ë¶„ì„í•  ë•Œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬ëŠ” **Grammar**ì´ë‹¤. ì´ëŠ” íŠ¹ì • languageì—ì„œ í—ˆìš©ë˜ëŠ” ê·œì¹™ì˜ ì§‘í•©ì„ ì •ë¦¬í•œ ê²ƒì´ë‹¤. ì´ê²ƒì˜ ì¢…ë¥˜ëŠ” í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤.\n\n- **Classic Grammar**  \n  ì‚¬ëŒì´ ì‹¤ì œë¡œ ì–¸ì–´ë¥¼ ì‚¬ìš©í•¨ì— ìˆì–´ ë°œìƒí•˜ëŠ” ì´ìƒí•œ ìŠµê´€ê³¼ ê°™ì€ ì–¸ì–´ í‘œí˜„ì´ë‹¤. ì´ëŸ¬í•œ ë²•ì¹™ë“¤ì€ ëŒ€ê²Œ ì˜ˆì œë“¤ì„ í†µí•´ì„œ ì •ì˜ë˜ëŠ”ë° ì´ëŸ° ê²ƒì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ëª…ë°±í•œ ë„êµ¬ê°€ ì¡´ì¬í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ê°íƒ„ì‚¬ì™€ ê°™ì€ ê²ƒë“¤ì´ ì—¬ê¸°ì— í¬í•¨ë˜ê² ë‹¤. ì´ëŠ” ì´ëŸ¬í•œ ë³€ì¹™ì ì¸ í˜•íƒœ ë•Œë¬¸ì— programmingì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.\n- **Explicit Grammar**  \n  ëª…ë°±í•˜ê²Œ ì •ì˜ë˜ì–´ ìˆëŠ” ì–¸ì–´ ê·œì¹™ì„ ì˜ë¯¸í•œë‹¤. ì´ëŠ” Programmìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìœ¼ë©°, ì—¬ëŸ¬ Grammar ì •ë¦¬ ë‚´ìš©ì´ ì´ë¯¸ ì •ë¦¬ë˜ì–´ ìˆë‹¤. (CFG, LFG, GPSG, HPSG, ....)  \n  ì´ë¥¼ ë¬¸ë²•ì ìœ¼ë¡œ ë¶„ì„í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” 6ë‹¨ê³„ì˜ ìˆœì°¨ì ì¸ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤.\n\n### 6 Layers in Language\n\nê° ë‹¨ê³„ëŠ” inputê³¼ outputì„ ê°€ì§„ë‹¤. ë‹¨ê³„ì ìœ¼ë¡œ ì§„í–‰ë˜ê¸° ë•Œë¬¸ì— ì´ì „ ë‹¨ê³„ì˜ outputì´ ë‹¤ìŒ ë‹¨ê³„ì˜ inputì´ ë˜ë©°, ë•Œë•Œë¡œ ëª‡ ë‹¨ê³„ëŠ” ìƒëµë  ìˆ˜ ìˆê¸°ì— ìœ ì—°í•˜ê²Œ ìƒê°í•˜ë„ë¡ í•˜ì.\n\nê° ë‹¨ê³„ì—ì„œ ì‹¤ì œë¡œ íŠ¹ì • ë¬¸ì¥ì´ ì²˜ë¦¬ë˜ëŠ” ê³¼ì •ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œ \"Astronomers saw stars with telescope\"ë¼ëŠ” ë¬¸ì¥ì´ ìŒì„± ë˜ëŠ” textë¡œ ë“¤ì–´ì™”ì„ ë•Œë¥¼ ê°€ì •í•˜ì—¬ ê° ë‹¨ê³„ì—ëŠ” ë¬´ì—‡ì„ í•˜ê³  ì´ë¥¼ í†µí•´ì„œ ì–´ë–»ê²Œ ì´ ë¬¸ì¥ì„ ë°”ê¿€ ìˆ˜ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•´ë³´ê² ë‹¤.\n\n> **1. Phonetics/Orthography(ìŒì„±í•™/ë§ì¶¤ë²•)**\n\në¨¼ì € OrthographyëŠ” ë§ì¶¤ë²• ê²€ì‚¬ë¥¼ ì˜ë¯¸í•˜ë©°, character sequenceë¡œ inputì´ ë“¤ì–´ì˜¤ë©´, ì´ë¥¼ ë§ì¶¤ë²•ì— ë§ëŠ”ì§€ë¥¼ í™•ì¸í•˜ì—¬ ì´ê²ƒì´ ìˆ˜ì •ëœ sequenceë¡œ ë°˜í™˜í•œë‹¤.  \nì˜ˆì‹œ ë¬¸ì¥ì— ìˆëŠ” \"telescope\"ëŠ” ë¬¸ë²•ì— ë§ì§€ ì•Šìœ¼ë¯€ë¡œ \"telescopes\"ë¡œ ë°”ë€Œì–´ì•¼ í•œë‹¤.\n\n| input                                 | output                                 |\n| :------------------------------------ | :------------------------------------- |\n| Astronomers saw stars with telescope. | Astronomers saw stars with telescopes. |\n\nPhoneticsëŠ” ìŒì„±í•™ì„ ì˜ë¯¸í•˜ë©°, í˜€ì™€ ìŒì„±ì˜ ì˜í–¥ì„ ì£¼ëŠ” ë‹¤ì–‘í•œ ê·¼ìœ¡ì˜ ìœ„ì¹˜ í˜•íƒœ, ëª¨ì–‘, ë¹ˆë„ë¥¼ í™œìš©í•˜ì—¬ ììŒê³¼ ëª¨ìŒì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤. Orthoographyì™€ëŠ” ë‹¬ë¦¬ ì–µì–‘ì´ë¼ëŠ” ê²ƒì„ ì¶”ê°€ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.\n\n| input                                                       | output                                 |\n| :---------------------------------------------------------- | :------------------------------------- |\n| Astronomers saw stars with telescopes.ë¥¼ ì˜ë¯¸í•˜ëŠ” ìŒì„± ì‹ í˜¸ | É™sËˆtrÉ’nÉ™mÉ™z sÉ”Ë stÉ‘Ëz wÉªÃ° ËˆtÉ›lÉªskÉ™ÊŠps. |\n\n*<https://tophonetics.com/> ì„ í†µí•´ì„œ ë³€í™˜í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n> **2. Phonology/Lexicalization(ìŒìš´ë¡ /ì–´íœ˜í™”)**\n\nPhonologyì€ ìŒìš´ë¡ ìœ¼ë¡œ ì†Œë¦¬ì™€ phonemes(ìŒì†Œ)ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬, ìŒì†Œë¥¼ íŠ¹ì • wordë¡œ ë³€í™˜í•˜ê³ , Lexicalizationì—ì„œëŠ” í•´ë‹¹ ë‹¨ì–´ë¥¼ ì‚¬ì „ì—ì„œì˜ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤.\n\n| input                                  | output                                 |\n| :------------------------------------- | :------------------------------------- |\n| É™sËˆtrÉ’nÉ™mÉ™z sÉ”Ë stÉ‘Ëz wÉªÃ° ËˆtÉ›lÉªskÉ™ÊŠps. | Astronomers saw stars with telescopes. |\n\n> **3. Morphology(ì–´í˜•ë¡ )**\n\nMorphologyëŠ” ì–´í˜•ë¡ ìœ¼ë¡œ ìŒì†Œì˜ êµ¬ì„±ì„ ê¸°ë³¸í˜•(lemma)ì˜ í˜•íƒœë¡œ ë³€í™˜í•˜ë©°, ê° ë‹¨ì–´ë“¤ì„ í˜•íƒœí•™ì ì¸ ì˜ë¯¸ë¥¼ ê°–ëŠ” ì¹´í…Œê³ ë¦¬(category, tag)ë¡œ ë¶„ë¥˜í•œë‹¤.\nì—¬ê¸°ì„œ ì‚¬ìš©ë˜ëŠ” lemmaì™€ categoryê°€ ë¬´ì—‡ì¸ì§€ ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ì.\n\n- **lemma**  \n  - ì‚¬ì „ì— í‘œê¸°ë˜ëŠ” ë‹¨ì–´ì˜ ê¸°ë³¸í˜•ìœ¼ë¡œ, ì‚¬ì „ì—ì„œ wordë¥¼ ì°¾ëŠ” pointerê°€ ëœë‹¤.  \n  - ë™ìŒì´ì˜ì–´ì˜ ê²½ìš° íŠ¹ì • ëœ»ì„ ì§€ì¹­í•˜ê³  ì‹¶ì€ ê²½ìš°ì—ëŠ” numberingì„ ìˆ˜í–‰í•˜ê¸°ë„ í•œë‹¤.\n  - ë” ë‚˜ì•„ê°€ì„œëŠ” í˜•íƒœì†Œ(morpeheme)ê¹Œì§€ êµ¬ë¶„í•˜ê¸°ë„ í•œë‹¤. ì´ëŠ” í˜¼ìì„œ ì“°ì¼ ìˆ˜ ìˆëŠ” ìë¦½ í˜•íƒœì†Œ(root)ì™€ ì˜ì¡´ í˜•íƒœì†Œ(stem)ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.  \n    - ì˜ˆë¥¼ ë“¤ë©´, quotations -> quote[root] + -ation[stem] + -s[stem]\n    - ìœ„ì™€ ê°™ì€ í˜•íƒœë¡œ ì„¸ë¶„í™”í•  ìˆ˜ë„ ìˆì§€ë§Œ, ëŒ€ê²ŒëŠ” lemma ë‹¨ìœ„ì—ì„œ ê·¸ì¹œë‹¤.\n- **categorizing**  \n  - categoryëŠ” ì •í•˜ê¸° ë‚˜ë¦„ì´ë©°, ì´ë¯¸ ì •í•´ì ¸ìˆëŠ” tagsetë“¤ë„(Brown, Penn, Multext) ë§ì´ ì¡´ì¬í•˜ê³ , ì–µì–‘ì´ë‚˜ ì‹¤ì œ ë¶„ë¥˜ ë“±ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤.\n  - **POS tagging**ì€ categoryë¥¼ ë¶„ë¥˜í•˜ëŠ” ë°©ë²• ì¤‘ì—ì„œ ê°€ì¥ ìœ ëª…í•œë°, ì´ëŠ” ì—¬ëŸ¬ ì–¸ì–´ì—ì„œ ê±°ì˜ í˜¸í™˜ë˜ê¸° ë•Œë¬¸ì— ì´ ë°©ì‹ì„ í™œìš©í•˜ì—¬ ë¶„ì„í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì•ˆì •ì ì¸ ë°©ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ë³„ë„ì˜ Postingì—ì„œ ë” ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.\n\në˜í•œ, ë‹¨ì–´ì˜ í˜•íƒœëŠ” ì–¸ì–´ë§ˆë‹¤ ë‹¤ì–‘í•˜ê¸° ë•Œë¬¸ì— ì–´ëŠì •ë„ ì–¸ì–´ë§ˆë‹¤ ë‹¤ë¥¸ ì‘ì—…ì„ í•´ì£¼ì–´ì•¼ í•œë‹¤. í¬ê²Œ êµ¬ë¶„ë˜ëŠ” í˜•íƒœë¡œ ì–¸ì–´ë¥¼ 3ê°œì˜ ì¢…ë¥˜ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.\n\n1. **Analytical Language(ê³ ë¦½ì–´)**  \n   í•˜ë‚˜ì˜ ë‹¨ì–´ê°€ ëŒ€ê²Œ í•˜ë‚˜ì˜ morphemeì„ ê°€ì§„ë‹¤. ë”°ë¼ì„œ, í•˜ë‚˜ ì´ìƒì˜ categoryë¡œ êµ¬ë¶„ë˜ì–´ì§ˆ ìˆ˜ ìˆë‹¤.  \n   ex. English, Chinese, Italian\n2. **Inflective Fusional Language(êµ´ì ˆì–´)**  \n   prefix/suffix/infixê°€ ëª¨ë‘ morphemeì— ì˜í–¥ì„ ë¯¸ì¹˜ë©°, morphemeì˜ ì •ì˜ ìì²´ê°€ ì• ë§¤í•´ì§€ëŠ” ì–¸ì–´ í˜•íƒœ  \n   ex. Czech, Russian, Polish, ...\n3. **Agglutinative Language(êµì°©ì–´)**  \n   í•˜ë‚˜ì˜ ë‹¨ì–´ì—ëŠ” morphemeì´ ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ê³ , prefix/suffix/infix ë˜í•œ ëª…í™•í•˜ê²Œ êµ¬ë¶„ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ, ê° morphemeì— ëª…í™•í•œ categoryë¥¼ mappingí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.  \n   ex. Korean, ...\n\n| input                                  | output(based on Brown tagset)                                         |\n| :------------------------------------- | :-------------------------------------------------------------------- |\n| Astronomers saw stars with telescopes. | (astronomer/NNS) (see/VBD) (star/NNS) (with/IN) (telescope/NNS) (./.) |\n\n> **4. Syntax(í†µì‚¬ë¡ )**\n\nlemmaë‚˜ morphemeì„ êµ¬ë¬¸ì˜ ìš”ì†Œì¸ S(Subject, ì£¼ì–´), V(Verb, ë™ì‚¬), O(Object, ëª©ì ì–´)ì™€ ê°™ì€ ìš”ì†Œë¡œ ë¶„ë¥˜í•œë‹¤. ì´ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•  ë•Œì—ëŠ” ë¬¸ì¥ì˜ êµ¬ì„±ìš”ì†Œë¥¼ ì•Œì•„ì•¼ í•œë‹¤. ì´ë¥¼ bottom-upìœ¼ë¡œ ì‚´í´ë³´ì.\n\n- **Word(ë‹¨ì–´)**  \n  ì‚¬ì „ì— ëª…ì‹œëœ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” ê´€ìš©ì–´(dark horse)ë¥¼ í¬í•¨í•œë‹¤.\n- **Phrase(êµ¬)**  \n  ë‘˜ ì´ìƒì˜ ë‹¨ì–´ ë˜ëŠ” êµ¬ì˜ ê²°í•©ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤. ëŒ€ê²Œ í•˜ë‚˜ì˜ ë¬¸ë²•ì ì¸ ì˜ë¯¸ë¡œ ë³€í™˜ë˜ì–´ì§„ë‹¤.  \n  - ëŒ€í‘œì ì¸ ì˜ˆì‹œ\n    - Noun : a new book\n    - Adjective : brand new\n    - Adverbial : so much\n    - Prepositional : in a class\n    - Verb : catch a ball\n  - **Elipse(ìƒëµ)**  \n    ëŒ€ê²Œ ë‹¨ì–´ ë˜ëŠ” êµ¬ê°€ ìƒëµë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. íŠ¹íˆ ë‹´í™”ì˜ ê²½ìš° ë”ìš± ê·¸ë ‡ë‹¤.  \n    ì´ë¥¼ ì¶”ë¡ ì„ í†µí•´ì„œ ì¶”ê°€í•  ìˆ˜ë„ ìˆë‹¤.\n- **Clause(ì ˆ)**  \n  ì ˆì€ ì£¼ì–´ì™€ ì„œìˆ ì–´ë¥¼ ê°–ì¶˜ í•˜ë‚˜ì˜ ë¬¸ì¥ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, ë¬¸ì¥ ìš”ì†Œë¡œì„œ ë” ìƒìœ„ ë¬¸ì¥ì— ì†í•˜ëŠ” ê²½ìš°ì´ë‹¤.  \n  ë˜í•œ, ì˜ì–´ì—ì„œëŠ” íŠ¹íˆ ì ‘ì†ì‚¬ë¡œ ì—°ê²°ëœ ì ˆì´ ì•„ë‹Œ ê²½ìš°ì—ëŠ” í•´ë‹¹ ì ˆì´ ì§€ì¹­í•˜ëŠ” ëŒ€ìƒì´ ì ˆ ë‚´ë¶€ì—ì„œ ìƒëµëœë‹¤. ì´ë¥¼ gapì´ë¼ê³  í•œë‹¤.\n- **Sentense**  \n  í•˜ë‚˜ ì´ìƒì˜ ì ˆë¡œ ì´ë£¨ì–´ì§€ê³ , ì˜ì–´ì—ì„œëŠ” ì‹œì‘ ì‹œì— ëŒ€ë¬¸ìë¡œ í‘œê¸°í•˜ë©° ì¢…ë£Œ ì‹œì—ëŠ” êµ¬ë¶„ìë¡œ .?!ë¡œ ëë‚œë‹¤.\n\nê²°êµ­ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ìš”ì†Œë¥¼ ì ì ˆí•˜ê²Œ í‘œì‹œí•´ì•¼ í•˜ëŠ”ë°, ì´ë¥¼ ìœ„í•´ì„œ tree êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ëŒ€í‘œì ìœ¼ë¡œ ë‘ ê°€ì§€ì˜ êµ¬ì¡°ê°€ ìˆë‹¤.\n\n1. **phrase structure(derivation tree)**\n   ë¬¸ì¥ì„ ê¸°ì ìœ¼ë¡œ ì ˆ, êµ¬, ë‹¨ì–´ë¡œ top-downìœ¼ë¡œ ë‚´ë ¤ê°€ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.  \n   ê° ë‹¨ìœ„ë¥¼ ë¬¶ì„ ë•Œì—ëŠ” ()ë¥¼ ì´ìš©í•˜ê³ , ê·¸ ë’¤ì•  í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì´ ë¬´ìŠ¨ êµ¬, ì ˆì¸ì§€ë¥¼ í‘œê¸°í•œë‹¤.\n2. **dependency structure**  \n   ë‹¨ì–´ ê°„ì˜ ê´€ê³„ì— ë” ì§‘ì¤‘í•˜ì—¬ ë‚˜íƒ€ë‚¸ë‹¤. ë”°ë¼ì„œ, ì‚¬ëŒì´ ë³´ê¸°ì—ëŠ” ë¶ˆëª…í™•í•´ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ íŠ¹ì • usecaseì—ì„œëŠ” ìœ ìš©í•˜ë‹¤.\n\n| input                                                                 | output (phrase structure)                                                              |\n| :-------------------------------------------------------------------- | :------------------------------------------------------------------------------------- |\n| (astronomer/NNS) (see/VBD) (star/NNS) (with/IN) (telescope/NNS) (./.) | ((astronomer/NNS)NP ((see/VBD)V ((star/NNS)NP ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S |\n\n![nlp-phase-structure](/images/nlp-phase-structure.jpg)\n\n> **5. Semantics(Meaning, ì˜ë¯¸ë¡ )**\n\nê°„ë‹¨í•˜ê²ŒëŠ” ì£¼ì–´, ëª©ì ì–´ì™€ ê°™ì€ tagë‚˜ \"Agent\"ë‚˜ \"Effect\"ì™€ ê°™ì€ tagë¥¼ ì ìš©í•˜ë©°, ì „ì²´ì ì¸ ì˜ë¯¸ë¥¼ ìœ ì¶”í•´ë‚¸ë‹¤.\n\n| input                                                                                  | output                                                                                                |\n| :------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------- |\n| ((astronomer/NNS)NP ((see/VBD)V ((star/NNS)NP ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S | ((astronomer/NNS)NP/agent ((see/VBD)V ((star/NNS)NP/affected ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S |\n\n> **6. Discourse/Pragmatics(ë‹´í™”/í™”ìš©ë¡ )**\n\nì‹¤ì œ ëŒ€í™” ë“±ê³¼ ê°™ì€ ëª©í‘œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì•ì„œ ë³´ì•˜ë˜ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì´ìš©í•œë‹¤.\n\në§Œì•½, í•´ë‹¹ ë°ì´í„°ë¥¼ í†µí•´ì„œ í•˜ê³ ì í•˜ëŠ” ê²ƒì´ ì´ ì´ì•¼ê¸°ë¥¼ í•œ ì‚¬ëŒì´ ì‹ë‹¹ ë‚´ë¶€ì— ìˆëŠ”ì§€ë¥¼ íŒë‹¨í•˜ê³ ì í•œë‹¤ê³  ê°€ì •í•´ë³´ì.\n\n| input                                                                                                 | output |\n| :---------------------------------------------------------------------------------------------------- | :----- |\n| ((astronomer/NNS)NP/agent ((see/VBD)V ((star/NNS)NP/affected ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S | False  |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- text to phonetic converter, <https://tophonetics.com>\n","slug":"nlp-linguistics","date":"2022-10-19 09:03","title":"[NLP] 1. Linguistics","category":"AI","tags":["NLP","Languagistics"],"desc":"Natural Language(ìì—°ì–´, ì‚¬ëŒì´ ì‚¬ìš©í•˜ëŠ” í†µìƒ ì–¸ì–´)ë¥¼ inputìœ¼ë¡œ í™œìš©í•˜ê³ ì í•˜ëŠ” ë…¸ë ¥ì€ ì»´í“¨í„°ì˜ ë“±ì¥ë¶€í„° ì‹œì‘í•˜ì—¬ ì—¬ëŸ¬ ë²ˆ ì‹œë„ë˜ì–´ ì™”ë‹¤. ì§€ê¸ˆê¹Œì§€ë„ ì™„ë²½í•˜ê²Œ ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ í˜ë“¤ë‹¤. ì™œ Natural Languageë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì€ ì–´ë µê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ NLPì—ì„œëŠ” ì–´ë–¤ ë°©ì‹ì„ í™œìš©í• ì§€ì— ëŒ€í•œ ê°œëµì ì¸ overviewë¥¼ ì œì‹œí•œë‹¤. ë˜í•œ, Natural Languageì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ë‹¨ê³„ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œ Linguistics(ì–¸ì–´í•™)ì„ ê°„ëµí•˜ê²Œ ì •ë¦¬í•œë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nì´ì „ Postingì—ì„œëŠ” SVMì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì•˜ë‹¤. ì¼ë°˜ì ì¸ Logistic Regressionì—ì„œëŠ” softmax functionì„ í†µí•´ì„œ ì—¬ëŸ¬ classë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆì—ˆì§€ë§Œ, SVMì˜ ê²½ìš° êµ¬ë¶„ ì„ ì´ ê²°êµ­ì€ hyperplaneìœ¼ë¡œë§Œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ SVMì—ì„œì˜ ì—¬ëŸ¬ í•´ê²°ì±…ì„ ì•Œì•„ë³´ì.\n\n## Multiclass in SVM\n\nê°€ì¥ ì‰½ê²Œ ìƒê°í•  ìˆ˜ ìˆëŠ” ê²ƒì€ SVMì„ ê²°í•©í•´ì„œ Multiclassë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤ëŠ” ideaì´ë‹¤. ì•„ë˜ì—ì„œ ê³§ë°”ë¡œ ì œì‹œí•  ì•„ì´ë””ì–´ë“¤ì´ ì´ì— ëŒ€í•œ ë‚´ìš©ì´ë‹¤.\n\n> **1. OvR SVM**\n\nOne vs Rest ì˜ ì•½ìë¡œ ë‹¤ì–‘í•œ ë³„ëª…ì´ ì¡´ì¬í•œë‹¤. (One vs All, OVA, One Against All, OAA)  \nì´ë¦„ì—ì„œë¶€í„° ëŠê»´ì§€ë‹¤ì‹œí”¼ í•˜ë‚˜ì˜ classì™€ ê·¸ ì™¸ì— ëª¨ë“  classë¥¼ í•˜ë‚˜ë¡œ ë¬¶ì–´ì„œ SVMì„ ì´ class ê°¯ìˆ˜ë§Œí¼ ë§Œë“¤ì–´ì„œ ê° decision boundaryë¡œ ë¶€í„° ê±°ë¦¬ê°€ ì–‘ì˜ ë°©í–¥ìœ¼ë¡œ ê°€ì¥ í° classë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹ì´ë‹¤.\n\n$$\n\\argmax_{k \\in [K]}(\\bold{w}_{(k)}^{\\top}\\phi(\\bold{x})+ b_{(k)})\n$$\n\n![svm-ovr](/images/svm-ovr.jpg)\n\nì´ ë°©ì‹ì€ í•˜ë‚˜ì˜ í° ë¬¸ì œë¥¼ ê°–ê³  ìˆëŠ”ë°, ê·¸ê²ƒì€ ê³¼ë„í•œ ë°ì´í„° ë¶ˆê· í˜•ì„ ìœ ë°œí•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œëŠ” classì˜ ìˆ˜ê°€ ë§ì•„ì§ˆ ìˆ˜ë¡ ë” ì‹¬í•´ì§„ë‹¤.\n\n> **2. OvO SVM**\n\nOne vs (Another) Oneì˜ ì•½ìë¡œ, í•´ë‹¹ ë°©ì‹ì€ 1ëŒ€1ë¡œ ë¹„êµí•˜ë©´ì„œ ê° SVMì—ì„œ ì„ íƒí•œ class ì¤‘ì—ì„œ ê°€ì¥ ë§ì€ ì„ íƒì„ ë°›ì€ classë¥¼ ìµœì¢…í•œë‹¤. OvRê³¼ëŠ” ë‹¤ë¥´ê²Œ ê° ê°ì˜ classë¥¼ 1ëŒ€1ë¡œ ë¹„êµí•˜ê¸° ë•Œë¬¸ì— ë°ì´í„°ì˜ ë¶ˆê· í˜•ì— ëŒ€í•œ ìœ„í˜‘ì€ ëœí•˜ë‹¤. í•˜ì§€ë§Œ, í•´ë‹¹ ê³¼ì •ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ K(K-1)/2ê°œì˜ SVMì´ í•„ìš”í•˜ë‹¤.\n\n![svm-ovo](/images/svm-ovo.jpg)\n\në˜í•œ, ê·¸ë¦¼ì—ì„œ \"?\"ë¡œ í‘œì‹œëœ ë¶€ë¶„ì„ ì–´ë–¤ classë¡œ ì„ íƒí• ì§€ì— ëŒ€í•œ ê¸°ì¤€ì´ ì—†ë‹¤. ì™œëƒí•˜ë©´, ê° ì˜ì—­ì—ì„œ í•œ í‘œì”©ë§Œ ë°›ê¸° ë•Œë¬¸ì´ë‹¤.\n\n> **3. DAG SVM**\n\nì• ì„œ ë³´ì•˜ë˜ OvOì™€ OvRì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì¥ë‹¨ì ì„ ì·¨í•˜ê¸° ìœ„í•´ì„œ ë‘˜ì„ ê²°í•©í•œ ë°©ì‹ì´ë‹¤. ê³„ì¸µ í˜•íƒœë¡œ SVMì„ êµ¬ì„±í•˜ê¸° ë•Œë¬¸ì— OvOë³´ë‹¤ëŠ” ì ì€ SVMì„ ì‚¬ìš©í•˜ë©´ì„œ, OvOì—ì„œì˜ ê³¼ë„í•œ ë°ì´í„° ë¶ˆê· í˜•ì„ í•´ê²°í•œë‹¤.\n\n![svm-multiclass-comparing](/images/svm-multiclass-comparing.jpg)\n\n> **4. WW SVM**\n\nmulticlass êµ¬ë¶„ì„ SVM ìµœì í™” ê³¼ì •ì— ì ìš©í•˜ê¸° ìœ„í•´ì„œ ëª©ì  í•¨ìˆ˜ì˜ í˜•íƒœë¥¼ ë³€í˜•í•˜ì—¬ êµ¬í˜„í•œ ë°©ë²•ìœ¼ë¡œ ìì„¸íˆ ë‹¤ë£¨ì§€ ì•Šì§€ë§Œ, ê¶ê¸ˆí•˜ë‹¤ë©´ í•´ë‹¹ [ğŸ”— link](https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf)ë¥¼ í†µí•´ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\n## Kernel Method\n\nì´ì „ê¹Œì§€ëŠ” ì‹¤ì œë¡œ SVMì˜ í˜•íƒœë¥¼ ë³€í˜•ì‹œí‚¤ê±°ë‚˜ SVMì„ ì—¬ëŸ¬ ê°œ í™œìš©í•˜ì—¬ multiclass classificationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ë³´ì•˜ë‹¤.\n\në˜ ë‹¤ë¥¸ ë°©ë²•ì´ ì¡´ì¬í•œë‹¤. ë°”ë¡œ input ê³µê°„ì„ í™•ì¥í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, ë” ë§ì€ ìœ ì˜ë¯¸í•œ featureë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ê¸°ì¡´ featureë¥¼ ê°€ê³µí•˜ì—¬ ìƒˆë¡œìš´ featureë¡œ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì‹œìŠ¤í…œì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ê¸°ì¡´ featureë¥¼ ê°€ê³µí•˜ì—¬ ìƒˆë¡œìš´ featureë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ì˜ ì˜ˆì‹œë¥¼ ë³´ì.\n\n![feature-transposing](/images/feature-transposing.jpg)\n\nì™¼ìª½ ê³µê°„ì—ì„œëŠ” SVMì€ decision vectorë¥¼ ì ì ˆí•˜ê²Œ ì„ íƒí•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤. í•˜ì§€ë§Œ, ê¸°ì¡´ x ë°ì´í„°ì— ì ˆëŒ€ê°’ì„ ì·¨í•˜ì—¬ ë‚˜íƒ€ë‚´ì–´ ë°ì´í„°ì— ì¶”ê°€í•˜ë©´, ì‰½ê²Œ decision boundaryë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ëŸ¬í•œ ì—¬ëŸ¬ ë³€í™˜ í•¨ìˆ˜ë¥¼ ì ìš©í•´ë³´ë©° ì—¬ëŸ¬ featureë¥¼ ë” ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì¢‹ì€ í•´ê²°ì±…ì„ ê°€ì ¸ë‹¤ ì¤„ ê²ƒì´ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ì˜ Soft margin SVMì˜ Dual Problemì„ ë‹¤ì‹œ í•œ ë²ˆ ì‚´í´ë³´ì.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & 0 \\leq \\alpha_{i} \\leq C, & i = 1, ..., N\n\\end{align*}\n$$\n\nì´ê²ƒì„ feature ë³€í™˜(basis functionì„ ì·¨í•œë‹¤.)ì„ í†µí•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\red{\\boldsymbol{\\phi}^{\\top}(\\bold{x}_{i})\\boldsymbol{\\phi}(\\bold{x}_{j})} + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & 0 \\leq \\alpha_{i} \\leq C, & i = 1, ..., N\n\\end{align*}\n$$\n\ní•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ìƒˆë¡œìš´ featureë¥¼ ìƒì„±í•  ìˆ˜ë¡, ê·¸ë¦¬ê³  ê¸°ì¡´ featureë¥¼ ë³µì¡í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ë¡ $\\boldsymbol{\\phi}(\\bold{x}_{i})$ë¥¼ ì—°ì‚°í•˜ëŠ” ë¹„ìš©ì´ ì»¤ì§ˆ ìˆ˜ ë°–ì— ì—†ë‹¤.  \n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì¼ì¢…ì˜ trickì„ í•˜ë‚˜ ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤. ë°”ë¡œ, ë§¤ bayese update ë§ˆë‹¤ ë³€í•˜ì§€ ì•Šê³  ì¬ì‚¬ìš©ë˜ëŠ” ê°’ì¸ $\\boldsymbol{\\phi}^{\\top}(\\bold{x}_{i})\\boldsymbol{\\phi}(\\bold{x}_{j})$ë¥¼ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ë©´ ì–´ë–¨ê¹Œ? ê·¸ë ‡ê²Œ í•˜ë©´ ìš°ë¦¬ëŠ” $\\boldsymbol{\\phi}(\\bold{x}_{i})$ë¥¼ ê³„ì‚°í•˜ê³  êµ¬ì„±í•˜ëŠ” ìˆ˜ê³ ë¥¼ ëœ ìˆ˜ ìˆë‹¤.\n\nì´ê²ƒì´ kernel method(trick)ì˜ í•µì‹¬ ì•„ì´ë””ì–´ì´ë‹¤.\n\nê°€ì¥ ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ ì•„ë˜ì™€ ê°™ì€ ë³µì¡í•œ $\\phi$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ,\n\n$$\n\\boldsymbol{\\phi}(x) = \\exp[{{-x^{2}}\\over{2\\sigma^{2}}}](1, \\sqrt{1\\over{1!\\sigma^{2}}}x, \\sqrt{1\\over{2!\\sigma^{4}}}x^{2}, \\sqrt{1\\over{3!\\sigma^{6}}}x^{3}, \\cdots)\n$$\n\nì•„ë˜ì˜ (RBF) kernelë¡œ ëŒ€ì²´ê°€ ê°€ëŠ¥í•´ì§„ë‹¤.\n\n$$\n\\kappa(x,x^{\\prime}) = \\exp(-{{(x - x^{\\prime})}\\over{2\\sigma^{2}}}) = \\boldsymbol{\\phi}^{\\top}(x)\\boldsymbol{\\phi}(x^{\\prime})\n$$\n\nëŒ€ê²Œ ìš°ë¦¬ê°€ í‘œí˜„í•˜ê³ ì í•˜ëŠ” í˜•íƒœì˜ $\\boldsymbol{\\phi}$ëŠ” ì´ë¯¸ íŠ¹ì • kernel í•¨ìˆ˜ë¡œ ë§¤í•‘ë˜ê³  ìˆìœ¼ë‹ˆ ì§ì ‘ $\\boldsymbol{\\phi}$ë¥¼ ê³„ì‚°í•˜ê¸° ì „ì— ì°¾ì•„ë³´ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ê²ƒì´ë‹¤.[ğŸ”— link](https://dataaspirant.com/svm-kernels/#t-1608054630726)\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- A Comparison of Methods for Multi-class Support Vector Machines, Chih-Wei Hsu and Chih-Jen Lin, <https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf>\n- SEVEN MOST POPULAR SVM KERNELS, <https://dataaspirant.com/svm-kernels/#t-1608054630726>\n","slug":"ml-multiclass-classification-in-svm","date":"2022-10-18 23:19","title":"[ML] 5. Multiclass Classification in SVM","category":"AI","tags":["ML","SVM","KernelMethod"],"desc":"ì´ì „ Postingì—ì„œëŠ” SVMì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì•˜ë‹¤. ì¼ë°˜ì ì¸ Logistic Regressionì—ì„œëŠ” softmax functionì„ í†µí•´ì„œ ì—¬ëŸ¬ classë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆì—ˆì§€ë§Œ, SVMì˜ ê²½ìš° êµ¬ë¶„ ì„ ì´ ê²°êµ­ì€ hyperplaneìœ¼ë¡œë§Œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ SVMì—ì„œì˜ ì—¬ëŸ¬ í•´ê²°ì±…ì„ ì•Œì•„ë³´ì.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nìš°ë¦¬ëŠ” Classificationì„ í•˜ê¸° ìœ„í•´ì„œ Logistic Regressionì„ ìˆ˜í–‰í•˜ì˜€ë‹¤. ê·¸ ê²°ê³¼ ê²°êµ­ Classificationë„ ê²°êµ­ì€ ì„ ì„ ê¸‹ëŠ” ê²ƒì´ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë¦¬ê²Œ ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ê·¸ì¹˜ì§€ ì•Šê³  í•˜ë‚˜ ë” ê³ ë¯¼í•´ ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ ìˆë‹¤. ë°”ë¡œ ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ì„œ ì™„ë²½í•˜ê²Œ êµ¬ë¶„í•˜ëŠ” decision boundaryê°€ ì—¬ëŸ¬ ê°œ ìˆì„ ë•Œ, ì–´ë–¤ ê²ƒì´ ê°€ì¥ ì¢‹ì€ ê²ƒì¼ê¹Œ? ì´ê²ƒì— ëŒ€í•œ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ” ê²ƒì´ SVMì´ë‹¤. í•´ë‹¹ Postingì—ì„œëŠ” ì´ì— ëŒ€í•´ì„œ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## (Hard Margin) SVM\n\nSoft Vector Machineì˜ ì•½ìë¡œ, ìœ„ì—ì„œ ì œì‹œí•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ Marginì´ë¼ëŠ” ê²ƒì„ ë„ì…í•˜ì˜€ë‹¤.\n\n> **Margin**\n\n**Margin**ì´ë€ decison boundaryì™€ ê°€ì¥ ê°€ê¹Œìš´ ê° classì˜ ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ 2ë¡œ ë‚˜ëˆˆ ê°’ì´ë‹¤.\n\n![svm-1](/images/svm-1.jpg)\n\nìœ„ì˜ ê·¸ë¦¼ì€ ë˜‘ê°™ì€ ë°ì´í„° ë¶„í¬ì—ì„œ ëŒ€í‘œì ì¸ decision boundary ë‘ ê°œë¥¼ ì œì‹œí•œ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” êµ‰ì¥íˆ ë§ì€ decision boundaryë¥¼ ê·¸ë¦´ ìˆ˜ ìˆë‹¤. ê·¸ ì¤‘ì—ì„œë„ íŒŒë€ìƒ‰ ì‹¤ì„ ì´ ì§ê´€ì ìœ¼ë¡œ ê°€ì¥ ì ì ˆí•œ decision boundaryê°€ ë  ê²ƒì´ë¼ê³  ì§ì‘í•  ìˆ˜ ìˆë‹¤. ê·¸ ì´ìœ ëŠ” í•„ì—°ì ìœ¼ë¡œ dataëŠ” noiseì— ì˜í•œ ì˜¤ì°¨ê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ”ë° ì‹¤ì œ ë°ì´í„°ì˜ ì˜¤ì°¨ì˜ í—ˆìš© ë²”ìœ„ë¥¼ ìš°ë¦¬ëŠ” **margin**(=capability of unexpected noise)ë§Œí¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ë¡œ ì´ë¥¼ í•´ì„í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì´ marginì„ í¬ê²Œ í•˜ë©´ í•  ìˆ˜ë¡ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§€ëŠ” ì„ ì„ ê·¸ì„ ìˆ˜ ìˆì„ ê²ƒì´ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.\n\nì´ê²ƒì´ SVMì˜ í•µì‹¬ ì•„ì´ë””ì–´ì´ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, marginì„ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•´ë³´ì. ìš°ë¦¬ê°€ decision boundaryë¥¼ $f(\\bold{x}) := \\bold{w}^{\\top}\\bold{x} + b = 0$ì´ë¼ê³  í•œë‹¤ë©´, ì ($\\bold{x}_{i}$)ê³¼ vector ì§ì„  vector ì‚¬ì´ì˜ ê±°ë¦¬ ê³µì‹ì„ í†µí•´ì„œ ${{|f(\\bold{x}_{i})|}\\over{||\\bold{w}||^{2}}}$ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\në”°ë¼ì„œ marginì€ ìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\min_{i}{{|f(\\bold{x}_{i})|}\\over{||\\bold{w}||^{2}}}\n$$\n\n```plaintext\n ğŸ¤” Canonical(ë²•ì¹™ê¹Œì§€ëŠ” ì•„ë‹ˆì§€ë§Œ ì‚¬ì‹¤ìƒ í‘œì¤€í™”ëœ) SVM\n\n SVMì—ì„œëŠ” f(x) = 0ì¸ ë“±ì‹ í˜•íƒœë¥¼ ê°™ëŠ”ë‹¤. ì¦‰ f(x)ì— ì–´ë–¤ ê°’ì„ ê³±í•´ë„ ë˜‘ê°™ë‹¤ëŠ” ê²ƒì´ë‹¤.\n ê·¸ëŸ°ë° marginì˜ í¬ê¸°ë¥¼ êµ¬í•  ë•Œì—ëŠ”, wì™€ bì— ì–´ë–¤ ê°’ì´ ê³±í•´ì§„ë‹¤ë©´ ì´ ê°’ì´ êµ‰ì¥íˆ ë‹¬ë¼ì§€ê²Œ ëœë‹¤.\n ë”°ë¼ì„œ, ì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” marginì—ì„œì˜ |f(x)| = 1ì´ ë  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•œë‹¤. \n ì´ë ‡ê²Œ í•˜ë©´ ê³„ì‚°ì´ êµ‰ì¥íˆ ì‰¬ì›Œì§„ë‹¤.\n```\n\n![svm-2](/images/svm-2.jpg)\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ í˜•íƒœë¡œ $\\bold{x}^{-}$ì™€ $\\bold{x}^{+}$ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\nì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ marginì„ ì •ì˜í•´ë³´ì.\n\n$$\n\\begin{align*}\n\\rho &= {1\\over2}\\{ {{|f(\\bold{x}^{+})|}\\over{||\\bold{w}||^{2}}} - {{|f(\\bold{x}^{-})|}\\over{||\\bold{w}||^{2}}}  \\} \\\\\n&= {1\\over2}{1\\over{||\\bold{w}||^{2}}}\\{\\bold{w}^{\\top}\\bold{x}^{+} - \\bold{w}^{\\top}\\bold{x}^{-}\\} \\\\\n&= {1\\over{||\\bold{w}||^{2}}}\n\\end{align*}\n$$\n\n> **Optimization**\n\nê·¸ë ‡ë‹¤ë©´, ì´ì œ ìš°ë¦¬ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ì¤€ë¹„ê°€ ëœ ê²ƒì´ë‹¤. ìš°ë¦¬ê°€ í•˜ê³ ì í•˜ëŠ” ê²ƒì€ marginì„ ìµœëŒ€í™”í•˜ë©´ì„œë„, ëª¨ë“  dataë¥¼ ì˜¤ë¥˜ì—†ì´ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ Constraint Optimization í˜•íƒœë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & {1\\over{||\\bold{w}||^{2}}} &\\\\\n  \\text{subject to} \\quad & y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\geq 1, & i = 1, ..., N\n\\end{align*}\n$$\n\nConditional Optimizationì€ ì´ì „ Posting([[ML] 0. Base Knowledge](/posts/ml-base-knowledge))ì—ì„œ ë‹¤ë£¬ë°” ìˆë‹¤. í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ ë¯¸ìˆ™í•˜ë‹¤ë©´ í•œ ë²ˆ ì‚´í´ë³´ê³  ì˜¤ë„ë¡ í•˜ì.\n\nìœ„ ë‚´ìš©ì„ ìˆ™ì§€í•˜ì˜€ë‹¤ë©´, ìœ„ì˜ í¼ì´ ë‹¤ì†Œ ë°”ë€Œì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ê²ƒì´ë‹¤. í•´ë‹¹ í˜•íƒœë¥¼ ë°”ê¾¸ë©´ì„œ, minimize í˜•íƒœë¥¼ ë¯¸ë¶„ì´ ê°„í¸í•  ìˆ˜ ìˆë„ë¡ ë°”ê¾¸ë„ë¡ í•˜ê² ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}||^{2} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\leq 0, & i = 1, ..., N\n\\end{align*}\n$$\n\nìš°ì„  lagrangianì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}\\alpha_{i}(1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b))\n$$\n\nì´ê²ƒì— KKT Conditionì„ ì ìš©í•˜ì—¬ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë“±ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\nì´ë¥¼ $\\mathcal{L}$ì— ëŒ€ì…í•˜ì—¬ ì‹ì„ ì •ë¦¬í•˜ë©´, ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\mathcal{L} = -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i}\n$$\n\nì´ì œ ì´ê²ƒì„ ì´ìš©í•´ì„œ Dual Problemì„ ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & \\alpha_{i} \\geq 0, & i = 1, ..., N\n\\end{align*}\n$$\n\nì´ ì‹ì—ì„œ ëˆˆì—¬ê²¨ ë³¼ì ì€ ë°”ë¡œ constraint ë¶€ë¶„ì´ë‹¤. ì´ ê³¼ì •ì„ í†µí•´ì„œ ê²°ë¡ ì ìœ¼ë¡œ constraint ë¶€ë¶„ì´ ë¶€ë“±ì‹ì—ì„œ ë“±ì‹ì´ ë˜ì—ˆë‹¤. ì´ëŠ” ì—°ì‚° ê³¼ì •ì„ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ í•œë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼ $\\bold{x}_{i}^{\\top}\\bold{x}_{j}$ëŠ” í•œ ë²ˆ ê³„ì‚°í•˜ë©´, ì „ì²´ ê³¼ì •ì—ì„œ ê³„ì†í•´ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì»´í“¨íŒ… ì‹œì—ëŠ” êµ‰ì¥í•œ ì´ì ì„ ë°œíœ˜í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì‹¤ì œë¡œ ê°’ì„ êµ¬í•  ë•Œì—ëŠ” ì´ê²ƒì„ ì´ìš©í•˜ì—¬ ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¼ë°˜ì ì´ë‹¤.\n\n## (Soft Margin) SVM\n\nSVMì˜ ëª¨ë“  ì ˆì°¨ë¥¼ ì‚´í´ë³¸ ê²ƒ ê°™ì§€ë§Œ, ìš°ë¦¬ê°€ ê°„ê³¼í•œ ì‚¬ì‹¤ì´ í•˜ë‚˜ ìˆë‹¤. ë°”ë¡œ ê·¸ê²ƒì€ ìš°ë¦¬ëŠ” dataê°€ í•˜ë‚˜ì˜ ì„ ì„ í†µí•´ì„œ ì™„ë²½í•˜ê²Œ ë‚˜ë‰˜ì–´ì§„ë‹¤ê³  ê°€ì •í–ˆë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ëŠ” ê·¸ë ‡ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ í¬ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì–´ëŠ ì •ë„ì˜ ì˜¤ì°¨ë¥¼ í—ˆìš©í•  ìˆ˜ ìˆë„ë¡ í•´ì•¼ í•œë‹¤. ì´ë¥¼ slack($\\zeta$)ì´ë¼ê³  í•œë‹¤.\n\n![svm-2](/images/svm-2.jpg)\n\nì´ë¥¼ ì ìš©í•˜ë©´, ìš°ë¦¬ì˜ ëª©ì í•¨ìˆ˜ì™€ ì œì•½ ì¡°ê±´ì„ ë³€ê²½í•´ì•¼ í•œë‹¤. ì´ë¥¼ ë³€ê²½í•˜ëŠ” ë°©ë²•ì€ ë‘ ê°€ì§€ê°€ ì¡´ì¬í•˜ëŠ”ë° ê° ê° slack variableì˜ L2-normì„ ëª©ì í•¨ìˆ˜ì— ë”í•˜ëŠ” ë°©ì‹ê³¼ L1-normì„ ë”í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n\n> **L2-norm Optimization**\n\në¨¼ì € L2-normì„ ë”í•˜ëŠ” ë°©ì‹ì„ ì•Œì•„ë³´ì\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i}^{2} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, & i = 1, ..., N\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ $C$ëŠ” margin ìµœëŒ€í™”ì™€ slackness ì •ë„ì˜ ìƒëŒ€ê°’ì„ ì˜ë¯¸í•œë‹¤. ë§Œì•½, slacknessë³´ë‹¤ marginì˜ ìµœëŒ€í™”ê°€ ì¤‘ìš”í•˜ë‹¤ë©´, Cê°’ì€ ì»¤ì§€ê³  ë°˜ëŒ€ë¼ë©´ ì´ ê°’ì€ ì‘ì•„ì§„ë‹¤.\n\nìš°ì„  lagrangianì„ ë¨¼ì € êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}||^{2} + {C\\over2}\\sum_{i=1}^{N}\\zeta_{i}^{2} + \\sum_{i=1}^{N}\\alpha_{i}(1 - \\zeta_{i} - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b))\n$$\n\nKKT conditionì„ ì´ìš©í•˜ì—¬ ì£¼ìš” ê°’ë“¤ì„ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë“±ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n$$\n\\boldsymbol{\\zeta} = {\\alpha\\over{C}}\n$$\n\në§ˆì§€ë§‰ìœ¼ë¡œ ì´ë¥¼ Dual Problemìœ¼ë¡œ ì¬ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\bold{x}_{i}^{\\top}\\bold{x}_{j} + {1\\over{C}}\\delta_{ij}) + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & \\alpha_{i} \\geq 0, & i = 1, ..., N\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ $\\delta_{ij}$ëŠ” ë‹¨ìœ„í–‰ë ¬ì´ë‹¤. ê¸°ì¡´ hard margin svmê³¼ ë¹„êµí–ˆì„ ë•Œ, ${1\\over{C}}\\delta_{ij}$ ì™¸ì—ëŠ” ë°”ë€Œì§€ ì•ŠëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n> **L1-norm Optimization**\n\nê·¸ ë‹¤ìŒì€ L1-normì´ë‹¤.\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, & \\\\\n  & \\zeta_{i} \\geq 0 & i = 1, ..., N\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œëŠ” slack variableì´ ë°˜ë“œì‹œ 0ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ë‹¤ëŠ” ê²ƒì„ ì£¼ì˜í•˜ì.\n\nlagrangianì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\mathcal{L} = {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i} + \\sum_{i=1}^{N}\\alpha_{i}(1 - \\zeta_{i} - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b)) -  \\sum_{i=1}^{N}\\beta_{i}\\zeta_{i}\n$$\n\nKKT conditionì„ ì´ìš©í•˜ì—¬ ì£¼ìš” ê°’ë“¤ì„ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë“±ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\bold{w} = \\sum_{i=1}^{N}\\alpha_{i}y_{i}\\bold{x}_{i}\n$$\n\n$$\n\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0\n$$\n\n$$\n\\sum_{i=1}^{N}\\beta_{i} = C\n$$\n\në§ˆì§€ë§‰ìœ¼ë¡œ ì´ë¥¼ Dual Problemìœ¼ë¡œ ì¬ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad & -{1\\over2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\bold{x}_{i}^{\\top}\\bold{x}_{j} + \\sum_{i=1}^{N}\\alpha_{i} &\\\\\n  \\text{subject to} \\quad & \\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0, & \\\\\n  & 0 \\leq \\alpha_{i} \\leq C, & i = 1, ..., N\n\\end{align*}\n$$\n\nê²°êµ­ ê¸°ì¡´ Hard marginê³¼ ë¹„êµí–ˆì„ ëŒ€ëŠ” ë§ˆì§€ë§‰ constraintì— $\\alpha_{i} \\leq C$ê°€ ì¶”ê°€ëœ ê²ƒ ë°–ì— ì—†ë‹¤.\n\n---\n\në§ˆì§€ë§‰ìœ¼ë¡œ ì—¬ê¸°ì„œ í•˜ë‚˜ì˜ insightë¥¼ ë” ì–»ì„ ìˆ˜ ìˆë‹¤.  \nL1-normì˜ optimizationìœ¼ë¡œ ëŒì•„ê°€ë³´ì.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & {1\\over2}||\\bold{w}||^{2} + C\\sum_{i=1}^{N}\\zeta_{i} &\\\\\n  \\text{subject to} \\quad & 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) - \\zeta_{i} \\leq 0, & \\\\\n  & \\zeta_{i} \\geq 0 & i = 1, ..., N\n\\end{align*}\n$$\n\nëª©ì  í•¨ìˆ˜ì˜ slack variableì— constraintì˜ ê°’ì„ ëŒ€ì…í•˜ì—¬, ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë‹¤.\n\n$$\n\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}max\\{ 0, 1 - y_{i}(\\bold{w}^{\\top}\\bold{x}_{i} + b) \\}\n$$\n\nì´ í˜•íƒœëŠ” logistric regressionì— regularizationì„ ìˆ˜í–‰í•œ ê²ƒê³¼ ë™ì¼í•œ í˜•íƒœë¥¼ ê°€ì§€ê²Œ ëœë‹¤. ì¦‰, ì´ì „ logistic regressionì—ì„œ regularizationì„ ë‹¤ë£¨ì§€ ì•Šì•˜ëŠ”ë°, ê²°êµ­ì€ soft margin svmì˜ L1-norm ëª©ì í•¨ìˆ˜ê°€ logistic regression ì¤‘ì—ì„œë„ hinge functionì´ë¼ëŠ” ê²ƒì„ ì´ìš©í–ˆì„ ë•Œì˜ regularizationì´ ë˜ëŠ” ê²ƒì´ë‹¤.\n\n## Generalization\n\nì—¬íƒœê¹Œì§€ ì‚´í´ë³¸ Regressionì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” Generalí•œ Classification ë°©ì‹ì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤. ìš°ì„  ì•„ë˜ ì‹ì„ ì‚´í´ë³´ì.\n\n- Linear Regression(Quadratic Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}{1\\over2}(1 - y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})) )^{2}$\n- Logit Regresion(Log Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}\\log( 1 + \\exp[-y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})]) )$\n- Binary SVM(Hinge Loss)  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}max\\{ 0, 1 - y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})) \\}$\n\nì—¬íƒœê¹Œì§€ ë‚˜ì˜¨ ì‹ë“¤ì„ ì‚´í´ë³´ë©´ ìœ„ì™€ ê°™ë‹¤. ìš°ë¦¬ëŠ” ì—¬ê¸°ì„œ ì•„ë˜ì™€ ê°™ì€ ì¼ë°˜ì ì¸ í˜•íƒœì˜ Classificationì„ ì œì‹œí•  ìˆ˜ ìˆë‹¤.\n\n- General Classification  \n  $\\min {C^{\\prime}\\over2}||\\bold{w}||^{2} + \\sum_{i=1}^{N}\\varepsilon\\log( 1 + \\exp[-y_{i}(\\bold{w}^{\\top}\\phi(\\bold{x}_{i})]) )$\n\nì—¬ê¸°ì„œ $\\varepsilon$ì´ 1ì´ë©´ ë°”ë¡œ logistic regressionì´ ë˜ê³ , $\\varepsilon$ì´ 0ì— ìˆ˜ë ´í•  ìˆ˜ë¡ SVMì´ ëœë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ ì´ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.\n\n![compare-regressions](/images/compare-regressions.jpg)\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-svm","date":"2022-10-18 17:29","title":"[ML] 4. SVM","category":"AI","tags":["ML","SVM","GeneralClassifier"],"desc":"ìš°ë¦¬ëŠ” Classificationì„ í•˜ê¸° ìœ„í•´ì„œ Logistic Regressionì„ ìˆ˜í–‰í•˜ì˜€ë‹¤. ê·¸ ê²°ê³¼ ê²°êµ­ Classificationë„ ê²°êµ­ì€ ì„ ì„ ê¸‹ëŠ” ê²ƒì´ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë¦¬ê²Œ ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ê·¸ì¹˜ì§€ ì•Šê³  í•˜ë‚˜ ë” ê³ ë¯¼í•´ ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ ìˆë‹¤. ë°”ë¡œ ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ì„œ ì™„ë²½í•˜ê²Œ êµ¬ë¶„í•˜ëŠ” decision boundaryê°€ ì—¬ëŸ¬ ê°œ ìˆì„ ë•Œ, ì–´ë–¤ ê²ƒì´ ê°€ì¥ ì¢‹ì€ ê²ƒì¼ê¹Œ? ì´ê²ƒì— ëŒ€í•œ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ” ê²ƒì´ SVMì´ë‹¤. í•´ë‹¹ Postingì—ì„œëŠ” ì´ì— ëŒ€í•´ì„œ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nì´ì „ê¹Œì§€ ìš°ë¦¬ëŠ” input dataê°€ ë“¤ì–´ì™”ì„ ë•Œ, continuosí•œ outputì„ ì–»ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ” ëŒ€ê²Œ ì •í™•í•œ ìˆ˜ì¹˜ë³´ë‹¤ëŠ” íŠ¹ì • ë¶„ë¥˜ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ íš¨ê³¼ì ì¸ ê²½ìš°ê°€ ë§ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, spam í•„í„°ë§, object detection, ë“± ë“±. ë”°ë¼ì„œ, í•´ë‹¹ í¬ìŠ¤íŒ…ì—ì„œëŠ” classificationì„ ìœ„í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” logistic regressionì— ëŒ€í•´ì„œ ì‚´í´ë³¼ ê²ƒì´ë‹¤.\n\n## Classification\n\n**Classification**ì´ë€ ê²°êµ­ íŠ¹ì • inputì´ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ í•˜ë‚˜ì˜ Classë¼ëŠ” outputì„ ë‚´ë³´ë‚´ëŠ” ê²ƒì´ë‹¤. ì¦‰, outputì€ ì—°ì†ì ì´ì§€ ì•Šê³ , descretí•˜ë‹¤. ëŒ€ê²Œ Classificationì—ì„œëŠ” Classì˜ ê°¯ìˆ˜ë¥¼ Kë¼ê³  í‘œê¸°í•˜ê³ , $C_k$ëŠ” k ë²ˆì§¸ Classë¼ëŠ” ì˜ë¯¸ë¡œ ì‚¬ìš©ë˜ì–´ì§„ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ì–´ë–»ê²Œ Classë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ê²ƒì¼ê¹Œ? ë§¤ìš° ë‹¨ìˆœí•˜ê²Œë„ ì´ëŠ” **Decision Boundary**ë¼ëŠ” ì„ ì„ ê·¸ì–´ì„œ í•´ê²° í•  ìˆ˜ ìˆë‹¤.\n\n![decision-boundary](/images/decision-boundary.jpg)\n\nìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼ ìš°ë¦¬ëŠ” ì„ ì„ í•˜ë‚˜ ê·¸ì–´ì„œ $\\red{\\text{x}}$ì™€ $\\blue{\\text{o}}$ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” Class 1ì— í•´ë‹¹í•  ê²ƒì´ë¼ê³  ì˜ˆì¸¡í•˜ëŠ” êµ¬ê°„ $R_1$ì´ ë§Œë“¤ì–´ì§€ê³ , Class 2ë¼ê³  ì˜ˆì¸¡í•˜ëŠ” êµ¬ê°„ $R_2$ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.\n\nì¦‰, classificationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ í•´ì•¼í•  ì¼ì€ ê¸°ì¡´ì˜ Regression ê³¼ì •ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\nê²°êµ­ ì°¾ê³ ì í•˜ëŠ” ê²ƒì´ ì„ ì´ë¼ë©´, ì´ê²ƒì„ Linear Regressionìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ Linear Regressionì„ ë°”ê¿”ì„œ ìƒê°í•  ìˆ˜ ìˆë‹¤.\n\n- ì˜ˆì¸¡ê°’($\\hat{y}$, $h(\\bold{x})$)  \n  $h(\\bold{x}) = \\text{sign}(\\bold{w}^{\\top}\\bold{x}) = \\begin{cases} +1 & \\bold{w}^{\\top}\\bold{x} \\geq 0 \\\\ -1 & \\text{otherwise}\\end{cases}$\n- Least Squared Error(LS, MLE)  \n  ì‹¤ì œë¡œ parameterë¥¼ êµ¬í•  ë•Œì—ëŠ” signì„ ì·¨í•˜ì§€ ì•ŠëŠ”ë°, signì„ ì·¨í•˜ê²Œ ë˜ë©´ ëª¨ë‘ LSëŠ” ê²°êµ­ ì˜¤ë‹µì˜ ê°¯ìˆ˜ ì •ë„ë¡œ ì·¨ê¸‰ëœë‹¤. ì¦‰, ì–¼ë§ˆë‚˜ ì˜ˆì¸¡ì´ ì˜ëª»ë˜ì—ˆëŠ”ì§€ë¥¼ ë°˜ì˜í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì´ëŠ” ê¸°ì¡´ Linear Regressionì˜ LSë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ ìˆ˜í–‰í•œë‹¤.  \n  $\\argmin_{w} {1\\over2}\\sum_{n=1}^{N}{(y_n - (\\bold{w}^{\\top}\\bold{x}))^2}$\n\nì´ë ‡ê²Œ Linear Regressionì„ ì ìš©í•˜ë©´ ë¬¸ì œê°€ ì—†ì„ ê±° ê°™ë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë¬¸ì œê°€ ìˆë‹¤. ë°”ë¡œ, ë°ì´í„°ê°€ ë¶ˆê· í˜•í•  ë•Œì´ë‹¤. ë§Œì•½ ë°ì´í„°ê°€ decision boundaryë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ì¹­(symmetric)ì¸ í˜•íƒœë¡œ ì¡´ì¬í•œë‹¤ë©´, ë¬¸ì œê°€ ì—†ë‹¤. í•˜ì§€ë§Œ, ë¹„ëŒ€ì¹­(asymmetric)ì¸ ê²½ìš° ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤. ì™œëƒí•˜ë©´, linear regressionì€ ìµœì ì—ì„œ ë°ì´í„°ì˜ í‰ê· ì„ ë°˜ì˜í•˜ëŠ”ë° ë¶ˆê· í˜•í•œ ê²½ìš° ë°ì´í„°ì˜ í‰ê· ì´ Decision Boundaryê°€ ë˜ëŠ” ê²ƒì€ ë¬¸ì œê°€ ìˆë‹¤.\n\n![linear-in-classification](/images/linear-in-classification.jpg)\n\n## Logistic Regression\n\nìœ„ì—ì„œ ì œì‹œí•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ Classificationì—ì„œëŠ” Linear Regressionì´ ì•„ë‹Œ Logistic Regressionì„ í™œìš©í•œë‹¤. ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œ ê¸°ë°˜ì´ ë  ìš”ì†Œë“¤ì„ ë¨¼ì € ì‚´í´ë³´ì.\n\n> **Discriminant Function**\n\níŒë³„í•¨ìˆ˜(Discriminant Function, Score Function) ë“±ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” í•´ë‹¹ í•¨ìˆ˜ëŠ” íŠ¹ì • dataê°€ íŠ¹ì • classì— ì†í•  ê°€ëŠ¥ì„±(likelihood, probability, score)ì„ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ì´ë‹¤. ì¦‰, inputìœ¼ë¡œ dataë¥¼ ë°›ê³ , outputìœ¼ë¡œ classì— ì†í•  í™•ë¥ ì„ ë‚´ë³´ë‚¸ë‹¤.\n\nì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ í•  ìˆ˜ ìˆë‹¤.\n\në§Œì•½, $f_k(\\bold{x}) \\gt f_j(\\bold{x})$ì´ë¼ë©´, $\\bold{x}$ì˜ classëŠ” $C_k$ì´ë‹¤.\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ Classê°€ ìˆëŠ” ê³µê°„ì—ì„œ dataë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.\n\n$$\nh(\\bold{x}) = \\argmax_{k}f_{k}(\\bold{x})\n$$\n\nê·¸ë ‡ë‹¤ë©´, Discriminant Functionìœ¼ë¡œ ì–´ë–¤ ê°’ì„ ì“°ë©´ ì¢‹ì„ê¹Œ? ì´ì— ëŒ€í•œ í•´ê²°ì±…ì„ Bayes Decision Ruleì—ì„œ ì œì‹œí•œë‹¤.\n\n> **Bayes Decision Rule**\n\në§Œì•½ ìš°ë¦¬ê°€ íŠ¹ì • dataê°€ íŠ¹ì • Classì— ì†í•  í™•ë¥ ì„ êµ¬í•œë‹¤ê³  í•˜ì. ìš°ë¦¬ëŠ” ë¨¼ì € Likelihoodë¥¼ ìƒê°í•  ìˆ˜ ìˆë‹¤. $P(x|C = k), P(x|C = j)$ë¥¼ êµ¬í•˜ì—¬ ê° Classì— ì†í•  í™•ë¥ ì„ ë¹„êµí•  ìˆ˜ ìˆì„ê¹Œ?  \në¬¼ë¡  ë¹„êµëŠ” ê°€ëŠ¥í•˜ë‹¤ í•˜ì§€ë§Œ, ë°˜ìª½ì§œë¦¬ ë¹„êµë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë§Œì•½, class kì— ì†í•˜ëŠ” ë°ì´í„°ë³´ë‹¤ class jì— ì†í•˜ëŠ” ë°ì´í„°ê°€ í›¨ì”¬ ë§ë‹¤ê³  í•˜ì. ê·¸ëŸ¬ë©´, ì¼ë°˜ì ìœ¼ë¡œ class jê°€ ë°œìƒí•  í™•ë¥  ìì²´ê°€ ë†’ë‹¤. í•˜ì§€ë§Œ, likelihoodëŠ” ì´ëŸ¬í•œ ê²½í–¥ì„ ë°˜ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤. ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ì.\n\n```plaintext\n ğŸ¤” ì–´ë–¤ ë™ë¬¼ì˜ í„¸ì— ì¡´ì¬í•˜ëŠ” ìƒ‰ì˜ ê°¯ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê³ ì–‘ì´ì¼ í™•ë¥ ê³¼ í˜¸ë‘ì´ì¼ í™•ë¥ ì´ë¼ê³  í•˜ì.\n\n  ê·¸ë¦¬ê³ , input dataëŠ” í„¸ì— ì¡´ì¬í•˜ëŠ” ìƒ‰ì˜ ìˆ˜ë¼ê³  í•˜ì. (í˜¸ë‘ì´ëŠ” ëŒ€ê²Œ 3ê°€ì§€ ìƒ‰, ë°±í˜¸ = 2ê°€ì§€ ìƒ‰, ê³ ì–‘ì´ëŠ” ë§¤ìš° ë‹¤ì–‘)\n  ê·¸ë ‡ë‹¤ë©´, P(í„¸ì˜ ìƒ‰ = 3|C = í˜¸ë‘ì´), P(í„¸ì˜ ìƒ‰ = 3|C = ê³ ì–‘ì´)ë¥¼ ë¹„êµí–ˆì„ ë•Œ, ìš°ë¦¬ëŠ” ë‹¹ì—°íˆ ì „ìê°€ í¬ë‹¤ê³  ìƒê°í•  ê²ƒì´ë‹¤.\n  í•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ê³ ë ¤í•˜ì§€ ì•Šì€ ê²ƒì´ ìˆë‹¤. ë°”ë¡œ ì „ì²´ ê³ ì–‘ì´ì™€ í˜¸ë‘ì´ì˜ ë¹„ìœ¨ì´ë‹¤. \n  ìƒëŒ€ì ìœ¼ë¡œ ê³ ì–‘ì´ê°€ í˜¸ë‘ì´ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ë‹¤ëŠ” ê²ƒì„ ê³ ë ¤í–ˆì„ ë•Œ, ê³ ì–‘ì´ì˜ í™•ë¥ ì´ ë” ë†’ì„ ìˆ˜ë„ ìˆë‹¤. \n\n  ì¦‰, ì–´ë–¤ ë™ë¬¼ì˜ í„¸ì— ì¡´ì¬í•˜ëŠ” ìƒ‰ì˜ ê°¯ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê³ ì–‘ì´ì¼ í™•ë¥ ì€ \n  P(C=ê³ ì–‘ì´|í„¸ì˜ ìƒ‰=3) =  P(í„¸ì˜ ìƒ‰ = 3|C = ê³ ì–‘ì´)P(C=ê³ ì–‘ì´)ì´ë‹¤. (ë¶„ëª¨ëŠ” ìƒëµí•¨.)\n```\n\nì¦‰, Bayes Ruleì— ê¸°ë°˜í•˜ì—¬ ìš°ë¦¬ê°€ ì›í•˜ëŠ” outputì€ Posteriorë¼ëŠ” ê²ƒì„ ëª…í™•íˆ ì•Œ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\np(C_{k}|\\bold{x}) &= {{p(\\bold{x}| C_{k}) p(C_{k})}\\over{\\sum_{j=1}^{K}{p(\\bold{x}|C_{j})p(C_{j})}}} \\\\\n&\\propto p(\\bold{x}| C_{k}) p(C_{k})\n\\end{align*}\n$$\n\nìœ„ì˜ ê²½ìš° Classê°„ì˜ ìƒëŒ€ ë¹„êµì— ì‚¬ìš©í•˜ëŠ” ì§€í‘œë¡œ ì´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ë¶„ëª¨(Normalization Factor, í™•ë¥ ì˜ ì´í•©ì´ 1ì´ ë˜ë„ë¡ í•˜ëŠ” ì—­í• )ë¥¼ ì œì™¸í•˜ì—¬ë„ ìƒê´€ì—†ê¸°ì— ëŒ€ê²Œ ë³µì¡í•œ ë¶„ëª¨ ê³„ì‚°ì„ ì œì™¸í•˜ê³  í‘œí˜„í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\n\në˜í•œ, ì•ì„  ì˜ˆì‹œì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” insightëŠ” í¸í–¥ëœ ë°ì´í„°ì¼ìˆ˜ë¡ MLEë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ìœ„ì—ì„œ Linear Regressionì´ Classificationì— ë¶€ì í•¨í•œ ê²½ìš°ë„ ë°ì´í„°ì˜ í¸í–¥ì´ ìˆì„ ê²½ìš°ì´ë‹¤. ì´ ì—­ì‹œ Linear Regressionì´ ê²°êµ­ì€ MLEì— ê¸°ë°˜í•˜ê¸° ë•Œë¬¸ì¸ ê²ƒì´ë‹¤.\n\nìš°ë¦¬ëŠ” ê° Class ìì²´ì˜ í™•ë¥ (Prior)ê³¼ Likelihoodë¥¼ ì´ìš©í•  ìˆ˜ ìˆëŠ” Discriminant Functionì„ êµ¬í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n> **Logistic Regression**\n\nì ì´ì œ ë“œë””ì–´ Logistric Regressionì„ ì‹œì‘í•´ë³´ì. ìš°ë¦¬ëŠ” Discriminant Functionì„ ë¨¼ì € ì§€ì •í•´ì•¼ í•œë‹¤. ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ì´ ìˆì§€ë§Œ, ê°€ì¥ ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€ **Softmax**ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤. **Softmax**ë¥¼ í™œìš©í•˜ì—¬ ì‹ì„ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\n$$\np(y_n = k | \\bold{x}_n, \\bold{w}) = {{\\exp(\\bold{w}_{k}^{\\top}\\bold{x}_n)}\\over{\\sum_{j=1}^{K}{\\exp(\\bold{w}_{j}^{\\top}\\bold{x}_n)}}}\n$$\n\në§Œì•½, classê°€ 2ê°œì¸ Binary Classificationì¸ ê²½ìš°ì— **Softmax**ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤. íŠ¹íˆ ì´ë¥¼ **Sigmoid**(**Logit**)ë¼ê³  ì •ì˜í•œë‹¤.\n\n$$\np(y_n = k | \\bold{x}_n, \\bold{w}) = {1\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}\n$$\n\nì´ë¥¼ ìœ ë„í•˜ëŠ” ê³¼ì •ì€ ìƒëµí•˜ì§€ë§Œ, ì—¬íƒ€ ë‹¤ë¥¸ ë¸”ë¡œê·¸ë¥¼ ë” ì°¸ê³ í•˜ë©´ ì¢‹ë‹¤.\n\nì´ë¥¼ Linear Regressionê³¼ ë¹„êµí•´ì„œ ì‚´í´ë³´ì.\n\n![logistic-vs-linear](/images/logistic-vs-linear.jpg)\n\nLinear Regressionì€ íŠ¹ì •ê°’ì„ í–¥í•´ ë‚˜ì•„ê°€ê³  ìˆë‹¤. í•´ë‹¹ ë°©ì‹ì„ ë³´ë©´ xê°€ ëŒ€ìƒì˜ íŠ¹ì„±ì„ ê°•í•˜ê²Œ ê°€ì§€ê³  ìˆë‹¤ë©´, ëª…í™•í•˜ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” **sigmoid**($\\sigma$) í•¨ìˆ˜ê°€ [0, 1] ë²”ìœ„ ë‚´ì—ì„œ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— Regression ê³¼ì •ì—ì„œ ê·¹ë‹¨ ë°ì´í„°(outlier)ê°€ ê°€ì§€ëŠ” ì˜í–¥ë ¥ì´ Linear Regressionë³´ë‹¤ ê·¹ë‹¨ì ìœ¼ë¡œ ì ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\nì ì´ê²ƒì´ ê°€ì§€ëŠ” ì˜ë¯¸ë¥¼ ì´ì „ì— ì‚´í´ë³¸ **Bayes Decision Rule**ì— ê¸°ë°˜í•´ì„œ ìƒê°í•´ë³´ì. **sigmoid**($\\sigma$)ëŠ” ê²°êµ­ ê·¹ë‹¨ì ì¸ ë°ì´í„°ì´ë“ , ì• ë§¤í•œ ë°ì´í„°ì´ë“  ê±°ì˜ ë¹„ìŠ·í•œ ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤. ê·¸ë ‡ë‹¤ëŠ” ê²ƒì€ ê¸°ì¡´ì—ëŠ” í‰ê· ì„ êµ¬í•˜ëŠ”ë°ì— input(x)ì˜ ê°’ì´ í° ì˜í–¥ì„ ë¯¸ì³¤ë‹¤ë©´, **sigmoid**($\\sigma$)ì—ì„œëŠ” íŠ¹ì • classì— ì†í•˜ëŠ” xì˜ ê°¯ìˆ˜ê°€ ë§ì€ ì˜í–¥ì„ ì£¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ì„œ **sigmoid**($\\sigma$)ê°€ ì™„ë²½í•˜ì§€ëŠ” ì•Šì§€ë§Œ, **Bayes Decision Rule**ì„ ë°˜ì˜í–ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\në§ˆì§€ë§‰ìœ¼ë¡œ, MLEë¥¼ í†µí•´ì„œ Logistic Regressionì˜ parameterë¥¼ ì¶”ì •í•´ë³´ì. (MAPëŠ” ê¸°ì¡´ì— ì‚´í´ë³¸ Linear Regressionê³¼ ë™ì¼í•˜ê²Œ regularizerë¥¼ ë”í•´ì£¼ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì— ìƒëµí•œë‹¤.)\n\n$$\n\\begin{align*}\n\\argmax_{w}\\log{p(\\mathcal{D}|\\bold{w})} &= \\argmax_{w}\\sum_{n=1}^{N}{\\log p(y_{n}|\\bold{x}_{n}, \\bold{w})} \\\\\n&= \\argmax_{w}\\sum_{n=1}^{N}{\\log ({1\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}) } \\\\\n&= \\argmax_{w}\\sum_{n=1}^{N}{-\\log (1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})) } \\\\\n&= \\argmin_{w}\\sum_{n=1}^{N}{\\log (1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})) } \\\\\n\\end{align*}\n$$\n\n## Gradient Descent/Ascent\n\nìœ„ì˜ ë³µì¡í•œ ì‹ì„ ë´¤ìœ¼ë©´ ì•Œê² ì§€ë§Œ, ì•ˆíƒ€ê¹ê²Œë„ ì¼ë°˜ì‹ìœ¼ë¡œ $\\bold{w}_{MLE}, \\bold{w}_{MAP}$ ë“±ì„ êµ¬í•  ìˆ˜ëŠ” ì—†ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ê°€ ë¯¿ì„ ê²ƒì€ Gradientë¥¼ ì´ìš©í•œ ë°©ì‹ì´ë‹¤.\n\n> **Gradient Descent**\n\në¨¼ì €, ìœ„ì—ì„œ ë´¤ê² ì§€ë§Œ, LossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\mathcal{L} = \\sum_{n=1}^{N}{\\log(1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n}))}\n$$\n\nì´ì œ ì´ë¥¼ ë¯¸ë¶„í•´ì„œ Gradientë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}) = \\sum_{n=1}^{N}{{{-y_{n}\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}\\bold{x}_{n}}\n$$\n\në”°ë¼ì„œ, Gradient Descent ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ëœë‹¤.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\alpha\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}_{t})\n$$\n\n> **Gradient Ascent**\n\nìœ„ì˜ ë°©ì‹ì´ ê°€ì¥ ì¼ë°˜ì ì´ì§€ë§Œ, ìš°ë¦¬ê°€ sigmoidì˜ classê°’ìœ¼ë¡œ $y \\in \\{-1, 1\\}$ ëŒ€ì‹  $y \\in \\{0, 1\\}$ì„ ì‚¬ìš©í–ˆì„ ê²½ìš° ë‹¤ë¥¸ ì‹ìœ¼ë¡œë„ ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë‹¤.\n\nì´ ê²½ìš°ì—ëŠ” Lossë¼ê¸° ë³´ê¸° ì–´ë µì§€ë§Œ, ë‹¤ë¥¸ í˜•íƒœì˜ optimization í˜•íƒœê°€ ë§Œë“¤ì–´ì§„ë‹¤. (ì—¬ê¸°ì„œ $\\sigma$ëŠ” sigmoid í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.)\n\n$$\n\\argmax_{\\bold{w}} \\sum_{n=1}^{N}y_{n}\\log{\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}) + (1-y_{n})\\log{(1-\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}))} }\n$$\n\nì´ë¥¼ ë˜‘ê°™ì´ ë¯¸ë¶„í•˜ì—¬ ì‚¬ìš©í•˜ì§€ë§Œ, ë°˜ëŒ€ë¡œ ì´ ê²½ìš°ì—ëŠ” maximization ì´ê¸° ë•Œë¬¸ì— Gradient Ascentë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤.\n\nìš°ì„  ë¯¸ë¶„ ê²°ê³¼ ì–»ëŠ” GradientëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}) = \\sum_{n=1}^{N}{[y_{n} - \\sigma(\\bold{w}^{\\top}\\bold{x}_{n})]\\bold{x}_{n}}\n$$\n\nêµ‰ì¥íˆ ê°„ë‹¨í•˜ê²Œ ì •ë¦¬ê°€ ë˜ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\alpha\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}_{t})\n$$\n\në”°ë¼ì„œ, ì•„ë˜ì™€ ê°™ì´ Gradient Ascentë¥¼ í™œìš©í•˜ì—¬ ê³„ì‚°í•˜ëŠ” ê²ƒë„ ì¶©ë¶„íˆ ê°€ëŠ¥í•˜ë‹¤.\n\n> **Newton Method**\n\nì´ëŸ¬í•œ í˜•íƒœë¡œ ë„˜ì–´ì˜¤ê²Œ ë˜ë©´, êµ‰ì¥íˆ ë§ì€ ì—°ì‚°ì´ ê° updateë§ˆë‹¤ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ ê³¼ì •ì„ ì¶•ì•½í•  ë°©ë²•ì„ ì°¾ê²Œ ëœë‹¤. ê·¸ ì•„ì´ë””ì–´ëŠ” ë°”ë¡œ gradientë¥¼ ì—…ë°ì´íŠ¸ í•  ë•Œ, linear í•˜ê²Œ updateí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ Quadraticí•˜ê²Œ updateí•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•œ ë°©ë²•ë¡ ì´ **Newton Method**ì´ë‹¤. ì´ ë°©ì‹ì„ Logistic Regressionì— ì ìš©í•˜ì˜€ì„ ë•Œ, ì´ë¥¼ IRLS(Iterative Re-weighted Least Squared) Algorithm ì´ë¼ê³  ë¶€ë¥¸ë‹¤.\n\n![newton-method](/images/newton-method.jpg)\n\nìœ„ ê·¸ë˜í”„ì—ì„œ f(x)ê°€ Loss ë¼ê³  í•  ë•Œ, ìš°ë¦¬ëŠ” $x_k$ì—ì„œ ì§ì„ í˜•ì˜ gradientë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ quadratic í˜•íƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´ê°’ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\nì´ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒ 2ê°€ì§€ì— ëŒ€í•œ ì‚¬ì „ ì´í•´ê°€ í•„ìš”í•˜ë‹¤.\n\n- Taylor Series  \n  smoothí•œ í˜•íƒœë¥¼ ê°€ì§„ xì— ëŒ€í•œ í•¨ìˆ˜ë¥¼ xì— ëŒ€í•œ ê¸‰ìˆ˜ì˜ í˜•íƒœë¡œ ë³€í™˜í•œ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n  $T_{\\infin}(x) = \\sum_{k=0}^{\\infin}{f^{(k)}(x_{0})\\over{k\\!}}(x-x_{0})^{k} $  \n  ì¦‰, sine í•¨ìˆ˜ì™€ ê°™ì€ í˜•íƒœì˜ ê·¸ë˜í”„ë„ xì˜ ê¸‰ìˆ˜ í˜•íƒœë¡œ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. Newton Methodì—ì„œëŠ” ë¬´í•œëŒ€ê¹Œì§€ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , ëŒ€ê²Œ K=2ê¹Œì§€ë¥¼ ì“´ë‹¤.\n- Hessian Matrix  \n  íŠ¹ì • í•¨ìˆ˜ $f(\\bold{x})$ë¥¼ ê° featureì— ëŒ€í•´ì„œ ì´ì¤‘ í¸ë¯¸ë¶„í•œ ê²°ê³¼ë¥¼ ì €ì¥í•œ í–‰ë ¬ì´ë‹¤. ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n  $\n  H = \\nabla^{2}f(x) =\n  \\left[\n    \\begin{array}{ccc}\n      \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{1}^{2}} & \\cdots & \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{1} \\partial x_{D}} \\\\\n      \\vdots & \\ddots & \\vdots \\\\\n      \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{D} \\partial x_{1}} & \\cdots & \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{n}^{2}}\n    \\end{array}\n  \\right]\n  $\n\nì´ë¥¼ ì´ìš©í•´ì„œ, Newton Methodì˜ ê²°ê³¼ê°’ì„ ì •ë¦¬í•˜ë©´ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\bold{w}^{(k+1)} = \\bold{w}^{(k)} - [\\nabla^{2}\\mathcal{J}(\\bold{w}^{(k)})]^{-1}\\nabla\\mathcal{J}(\\bold{w}^{(k)})\n$$\n\nì ì´ì œ ì´ê²ƒì„ ì‹¤ì œë¡œ Logistic Regression ì‹ì— ëŒ€ì…í•´ë³´ì.\n\n$$\n\\begin{align*}\n  \\nabla\\mathcal{J}(w) &= - \\sum_{n=1}^{N}(y_{n}-\\hat{y}_{n})x_{n} \\\\\n  \\nabla^{2}\\mathcal{J}(w) &= \\sum_{n=1}^{N}\\hat{y}_{n}(1-\\hat{y}_{n})\\bold{x}_{n}\\bold{x}_{n}^{\\top}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ, ì•„ë˜ì™€ ê°™ì´ ë³€ìˆ˜ë¥¼ ì •ì˜í•˜ë©´,\n\n$$\nS =\n  \\begin{bmatrix}\n    \\hat{y}_{1}(1-\\hat{y}_1)  & \\cdots  & 0                         \\\\\n    \\vdots                    & \\ddots  & \\vdots                     \\\\\n    0                         & \\cdots  & \\hat{y}_{N}(1-\\hat{y}_N)  \\\\\n  \\end{bmatrix},\n\n\\bold{b} =\n  \\begin{bmatrix}\n    {{y_{1} - \\hat{y}_{1}}\\over{\\hat{y}_{1}(1-\\hat{y}_{1})}} \\\\\n    \\vdots \\\\\n    {{y_{N} - \\hat{y}_{N}}\\over{\\hat{y}_{N}(1-\\hat{y}_{N})}}\n  \\end{bmatrix}\n$$\n\nê²°ê³¼ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\bold{w}_{k+1} &= \\bold{w}_{k} + (XS_{k}X^{\\top})^{-1}XS_{k}\\bold{b}_{k} \\\\\n&= (XS_{k}X^{\\top})^{-1}[(XS_{k}X^{\\top})\\bold{w}_{k} + XS_{k}\\bold{b}_{k}] \\\\\n&= (XS_{k}X^{\\top})^{-1}XS_{k}[X^{\\top}\\bold{w}_{k} + \\bold{b}_{k}]\n\\end{align*}\n$$\n\nì´ëŠ” ê²°ì½” ê³„ì‚° ê³¼ì •ì´ ë‹¨ìˆœí•˜ë‹¤ê³ ëŠ” í•  ìˆ˜ ì—†ì§€ë§Œ, ë¹ ë¥´ê²Œ ìˆ˜ë ´í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê°€ì¹˜ìˆëŠ” ë°©ë²•ì´ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-logistic-regression","date":"2022-10-18 09:58","title":"[ML] 3. Logistic Regression","category":"AI","tags":["ML","LogisticRegression","Classification","SigmoidFunction","SoftmaxFunction","NewtonMethod"],"desc":"ì´ì „ê¹Œì§€ ìš°ë¦¬ëŠ” input dataê°€ ë“¤ì–´ì™”ì„ ë•Œ, continuosí•œ outputì„ ì–»ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ” ëŒ€ê²Œ ì •í™•í•œ ìˆ˜ì¹˜ë³´ë‹¤ëŠ” íŠ¹ì • ë¶„ë¥˜ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ íš¨ê³¼ì ì¸ ê²½ìš°ê°€ ë§ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, spam í•„í„°ë§, object detection, ë“± ë“±. ë”°ë¼ì„œ, í•´ë‹¹ í¬ìŠ¤íŒ…ì—ì„œëŠ” classificationì„ ìœ„í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” logistic regressionì— ëŒ€í•´ì„œ ì‚´í´ë³¼ ê²ƒì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nRegression(íšŒê·€)ì´ë¼ëŠ” ë‹¨ì–´ëŠ” \"ì›ë˜ì˜ ìƒíƒœë¡œ ëŒì•„ê°„ë‹¤\"ë¡œ ëŒì•„ê°„ë‹¤ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤. ê²°êµ­ ì–´ë–¤ ì¼ë ¨ì˜ Eventë¡œ ì¸í•´ì„œ ë°ì´í„°ì— Noiseê°€ ë°œìƒí•  ìˆ˜ ìˆì–´ë„ ê²°êµ­ì€ í•˜ë‚˜ì˜ \"ë³´í¸\"ìœ¼ë¡œ ì‹œê°„ì´ ì§€ë‚˜ë©´ ìˆ˜ë ´(íšŒê·€)í•  ê²ƒì´ë¼ëŠ” ìƒê°ì— ê¸°ë°˜í•˜ëŠ” ê²ƒì´ë‹¤.  \në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ \"ë³´í¸\"ì„ ì°¾ê¸° ìœ„í•´ì„œ ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” ë…ë¦½ ë°ì´í„° Xë¥¼ í†µí•´ì„œ ì•Œê³ ì í•˜ëŠ” ê°’ Yë¥¼ ë³´í¸ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤. ì´ ê³¼ì •ì„ ìš°ë¦¬ëŠ” Regressionì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë˜í•œ, Xì— ì˜í•´ ë…ë¦½ì ì´ì§€ ì•Šê³  ì¢…ì†ì ì¸ Yì˜ ê´€ê³„ê°€ Linearí•˜ê²Œ í‘œí˜„ë  ë•Œ ì´ë¥¼ ìš°ë¦¬ëŠ” Linear Regressionì´ë¼ê³  í•œë‹¤.  \në”°ë¼ì„œ, í•´ë‹¹ Postingì—ì„œëŠ” Linear Regressionì„ ë°”íƒ•ìœ¼ë¡œ Machine Learningì´ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.\n\n## Regression\n\n> **ì •ì˜**\n\në…ë¦½ ë³€ìˆ˜ Xë¡œ ë¶€í„° ì¢…ì† ë³€ìˆ˜ Yì— ëŒ€ì‘ë˜ëŠ” í•¨ìˆ˜ fë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•œë‹¤.\n\n$$\n\\bold{y} = f(\\bold{x}) + \\epsilon\n$$\n\n$$\nf(\\bold{x}) = \\bold{w}^{\\top}\\bold{x},\n(\\bold{w} = \\begin{bmatrix} w_{0} \\\\ w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{N} \\\\ \\end{bmatrix}, \\bold{x} = \\begin{bmatrix} 1 \\\\ x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{N} \\\\ \\end{bmatrix} )\n$$\n\nì—¬ê¸°ì„œ ê° ë³€ìˆ˜ $x$, $y$, $\\epsilon$, $w$ì€ ë‹¤ìŒê³¼ ê°™ì´ ì—¬ëŸ¬ ì´ë¦„ìœ¼ë¡œ ë¶ˆë ¤ì§„ë‹¤.\n\n- $x$ : input, ë…ë¦½ ë³€ìˆ˜, predictor, regressor, covariate\n- $y$ : output, ì¢…ì† ë³€ìˆ˜, response\n- $\\epsilon$ : noise, ê´€ì¸¡ë˜ì§€ ì•Šì€ ìš”ì†Œ\n- $w$ : weight, ê°€ì¤‘ì¹˜, parameter\n\n> <mark>**ì„±ëŠ¥ í‰ê°€(MSE)**</mark>\n\nìš°ë¦¬ê°€ ë§Œë“  Regressionì´ ì–¼ë§ˆë‚˜ ë°ì´í„°ë¥¼ ì˜ ë°˜ì˜í•˜ëŠ”ì§€ë¥¼ ì•Œê³  ì‹¶ì„ ë•Œ, ì¦‰ í‰ê°€í•˜ê³ ì í•  ë•Œ, ìš°ë¦¬ëŠ” Mean Squared Error(MSE)ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ëŠ” ì´ì „ í¬ìŠ¤íŒ…ì¸ [Parametric Estimation](/posts/ml-parametric-estimation)ì—ì„œë„ ì‚´í´ë³´ì•˜ì—ˆë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, MSEë¥¼ ìµœì†Œë¡œ í•˜ëŠ” f(x)ëŠ” ë¬´ì—‡ì¼ê¹Œ? ì´ë¥¼ í†µí•´ì„œ ë˜, í•˜ë‚˜ì˜ ì‹ê²¬ì„ ë„“í ìˆ˜ ìˆë‹¤. í•œ ë²ˆ MSE ì‹ì„ ì •ë¦¬í•´ë³´ì.\n\n$$\n\\begin{align*}\n\\Epsilon(f) &= E[||\\bold{y}_*-f(\\bold{x})||^2] \\\\\n&= \\int\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{x}, \\bold{y}_*)d\\bold{x}d\\bold{y}_* \\\\\n&= \\int\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{x})p(\\bold{y}_* | \\bold{x})d\\bold{y}_*d\\bold{x} \\\\\n&= \\int p(\\bold{x}) \\red{\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}d\\bold{x}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê²ƒì€ ë°”ë¡œ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ìƒ‰ì¹ í•œ ë¶€ë¶„ì´ë‹¤. ìš°ë¦¬ê°€ ë°”ê¿€ ìˆ˜ ìˆëŠ” ê°’ì€ f(x)ë¥¼ êµ¬ì„±í•˜ëŠ” wë°–ì— ì—†ë‹¤ ì¦‰, ìœ„ ì‹ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì€ ë¹¨ê°„ìƒ‰ ë¶€ë¶„ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒê³¼ ê°™ì•„ì§„ë‹¤.  \në”°ë¼ì„œ, ì´ ë¶€ë¶„ì„ ë¯¸ë¶„í•´ì„œ ìµœì†Ÿê°’ì„ êµ¬í•  ìˆ˜ ìˆëŠ”ë° ì´ë¥¼ í™•ì¸í•´ë³´ì.\n\n$$\n\\begin{align*}\n&{\\partial\\over{\\partial{f(\\bold{x})}}}({\\int||\\bold{y}_*-f(\\bold{x})||^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}) = 0 \\\\\n&{\\partial\\over{\\partial{f(\\bold{x})}}}({\\int||\\bold{y}_*^2p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} - 2f(\\bold{x}){\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} + f(\\bold{x})^2{\\int p(\\bold{y}_* | \\bold{x})d\\bold{y}_*}) = 0 \\\\\n&-2{\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} + 2f(\\bold{x}) = 0 \\\\\n&f(\\bold{x}) = {\\int\\bold{y}_*p(\\bold{y}_* | \\bold{x})d\\bold{y}_*} = E[\\bold{y}_*|\\bold{x}]\n\\end{align*}\n$$\n\nì¦‰, ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” Linear Regression í•¨ìˆ˜ëŠ” data xì— ë”°ë¥¸ ì‹¤ì œ y ê°’ì˜ í‰ê· ì„ ì˜ë¯¸í•œë‹¤. Regressionì˜ ì •ì˜ë¥¼ ìƒê°í–ˆì„ ë•Œ, ì–´ëŠì •ë„ í•©ë¦¬ì ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. \"ë³´í¸\"ì ì´ë‹¤ëŠ” ì˜ë¯¸ì—ì„œ \"í‰ê· \"ì„ ì“°ëŠ” ê²½ìš°ê°€ ë§ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\n\nìœ„ì—ì„œëŠ” MSEë¥¼ ì´ìš©í•´ì„œ ë¶„ì„í•˜ì˜€ì§€ë§Œ, MAE(Mean Absolute Error)ë¥¼ í™œìš©í•˜ì—¬ êµ¬í•  ìˆ˜ ìˆëŠ”ë° ì´ ê²½ìš°ì—ëŠ” Regressionì˜ í˜•íƒœê°€ ë˜ ë‹¬ë¼ì§„ë‹¤. ì¦‰, MSEë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ì€ ìš°ë¦¬ê°€ \"ë³´í¸\"ì ì¸ ë‹µì„ êµ¬í•˜ëŠ”ë° ìˆì–´ \"í‰ê· \"ì„ í™œìš©í•œ ê²ƒì´ê³ , MAEë¥¼ ì‚¬ìš©í•œë‹¤ë©´, ë˜ ë‹¤ë¥¸ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ë  ê²ƒì´ë‹¤.\n\n## MLE of Linear Regression\n\nì´ì œ Linear Regressionì—ì„œ $\\bold{w}$ë¥¼ ì–´ë–»ê²Œ ì°¾ì•„ ë‚˜ê°ˆì§€ì— ëŒ€í•´ì„œ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ìˆœì„œëŠ” ì´ì „ Posting [Parametric Estimation](/posts/ml-parametric-estimation)ì—ì„œ ì‚´í´ë´¤ë˜ ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ MLE, MAP ìˆœìœ¼ë¡œ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ê²ƒì´ ì™œ MLEê³ , MAPë‘ ê´€ë ¨ì´ ìˆëŠ”ì§€ë„ ì‚´í´ë³¼ ê²ƒì´ë‹¤.\n\në“¤ì–´ê°€ê¸°ì— ì• ì„œ, í‘œê¸°ë²•ê³¼ ìš©ì–´ë¥¼ ëª‡ ê°œ ì •ë¦¬í•  í•„ìš”ê°€ ìˆë‹¤.\n\n- $\\bold{x}, \\bold{y}, \\bold{w}$ ë“± êµµì€ ì„  ì²˜ë¦¬ë˜ì–´ ìˆëŠ” ë³€ìˆ˜ëŠ” vectorë¥¼ ì˜ë¯¸í•œë‹¤.\n- $\\bold{X}$ ë“± êµµê³  ëŒ€ë¬¸ìë¡œ ì²˜ë¦¬ë˜ì–´ ìˆëŠ” ë³€ìˆ˜ëŠ” Matrixë¥¼ ì˜ë¯¸í•œë‹¤.\n- $\\bold{w^{\\top}}$, $\\bold{X}^{\\top}$ ì—ì„œ TëŠ” Transposeë¥¼ ì˜ë¯¸í•œë‹¤.\n- feature : input ë°ì´í„°ì˜ ê° ê° ë¶„ë¥˜ ê¸°ì¤€ë“¤ì„ ì˜ë¯¸í•œë‹¤. ìˆ˜ì‹ìœ¼ë¡œëŠ” $x_1, x_2, x_3$ ì´ëŸ° ì‹ìœ¼ë¡œ í‘œí˜„ëœ inputë“¤ ì¤‘ì— ê° ê°ì˜ inputì„ featureë¼ê³  í•˜ë©°, ì‹¤ì œ ì˜ˆì‹œë¡œëŠ” ë°ì´í„° ìˆ˜ì§‘ ì‹œì— ê° ë°ì´í„°ì˜ column(ë‚˜ì´, ì„±ë³„, ë“± ë“±)ì´ ë  ê²ƒì´ë‹¤.\n\nìœ„ì˜ ìš©ì–´ ì •ë¦¬ì— ì˜í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‚¬ì‹¤ì„ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•˜ì.\n\në¨¼ì €, ë‹¨ì¼ Linear Regressionì´ë‹¤.\n\n$$\n\\hat{y} = f(\\bold{x}) = \\bold{w}^{\\top}\\bold{x} = \\bold{x}^{\\top}\\bold{w}\n$$\n\nì´ë²ˆì—ëŠ” ì—¬ëŸ¬ ê°œì˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¶”ì¸¡í•œ ê²°ê³¼ê°’ $\\hat{\\bold{y}}$ ì´ë‹¤.\n\n$$\n\\hat{\\bold{y}} = \\bold{X}\\bold{w}\n$$\n\nê° ì˜ë¯¸ë¥¼ ê³±ì”¹ì–´ë³´ë©´ ì–´ë–»ê²Œ ìƒê²¼ì„ì§€ ì–´ë µí’‹ì´ ì§ì‘ì´ ì˜¬ ê²ƒì´ë‹¤.\n\n> **basis function**\n\nì—¬ê¸°ì„œ ë˜ í•˜ë‚˜ ì§šì–´ë³¼ ê²ƒì€ ë°”ë¡œ $\\bold{x}$ë¥¼ ë³€í˜•í•˜ëŠ” ë°©ë²•ì´ë‹¤. ë°”ë¡œ, ìš°ë¦¬ëŠ” ë°ì´í„°ë¡œ ì…ë ¥ ë°›ì€ ë°ì´í„°ë¥¼ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, í•´ë‹¹ input ê°’ì„ ì œê³±í•´ì„œ ì‚¬ìš©í•´ë„ ë˜ê³ , ì„œë¡œ ë”í•´ì„œ ì‚¬ìš©í•´ë„ ë˜ê³ , ë‚˜ëˆ„ì–´ì„œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ ìš°ë¦¬ê°€ êµ¬í•˜ê³  ì‹¶ì€ ê°’ì´ ëŒ€í•œë¯¼êµ­ ì¸êµ¬ì˜ í‰ê·  ë‚˜ì´ë¼ê³  í•˜ì. ì´ë•Œ, ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì˜ ê°’ì´ ê°€êµ¬ ë‹¨ìœ„ë¡œ ì¡°ì‚¬ë˜ì–´ ë¶€,ëª¨,ìì‹1, ìì‹2, ... ë¡œ ë¶„ë¥˜ë˜ì–´ ë‚˜ì´ê°€ ì í˜€ìˆë‹¤ê³  í•˜ì. ì´ë•Œ ìš°ë¦¬ê°€ í•„ìš”í•œ ê²ƒì€ ê²°êµ­ ì „ì²´ ì¸êµ¬ì˜ ë‚˜ì´ ë°ì´í„°ì„ìœ¼ë¡œ ëª¨ë‘ í•˜ë‚˜ì˜ featureë¡œ í•©ì³ë²„ë¦´ ìˆ˜ë„ ìˆë‹¤.\n\nì´ëŸ¬í•œ ê³¼ì •ì„ ìœ„í•´ì„œ ìš°ë¦¬ëŠ” basis function($\\phi(\\bold{x})$)ì´ë¼ëŠ” ê²ƒì„ ì´ìš©í•œë‹¤. ë‹¨ìˆœíˆ input dataë¥¼ í•©ì„±í•´ì„œ í•˜ë‚˜ì˜ inputì„ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” í•„ìš”ì— ë”°ë¼ input dataë¥¼ ê°€ê³µí•˜ì—¬ ì‚¬ìš©í•˜ë©° ì—¬ëŸ¬ $\\phi$ë¥¼ ì ìš©í•˜ì—¬ ë‚˜íƒ€ë‚¼ ê²½ìš° linear regressionì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœê°€ ëœë‹¤.\n\n$$\nf(\\bold{x}) = \\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x})\n$$\n\nëŒ€í‘œì ì¸ Basis functionì„ ì‚´í´ë³´ì.\n\n- Polynomial basis : í•˜ë‚˜ì˜ input featureì— ëŒ€í•´ì„œ n-ì œê³±í˜•íƒœì˜ vectorë¡œ ë³€í™˜í•˜ëŠ” í˜•ì‹ì´ë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒê³¼ ê°™ì´ í‘œê¸° ëœë‹¤.  \n  $\\boldsymbol{\\phi}(\\bold{x}) = \\begin{bmatrix} 1 \\\\ x \\\\ x^{2} \\\\ \\vdots \\\\ x^{n} \\\\ \\end{bmatrix}$, $\\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x}) = w_{0} + w_{1}x + w_{2}x^{2} + ... + w_{n}x^{n}$  \n  ëŒ€ê²Œ ì´ëŸ¬í•œ í˜•íƒœë¡œ ë³€í˜•í•œ Linear Regressionì„ Polinomial Regressionì´ë¼ê³  ë¶€ë¥´ëŠ”ë°, ì´ë¥¼ í†µí•œ ê²°ê³¼ ê°’ì´ ë§ˆì¹˜ ë‹¤í•­ì‹ì˜ í˜•íƒœë¥¼ ë„ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ, featureì˜ ê°’ì´ polynomialì´ ë˜ì—ˆë”ë¼ë„ $\\bold{w}$ê°€ ì„ í˜•ì„ì„ ìŠì–´ì„œëŠ” ì•ˆëœë‹¤.  \n  ì´ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´, ìš°ë¦¬ëŠ” 1ì°¨ì›ì˜ input ê³µê°„ì—ì„œ ì„ í˜•ìœ¼ë¡œëŠ” ë‚˜ëˆŒ ìˆ˜ ì—†ë˜ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n- Gaussian basis : ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒìœ¼ë¡œ íŠ¹ì • featureë¥¼ gaussianìœ¼ë¡œ ë³€í™˜í•˜ê²Œ ë˜ë©´, ë°ì´í„°ì˜ ê²½í–¥ì„±ì´ íŒŒì•…ëœë‹¤. ì´ëŠ” í›„ì— ë” ìì„¸íˆ ë‹¤ë£° ê¸°íšŒê°€ ì˜¨ë‹¤.  \n- Spline basis: íŠ¹ì • êµ¬ê°„ë§ˆë‹¤ ë‹¤ë¥¸ Polynomial í˜•íƒœì˜ featureë¥¼ ì ìš©í•˜ë„ë¡ í•˜ëŠ” ë°©ì‹ì´ë‹¤. ëŒ€ê²Œ êµ¬ê°„ë§ˆë‹¤ ë‹¤ë¥¸ í™•ë¥  ë¶„í¬ë¥¼ ì ìš©í•˜ê³ ì í•  ë•Œ ì‚¬ìš©í•œë‹¤.\n- Fourier basis, Hyperbolic tangent basis, wavelet basis ë“± ì—¬ëŸ¬ ê°€ì§€ ë°©ì‹ì´ ì¡´ì¬í•œë‹¤.\n\n> **Design Matrix**\n\në§ˆì§€ë§‰ìœ¼ë¡œ, ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ $\\phi(\\bold{x})$ë¥¼ í•˜ë‚˜ì˜ Matrixë¡œ í•©ì¹œ ê²ƒì„ Design Matrixë¼ê³  í•œë‹¤. Nê°œì˜ ë°ì´í„°ë¥¼ Lê°œì˜ ì„œë¡œ ë‹¤ë¥¸ basis functionìœ¼ë¡œ ë³€í™˜í•œ ë°ì´í„°ë¥¼ í–‰ë ¬ë¡œ í‘œí˜„í•˜ë©´, ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\Phi =\n  \\begin{bmatrix}\n    \\phi_1({\\bold{x_1}})  & \\phi_2(\\bold{x_1})  & \\cdots  & \\phi_L(\\bold{x_1})  \\\\\n    \\phi_1({\\bold{x_2}})  & \\phi_2(\\bold{x_2})  & \\cdots  & \\phi_L(\\bold{x_2})  \\\\\n    \\vdots                & \\vdots              & \\ddots  & \\vdots              \\\\\n    \\phi_1({\\bold{x_N}})  & \\phi_2(\\bold{x_N})  & \\cdots  & \\phi_L(\\bold{x_N})  \\\\\n  \\end{bmatrix}\n$$\n\nì´ë¥¼ í†µí•´ì„œ í‘œí˜„í•œ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•œ Linear Regressionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\hat{\\bold{y}} = \\Phi\\bold{w}\n$$\n\nì ì´ì œë¶€í„° ìš°ë¦¬ëŠ” ë³¸ë¡ ìœ¼ë¡œ ë“¤ì–´ì™€ì„œ ìš°ë¦¬ì˜ Linear Regressionì˜ Weight(Parameter, $\\bold{w}$)ë¥¼ ì–´ë–»ê²Œ ì¶”ì •í•  ìˆ˜ ìˆì„ì§€ë¥¼ ì•Œì•„ë³´ì.\n\nìš°ë¦¬ëŠ” ìµœì¢…ì ìœ¼ë¡œ ìš°ë¦¬ì˜ Linear Regressionì´ ì •ë‹µê³¼ ë§¤ìš° ìœ ì‚¬í•œ ê°’ì„ ë‚´ë†“ê¸°ë¥¼ ì›í•œë‹¤. ë”°ë¼ì„œ, ì´ë•Œ ìš°ë¦¬ëŠ” Least Square Errorë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ëª¨ë“  ë°ì´í„°ì—ì„œ ì–»ì€ ì˜ˆì¸¡ê°’(Linear Regressionì˜ output)ê³¼ ì‹¤ì œ yì˜ ê°’ì˜ Square Errorì˜ í•©ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\varepsilon_{LS}(\\bold{w}) = {1\\over2}\\sum_{n=1}^{N}(y_n - \\bold{w}^{\\top}\\boldsymbol{\\phi}(\\bold{x_n}))^2 = {1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2\n$$\n\nì´ì œ $\\argmin_{\\bold{w}}\\varepsilon_{LS}(\\bold{w})$ì„ í’€ê¸° ìœ„í•´ì„œ ë¯¸ë¶„ì„ í•´ë³´ì.\n\n$$\n\\begin{align*}\n&{\\partial\\over\\partial\\bold{w}}{1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2 = 0 \\\\\n&\\Phi^{\\top}(\\bold{y}_* - \\Phi\\bold{w}) = 0 \\\\\n&\\Phi^{\\top}\\Phi\\bold{w} = \\Phi^{\\top}\\bold{y_*} \\\\\n&\\bold{w} = (\\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y_*} \\\\\n&\\bold{w} = \\Phi^{\\dagger}\\bold{y_*}\n\\end{align*}\n$$\n\nì´ë¥¼ í†µí•´ì„œ, ìœ„ì™€ ê°™ì€ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n---\n\nê·¸ëŸ¼ ì´ ì‹ì´ ì™œ MLEë‘ ê´€ë ¨ì´ ìˆëŠ” ê²ƒì¼ê¹Œ? ê·¸ê²ƒì€ ë‹¤ìŒì˜ ê³¼ì •ì„ í†µí•´ì„œ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.\n\nìš°ë¦¬ëŠ” ê° dataë§ˆë‹¤ ì¡´ì¬í•˜ëŠ” error(noise, $y_* - \\hat{y}$, $\\varepsilon$)ê°€ ê·¸ ì–‘ì´ ë§ì•„ì§ì— ë”°ë¼ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. (Central Limit Theorem)\n\n$$\n\\begin{align*}\n\\varepsilon &= y_* - \\hat{y} = y_*-\\phi(\\bold{x}) \\\\\ny_* &= \\phi(\\bold{x}) + \\varepsilon\n\\end{align*}\n$$\n\nì´ë¥¼ ì¢Œí‘œ í‰ë©´ ìƒì—ì„œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n\n![gaussian-error](/images/gaussian-error.jpeg)\n\në˜í•œ, $\\varepsilon$ì˜ í™•ë¥ ì„ ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ í™•ë¥ ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\np(\\varepsilon) &= {1\\over{\\sqrt{2\\pi}\\sigma}}\\exp{[-{\\varepsilon^2\\over{2\\sigma^2}}]} \\\\\np(\\varepsilon) &= {1\\over{\\sqrt{2\\pi}\\sigma}}\\exp{[-{(y_*-\\phi(\\bold{x}))^2\\over{2\\sigma^2}}]}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” $p(\\varepsilon)$ì„ $p(y_*|\\bold{x}; \\theta)$ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ($\\theta = (\\bold{w}, \\phi, \\sigma)$)\n\nìš°ë¦¬ëŠ” ì´ë¥¼ ì´ìš©í•´ì„œ Likelihoodë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L} &= \\log{p(\\bold{y}_*|\\bold{X}; \\theta)} = \\sum_{i=1}^{N}{\\log{p(y_{*(i)}|\\bold{x}_{(i)}; \\theta)}} \\\\\n&= N\\log{{1\\over{\\sqrt{2\\pi}\\sigma}}} + \\sum_{i=1}^{N}{-{(y_*-\\phi(\\bold{x}))^2\\over{2\\sigma^2}}} \\\\\n&= N\\log{{1\\over{\\sqrt{2\\pi}\\sigma}}} - {1\\over{\\sigma^2}}\\red{{1\\over{2}}\\sum_{i=1}^{N}{(y_*-\\phi(\\bold{x}))^2}}\n\\end{align*}\n$$\n\nìš°ë¦¬ê°€ ë³€ê²½í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ëŠ” $\\phi(\\bold{x})$ ë°–ì— ì—†ë‹¤. ë”°ë¼ì„œ, ë¹¨ê°„ìƒ‰ì„ ì œì™¸í•œ ë¶€ë¶„ì€ Likelihoodì˜ ìµœëŒ“ê°’ì„ êµ¬í•  ë•Œ, ê³ ë ¤í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” ìƒìˆ˜ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” Likelihoodì˜ ìµœëŒ“ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ ë¹¨ê°„ìƒ‰ í‘œì‹œëœ ë¶€ë¶„ì„ ìµœì†Œí™”í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , ì´ëŠ” ìš°ë¦¬ê°€ ì•ì—ì„œ ì‚´í´ë´¤ë˜, Least Squared Errorì™€ ê°™ë‹¤.\n\n<mark>ì¦‰, $\\bold{w}_{LS}=\\bold{w}_{MLE}$ ë¼ëŠ” ê²ƒì´ë‹¤.</mark>\n\n## MAP of Linear Regression\n\nì´ë²ˆì—ëŠ” Linear Regressionì—ì„œ $\\bold{w}$ë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” ê³¼ì •ì—ì„œ MAPë¥¼ í™œìš©í•˜ëŠ” ê³¼ì •ì„ ì•Œì•„ë³¼ ê²ƒì´ë‹¤.\n\n> **overfitting**\n\nìš°ë¦¬ê°€ MLEë¥¼ í†µí•´ì„œ Linear Regressionì„ ì°¾ëŠ” ê²ƒì´ ì¶©ë¶„í•˜ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” ì–´ì©” ìˆ˜ ì—†ì´ **overfitting**ì´ë¼ëŠ” ë¬¸ì œì— ì§ë©´í•˜ê²Œ ëœë‹¤.\n\n![over-fitting-example](/images/over-fitting-example.jpg)\n\n**overfitting**ì´ë€ ë°ì´í„°ë¥¼ í†µí•´ì„œ êµ¬í•  ìˆ˜ ìˆëŠ” ë¶„í¬ê°€ í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì—ëŸ¬ê°€ ê±°ì˜ ì—†ëŠ” í˜•íƒœë¡œ ì˜ˆì¸¡í•˜ì§€ë§Œ, ê·¸ ì™¸ì— ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì—ëŸ¬ê°€ í¬ê²Œ ë°œìƒí•˜ëŠ” ê²½ìš°ë¥¼ ì˜ë¯¸í•œë‹¤. ìœ„ì˜ ì˜ˆì‹œì—ì„œ ì²˜ëŸ¼ ë°ì´í„°ê°€ ì „ì²´ Sample spaceë³´ë‹¤ í„±ì—†ì´ ì ì€ ê²½ìš°ì— ë°œìƒí•˜ê¸° ì‰½ë‹¤.\n\nì´ëŸ¬í•œ ë¬¸ì œëŠ” ì‚¬ì‹¤ basis functionì„ ì˜ ì„ íƒí•˜ë©´ í•´ê²°í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ì–´ë–»ê²Œ ë§¤ë²ˆ ì ì ˆí•œ basis functionì„ ì°¾ê¸° ìœ„í•´ì„œ iterationì„ ë°˜ë³µí•˜ëŠ” ê²ƒì´ ì˜¬ë°”ë¥¼ê¹Œ? ê·¸ë¦¬ê³  ì´ëŠ” ì‹¤ì œ ì í•©í•œ ê°’ì„ ì°¾ê¸° ìœ„í•œ ìˆ˜í•™ì  ì‹ë„ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n> **Regularization**\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” **regularization**ì„ ìˆ˜í–‰í•œë‹¤. ìœ„ì˜ overfittingëœ ê·¸ë˜í”„ë¥¼ ë³´ë©´ í•˜ë‚˜ì˜ insight(ë²ˆëœ©ì´ëŠ” idea?)ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ë°”ë¡œ, ê¸‰ê²©í•œ ê¸°ìš¸ê¸°ì˜ ë³€í™”ëŠ” overfittingê³¼ ìœ ì‚¬í•œ ì˜ë¯¸ë¡œ ë³¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì¦‰, ê·¸ë˜í”„ì˜ í˜•íƒœê°€ smooth í•´ì•¼í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ errorì— ëŒ€í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì¬ì •ì˜í•´ì„œ smoothing(regularization)ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\varepsilon = {1\\over2}||\\bold{y}_* - \\Phi\\bold{w}||^2 + {\\lambda\\over{2}}||\\bold{w}||^2\n$$\n\n$\\bold{w}$ì˜ L2 normì„ errorì— ì¶”ê°€í•˜ì—¬ $\\bold{w}$ì˜ í¬ê¸°ê°€ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. (ë¬¼ë¡  L1 normì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ì´ ë˜í•œ, í›„ì— ë‹¤ë£° ê²ƒì´ë‹ˆ ì—¬ê¸°ì„œëŠ” ë„˜ì–´ê°€ê² ë‹¤. ì¶”ê°€ë¡œ ì´ë ‡ê²Œ L2 normì„ ì´ìš©í•˜ë©´ **Ridge Regression**, L1 normì„ ì´ìš©í•˜ë©´ **Lasso Regression**ì´ë¼ê³  í•œë‹¤.)\n\nì ì´ì œ ìœ„ì˜ ì‹ì„ ë¯¸ë¶„í•´ì„œ ìµœì†Œê°’ì´ ë˜ê²Œ í•˜ëŠ” $\\bold{w}$ë¥¼ ì°¾ì•„ë³´ì. ê³¼ì •ì€ ì—°ì‚°ì´ ê·¸ë ‡ê²Œ ì–´ë µì§€ ì•Šìœ¼ë¯€ë¡œ ë„˜ì–´ê°€ê³  ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\n$$\n\\bold{w}_{ridge} = (\\lambda I + \\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y}_*\n$$\n\n---\n\nê·¸ëŸ¼ ì´ ì—­ì‹œ MAPë¥¼ í†µí•´ì„œ í•´ì„í•´ë³´ë„ë¡ í•˜ì.\n\nìœ„ì—ì„œ ì‚´í´ë³¸ ë°”ì™€ ê°™ì´ ìš°ë¦¬ëŠ” wê°’ì´ ì‘ì„ í™•ë¥ ì´ ë†’ì„ ìˆ˜ë¡ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§ˆ ê²ƒì´ë¼ëŠ” Priorë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\nì¦‰,$p(\\bold{w})$ê°€ zero-mean gaussian(í‘œì¤€ì •ê·œë¶„í¬)í˜•íƒœë¥¼ ì´ë£¨ê¸°ë¥¼ ë°”ë„ ê²ƒì´ë‹¤.\n\n$$\np(\\bold{w}) = \\mathcal{N}(\\bold{w}|0, \\Sigma)\n$$\n\nê·¸ë¦¬ê³ , ì´ì „ì— MLEë¥¼ êµ¬í•  ë•Œ, Likelihoodë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í–ˆë‹¤.\n\n$$\n\\begin{align*}\np(\\bold{y}_*|\\bold{X}; \\theta) &= \\mathcal{N}(\\bold{y}_*-\\Phi\\bold{w}, \\sigma I) \\\\\np(\\bold{y}_*|\\Phi, \\bold{w}) &= \\mathcal{N}(\\bold{y}_*-\\Phi\\bold{w}, \\sigma I)\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ë¥¼ ì´ìš©í•´ì„œ posteriorë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤.\n\n$$\np(\\bold{w}|\\bold{y}_*, \\Phi) = {{p(\\bold{y}_*| \\Phi, \\bold{w})p(\\bold{w})}\\over{p(\\bold{y}_*|\\Phi)}}\n$$\n\nì—¬ê¸°ì„œ MAPë¥¼ êµ¬í•  ë•Œì—ëŠ” Lemma ì •ë¦¬(ë‘ ì •ê·œë¶„í¬ì˜ conditional Probabilityë¥¼ êµ¬í•˜ëŠ” ê³µì‹)ë¥¼ ì´ìš©í•˜ë©´ í¸í•˜ë‹¤. ë”°ë¡œ ì—°ì‚°ì€ ìˆ˜í–‰í•˜ì§€ ì•Šì§€ë§Œ ê²°ê³¼ ê°’ì€ ì•„ë˜ì™€ ê°™ë‹¤.\n\n$$\n\\bold{w}_{MAP} = (\\sigma^2\\Sigma^{-1}+\\Phi^{\\top}\\Phi)^{-1}\\Phi^{\\top}\\bold{y}\n$$\n\nì—¬ê¸°ì„œ ë§Œì•½ ìš°ë¦¬ê°€ $\\Sigma = {\\sigma^2\\over{\\lambda}}I$ë¼ê³  ê°€ì •í•˜ë©´, ìœ„ì˜ MAP ì‹ì€ Ridge Regressionê³¼ ë™ì¼í•´ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\nì¦‰, Ridge Regressionì€ MAPì˜ í•œ ì¢…ë¥˜ë¼ê³  ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\bold{w}_{ridge} \\in {(\\bold{w}_{MAP}, \\Sigma)}\n$$\n\n## Gradient Descent\n\nì—¬íƒœê¹Œì§€ ìš°ë¦¬ëŠ” Lossë¥¼ ì •ì˜í•˜ê³ , ì´ Lossê°€ ìµœì†Ÿê°’ì„ ê°–ëŠ” $\\bold{w}$ë¥¼ ì°¾ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ëª¨ë“  Lossê°€ ë¯¸ë¶„ì´ í•­ìƒ ì‰¬ìš´ ê²ƒì€ ì•„ë‹ˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼, Lossì˜ ë¯¸ë¶„ ê°’ì´ 5ì°¨ì› ì´ìƒì˜ ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤ë©´, ìš°ë¦¬ëŠ” ì´ë¥¼ í’€ ìˆ˜ ì—†ì„ ìˆ˜ë„ ìˆë‹¤. 5ì°¨ì› ì´ìƒì˜ polynomialì—ì„œëŠ” ì„ í˜•ëŒ€ìˆ˜ì ì¸ í•´ê²°ë²•(ê·¼ì˜ ë°©ì •ì‹)ì´ ì—†ë‹¤ëŠ” ê²ƒì´ ì¦ëª…ë˜ì–´ìˆë‹¤.(Abel-Ruffini theorem)\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” Lossê°€ 0ì´ ë˜ëŠ” ì§€ì ì„ ì°¾ê¸° ìœ„í•´ì„œ, wì˜ ê°’ì„ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì‹ì„ í™œìš©í•œë‹¤. ì´ë•Œ, ìš°ë¦¬ëŠ” wì˜ ê°’ì´ ê³„ì†í•´ì„œ Lossë¥¼ ê°ì†Œì‹œí‚¤ê¸°ë¥¼ ì›í•œë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” í˜„ì¬ $\\bold{w}$ì—ì„œ Gradientë¥¼ í˜„ì¬ $\\bold{w}$ì— ë¹¼ì¤€ë‹¤. ì´ë¥¼ ìš°ë¦¬ëŠ” **Gradient Descent**ë¼ê³  í•œë‹¤.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\gamma((\\nabla L)(\\bold{w}_{t}))^{\\top}\n$$\n\nì—¬ê¸°ì„œ $\\gamma$ëŠ” step size(learning rate)ë¼ê³  í•˜ë©°, ê¸°ìš¸ê¸°ê°’ì„ ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.\n\n---\n\nì´ì œë¶€í„°ëŠ” Gradient Descentë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ì§„í–‰í•˜ê¸° ìœ„í•œ 3ê°€ì§€ì˜ ê¸°ìˆ ë“¤ì„ ì¶”ê°€ì ìœ¼ë¡œ ì œì‹œí•œë‹¤.\n\n> **1. optimize stepsize**\n\nstepsize($\\gamma$)ê°€ íŠ¹ì • ìƒìˆ˜ë¡œ ì œì‹œëœ ê²Œ ì•„ë‹ˆë¼ ë³€ìˆ˜ë¡œ í‘œí˜„ëœ ì´ìœ ëŠ” linear regressionë§ˆë‹¤ ì ì ˆí•œ $\\gamma$ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ë‚˜ì˜ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ì.\n\n![loss-divergence](/images/loss-divergence.jpg)\n\nìœ„ëŠ” Loss functionì´ convexí•  ë•Œ, ìµœì†Ÿê°’ì„ ì°¾ì•„ë‚˜ê°€ëŠ” ê³¼ì •ì´ë‹¤. ë§Œì•½, $\\gamma$ê°€ í¬ë‹¤ë©´, Lossê°€ íŠ¹ì •ê°’ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë°œì‚°í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë¥¼ ë§‰ê¸° ìœ„í•´ $\\gamma$ë¥¼ êµ‰ì¥íˆ ì‘ì€ ìˆ˜ë¡œ í•˜ëŠ” ê²½ìš°ì—ëŠ” Lossì˜ ìµœì†Ÿê°’ì„ ì°¾ê¸°ë„ ì „ì— íŠ¹ì • ì§€ì ì—ì„œ ë©ˆì¶°ë²„ë¦´ ìˆ˜ë„ ìˆë‹¤. ë˜í•œ, Lossì˜ graphí˜•íƒœëŠ” dataë§ˆë‹¤ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ì ˆëŒ€ì ì¸ $\\gamma$ì—­ì‹œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë§¤ updateë§ˆë‹¤ ì ì ˆí•œ $\\gamma$ë¥¼ ì°¾ì„ë ¤ê³  ë…¸ë ¥í•œë‹¤. ì—¬ê¸°ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•Šì§€ë§Œ í›„ì— ë” ë‹¤ë£° ê¸°íšŒê°€ ìˆì„ ê²ƒì´ë‹¤. ê°„ë‹¨íˆ í”„ë¡œê·¸ë˜ë°ì ìœ¼ë¡œ(systemical) ìƒê°í•˜ë©´, ì—…ë°ì´íŠ¸ ì´í›„ lossê°€ ë§Œì•½ ê·¸ì „ Lossë³´ë‹¤ ì»¤ì§„ë‹¤ë©´, ì´ë¥¼ ì·¨ì†Œí•˜ê³  ë” ì‘ì€ $\\gamma$ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ê³ , ì—…ë°ì´íŠ¸ ëœ í›„ì˜ Lossì™€ ê·¸ì „ Lossê°€ ê°™ë‹¤ë©´, ì§„ì§œ ìˆ˜ë ´í•˜ëŠ”ì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ $\\gamma$ë¥¼ í‚¤ì›Œë³¼ ìˆ˜ë„ ìˆë‹¤.\n\n> **2. momentum**\n\nìš°ë¦¬ê°€ Gradient Descentë¥¼ ì§„í–‰í•˜ë‹¤ë³´ë©´, ë‹¤ìŒê³¼ ê°™ì€ í˜„ìƒì„ ìì£¼ ë§ˆì£¼í•˜ê²Œ ëœë‹¤.\n\n![momentum-example-1](/images/momentum-example-1.jpg)\n\nìš°ë¦¬ê°€ ì°¾ê³ ì í•˜ëŠ” Lossë¥¼ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì—ì„œ ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ê¸°ìš¸ê¸°ê°€ ë°”ë€ŒëŠ” ê²½ìš°ì´ë‹¤.(ì§„ë™í•œë‹¤) ì´ëŠ” ìµœì¢…ìœ¼ë¡œ ì°¾ê³ ì í•˜ëŠ” ê°’ì„ ì°¾ëŠ” ê³¼ì •ì´ ë” ì˜¤ë˜ ê±¸ë¦¬ê²Œ í•œë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì§„ë™ì„ ë§‰ê¸° ìœ„í•´ì„œ Momentumì„ ì‚¬ìš©í•œë‹¤. ì¦‰, ì´ì „ ì°¨ì‹œì—ì„œì˜ gradientë¥¼ ì €ì¥í•´ë‘ê³ , ì´ë¥¼ ë”í•´ì„œ ì§„ë™í•˜ëŠ” ê²ƒì„ ë§‰ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\bold{w}_{i+1} = \\bold{w}_{i} - \\gamma_{i}((\\nabla L)(\\bold{w}_{i}))^{\\top} + \\alpha \\Delta \\bold{w}_i ,( \\alpha \\in [0, 1] )\n$$\n\n$$\n\\Delta \\bold{w}_i = \\bold{w}_{i} - \\bold{w}_{i-1} = \\alpha \\Delta \\bold{w}_{i-1} - \\gamma_{i-1}((\\nabla L)(\\bold{w}_{i-1}))^{\\top}\n$$\n\nì¦‰, ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ë©´, ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n![momentum-example-2](/images/momentum-example-2.jpg)\n\nì´ì „ ë³€í™”ëŸ‰ê³¼ í˜„ì¬ ë³€í™”ëŸ‰ì„ í•©í•˜ì—¬ ì´ë™í•˜ê¸° ë•Œë¬¸ì— ìœ„ì— ìƒˆë¡œ ì¶”ê°€ëœ ê²ƒì²˜ëŸ¼ ì§„ë™í•˜ì§€ ì•Šê³ , ì§„í–‰í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.\n\n> **3. Stochastic Gradient Descent**\n\nìš°ë¦¬ì˜ Gradient Descentì˜ ê°€ì¥ í° ë¬¸ì œëŠ” ë°”ë¡œ Global Minimumì„ ì°¾ì„ ê±°ë¼ëŠ” í™•ì‹ ì„ ì¤„ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ì.\n\n![gradient-descent-example](/images/gradient-descent-example.jpg)\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ì´ˆê¸° w ê°’ì„ ì–´ë–»ê²Œ ì •í•˜ëƒì— ë”°ë¼ì„œ, **local minimum**ì„ ì–»ê²Œ ë˜ê±°ë‚˜ **global minimum**ì„ ì–»ê²Œ ëœë‹¤. ì¦‰, ì´ˆê¸°ê°’ì´ ê²°ê³¼ì— êµ‰ì¥íˆ í° ì˜í–¥ì„ ì¤€ë‹¤ëŠ” ê²ƒì´ë‹¤.\n\nì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë©°, í•™ìŠµ íš¨ìœ¨ë„ ë†’ì¼ ìˆ˜ ìˆëŠ” ê²ƒì´ Stochastic Gradient Descentì´ë‹¤. ì›ë¦¬ëŠ” Lossë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œ ì „ì²´ ë°ì´í„°(ëª¨ì§‘ë‹¨)ë¥¼ ì‚¬ìš©í–ˆì—ˆëŠ”ë° ê·¸ëŸ¬ì§€ë§ê³  ì¼ë¶€ ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ì¶”ì¶œ(sampling)í•´ì„œ(í‘œë³¸ ì§‘ë‹¨) ì´ë“¤ì„ í†µí•´ì„œ Loss functionì„ êµ¬í•˜ê¸°ë¥¼ ë°˜ë³µí•˜ìëŠ” ê²ƒì´ë‹¤.\n\nì´ ë°©ì‹ì„ í†µí•´ì„œ êµ¬í•œ Gradientì˜ í‰ê· ì´ ê²°êµ­ì€ ì „ì²´ batchì˜ í‰ê· ê³¼ ê°™ë‹¤ëŠ” ê²ƒì€ Central Limit Theorem(ì¤‘ì‹¬ ê·¹í•œ ì •ë¦¬)ì— ì˜í•´ ì¦ëª…ì´ ëœë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ë¥¼ í†µí•œ gradient descentë„ íŠ¹ì • minimumì„ í–¥í•´ ë‚˜ì•„ê°€ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\nê·¸ë ‡ì§€ë§Œ, í‘œë³¸ ì§‘ë‹¨ì„ ì´ìš©í•œ í‰ê· ì„ êµ¬í–ˆì„ ë•Œì— ìš°ë¦¬ëŠ” noiseì— ì˜í•´ì„œ local minimumìœ¼ë¡œë§Œ ìˆ˜ë ´í•˜ëŠ” í˜„ìƒì„ ë§‰ì„ ìˆ˜ ìˆë‹¤. ì¦‰, gradient descentë¥¼ ë°˜ë³µí•˜ë‹¤ë³´ë©´, ë‹¤ë¥¸ local minimumìœ¼ë¡œ íŠ€ì–´ë‚˜ê°€ê¸°ë„ í•˜ë©° global minimumì„ ë°œê²¬í•  í™•ë¥ ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n- [Probabilistic interpretation of linear regression clearly explained](https://towardsdatascience.com/probabilistic-interpretation-of-linear-regression-clearly-explained-d3b9ba26823b), Lily Chen\n","slug":"ml-linear-regression","date":"2022-10-17 09:46","title":"[ML] 2. Linear Regression","category":"AI","tags":["ML","LinearRegression","BasisFunction","Regularization","GradientDescent","Momentum","StochasticGradientDescent"],"desc":"Regression(íšŒê·€)ì´ë¼ëŠ” ë‹¨ì–´ëŠ” \"ì›ë˜ì˜ ìƒíƒœë¡œ ëŒì•„ê°„ë‹¤\"ë¡œ ëŒì•„ê°„ë‹¤ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤. ê²°êµ­ ì–´ë–¤ ì¼ë ¨ì˜ Eventë¡œ ì¸í•´ì„œ ë°ì´í„°ì— Noiseê°€ ë°œìƒí•  ìˆ˜ ìˆì–´ë„ ê²°êµ­ì€ í•˜ë‚˜ì˜ \"ë³´í¸\"ìœ¼ë¡œ ì‹œê°„ì´ ì§€ë‚˜ë©´ ìˆ˜ë ´(íšŒê·€)í•  ê²ƒì´ë¼ëŠ” ìƒê°ì— ê¸°ë°˜í•˜ëŠ” ê²ƒì´ë‹¤.  ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ \"ë³´í¸\"ì„ ì°¾ê¸° ìœ„í•´ì„œ ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” ë…ë¦½ ë°ì´í„° Xë¥¼ í†µí•´ì„œ ì•Œê³ ì í•˜ëŠ” ê°’ Yë¥¼ ë³´í¸ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤. ì´ ê³¼ì •ì„ ìš°ë¦¬ëŠ” Regressionì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë˜í•œ, Xì— ì˜í•´ ë…ë¦½ì ì´ì§€ ì•Šê³  ì¢…ì†ì ì¸ Yì˜ ê´€ê³„ê°€ Linearí•˜ê²Œ í‘œí˜„ë  ë•Œ ì´ë¥¼ ìš°ë¦¬ëŠ” Linear Regressionì´ë¼ê³  í•œë‹¤.  ë”°ë¼ì„œ, í•´ë‹¹ Postingì—ì„œëŠ” Linear Regressionì„ ë°”íƒ•ìœ¼ë¡œ Machine Learningì´ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nMachine Learningì€ íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„°ë¡œ ë¶€í„° pattern ë˜ëŠ” ê°€ì • ë“±ì„ ìœ ë„í•´ë‚´ëŠ” ë°©ë²•ì´ë‹¤.\nì´ë¥¼ ìœ„í•œ ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì€ ì—¬ëŸ¬ ê°œì˜ í™•ë¥ ë¶„í¬ì™€ ì´ê²ƒì˜ parameterì˜ ì¡°í•©(probabilistic model)ë“¤ ì¤‘ì—ì„œ ì¸¡ì •ëœ ë°ì´í„°ë“¤ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” í•˜ë‚˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤.\nê·¸ ì¤‘ì—ì„œ, í™•ë¥  ë¶„í¬ë¥¼ ê²°ì •í•œ ìƒíƒœì—ì„œ parameterë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” í˜•íƒœì˜ ì ‘ê·¼ë²•ì„ ìš°ë¦¬ëŠ” Parametric Estimationì´ë¼ê³  í•œë‹¤. ê·¸ ì™¸ì—ë„ Nonparametric, Semi-parametric ë°©ì‹ë„ ì¡´ì¬í•˜ì§€ë§Œ ì´ëŠ” ì—¬ê¸°ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤.\n\n## Small Example\n\nê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ í†µí•´ì„œ Parametric Estimationì˜ íë¦„ì„ ìµí˜€ë³´ì.\n\ní•œ í•™ê¸‰ì—ì„œ í•™ìƒë“¤ì˜ í˜•ì œìë§¤ ìˆ˜ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•˜ê³  ì‹¶ë‹¤ê³  í•˜ì.  \nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ë¨¼ì € ì¡°ì‚¬(ê´€ì¸¡)ë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. ì´ë¥¼ í†µí•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ë¥¼ ì–»ê²Œ ë˜ì—ˆë‹¤ê³  í•˜ì.\n\n| x        | 1    | 2    | 3    | 4    | 5    | 6    | x$\\geq$7 |\n| :------- | :--- | :--- | :--- | :--- | :--- | :--- | :------- |\n| $p(X=x)$ | 17   | 59   | 15   | 6    | 2    | 0    | 1        |\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ì—¬ëŸ¬ ì‚¬ì „ ì§€ì‹ì„ í™œìš©í•˜ì—¬ í•´ë‹¹ ë°ì´í„°ë¥¼ ë³´ì•˜ì„ ë•Œ, í•´ë‹¹ ë¶„í¬ê°€ Poisson ë¶„í¬ì˜ í˜•íƒœë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.  \në”°ë¼ì„œ, ìš°ë¦¬ëŠ” í•´ë‹¹ ë¶„í¬ë¥¼ Poissonì´ë¼ê³  ê°€ì •í•œ ë‹¤ìŒì—ëŠ” ë‹¨ìˆœíˆ í•´ë‹¹ ë¶„í¬ì— ëŒ€ì…í•˜ë©°, ê°€ì¥ ì ì ˆí•œ parameterë§Œ ì°¾ìœ¼ë©´ ëœë‹¤.  \n\nì´ ê³¼ì •ê³¼ ë‹¨ìˆœíˆ ê° xì—ì„œì˜ í™•ë¥ ê°’ì„ êµ¬í•˜ëŠ” ë°©ì‹ì´ë‘ ë¬´ì—‡ì´ ë‹¤ë¥¸ì§€ë¥¼ ì•Œì•„ì•¼ì§€ í•´ë‹¹ ê³¼ì •ì˜ ì˜ì˜ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.\në¨¼ì €, ìš°ë¦¬ê°€ í•˜ê³ ì í•˜ëŠ” ì¼ì´ í˜•ì œìë§¤ì˜ í‰ê·  ìˆ˜ë¥¼ êµ¬í•œë‹¤ê³  í•˜ì. ì´ë•Œì˜ í‰ê·  ê°’ê³¼ Poisson ë¶„í¬ì—ì„œì˜ í™•ë¥ ê°’ì€ ë‹¤ë¥¼ ìˆ˜ ë°–ì— ì—†ë‹¤.\n\nì´ë ‡ê²Œ í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•˜ëŠ” ê²ƒì˜ ì˜ë¯¸ëŠ” ì´ê²ƒë§ê³ ë„ ë³´ì§€ ì•Šì€ ë°ì´í„°(unseen data)ë¥¼ ì²˜ë¦¬í•¨ì— ìˆë‹¤. ìš°ë¦¬ê°€ ë§Œì•½ ëª¨ë“  ê°€ëŠ¥í•œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ëª¨ë‘ ì•Œê³  ìˆê³ , ì´ë¥¼ ì €ì¥í•  ê³µê°„ì´ ì¶©ë¶„í•˜ë‹¤ë©´,\nì´ëŸ¬í•œ í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•  í•„ìš”ê°€ ì—†ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì¶”ì¸¡ì€ unseen dataì— ëŒ€í•´ì„œë„ ê·¸ëŸ´ì‚¬í•´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ê²°êµ­ í™•ë¥  ë¶„í¬ê°€ í•„ìš”í•˜ë‹¤.\n\nìœ„ì˜ ì˜ˆì‹œì—ì„œ ë§Œì•½, í˜•ì œìë§¤ê°€ 3ëª…ì¸ ê²½ìš°ì˜ ë°ì´í„°ê°€ ì—†ë‹¤ê³  í•˜ì. ì´ ê²½ìš°ì—ë„ í™•ë¥ ë¶„í¬ë¥¼ í†µí•œ ì¶”ì¸¡ì„ í•œë‹¤ë©´, ìš°ë¦¬ëŠ” ìœ ì˜ë¯¸í•œ ê°’ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n## Parametric Estimation\n\n> **ì •ì˜**\n\nsample space $\\Omega$ì—ì„œ í†µê³„ ì‹¤í—˜ì˜ ê´€ì¸¡ ê²°ê³¼ë¥¼ í†µí•´ì„œ ì–»ì€ sample $X_1$, $X_2$, ... , $X_n$ì´ ìˆë‹¤ê³  í•˜ì. ê° sampleì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ìš°ë¦¬ëŠ” $p_\\theta$ë¼ê³  í•œë‹¤.\nì—¬ê¸°ì„œ $\\theta$ëŠ” íŠ¹ì • í™•ë¥  ë¶„í¬ì—ì„œì˜ parameterë¥¼ ì˜ë¯¸í•œë‹¤. ë§Œì•½, bernoulli ë¼ë©´, ë‹¨ì¼ ì‹œí–‰ì— ëŒ€í•œ í™•ë¥ ì´ ë  ê²ƒì´ê³ , binomialì´ë¼ë©´, ë‹¨ì¼ ì‹œí–‰ì˜ í™•ë¥ ê³¼ íšŸìˆ˜ê°€ í•´ë‹¹ ê°’ì´ ë  ê²ƒì´ë‹¤.\n\n> **ì„±ëŠ¥ í‰ê°€**\n\nì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì°¾ê¸°ë¥¼ ì›í•˜ëŠ” ê²ƒì€ ì „ì²´ sample space $\\Omega$ë¥¼ ëª¨ë‘ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ” $\\theta_{*}$(ì‹¤ì œ true $\\theta$)ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.(ì´ë¯¸ í™•ë¥  ë¶„í¬ì˜ í˜•íƒœ(í•¨ìˆ˜, ex. Bernoulli, Binomial)ëŠ” ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤.)  \nê·¸ë ‡ë‹¤ë©´, ì‹¤ì œ $\\theta_*$ì™€ ì¶”ì¸¡ì„ í†µí•´ ë§Œë“  $\\hat{\\theta}$ ì‚¬ì´ì˜ ë¹„êµë¥¼ ìœ„í•œ ì§€í‘œë„ í•„ìš”í•  ê²ƒì´ë‹¤. ì¦‰, ìš°ë¦¬ê°€ ë§Œë“  í™•ë¥  ë¶„í¬ì˜ ì˜ˆì¸¡ ì„±ëŠ¥í‰ê°€ê°€ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” **Risk**ë¼ëŠ” ê²ƒì„ ì‚¬ìš©í•œë‹¤.  \nê°„ë‹¨í•˜ê²Œë„ ì‹¤ì œ $\\theta_*$ì™€ $\\hat{\\theta}$ì˜ Mean Square Errorë¥¼ ê³„ì‚°í•œë‹¤.\n\n$$\n\\begin{align*}\nRisk &= E[(\\hat{\\theta} - \\theta_*)^2] = E[\\hat{\\theta}^2 - 2\\hat{\\theta}\\theta_* + \\theta_*^2] \\\\\n&= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 \\\\\n&= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 + (E^2[\\hat{\\theta}] - E^2[\\hat{\\theta}]) \\\\\n&= (E[\\hat{\\theta}] - \\theta_*)^2 + E[\\hat{\\theta}^2] - E^2[\\hat{\\theta}] \\\\\n&= {Bias}^2 + Var[\\hat{\\theta}]\n\\end{align*}\n$$\n\ní•´ë‹¹ ì‹ì„ ë¶„ì„í•´ë³´ë©´, ì´ì™€ ê°™ì€ ì˜ë¯¸ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. ìš°ë¦¬ê°€ íŠ¹ì • í™•ë¥  ë¶„í¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë‹¨ í•˜ë‚˜ë¡œ ë‹¨ì •í•˜ê³  Riskë¥¼ ê³„ì‚°í•˜ëŠ” ê²½ìš°ëŠ” Variance ê°’ì€ 0ì´ë‹¤. ì¦‰, í•´ë‹¹ í™•ë¥  ë¶„í¬ê°€ ê°€ì§€ëŠ” RiskëŠ” ë‹¨ìˆœíˆ í•´ë‹¹ parameterì™€ ì‹¤ì œ parameterê°€ ì–¼ë§ˆë‚˜ ì°¾ì´ê°€ ë‚˜ëŠ”ê°€ë¥¼ ì˜ë¯¸í•œë‹¤.\n\ní•˜ì§€ë§Œ, parameterë¥¼ íŠ¹ì •í•˜ì§€ ì•Šê³ , ë²”ìœ„ë¡œ ì§€ì •í•œë‹¤ë©´, (ì˜ˆë¥¼ ë“¤ì–´, ì£¼ì‚¬ìœ„ë¥¼ ë˜ì ¸ 3ì´ ë‚˜ì˜¬ í™•ë¥ ì€ 1/6 ~ 1/3ì´ë‹¤.) í•´ë‹¹ í™•ë¥ ì˜ í‰ê· ê³¼ Varianceê°€ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë‹¤.  \në‹¤ì†Œ ì²˜ìŒì—ëŠ” í—·ê°ˆë¦´ ìˆ˜ ìˆì§€ë§Œ, í•´ë‹¹ ì‹ì—ì„œ í‰ê· ì´ ì˜ë¯¸ëŠ” ì˜ í™•ì¸í•˜ì. íŠ¹ì • í™•ë¥  ë¶„í¬ë¥¼ ê°€ì§€ë„ë¡ í•˜ëŠ” $\\theta$ê°€ $\\theta_*$ ì— ì–¼ë§ˆë‚˜ ê·¼ì ‘í•œì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì‹ì´ë¼ëŠ” ê²ƒì„ ë‹¤ì‹œ í•œ ë²ˆ ê¸°ì–µí•˜ì.\n\n> **Estimation**\n\nì´ì œë¶€í„°ëŠ” ì•ì—ì„œ ì‚´í´ë³´ì•˜ë˜, parameteric estimationì—ì„œ ì–´ë–»ê²Œ $\\hat{\\theta}$ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë‹¤ë£° ê²ƒì´ë‹¤. í™•ë¥ /í†µê³„ ì´ë¡ ì—ì„œëŠ” í¬ê²Œ 3ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê° ê°ì„ ì‚´í´ë³´ë„ë¡ í•˜ì.\n\n<mark>**1. MLE**</mark>\n\nMaximum Likelihood Estimationì˜ ì•½ìì´ë‹¤. ì—¬ê¸°ì„œ, LikelihoodëŠ” ê°€ëŠ¥ì„±ì´ë¼ëŠ” ëœ»ì„ ê°€ì§€ë©°, í™•ë¥ /í†µê³„ ì´ë¡ ì—ì„œ ì´ëŠ” í™•ë¥ ì„ í•´ë‹¹ ì‚¬ê±´ì´ ë°œìƒí•  ê°€ëŠ¥ì„±ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ì´ìš©í•´ì„œ ìš°ë¦¬ê°€ í’€ê³ ì í•˜ëŠ” ë¬¸ì œ, ìš°ë¦¬ê°€ ì¶”ì¸¡í•œ $\\theta$ê°€ ìš°ë¦¬ê°€ ê°€ì§„ Datasetë¥¼ ë§Œì¡±ì‹œí‚¬ ê°€ëŠ¥ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤. ì•„ë˜ ìˆ˜ì‹ì„ ë³´ì.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta;\\mathcal{D}) &= p(\\mathcal{D}|\\theta) = p(x_1, x_2, ..., x_n|\\theta) \\\\\n&= \\prod_{i=1}^{n}{p(x_i|\\theta)}\n\\end{align*}\n$$\n\n(ìœ„ ì‹ì„ ì´í•´í•˜ë ¤ë©´, ë¨¼ì € Datasetì˜ ê° dataë“¤ì€ ì„œë¡œ independentí•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¸°ì–µí•˜ì.)  \nê²°êµ­ $\\theta$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Datasetì¼ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ë‹¤ì‹œ ìƒê°í•˜ë©´, $\\theta$ê°€ ì–¼ë§ˆë‚˜ ë°ì´í„°ì…‹ì˜ í™•ë¥ ì„ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ê°€ì™€ ê°™ë‹¤.\n\nì´ê²ƒì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ë ¤ë©´ í•˜ë‚˜ì˜ ì˜ˆì‹œë¥¼ ë³´ë©´ ì¢‹ë‹¤.\n\n![MLE example](/images/MLE-example.png)\n\nì²« ë²ˆì§¸ ê·¸ë˜í”„ëŠ” ê°™ì€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ í•¨ìˆ˜ë¥¼ ì“°ë©´ì„œ, parameterë§Œ ë‹¤ë¥´ê²Œ í•œ ê²½ìš°ì´ê³ , ì•„ë˜ëŠ” ì‹¤ì œ ë°ì´í„°ì˜ ë¶„í¬ë¼ê³  í•˜ì.(ë¹¨ê°„ìƒ‰ ì„  í•˜ë‚˜ í•˜ë‚˜ê°€ ë°ì´í„°ë¥¼ ì˜ë¯¸)  \nì´ë•Œ, Likelihoodë¥¼ ê° ê° êµ¬í•˜ë©´ ê° xì—ì„œì˜ í™•ë¥ ë¶„í¬ì˜ í™•ë¥ ê°’ì„ ëª¨ë‘ ê³±í•˜ë©´ ëœë‹¤. ê·¸ ê²½ìš° ì–´ë–¤ ê²ƒì´ ì œì¼ í´ì§€ëŠ” ë¶„ëª…í•˜ë‹¤. ë°”ë¡œ íŒŒë€ìƒ‰ ë¶„í¬ì¼ ê²ƒì´ë‹¤.  \n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€? ë°”ë¡œ ê°€ì¥ ë†’ì€ ê°€ëŠ¥ì„±ì„ ê°€ì§€ê²Œ í•˜ëŠ” $\\theta$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ì‹ìœ¼ë¡œ í‘œì‹œí•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\n$$\n\\hat{\\theta}_{MLE} = \\argmax_{\\theta}\\mathcal{L}(\\theta;\\mathcal{D})\n$$\n\nì—¬ê¸°ì„œ í•˜ë‚˜ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆë‹¤. ë°”ë¡œ, ì»´í“¨í„°ë¡œ ì—°ì‚°í•˜ê²Œ ë˜ë©´ underflowê°€ ë°œìƒí•˜ëŠ” ê²ƒì´ë‹¤. íŠ¹ì • ì–¸ì–´ê°€ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ì†Œìˆ˜ì  ë²”ìœ„ë¥¼ ë²—ì–´ë‚œë‹¤ë©´, ì œëŒ€ë¡œ ëœ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ì—†ë‹¤. ì´ì™€ ê°™ì€ ë¬¸ì œë¥¼ **vanishing likelihood**ë¼ê³  í•œë‹¤.  \në”°ë¼ì„œ, ìš°ë¦¬ëŠ” logë¥¼ ì·¨í–ˆì„ ë•Œì™€ logë¥¼ ì·¨í•˜ì§€ ì•Šì•˜ì„ ë•Œì˜ ê²½í–¥ì„±ì´ ê°™ìŒì„ ë°”íƒ•ìœ¼ë¡œ likelihoodì— logë¥¼ ì·¨í•œ ê°’ì„ ì´ìš©í•˜ì—¬ MLEë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ì´ ë°©ì‹ì„ maxmum log likelihood estimation ì´ë¼ê³  ë¶€ë¥¸ë‹¤.\n\n$$\n\\mathcal{l}(\\theta;\\mathcal{D}) = \\sum_{i=1}^{n}{\\log{(p(x_i|\\theta))}}\n$$\n\nì´ ë°©ì‹ì„ ì´ìš©í•˜ê²Œ ë˜ë©´, ê³±ì…ˆì´ ëª¨ë‘ ë§ì…ˆìœ¼ë¡œ ë°”ë€Œê¸° ë•Œë¬¸ì— ê³„ì‚°ì—ì„œë„ ìš©ì´í•˜ë‹¤.\n\nì—¬ê¸°ê¹Œì§€ ì‚´í´ë³´ë©´, í•˜ë‚˜ì˜ ì˜ë¬¸ì´ ë“¤ ìˆ˜ë„ ìˆë‹¤. ë°”ë¡œ, $p(\\theta|\\mathcal{D})$ë„ ì¸¡ì • ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ ì•ŠëƒëŠ” ê²ƒì´ë‹¤. ì´ ì—­ì‹œë„ Datasetì´ ì£¼ì–´ì§ˆ ë•Œ, $\\theta$ì¼ í™•ë¥ ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.  \nì–´ì°Œë³´ë©´, ì‚¬ëŒì˜ ìƒê°ìœ¼ë¡œëŠ” ì´ê²Œ ë” ë‹¹ì—°í•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ë„ ìˆë‹¤. ì´ëŠ” ë°”ë¡œ ë‹¤ìŒ MAPì—ì„œ ë‹¤ë£° ê²ƒì´ë‹¤. ìš°ì„  MLEë¥¼ ë¨¼ì €í•œ ì´ìœ ëŠ” ì´ê²ƒì´ ë” êµ¬í•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì„ì„ ê¸°ì–µí•´ë‘ì.\n\n```plaintext\n ğŸ¤” ì¦ëª…\n\n (*í•´ë‹¹ ë‚´ìš©ì€ ì •ë³´ ì´ë¡ ì— ê¸°ë°˜í•œ MLEì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì´í•´ë¥¼ ìœ„í•œ ë‚´ìš©ì…ë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ìì„¸íˆ ì•Œ í•„ìš”ê¹Œì§€ëŠ” ì—†ìŠµë‹ˆë‹¤.)\n\n ë‘ í™•ë¥  ë¶„í¬ ê°„ information Entropyì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” KL divergenceì˜ ìµœì†Ÿê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ ëª©í‘œë¼ê³  ì •ì˜í•  ìˆ˜ ìˆë‹¤.  \n ë”°ë¼ì„œ, ìš°ë¦¬ê°€ ê²°êµ­ ì–»ê³ ì í•˜ëŠ” ê²ƒì€ í™•ë¥  ë¶„í¬ í•¨ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ,  \n nì´ ë¬´í•œëŒ€ë¡œ ê°ˆ ë•Œ, ê²½í—˜ì  í™•ë¥ (empirical probability)ì— ê°€ì¥ ê·¼ì‚¬í•˜ëŠ” parameterë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.  \n ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” KL divergenceì˜ ìµœì†Ÿê°’ì„ êµ¬í•˜ë©´ ëœë‹¤.\n```\n\n$$\n\\begin{align*}\n\\argmin_\\theta KL(\\tilde{p}||p_\\theta) &= \\argmin_\\theta \\int\\tilde{p}(x)\\log{\\tilde{p}(x)\\over{p_\\theta(x)}}dx \\\\\n&=\\argmin_\\theta[-\\int\\tilde{p}(x)\\log{\\tilde{p}(x)dx} - \\int\\tilde{p}(x)\\log{p_\\theta(x)dx}] \\\\\n&= \\argmax_\\theta\\int{\\tilde{p}(x)\\log{p_\\theta(x)}dx} \\\\\n&= \\argmax_\\theta\\sum_{i=1}^{n}{\\log{p_\\theta(x_i)}} \\\\\n&= \\theta_{MLE}\n\\end{align*}\n$$\n\n<mark>**2. MAP**</mark>\n\nMaximum A Posterioriì˜ ì•½ìì´ë‹¤. PosterioriëŠ” ì‚¬í›„ í™•ë¥ ì´ë¼ê³ ë„ ë¶€ë¥´ë©°, datasetì´ ì£¼ì–´ì¡Œì„ ë•Œ, $\\theta$ì¼ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.  \nì´ë¥¼ ë°”ë¡œ êµ¬í•˜ëŠ” ê²ƒì€ ë‹¤ì†Œ ì–´ë µë‹¤. ì™œëƒí•˜ë©´, Datasetì´ ì¡°ê±´ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” í˜•íƒœì´ê¸° ë•Œë¬¸ì´ë‹¤. ($p(\\theta|\\mathcal{D})$)  \në”°ë¼ì„œ, ìš°ë¦¬ëŠ” Bayes' Theoremì— ë”°ë¼ì„œ ì´ì „ì— ë°°ìš´ Likelihoodì™€ parameterì˜ í™•ë¥ , ê·¸ë¦¬ê³  Datasetì˜ í™•ë¥ ì„ í™œìš©íˆì—¬ í’€ì–´ë‚¼ ê²ƒì´ë‹¤.\n\n$$\np(\\theta|\\mathcal{D}) = {p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}}\n$$\n\nì—¬ê¸°ì„œ ì£¼ì˜í•´ì„œ ë³¼ ê²ƒì€ ë°”ë¡œ $p(\\theta|\\mathcal{D})$ì™€ $p(\\theta)$ì˜ ê´€ê³„ì´ë‹¤. datasetì´ ì£¼ì–´ì§ˆ ë•Œì˜ parameterì˜ í™•ë¥ ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ ì›ë˜ parameterì˜ í™•ë¥ ì´ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.  \nì–´ì°Œë³´ë©´ êµ‰ì¥íˆ ëª¨ìˆœë˜ì–´ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ìš°ë¦¬ê°€ ì´ê²ƒì„ ì‚¬ì „ í™•ë¥ (priori)ë¡œ ë³¸ë‹¤ë©´ ë‹¤ë¥´ê²Œ ë³¼ ì—¬ì§€ê°€ ìˆë‹¤.  \nì˜ˆë¥¼ ë“¤ë©´, ìš°ë¦¬ê°€ ìˆ˜ìƒí•œ ì£¼ì‚¬ìœ„ë¡œ í•˜ëŠ” ê²Œì„ì— ì°¸ê°€í•œë‹¤ê³  í•˜ì. ì´ë•Œ, ìš°ë¦¬ëŠ” ìˆ˜ìƒí•œ ì£¼ì‚¬ìœ„ì˜ ì‹¤ì œ í™•ë¥ ì€ ì•Œ ìˆ˜ ì—†ì§€ë§Œ, ì£¼ì‚¬ìœ„ ìì²´ì˜ í™•ë¥ ì€ ëª¨ë‘ 1/6ì´ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤. ë”°ë¼ì„œ, $p(\\theta={1\\over6}) = \\alpha, p(\\theta\\neq{1\\over6}) = \\beta$ ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë§Œì•½ ì •ë§ ìˆ˜ìƒí•´ë³´ì¸ë‹¤ë©´, ìš°ë¦¬ëŠ” $\\alpha$ê°€ ì ì  ì‘ì•„ì§„ë‹¤ëŠ” ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê³ , í•˜ë‚˜ë„ ìˆ˜ìƒí•´ë³´ì´ì§€ ì•ŠëŠ” ì¼ë°˜ ì£¼ì‚¬ìœ„ë¼ë©´, $\\alpha=1, \\beta=0$ìœ¼ë¡œ í•  ìˆ˜ë„ ìˆë‹¤. ì´ ê²½ìš°ì—ëŠ” likelihood ê°’ì— ìƒê´€ì—†ì´ ë‹¤ë¥¸ ëª¨ë“  ê°’ì´ 0ì´ê¸° ë•Œë¬¸ì— ê²°êµ­ì€ $p(\\theta|\\mathcal{D}) = p(\\theta)$ ê°€ ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\nìµœì¢…ì ìœ¼ë¡œ, MAPë„ ê²°êµ­ì€ Datasetì„ ì–¼ë§ˆë‚˜ parameterê°€ ì˜ í‘œí˜„í•˜ëŠ”ê°€ì— ëŒ€í•œ ì§€í‘œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\në”°ë¼ì„œ, ì´ë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” parameterëŠ” $\\theta_*$ì™€ êµ‰ì¥íˆ ê·¼ì ‘í•  ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{MAP} &= \\argmax_{\\theta}p(\\theta|\\mathcal{D}) \\\\\n&= \\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}} \\\\\n&=\\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)} \\\\\n&=\\argmax_\\theta{[\\red{\\log{p(\\mathcal{D}|\\theta)}} + \\blue{\\log{p(\\theta)}}]}\n\\end{align*}\n$$\n\nMLEì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì´ ë˜í•œ ì—°ì‚° ë° **vanishing**ì„ ë§‰ê¸° ìœ„í•´ì„œ logë¥¼ ì·¨í•œë‹¤. ì‚¬ì‹¤ìƒ likelihoodì™€ ì‚¬ì „ í™•ë¥ ì˜ í•©ì„ ìµœëŒ€ë¡œ í•˜ëŠ” $\\theta$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\n<mark>**3. Bayesian Inference**</mark>\n\nì´ì œ ë§ˆì§€ë§‰ ë°©ë²•ìœ¼ë¡œ ì œì‹œë˜ëŠ” Bayesian Inferenceì´ë‹¤. ì´ëŠ” ëŒ€ê²Œ Bayesian Estimationì´ë¼ê³  ë§ì´ ë¶ˆë¦¬ëŠ” ê²ƒ ê°™ë‹¤. ì´ì „ê¹Œì§€ MLE, MAPëŠ” ê²°êµ­ ì£¼ì–´ì§„ ì‹ì„ ìµœëŒ€ë¡œ í•˜ëŠ” í™•ì •ì  $\\theta$ í•˜ë‚˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆë‹¤.\n\nBayesian InferenceëŠ” Datasetì´ ì£¼ì–´ì¡Œì„ ë•Œ, $\\theta$ì˜ í‰ê· ê°’ì„ í™œìš©í•œë‹¤. ë” ìì„¸íˆ ë§í•˜ë©´, Posteriori(ì‚¬í›„ í™•ë¥ )ì˜ í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.  \nì´ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ë©´ ì´í•´í•˜ëŠ”ë° ë„ì›€ì´ ë  ê²ƒì´ë‹¤. í•œ ë²ˆ ì‚´í´ë³´ì.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{BE}&= E[\\theta|\\mathcal{D}] \\\\\n&= {\\int_{0}^{1}{{\\theta}p(\\theta|\\mathcal{D})}d\\theta} \\\\\n&= {\\int_{0}^{1}{\\theta}{{p(\\mathcal{D}|\\theta)p(\\theta)}\\over{p(\\mathcal{D})}}d\\theta} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)p(\\mathcal{D}|\\theta)}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\mathcal{D}|\\theta)p(\\theta)}d\\theta}} \\\\\n&= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\theta)\\prod_{i=1}^{n}p(x_i|\\theta)}d\\theta}} \\\\\n\\end{align*}\n$$\n\nì´ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì€ ì´ì „ê³¼ëŠ” ë‹¤ë¥´ê²Œ ìƒëŒ€ê°’ì´ ì•„ë‹Œ í‰ê· ì„ êµ¬í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— posteriori(ì‚¬í›„ í™•ë¥ ,$p(\\theta|\\mathcal{D})$)ë¥¼ êµ¬í•´ì•¼ í•œë‹¤.\n\ní•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ì¡ê¸°ìˆ ì´ í•˜ë‚˜ ì¡´ì¬í•œë‹¤. ë°”ë¡œ **Conjugate Prior**ì´ë‹¤.\n\në°”ë¡œ ë‘ í™•ë¥  ë¶„í¬ í•¨ìˆ˜(likelihood, prior)ì— ì˜í•œ posteriorì˜ í˜•íƒœê°€ ì •í•´ì§„ ê²½ìš°ê°€ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\n\n| Prior $p(\\theta \\mid \\alpha)$  | Likelihood $p(\\mathcal{D} \\mid \\theta)$                 | Posterior $p(\\theta \\mid \\mathcal{D}, \\alpha)$                                                                                                                                                                                                                   | Expectation of Posterior                                                                                                                                                       |\n| :----------------------------- | :------------------------------------------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Beta ($\\alpha, \\beta$)         | Benoulli ($\\sum _{i=1}^{n}x_{i}$)                       | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +n-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                                            | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + n}}$                                                                                                                   |\n| Beta ($\\alpha, \\beta$)         | Binomial ($\\sum _{i=1}^{n}N_{i}, \\sum _{i=1}^{n}x_{i}$) | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                         | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + \\sum _{i=1}^{n}N_{i}}}$                                                                                                |\n| Gaussian ($\\mu_0, \\sigma_0^2$) | Gaussian ($\\mu, \\sigma^2$)                              | Gaussian (${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu_{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum_{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right),\\left({\\frac {1}{\\sigma_{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}\\right)^{-1}}$) | ${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu_{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum_{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right)}$ |\n\nì´ë¥¼ ì´ìš©í•˜ë©´, ìš°ë¦¬ëŠ” ê°„ë‹¨í•˜ê²Œ Posterioriì˜ í‰ê· ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n  ","slug":"ml-parametric-estimation","date":"2022-10-15 11:25","title":"[ML] 1. Parametric Estimation","category":"AI","tags":["ML","MLE","MAP","Bayesian"],"desc":"Machine Learningì€ íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„°ë¡œ ë¶€í„° pattern ë˜ëŠ” ê°€ì • ë“±ì„ ìœ ë„í•´ë‚´ëŠ” ë°©ë²•ì´ë‹¤.ì´ë¥¼ ìœ„í•œ ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì€ ì—¬ëŸ¬ ê°œì˜ í™•ë¥ ë¶„í¬ì™€ ì´ê²ƒì˜ parameterì˜ ì¡°í•©(probabilistic model)ë“¤ ì¤‘ì—ì„œ ì¸¡ì •ëœ ë°ì´í„°ë“¤ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” í•˜ë‚˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤.ê·¸ ì¤‘ì—ì„œ, í™•ë¥  ë¶„í¬ë¥¼ ê²°ì •í•œ ìƒíƒœì—ì„œ parameterë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” í˜•íƒœì˜ ì ‘ê·¼ë²•ì„ ìš°ë¦¬ëŠ” Parametric Estimationì´ë¼ê³  í•œë‹¤. ê·¸ ì™¸ì—ë„ Nonparametric, Semi-parametric ë°©ì‹ë„ ì¡´ì¬í•˜ì§€ë§Œ ì´ëŠ” ì—¬ê¸°ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"},{"content":"\n## Intro\n\nMachine Learningì€ dataë“¤ë¡œ ë¶€í„° íŠ¹ì • patternì„ ë‚˜íƒ€ë‚´ëŠ” functionì„ ë§Œë“œëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì¦‰, patternì€ dataì— ëŒ€í•œ ê°„ë‹¨í•œ ìš”ì•½ë³¸ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.\ní™•ë¥ /í†µê³„ ì´ë¡  ë° ì„ í˜•ëŒ€ìˆ˜, ë¯¸ì ë¶„, ì •ë³´ ì´ë¡  ê´€ë ¨ ê¸°ë³¸ ë‚´ìš©ì„ í•´ë‹¹ í¬ìŠ¤íŒ…ì— ì •ë¦¬í•œë‹¤. ì—¬ê¸°ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ëŒ€ê²Œ ë§ì´ ì¶”ìƒì ì¸ ë‚´ìš©ì´ë©°, í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ ë‚´ìš©ì´ë‹¤. ë§Œì•½, ì¶”ê°€ì ì¸ ì„¤ëª…ì´ í•„ìš”í•˜ë‹¤ë©´ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë” ê²€ìƒ‰ì„ í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì´ë‹¤.\n\n## Probability/Statisics\n\ní™•ë¥ ê³¼ í†µê³„ëŠ” ëŒ€ê²Œ ê±°ì˜ ë™ì˜ì–´ì²˜ëŸ¼ ì‚¬ìš©ë˜ì§€ë§Œ, StatisticsëŠ” ëŒ€ê²Œ ê³¼ê±°ë¥¼ ì˜ë¯¸í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´ ProbabilityëŠ” ë¯¸ë˜ë¥¼ ì˜ë¯¸í•˜ëŠ” ìš©ë„ë¡œ ë§ì´ ì‚¬ìš©ë˜ì–´ì§„ë‹¤.\n\n### Probability Space\n\ní™•ë¥  ê³µê°„ì„ ì •ì˜í•˜ëŠ” ê²ƒì€ í™•ë¥ ì„ ì´í•´í•˜ëŠ” í† ëŒ€ê°€ ëœë‹¤. í™•ë¥ ì„ ì ìš©í•˜ê¸° ìœ„í•œ ê³µê°„ì„ ë¨¼ì € ì‚´í´ë³´ì.\n\n- Sample Space($\\Omega$)  \n  ê°€ëŠ¥í•œ ëª¨ë“  ê²°ê³¼ê°’ì˜ ì§‘í•©ì´ë‹¤.  \n  ex. ë™ì „ì„ ë‘ ë²ˆì„ ë˜ì ¸ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  ê²°ê³¼ê°’ì€ $\\Omega = $ $\\{ hh, ht, th, tt \\}$\n- Event($E$)  \n  Sample Spaceì˜ Subsetì´ë‹¤. Sample Spaceì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” eventë¼ëŠ” ì˜ë¯¸ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.  \n  ex. ë™ì „ì„ ë‘ ë²ˆì„ ë˜ì ¸ì„œ ëª¨ë‘ ê°™ì€ ë©´ì´ ë‚˜ì˜¤ëŠ” EventëŠ” $E = $ $\\{ hh, tt \\}$\n- Field($\\mathcal{F}$)  \n  Sample Spaceì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ëª¨ë“  Eventë“¤ì˜ ì§‘í•©ì´ë‹¤.  \n  ex ë™ì „ì„ ë‘ ë²ˆ ë˜ì ¸ì„œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ê°’ì˜ FieldëŠ” $\\mathcal{F} = $ $\\{$ $\\emptyset$, $\\{hh\\}$, $\\{ht\\}$, $\\{th\\}$, $\\{tt\\}$, $\\{hh, ht\\}$, $\\{hh, th\\}$, $\\{hh, tt\\}$, $\\{ht, th\\}$, $\\{ht, tt\\}$, $\\{th, tt\\}$, $\\{hh, ht, th\\}$, $\\{hh, ht, tt\\}$, $\\{hh, th, tt\\}$, $\\{ht, th, tt\\}$, $\\{hh, ht, th, tt\\}$ $\\}$\n- $\\sigma$-field  \n  ìì‹  ë‚´ë¶€ì˜ ì›ì†Œë¥¼ í¬í•¨í•˜ëŠ” í•©ì§‘í•©ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì…€ ìˆ˜ ìˆëŠ” fieldë¥¼ sigma fieldë¼ê³  í•œë‹¤.  \n  ì´ $\\sigma$-fieldëŠ” ì¼ë°˜ì ì¸ í™•ë¥ ê³¼ íŠ¹ì • domainì—ì„œì˜ í™•ë¥ ì„ ì •ì˜í•˜ëŠ”ë° í•„ìš”í•˜ë‹¤.  \n  ìš°ë¦¬ê°€ sample space($\\Omega$)ì™€ $\\sigma$-field $\\mathcal{F} \\subset 2^{\\Omega}$ê°€ ì£¼ì–´ì§ˆ ë•Œ, í™•ë¥ Pê°€ ë‹¤ìŒê³¼ ê°™ì´ mappingí•œë‹¤ê³  í•˜ì. $P: \\mathcal{F} \\mapsto [0, 1]$ ì´ë•Œ PëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§„ë‹¤.\n  - $A \\in \\mathcal{F}$ì¸ ëª¨ë“  Aì— ëŒ€í•´ì„œ $P(A) \\leq 0$ ì´ë‹¤.  \n    $P(\\emptyset) = 0, P(\\Omega) = 1$\n  - $\\{A_i\\}_{i \\in I}$ì´ê³ , ì„œë¡œ ë‹¤ë¥¸ ëª¨ë“  i, jì— ëŒ€í•´ $ A_{i}\\cup A_{j} = \\emptyset$ì´ë¼ë©´, ì•„ë˜ ì‹ì„ ë§Œì¡±í•œë‹¤.  \n    $$P(\\cup_{i \\in I}A_i) = \\sum_{i \\in I}P(A_i)$$\n\n### Important properties of Probability\n\n- **Joint Probability**  \n  ë‘ Eventì˜ Joint ProbabilityëŠ” ë‘ Eventì˜ í•©ì§‘í•©ì˜ í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤.\n  $P(A, B) = P(A \\cap B)$\n- **Marginal Probability**  \n  ëŒ€ê²Œ ë‘ ê°œ ì´ìƒì˜ Eventê°€ ìˆì„ ë•Œ, ê° ê°ì˜ Eventì˜ í™•ë¥ ì„ íŠ¹ì •í•  ë•Œ ì‚¬ìš©í•œë‹¤.\n  $P(A), P(B)$\n- **Independence**  \n  ë‘ Eventê°€ ë…ë¦½ì´ë¼ëŠ” ì˜ë¯¸ëŠ” ì„œë¡œì˜ Eventê°€ ì„œë¡œ ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. <mark>**ì£¼ì˜í•  ê²ƒì€ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ ë‘ Eventì˜ êµì§‘í•©ì´ ì—†ë‹¤ëŠ” ì˜ë¯¸ê°€ ì•„ë‹ˆë‹¤.**</mark>  \n  ì˜ˆë¥¼ ë“¤ì–´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ìš°ë¦¬ê°€ ìœ„ì—ì„œ ì˜ˆì‹œë¡œ ì‚¬ìš©í•œ ë‘ ê°œì˜ ë™ì „ì„ ë˜ì§„ ê²°ê³¼ë¥¼ ë³´ì. ë‘ ê°œì˜ ë™ì „ì´ ëª¨ë‘ ì•ë©´ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ì™€ ëª¨ë‘ ë’·ë©´ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ëŠ” ì„œë¡œ ë…ë¦½ì¼ê¹Œ? ì´ëŠ” ë…ë¦½ì´ ì•„ë‹ˆë‹¤. ì™œëƒí•˜ë©´, ë™ì „ì´ ëª¨ë‘ ì•ë©´ì´ ë‚˜ì˜¤ëŠ” ì‚¬ê±´ì€ í•„ì—°ì ìœ¼ë¡œ ëª¨ë‘ ë’·ë©´ì´ ë‚˜ì˜¤ëŠ” ì‚¬ê±´ì€ ë°˜ë“œì‹œ ì¼ì–´ë‚˜ì§€ ì•Šì„ ê²ƒì´ë¼ëŠ” ì¦ê±°ê°€ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ë°˜ëŒ€ë¡œ, ëª¨ë‘ ì•ë©´ì´ ë‚˜ì˜¤ëŠ” ì‚¬ê±´ê³¼ í•œ ë²ˆë§Œ ì•ë©´ì´ ë‚˜ì˜¤ëŠ” ì‚¬ê±´ì„ ìƒê°í•´ë³´ì. í•˜ë‚˜ì˜ ì‚¬ê±´ì´ ì¼ì–´ë‚¬ë‹¤ê³ , ë°˜ë“œì‹œ ê·¸ ì‚¬ê±´ì´ ì¼ì–´ë‚¬ê±°ë‚˜ ì•ˆì¼ì–´ë‚¬ë‹¤ëŠ” ê´€ê³„ë¥¼ ë°í˜€ë‚¼ ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ, ì´ëŸ¬í•œ ê²½ìš° ë‘ ì‚¬ê±´ì´ ë…ë¦½ì ì´ë¼ê³  í•œë‹¤.  \n  ì´ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.  \n  $P(A, B)=P(A)P(B)$  \n  ì¦‰ ìœ„ ê³µì‹ì´ ì„±ë¦½í•˜ë©´ ë…ë¦½ì´ë©°, ë…ë¦½ì´ë¼ë©´ ìœ„ì˜ ì‹ì´ ì„±ë¦½í•œë‹¤.\n- **Conditional Probability**  \n  ë‘ Eventê°€ ìˆì„ ë•Œ, í•˜ë‚˜ì˜ Eventê°€ ë°œìƒí–ˆì„ ë•Œ ë‹¤ë¥¸ í•˜ë‚˜ì˜ Eventê°€ ë°œìƒí•  í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ, ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.  \n  $P(A|B) = {{P(A, B)}\\over{P(B)}}, (P(B) \\neq 0)$  \n  ì—¬ê¸°ì„œ independence íŠ¹ì„±ì„ ë” ëª…í™•í•˜ê²Œ í™•ì¸í•  ìˆ˜ ìˆëŠ”ë°, ë§Œì•½ Aì™€ Bê°€ ë…ë¦½ì´ë¼ë©´, $P(A|B) = P(A)$ì´ë‹¤.  \n  ì¦‰, Bê°€ ë°œìƒí–ˆëŠ”ì§€ ì—¬ë¶€ëŠ” Aì˜ ê²°ê³¼ì— ì˜í–¥ì„ ì•ˆì¤€ë‹¤ëŠ” ê²ƒì´ë‹¤.\n- **Partition**  \n  Sample Space($\\Omega$)ë¥¼ ê²¹ì¹˜ì§€ ì•Šê³ , ëª¨ë‘ í¬í•¨í•˜ëŠ” Eventì˜ ì§‘í•©ì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ì‹ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.  \n  $\\cup_{i=1}^{n}{P_i} = \\Omega$ ì´ê³ , $\\cap_{i=1}^{n}{P_i} = \\emptyset$\n- **Marginalization**  \n  ì „ì²´ Sample space($\\Omega$)ì— ëŒ€í•˜ì—¬ **B**ê°€ ì´ì— ëŒ€í•œ partitionì¼ ë•Œ, ì•„ë˜ ê³µì‹ì´ ì„±ë¦½í•œë‹¤.  \n  $P(A) = \\sum_{i=1}^{n}{P(A,B_i)} = \\sum_{i=1}^{n}{P(A|B_i)P(B_i)}$\n- **Bayes' Theorem**  \n  ë§Œì•½ $P(B) \\neq 0$ë¼ë©´, ì•„ë˜ ê³µì‹ì´ ì„±ë¦½í•œë‹¤. ê°„ë‹¨íˆ conditional probabilityë¥¼ í’€ì–´ì£¼ë©´ ì•„ë˜ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.  \n  $P(A|B) = {P(B|A)P(A)\\over{P(B)}}$  \n  í•´ë‹¹ ì‹ì€ ë‹¨ìˆœíˆ Joint Probabilityë¡œ ë³€í™˜í•˜ê³ , ë‹¤ì‹œ ë°˜ëŒ€ í™•ë¥ ë¡œ ë³€ê²½í–ˆì„ ë¿ì´ë‹¤. ì´ ê³µì‹ì´ ì¤‘ìš”í•˜ë‹¤ê¸° ë³´ë‹¤ëŠ” ì´ ê³µì‹ì´ ê°€ì§€ëŠ” ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. í™•ë¥ ì„ ì‚¬ê±´ì˜ ë°œìƒì˜ ë¹ˆë„ë¡œ ì´í•´í•˜ëŠ” Frequentist Approachì—ì„œëŠ” ê´€ì¸¡ì„ í†µí•´ì„œ íŠ¹ì • ë°ì´í„°ê°€ ë°œìƒí•  í™•ë¥ ì„ ì–»ëŠ”ë‹¤. ë§Œì•½ ìš°ë¦¬ê°€ ì›í•˜ëŠ” í™•ë¥ ì´ ê´€ì¸¡ì„ í†µí•´ì„œëŠ” ì–»ì„ ìˆ˜ ì—†ëŠ” ë°ì´í„°ë¼ê³  í•˜ì. ì´ ê²½ìš°ì— ìš°ë¦¬ëŠ” í™•ë¥ ì˜ ì—­ì—°ì‚°ì´ í•„ìš”í•˜ë‹¤. ìœ„ì˜ ê³µì‹ì„ ë³´ë©´ íŠ¹ì´í•œ ê²ƒì´ ë³´ì´ëŠ”ë°, ë°”ë¡œ $P(A|B)$ì™€ $P(A)$ì´ë‹¤. ì´ëŠ” ì „ì²´ í™•ë¥ ì„ í†µí•´ì„œ **Conditional Probability**ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê¸°ì— ìš°ë¦¬ëŠ” ì´ë¥¼ ì—­ì—°ì‚°ì´ë¼ê³  ë¶€ë¥´ë©°, ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆëŠ” ê¸°ì¡´ **ì‚¬ì „ í™•ë¥ **(Priority, ì´ì „ê¹Œì§€ ë§ì„ ê±°ë¼ê³  ìƒê°í•œ í™•ë¥ )ì„ í†µí•´ì„œ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œì˜ ì‚¬ê±´ì˜ í™•ë¥ ì„ ë‹¤ì‹œ ê³„ì‚°í•´ë³´ëŠ” ê²ƒì´ë‹¤. ì´ ê³¼ì •ì„ **Bayesian Update**ë¼ê³  í•˜ëŠ”ë° ì´ ê³¼ì •ì„ í†µí•´ì„œ ì–»ì€ $P(A|B)$ë¥¼ ë‹¤ì‹œ ë‹¤ìŒ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” $P(A)$ë¡œì¨ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë ‡ê²Œ í•´ì„œ ìš°ë¦¬ëŠ” ì ì§„ì ìœ¼ë¡œ $P(A)$ë¥¼ ì°¾ì•„ë‚˜ê°ˆ ìˆ˜ ìˆë‹¤.\n\n### Random Variable\n\nRandom Variableì´ë¼ëŠ” ê²ƒì€ íŠ¹ì • ì‚¬ê±´ì„ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ ë³€í˜•í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•œë‹¤. ìš°ë¦¬ëŠ” ì´ì „ ì˜ˆì‹œì—ì„œ ë‘ ê°œì˜ ë™ì „ì„ ë™ì‹œì— ë˜ì ¸ì„œ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ Sample Spaceë¡œ ë‘ì—ˆê³ , ì´ë¥¼ $\\Omega = $ $\\{ hh, ht, th, tt \\}$ë¼ê³  í‘œí˜„í–ˆë‹¤. í•˜ì§€ë§Œ, ì´ì™€ ê°™ì€ í‘œê¸° ë°©ì‹ì€ ìˆ˜í•™ì ì¸ ì—°ì‚°ì„ ì ìš©í•˜ê¸° ì–´ë µë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì•ë©´ì´ ë‚˜ì˜¨ ê²½ìš°ë¥¼ $X=1$, ë’·ë©´ì´ ë‚˜ì˜¨ ê²½ìš°ë¥¼ $X=-1$ ë¼ê³  í•˜ëŠ” í˜•íƒœë¡œ ì¹˜í™˜í•˜ëŠ” ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ë§Œë“¤ì–´ì§„ Xë¥¼ ìš°ë¦¬ëŠ” Random Variableì´ë¼ê³  ë¶€ë¥¸ë‹¤. ì´ëŸ° ì¹˜í™˜ì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” í™•ë¥ ì„ Random Variableì— ëŒ€í•œ í•¨ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\në˜, Random Variableì„ ì •ì˜í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ê°’ì„ ì—°ì†ì ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n- **Mean**  \n  Random Variableì˜ í‰ê·  ë˜ëŠ” ê¸°ëŒ“ê°’ì´ë¼ê³  ë¶€ë¥¸ë‹¤.  \n  $\\mu_{X} = E[X] = \\sum_{x}{xP(X=x)}$\n- **Variance**  \n  í‰ê· ì—ì„œ ë°ì´í„°ê°€ ë–¨ì–´ì§„ ì •ë„ë¥¼ í‘œí˜„í•˜ëŠ” ê°’ìœ¼ë¡œ ë¶„ì‚°ì´ë¼ê³  ë¶€ë¥¸ë‹¤.  \n  $\\sigma_{X}^{2} = E[(X-\\mu_{X})^2] = E[X^2] -\\mu_{X}^{2}$\n- **Covariance**  \n  Random Variable Xì™€ Yì˜ ìƒê´€ê´€ê³„(Correlation)ì„ í™•ì¸í•˜ëŠ” ì²™ë„ë¡œ ì‚¬ìš©í•œë‹¤.  \n  $cov(X, Y) = E[(X-\\mu_{X})(Y-\\mu_{Y})] = E[XY] -\\mu_{X}\\mu_{Y}$  \n  ë§Œì•½, ë‘ Xì™€ Yê°€ ì„œë¡œ ì „í˜€ ìƒê´€ì´ ì—†ë‹¤(Independent)ë©´, $cov(X, Y) = 0$ì´ë‹¤. ê·¸ ë°˜ëŒ€ëŠ” ì„±ë¦½í•˜ì§€ ì•Šì§€ë§Œ, ê·¸ëŸ´ ê°€ëŠ¥ì„±ì´ êµ‰ì¥íˆ ë†’ì•„ì§„ë‹¤.\n- **Correlation Coefficient**  \n  Covarianceë³´ë‹¤ ë” ì—„ê²©í•œ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•˜ëŠ” ì²™ë„ë¡œ ì‚¬ìš©ë˜ëŠ”ë°, ë‹¨ìˆœíˆ Covarianceë¥¼ ê° í‘œì¤€í¸ì°¨($\\sigma$)ë¡œ ë‚˜ëˆˆ ê²ƒì´ë‹¤. ì´ë¡œ ì¸í•´ ê²°ê³¼ ê°’ì€ [-1, 1] ì‚¬ì´ ê°’ì´ ëœë‹¤.  \n  $corr(X, Y) = {cov(X,Y)\\over{\\sigma_{X}\\sigma_{Y}}}$  \n  ë”°ë¼ì„œ, <mark>1ì¼ ìˆ˜ë¡ ë‘ Random Variableì˜ ìƒê´€ì„±ì´ ë†’ìœ¼ë©° ë¹„ë¡€í•˜ëŠ” ê´€ê³„ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, -1ì¼ ê²½ìš°ì—ëŠ” ìƒê´€ì´ ë†’ì§€ë§Œ ë°˜ë¹„ë¡€í•˜ëŠ” ê´€ê³„ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë°˜ëŒ€ë¡œ, 0ì¸ ê²½ìš°ëŠ” ìƒê´€ ê´€ê³„ê°€ ì•„ì£¼ ë‚®ìŒìœ¼ë¡œ ë…ë¦½ì¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.</mark> ê·¸ë ‡ë‹¤ê³  100%ëŠ” ì•„ë‹ˆì§€ë§Œ, ë‹¨ì§€ ê·¸ëŸ´ í™•ë¥ ì´ êµ‰ì¥íˆ ë†’ë‹¤ëŠ” ê²ƒì´ë‹¤. ì£¼ì˜í•  ì ì€ Correlation Coefficientê°€ 1ì´ë¼ê³  Xê°€ Yì˜ ì›ì¸ì´ ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¼ëŠ” ê²ƒì„ ìœ ì˜í•´ì•¼ í•œë‹¤. ë‹¨ìˆœíˆ Xê°€ ì¼ì–´ë‚¬ì„ ë•Œ, Yê°€ ì¼ì–´ë‚  í™•ë¥ ì´ ë†’ë‹¤ëŠ” ê²ƒì´ë‹¤.  \n\n### Law of Large Numbers\n\nê²½í—˜ì  í™•ë¥ ê³¼ ìˆ˜í•™ì  í™•ë¥  ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²•ì¹™ìœ¼ë¡œ, ì „ì²´ ê²½ìš°ì˜ ìˆ˜ì™€ ì´ì— ë”°ë¥¸ í™•ë¥ (ëª¨ì§‘ë‹¨)ì´ ìˆì„ ë•Œ, ê´€ì¸¡í•œ ê²½ìš°ì˜ ìˆ˜ì™€ ì´ì— ë”°ë¥¸ í™•ë¥ (í‘œë³¸ ì§‘ë‹¨)ì€ ê´€ì¸¡ ë°ì´í„°ì˜ í¬ê¸°ê°€ ì»¤ì§ˆ ìˆ˜ë¡ í‘œë³¸ í‰ê· ì´ ëª¨í‰ê· ì— ê°€ê¹Œì›Œì§ì„ ì˜ë¯¸í•œë‹¤.\n\n### ìì£¼ ì‚¬ìš©ë˜ëŠ” Probability Distribution Function\n\níŠ¹ì • taskì˜ ê²½ìš° ì´ë¯¸ ì •ì˜ëœ í™•ë¥  ë¶„í¬ë¥¼ í†µí•´ì„œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê²½ìš°ê°€ ìˆë‹¤. ë”°ë¼ì„œ, ì•„ë˜ì™€ ê°™ì€ ëŒ€í‘œì ì¸ í™•ë¥  ë¶„í¬ëŠ” ì•Œì•„ë‘ëŠ” ê²ƒì€ ì¤‘ìš”í•˜ë‹¤.\n\n- **Bernoulli distribution**  \n  í•˜ë‚˜ì˜ ì‚¬ê±´ì´ ì¼ì–´ë‚  í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤. ë°œìƒí•˜ëŠ” ê²½ìš°ë¥¼ X=1, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ë¥¼ X=0ìœ¼ë¡œ random variableë¡œ ì¹˜í™˜í•˜ì—¬ ë‚˜íƒ€ë‚¸ í™•ë¥  ë¶„í¬(probability distribution)ì´ë‹¤. ëŒ€í‘œì ì¸ ì‚¬ê±´ì€ ë™ì „ ë˜ì§€ê¸°ì™€ ê°™ì€ ë‘ ê°œì˜ ê²°ê³¼ë§Œ ê°–ëŠ” binary eventì„ í‘œí˜„í•  ë•Œì´ë‹¤.  \n  ë”°ë¼ì„œ, ì‚¬ê±´ì´ ì¼ì–´ë‚  í™•ë¥ ì„ pë¼ê³  í•  ë•Œ, ë‹¤ìŒê³¼ ê°™ì´ Random Variableì— ëŒ€í•œ í™•ë¥ ì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.  \n  $P(X=x) = p^{x}(1-p)^{1-x}$\n  ë³µì¡í•´ë³´ì´ì§€ë§Œ, ì‹¤ìƒì€ Xê°€ 0 ë˜ëŠ” 1ì´ë¯€ë¡œ, $P(X=0)=1-p$ì´ê³ , $P(X=1)=p$ì´ë‹¤.\n  - í‰ê· \n    $E[X] = p$\n  - ë¶„ì‚°  \n    $Var[X] = E[X^2] - \\mu_{X}^2 = p - p^2 = p(1-p)$\n- **Binomial Distribution**  \n  í™•ë¥ ì´ pì¸ ì‚¬ê±´ì„ në²ˆ ìˆ˜í–‰í–ˆì„ ë•Œ, xë²ˆ ë°œìƒí•  í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ, Random Variable Xì˜ ë²”ìœ„ëŠ” {0, 1, â€¦, n}ì´ ëœë‹¤. ëŒ€í‘œì ì¸ ì‚¬ê±´ì€ ë™ì „ ë˜ì§€ê¸°ë¥¼ ì—¬ëŸ¬ ë²ˆ ë˜ì¡Œì„ ë•Œ, ì• ë©´ì´ xë²ˆ ë‚˜ì˜¬ ê²½ìš°ì˜ ìˆ˜ì´ë‹¤.  \n  ì´ì— ë”°ë¼ Random Variableì— ëŒ€í•œ í™•ë¥ ì„ ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n  $P(X=x) = {n \\choose x}p^x(1-p)^{n-x}$  \n  ì´ ë˜í•œ ë³µì¡í•´ ë³´ì´ì§€ë§Œ, ì‚¬ì‹¤ì€ ë…ë¦½ì ì¸ Bernoulliì˜ ì—°ì† ìˆ˜í–‰ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.  \n  - í‰ê·   \n    $E[X] = np$\n  - ë¶„ì‚°  \n    $Var[X] = Var[\\sum_{i}X_i]=\\sum_iVar[X_i]=np(1-p)$\n- **Beta Distribution**  \n  $\\alpha, \\beta > 0$ë¥¼ ë§Œì¡±í•˜ëŠ” ë‘ parameterë¥¼ ì´ìš©í•œ probability distributionì´ë‹¤.  \n  ì´ëŠ” [0, 1]ì—ì„œ continuousí•œ random variableë¥¼ ì´ìš©í•  ìˆ˜ ìˆë‹¤. ì´ì— ë”°ë¥¸ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n  $P(X=x) \\propto x^{\\alpha-1}(1-x)^{\\beta-1}$  \n  ì´ì— ëŒ€í•œ ì˜ë¯¸ë¥¼ ì´í•´í•˜ìë©´, í™•ë¥ ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ì´ë‹¤. ê° $\\alpha - 1$ì™€ $\\beta - 1$ë¥¼ ì„±ê³µ íšŸìˆ˜, ì‹¤íŒ¨ íšŸìˆ˜ë¼ê³  í•˜ì.  ì´ëŠ” ì´ë¯¸ ì•Œê³  ìˆëŠ” ëª¨ì§‘ë‹¨(ì „ì²´ ì§‘í•©)ì˜ ê³„ì‚° ê²°ê³¼ì´ë‹¤. ê·¸ë¦¬ê³  random variableì„ íŠ¹ì • eventì˜ í™•ë¥ ì´ë¼ê³  í•˜ì. ì˜ˆë¥¼ë“¤ë©´, ë™ì „ ë˜ì§€ê¸°ë¥¼ í•  ë•Œ, ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì´ $1\\over2$ì´ë¼ëŠ” ê²ƒì„ ì´ë¯¸ ì•Œê³  ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” $\\alpha - 1$ = $\\beta - 1$ ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œë¡œ ë™ì „ ë˜ì§€ê¸°ë¥¼ 5ë²ˆ ìˆ˜í–‰í–ˆì„ ë•Œ, 4ë²ˆ ì•ë©´ì´ ë‚˜ì™”ë‹¤ê³  í•˜ì. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ì¶”ì¸¡í•œ í•´ë‹¹ eventì˜ í™•ë¥ ì€ $4\\over5$ì´ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´, ì‹¤ì œë¡œ í•´ë‹¹ í™•ë¥ ì´ $4\\over5$ì¼ í™•ë¥ ì„ ì–¼ë§ˆë‚˜ ë ê¹Œ?  \n  ì´ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•œ ê²ƒì´ Beta distributionì¸ ê²ƒì´ë‹¤. ì´ì— ë”°ë¼, Beta distributionì„ PDFë¡œ í‘œí˜„í•˜ë©´ ${\\alpha\\over\\alpha+\\beta}$ì—ì„œ ë†’ì€ í™•ë¥ ê°’ì„ ê°€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.\n  - í‰ê·   \n    $E[X] = {\\alpha\\over{\\alpha+\\beta}}$\n  - ë¶„ì‚°  \n    $Var[X] = {\\alpha\\beta\\over{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}}$\n\n  ìœ„ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯ì´, ë§Œì•½ ì´ì „ ëª¨ì§‘ë‹¨ì—ì„œì˜ í‰ê· ê°’ì— ëŒ€í•œ ë¯¿ìŒì´ í¬ë‹¤ë©´, ê°ê°  $\\alpha, \\beta$ì˜ ë¹„ìœ¨ì€ ìœ ì§€í•˜ë©´ì„œ ìƒìˆ˜ë°°ë¥¼ ìˆ˜í–‰í•˜ì—¬ í‰ê· ì€ ë™ì¼í•˜ì§€ë§Œ ë¶„ì‚° ê°’ì„ ë” ì ê²Œ ë§Œë“¤ì–´ ë¾°ì¡±í•œ í˜•íƒœì˜ ë¶„í¬ë¥¼ ì™„ì„±í•  ìˆ˜ë„ ìˆë‹¤. ì´ ê²½ìš°ì—ëŠ” í‰ê· ê³¼ ë§ì§€ ì•ŠëŠ” í‘œë³¸ì§‘í•©ì—ì„œì˜ í‰ê· ì„ êµ‰ì¥íˆ í™•ë¥ ì´ ë‚®ì€ í™•ë¥ ë¡œ ì‹ë³„í•˜ëŠ” ê²ƒì´ë‹¤.\n- **Gaussian Distribution**  \n  $\\mu, \\sigma^2$ë¥¼ parameterë¡œ ê°–ëŠ” probability distributionì´ë‹¤.\n\n  ì´ëŠ” $[-\\infin, \\infin]$ë¥¼ êµ¬ê°„ìœ¼ë¡œ continuousí•œ random varibleì„ ì´ìš©í•œë‹¤. ì´ì— ë”°ë¥¸ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. (ë‹¨ì¼ random variableì¸ ê²½ìš°)\n  \n  $P(X) = {1\\over{\\sqrt{2\\pi\\sigma^2}}}\\exp(-{1\\over{2\\sigma^2}}(X-\\mu)^2)$\n\n  ì´ ë¶„í¬ëŠ” êµ‰ì¥íˆ ë§ì€ ê³³ì— ì‚¬ìš©ë˜ëŠ”ë° <mark>ìš°ë¦¬ê°€ ìƒê°í•  ìˆ˜ ìˆëŠ” ëŒ€ê²Œì˜ ìì—° ë°œìƒì— ì˜í•œ í˜„ìƒë“¤(ex. ì‚¬ëŒ í‚¤ì˜ ë¶„í¬)ì´ ì´ ë¶„í¬ë¥¼ ë”°ë¥´ê¸° ë•Œë¬¸ì´ë‹¤.</mark> ê·¸ë ‡ê¸°ì— ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¶„í¬ì´ë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼ (Lindeberg-Levy) **Central Limit Theoriem**ì— ë”°ë¥´ë©´, í‘œë³¸ì—ì„œ ì–»ì€ í‘œë³¸ í‰ê· ì„ êµ¬í•˜ë©´ êµ¬í•  ìˆ˜ë¡ ì ì  Gaussian Distributionì„ ë”°ë¼ê°„ë‹¤. ì¦‰, $n \\rarr \\infin$ì´ë©´, í‘œë³¸ í‰ê· ì´ ì´ë£¨ëŠ” ë¶„í¬ê°€ Gaussianì´ë¼ëŠ” ê²ƒì´ë‹¤.\n  \n  ì¶”ê°€ì ìœ¼ë¡œ ì•Œì•„ë³¼ ê²ƒì€ ë°”ë¡œ ì—¬ëŸ¬ ê°œì˜ Random Variableë¡œ Gaussian Distributionì„ ë” ë†’ì€ ì°¨ì›ìœ¼ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìˆ˜í–‰í•˜ë©´, Gaussian ë¶„í¬ê°€ í‰ê· ê³¼ ë¶„ì‚°ì„ í¬í•¨í•˜ê¸° ë•Œë¬¸ì— ë‘ ë°ì´í„°ì˜ ê²½í–¥ì„±(Covariance)ë¥¼ ì–´ëŠì •ë„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.\n\n## Calculus\n\nì¼ëª… ë¯¸ì ë¶„í•™ìœ¼ë¡œ, ëŒ€ê²Œì˜ ë¯¸ì ë¶„ ë²•ì¹™ì€ ëª¨ë‘ ì•Œê³  ìˆì„ ê²ƒì´ë¼ê³  ìƒê°í•˜ê³  ë„˜ì–´ê°„ë‹¤.\n\n### Optimization\n\nì •ë§ ëª¨ë‘ê°€ ì•Œê³  ìˆì„ ê±° ê°™ì§€ë§Œ, ê·¸ëŸ¼ì—ë„ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì— ì •ë¦¬í•˜ê³  ë„˜ì–´ê°€ì. ì¼ë°˜ì ìœ¼ë¡œ Optimizationì´ë€ ìµœì ê°’ì„ ì°¾ëŠ” ê³¼ì •ì´ë‹¤. ì´ ê³¼ì •ì—ì„œ ëŒ€ê²Œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ ìµœì†Ÿê°’ ë˜ëŠ” ìµœëŒ“ê°’ì´ë‹¤. ìš°ë¦¬ëŠ” ìµœì†Ÿê°’/ìµœëŒ“ê°’ì„ ë¯¸ë¶„ì„ í†µí•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\nì—¬ê¸°ì„œëŠ” Convexë¼ëŠ” ì„±ì§ˆì— ëŒ€í•´ì„œ ìì„¸íˆ ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤. ì‹œê°„ì´ ìˆë‹¤ë©´, ì´ì—ëŒ€í•œ ê°œë…ë„ ë°˜ë“œì‹œ ìˆ™ì§€í•˜ê¸°ë¥¼ ë°”ë€ë‹¤.\n\n> **ìµœëŒ€/ìµœì†Œ êµ¬í•˜ê¸°**\n\nëª¨ë‘ê°€ ì•Œë‹¤ì‹œí”¼ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ê¸°ìš¸ê¸°ë¥¼ ì˜ë¯¸í•œë‹¤. ë§Œì•½ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì— íŠ¹ì • ê°’ì„ ëŒ€ì…í•  ê²½ìš° ì´ëŠ” ê·¸ ì§€ì ì—ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ ì˜ë¯¸í•œë‹¤.\n\në¨¼ì €, í•¨ìˆ˜ì˜ ë¯¸ë¶„ì— ëŒ€ì…í•œ ê°’ì´ 0ì¸ ê²½ìš°ì— í•´ë‹¹ ê°’(ê·¹ê°’)ì´ ê°€ì§€ëŠ” ì„±ì§ˆì„ ê¸°ì–µí•´ì•¼ í•œë‹¤. ë§Œì•½, ê°’ì´ 0ìœ¼ë¡œ í•˜ëŠ” ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì¢Œìš° ë¶€í˜¸ê°€ ë°”ë€ë‹¤ë©´, ì´ëŠ” ì •ë§ ê·¹ëŒ€, ê·¹ì†Œë¼ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤. ì¦‰, í•´ë‹¹ êµ¬ê°„ì—ì„œ ìµœì†Œì™€ ìµœëŒ€ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ê²ƒì´ë‹¤.\n\nì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ì˜ ë¶€í˜¸ê°€ ë°”ë€Œì—ˆë‹¤ëŠ” ì˜ë¯¸ë¥¼ ì‚´í´ë³´ì•„ì•¼ í•œë‹¤. ì´ëŠ” í•¨ìˆ˜ì˜ ê°’ì´ êµ¬ê°„ ë‚´ì—ì„œ ê°€ì¥ ì‘ì€ ê°’(ê·¹ì†Œ) ë˜ëŠ” êµ¬ê°„ ë‚´ì—ì„œ ê°€ì¥ í° ê°’(ê·¹ëŒ€)ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì™œëƒí•˜ë©´, ì§ê´€ì ìœ¼ë¡œ ê¸°ìš¸ê¸°ê°€ 0ì´ ë˜ê¸° ì „ê¹Œì§€ëŠ” ê³„ì†í•´ì„œ ì¦ê°€/ê°ì†Œí•´ì™”ë‹¤ëŠ” ê²ƒì„ ì•Œê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê¸°ìš¸ê¸°ê°€ 0ì¸ ì§€ì ì„ ëª¨ë‘ ì°¾ì•„ ë¹„êµí•˜ë©´, ê·¸ ì•ˆì—ì„œ ìµœëŒ€/ìµœì†Œë¥¼ ì°¾ì„ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\nê·¸ëŸ°ë° ì–´ë–»ê²Œ í•˜ë©´, ê¸°ìš¸ê¸°ê°€ 0ì¸ ì§€ì ì´ ê·¹ëŒ€ì¸ì§€ ê·¹ì†Œì¸ì§€ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆì„ê¹Œ? ì´ê²ƒì€ ë°”ë¡œ ì§ì „ì˜ ê°’ì„ ë¯¸ë¶„ í•¨ìˆ˜ì— ëŒ€ì…í•´ë³´ë©´ ì‰½ê²Œ ì•Œ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ì´ê²ƒì´ ë§¤ë²ˆ ê·¸ë ‡ê²Œ ì‰½ê²Œ íŒë³„ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ì¤‘ ë¯¸ë¶„ì„ ì‚¬ìš©í•œë‹¤. ì´ì¤‘ ë¯¸ë¶„ í•¨ìˆ˜ì— ê·¹ê°’ì„ ëŒ€ì…í–ˆì„ ë•Œ ì–‘ìˆ˜ë¼ë©´ ì´ëŠ” ê·¹ì†Œë¥¼ ì˜ë¯¸í•˜ê³ , ìŒìˆ˜ì¸ ê²½ìš°ëŠ” ê·¹ëŒ€ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ ë˜í•œ, ì§ê´€ì ìœ¼ë¡œ ê¸°ìš¸ê¸°ì˜ ë³€í™”ëŸ‰ì´ë¼ëŠ” ì´ì¤‘ ë¯¸ë¶„ì˜ ì •ì˜ë¥¼ ì•Œë©´, ì§ê´€ì ìœ¼ë¡œ ì™€ë‹¿ì„ ìˆ˜ ìˆë‹¤.\n\nì´ë ‡ê²Œ í•´ì„œ ê·¹ëŒ€ì™€ ê·¹ì†Œë¥¼ ê³¨ë¼ë‚´ê³ , ì´ì¤‘ì—ì„œ ê°€ì¥ í° ê°’ê³¼ ê°€ì¥ ì‘ì€ ê°’ì„ ì°¾ì•„ë‚´ë©´, ìš°ë¦¬ëŠ” ì´ê²ƒì„ í•¨ìˆ˜ì˜ ìµœì í™”ë¥¼ ìˆ˜í–‰í–ˆë‹¤ê³  í•œë‹¤.\n\n### Constraint Optimization\n\nì—¬ê¸°ì„œëŠ” íŠ¹ë³„í•œ caseë¥¼ ìœ„í•œ ì˜ˆì‹œì´ë‹¤. íŠ¹ì • ì¡°ê±´ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ë¥¼ ë§Œì¡±í•˜ë©´ì„œ íŠ¹ì • í•¨ìˆ˜ì˜ optimizationì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤.\n\nê·¸ëŸ¬ë©´ ìš°ë¦¬ê°€ ìµœì í™”í•˜ê³ ì í•˜ëŠ” ëª©ì í•¨ìˆ˜($\\mathcal{J}(\\bold{x})$)ì™€ ë“±ì‹ ì œì•½ ì¡°ê±´($h_{j}(\\bold{x})$), ë¶€ë“±ì‹ ì œì•½ ì¡°ê±´($g_{i}(\\bold{x})$)ì„ ì‚´í´ë³´ì.\n\nìš°ë¦¬ëŠ” ëª¨ë“  ìµœì í™” ë¬¸ì œë¥¼ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ ë¬˜ì‚¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & \\mathcal{J}(\\bold{x}) &\\\\\n  \\text{subject to} \\quad & g_{i}(\\bold{x}) \\leq 0, & i = 1, ..., M \\\\\n                          & h_{j}(\\bold{x}) = 0, & j = 1, ..., L\n\\end{align*}\n$$\n\nmaximizationì¸ ê²½ìš°ëŠ” ìŒìˆ˜ë¥¼ ì·¨í•´ì„œ ê²°ê³¼ë¥¼ êµ¬í•œ í›„ ë³€í™˜ ì‹œì— ë‹¤ì‹œ ìŒìˆ˜ë¥¼ ì·¨í•´ì£¼ë©´ ëœë‹¤. ê·¸ë¦¬ê³  ë¶€ë“±í˜¸ê°€ ë°˜ëŒ€ì¸ ì œì•½ ì¡°ê±´ì¸ ê²½ìš°ì—ë„ ì–‘ë³€ì— ìŒìˆ˜ë¥¼ ì·¨í•´ì„œ ê°„ë‹¨í•˜ê²Œ ë’¤ì§‘ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.\n\nì´ëŸ¬í•œ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ì„œëŠ” ìš°ë¦¬ëŠ” ì‹ì„ **Lagrangian** í˜•íƒœë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤.\n\n> **Lagrangian**\n\nì´ëŠ” ì¡°ê±´ë¶€ì‹ì— ìˆëŠ” ì¡°ê±´ì— ë³€ìˆ˜($\\nu$, $\\lambda$)ë¥¼ ê³±í•œ ê°’ì˜ í•©ê³¼ ì›ë˜ ëª©ì  í•¨ìˆ˜($\\mathcal{J}(\\bold{x})$)ì˜ í•©ì´ë‹¤.\n\n$$\n\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) = \\mathcal{J}(\\bold{x}) + \\sum_{i=1}^{M}{\\nu_{i}g_{i}(\\bold{x})} + \\sum_{j=1}^{K}{\\lambda_{j}h_{j}(\\bold{x})}\n$$\n\nì—¬ê¸°ì„œ **Lagrangian** í•¨ìˆ˜ì˜ optimizationì´ ê³§ ëª©ì í•¨ìˆ˜ì˜ optimizationì´ë‹¤. ì¦ëª…ì€ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš°ì—ëŠ” ì§ì ‘ ì°¾ì•„ë³´ì•„ì•¼í•  ê²ƒì´ë‹¤.  \nì—¬ê¸°ì„œ ë§Œì•½ ë“±ì‹ë§Œ ìˆëŠ” ì‹ì¸ ê²½ìš°ì— ìš°ë¦¬ëŠ” ê°„ë‹¨íˆ ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•´ì„œ í¸ë¯¸ë¶„ì´ 0ì´ ë˜ëŠ” ë“±ì‹ì„ ì´ìš©í•´ì„œ, ìµœì  $\\bold{x}$ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤. ìœ„ì— ì‹ì—ì„œ ë¶€ë“±ì‹ ì¡°ê±´($g_{i}(\\bold{x})$)ì´ ì‚¬ë¼ì§„ë‹¤ë©´, ìš°ë¦¬ëŠ” ë¯¸ë¶„ì„ í†µí•´ì„œ ì²˜ë¦¬í•´ì•¼í•˜ëŠ” ê°’ì€ ì´ xì˜ í¬ê¸°(N)ì™€ Lì´ë‹¤. ì´ëŠ” ìš°ë¦¬ê°€ í¸ë¯¸ë¶„í•´ì„œ êµ¬í•  ìˆ˜ ìˆëŠ” ì‹ì˜ ê°¯ìˆ˜ì™€ ë˜‘ê°™ë‹¤. ì¦‰, ìš°ë¦¬ê°€ ëª¨ë¥´ëŠ” ë³€ìˆ˜ëŠ” N+Mê°œ ìš°ë¦¬ê°€ ê°€ì§„ ë“±ì‹ì€ N+Mê°œì´ë¯€ë¡œ ì—°ë¦½í•´ì„œ ê° ê°’ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\ní•˜ì§€ë§Œ, ë¶€ë“±ì‹ì¸ ê²½ìš°ì—ëŠ” ì¶”ê°€ì ìœ¼ë¡œ ê³ ë ¤í•´ì¤˜ì•¼í•  ê²ƒì´ ìˆë‹¤.\n\n> **KKT Condition**\n\nì´ëŠ” ìš°ë¦¬ê°€ ìµœì ê°’($\\bold{x}_{*}$)ë¥¼ ì°¾ì•˜ì„ ë•Œ, ë‹¤ìŒê³¼ ê°™ì€ $\\boldsymbol{\\nu_{*}}$, $\\boldsymbol{\\lambda_{*}}$ê°€ ì¡´ì¬í•´ì•¼ í•œë‹¤ëŠ” ì •ë¦¬ì´ë‹¤.\n\n1. **Optimality**  \n   $\\nabla\\mathcal{L} = \\nabla\\mathcal{J}(\\bold{x}_{*}) + \\sum_{i=1}^{M}{\\nu_{*(i)}\\nabla g_{i}(\\bold{x}_{*})} + \\sum_{j=1}^{L}{\\lambda_{*(j)}\\nabla h_{j}(\\bold{x}_{*})} = 0$  \n   ìœ„ì—ì„œ ë³´ì•˜ë˜ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì‹ì´ë‹¤.\n2. **Feasibility**  \n   $g_{i}(\\bold{x}_{*}) \\leq 0,  i = 1, ..., M$  \n   $h_{j}(\\bold{x}_{*}) = 0,  j = 1, ..., L$  \n   ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ë‹¤.\n3. **Complementary slackness**  \n   $\\nu_{*(i)}g_{i}(\\bold{x}_{*}) = 0, i = 1, ..., M\\quad(\\nu_{*(i)} \\geq 0)$  \n   ìœ„ì˜ ì‹ì€ ë‹¤ì†Œ í—·ê°ˆë¦´ ìˆ˜ ìˆëŠ”ë° ê°€ì¥ ì•Œì•„ë“£ê¸° ì‰¬ìš´ í˜•íƒœëŠ” ì•„ë˜ì´ë‹¤.  \n   $g_{i}(\\bold{x}_{*}) \\lt 0\\text{, then } \\nu_{*(i)} = 0$  \n   $g_{i}(\\bold{x}_{*}) = 0\\text{, then } \\nu_{*(i)} > 0$\n\nìœ„ì˜ ì‹ì„ ë§Œì¡±í•˜ëŠ” $\\bold{x}_{*}$, $\\boldsymbol{\\nu_{*}}$, $\\boldsymbol{\\lambda_{*}}$ë¥¼ ì°¾ìœ¼ë©´, ê·¸ê²ƒì´ ìµœì ê°’ì—ì„œì˜ $\\bold{x}_{*}$ì´ë‹¤.\n\n> **Lagrangian Dual Problem**\n\nì—¬ê¸°ì„œ í•œ ë°œ ë” ë‚˜ì•„ê°€ë©´, Lagrangianì— ë‹¤ì‹œ í•œë²ˆ Lagrangianì„ ì·¨í•  ìˆ˜ ìˆë‹¤.\n\nìš°ë¦¬ê°€ ë§Œì•½ Lagrangianì˜ í•˜í•œì„ $\\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda})$ì´ë¼ í•˜ê³ ,\n\n$$\n\\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) = \\inf_{\\bold{x} \\in \\mathcal{X}}\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda})\n$$\n\n$\\boldsymbol{f}^{*}$ì„ ìµœì ê°’ì´ë¼ê³  í•œë‹¤ë©´, ì•„ë˜ ì‹ì´ ì„±ë¦½í•œë‹¤.\n\n$$\n\\boldsymbol{f}^{*} \\geq \\min_{\\bold{x}}\\mathcal{L}(\\bold{x}, \\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) := \\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda})\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” $\\mathcal{G}$ì˜ ìµœëŒ“ê°’ì„ ì°¾ìœ¼ë©´ í•´ë‹¹ ê°’ì´ ìµœì í•´ì— ê·¼ì‚¬í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\nì´ëŠ” ìš°ë¦¬ê°€ í’€ê³ ì í•˜ëŠ” ë¬¸ì œì˜ í˜•ì‹ì„ ë‹¤ì‹œ í•œ ë²ˆ ë°”ê¾¸ê²Œ ëœë‹¤.\n\n$$\n\\begin{align*}\n  \\text{minimize}   \\quad & \\mathcal{G}(\\boldsymbol{\\nu}, \\boldsymbol{\\lambda}) &\\\\\n  \\text{subject to} \\quad & \\boldsymbol{\\nu}_{i} \\geq 0, & i = 1, ..., M\n\\end{align*}\n$$\n\nì´ ì‹ì„ KKT conditionì„ ì´ìš©í•˜ì—¬ í‘¸ëŠ” ê²ƒì´ ê¸°ì¡´ ì‹ë³´ë‹¤ ì‰½ê²Œ í’€ ìˆ˜ ìˆëŠ” ê²½ìš°ê°€ ë§ë‹¤. ë”°ë¼ì„œ, ì´ëŸ¬í•œ í˜•íƒœë¡œ ë¬¸ì œë¥¼ í’€ì´í•  ìˆ˜ë„ ìˆë‹¤.\n\n## Information Theory\n\n### Entropy\n\nìˆ˜í•™ì—ì„œ ì •ë³´ì˜ ë¶ˆí™•ì‹¤ì„±(uncertainty)ì„ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ ë¬¼ë¦¬ì˜ entropy ë¼ëŠ” ê°œë…ì„ ë„ì…í•œ ê²ƒì´ë‹¤. ì¦‰ ì •ë³´ê°€ ê°€ì§€ëŠ” \"surprise\" ì •ë„ê°€ í¬ë‹¤ë©´, entropyê°€ í° ì •ë³´ë¥¼ ì˜ë¯¸í•˜ê³ , ì¼ë°˜ì ì¸ ì •ë³´ë¼ë©´ ì´ëŠ” entropyê°€ ì‘ì€ ì •ë³´ì¸ ê²ƒì´ë‹¤.\n\nìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ì‹œ ì •ì˜í•˜ìë©´, ë‹¤ìŒê³¼ ê°™ë‹¤.  \nsample space $\\Omega$ì—ì„œ ì •ì˜ëœ random variable $X$ì™€ í™•ë¥  $p_{X}(x)$ì´ ì£¼ì–´ì§ˆ ë•Œ, Entropyë¥¼ $H(x)$ë¼ í•˜ì.\n\n$$\nH(X) = -\\sum_{x \\in \\Omega}p(x)\\log_{2}p(x)\n$$\n\nìœ„ ì‹ì—ì„œ logì˜ ë°‘ì´ 2ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆëŠ”ë° computer scienceì—ì„œëŠ” ì •ë³´ê°€ bitë‹¨ìœ„ë¡œ ì €ì¥ë˜ê¸° ë•Œë¬¸ì— ê¸°ë³¸ì ìœ¼ë¡œëŠ” 2ë¥¼ ì‚¬ìš©í•œë‹¤. í•˜ì§€ë§Œ, ìƒí™©ì— ë”°ë¼ì„œëŠ” ë‹¤ë¥¸ ë°‘ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤.\n\ní—·ê°ˆë¦´ ìˆ˜ ìˆëŠ”ë° í‘œê¸°ë²•ì´ êµ‰ì¥íˆ ë‹¤ì–‘í•˜ë‹ˆ ìœ ì˜í•´ì„œ ë³´ë„ë¡ í•˜ì.\n\n$$\nH(X) = H_{p}(X) = H(p) = H_{X}(p) = H(p_{X})\n$$\n\n> **ìµœëŒ“ê°’ê³¼ ìµœì†Ÿê°’**\n\nEntropyëŠ” ì •ë³´ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤ê³  í–ˆë‹¤. ì¦‰, ì •ë³´ê°€ í™•ì‹¤í•  ìˆ˜ë¡ EntrophyëŠ” 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ë©°, í™•ì‹¤íˆ ì•„ëŠ” ì •ë³´ì˜ ê²½ìš° Entropyê°€ ìµœì†Ÿê°’ì¸ 0ì´ ëœë‹¤.  \në°˜ëŒ€ë¡œ Entropyì˜ ìµœëŒ“ê°’ì˜ ê²½ìš° $|\\Omega| = n$ì´ë¼ê³  í•  ë•Œ, $\\log_{2}{n}$ì´ë‹¤. ì´ëŠ” uniform distribution(ëª¨ë“  Random Variableì˜ í™•ë¥ ì´ ê°™ì€ ë¶„í¬)ì¼ ê²½ìš°ì´ë‹¤.\n\n$$\n0 \\leq H(x) \\leq log_{2}{|\\Omega|}\n$$\n\n> **$\\bold{\\log_{2}({1 \\over p_{X}(x)})}$ì˜ í‰ê· **\n\nEntropyë¥¼ random variable xì˜ í™•ë¥ ì˜ ì—­ìˆ˜ì˜ logë¥¼ ì·¨í•œ ê°’ìœ¼ë¡œ í•´ì„í•  ìˆ˜ë„ ìˆë‹¤.\n\n$$\n\\begin{align*}\nE[\\log_{2}(({1 \\over p_{X}(x)})] &= \\sum_{x \\in X}p_{X}(x)\\log_{2}({1 \\over p_{X}(x)}) \\\\\n&= -\\sum_{x \\in X}p_{X}(x)\\log_{2}(p_{X}(x)) \\\\\n&= H(p_{X})\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” $\\log_{2}({1 \\over p_{X}(x)})$ì„ **ì •ë³´ëŸ‰**ì´ë¼ê³  ì •ì˜í•œë‹¤. ì‹ì—ì„œë„ ì•Œ ìˆ˜ ìˆì§€ë§Œ, ì •ë³´ëŸ‰ê³¼ í•´ë‹¹ ì •ë³´ëŸ‰ì„ ì–»ì„ í™•ë¥ ì€ ë°˜ë¹„ë¡€í•œë‹¤.\n\n> **Joint, Conditional Entropy**\n\nRandom Variableì´ ë‘ ê°œ ì´ìƒì¼ ë•Œ, ì´ë¥¼ ì ìš©í•  ìˆ˜ ìˆë‹¤. ìœ ë„ ê³¼ì •ì€ $H(X)$ê°€ Expectationê³¼ ì–´ë–¤ ê´€ê³„ì˜€ëŠ”ì§€ë¥¼ ë– ì˜¬ë ¤ ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤.\n\n- **Joint Entropy** : $H(X, Y) = -\\sum_{x \\in \\Omega_{X}}\\sum_{y \\in \\Omega_{Y}}p(x, y)\\log_{2}p(x, y)$\n- **Conditional Entropy** : $H(Y|X) = -\\sum_{x \\in \\Omega_{X}}\\sum_{y \\in \\Omega_{Y}}p(x, y)\\log_{2}p(y|x)$\n\n> **properties**\n\n1. **Chain Rule**  \n   $H(X, Y) = H(Y|X) + H(X)$  \n   $H(X, Y) = H(X|Y) + H(Y)$\n2. **Conditional Entropy's Maximum**  \n   $H(Y|X) \\leq H(Y)$  \n   ë…ë¦½ì¼ ë•Œ ê°™ì•„ì§€ë©° ê·¸ ì™¸ì—ëŠ” í•­ìƒ Conditionalì˜ Entropyê°€ ë” ë‚®ë‹¤. ì˜ë¯¸ë¥¼ ì´í•´í•˜ë©´ ì‰½ë‹¤. í•œ ë§ˆë””ë¡œ Xë¼ëŠ” ì •ë³´ê°€ Yë¼ëŠ” ì •ë³´ê°€ ë°œìƒí•  í™•ë¥ ì— ëŒ€í•œ í‹°ëŒì´ë¼ë„ì˜ íŒíŠ¸ë¥¼ ì¤€ë‹¤ë©´, í•´ë‹¹ ë¶ˆí™•ì‹¤ì„±ì€ ê°ì†Œí•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.\n3. **Joint Entropy's Maximum**  \n   $H(X, Y) \\leq H(X) + H(Y)$  \n   ë™ì¼í•˜ê²Œ ë…ë¦½ì¼ ë•Œ ê°™ì•„ì§€ë©°, ê·¸ ì™¸ì—ëŠ” í•­ìƒ Jointì˜ Entropyê°€ ë” ë‚®ë‹¤. ì´ ë˜í•œ, ë‘ ì‚¬ê±´ì´ í‹°ëŒì´ë¼ë„ ê²¹ì¹˜ëŠ” Eventê°€ ìˆë‹¤ë©´, ê° Entropyë¥¼ ë”í•˜ëŠ” ê²ƒë³´ë‹¤ ë‹¹ì—°íˆ ì‘ì„ ìˆ˜ ë°–ì— ì—†ëŠ” ê²ƒì´ë‹¤.\n4. **Concave**  \n   Entropyì˜ ê·¸ë˜í”„ëŠ” í•­ìƒ concaveí•˜ë‹¤.\n\n> **Coding**\n\nì •ë³´ ì´ë¡ ì´ í™œë°œí•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ì˜ˆì‹œ ì¤‘ì— í•˜ë‚˜ê°€ ë°”ë¡œ ë°ì´í„°ì˜ Encoding/Decodingì„ ìˆ˜í–‰í•˜ì—¬ bit dataë¡œ mappingí•  ë•Œì´ë‹¤. variable length encodingì„ ì•Œê³  ìˆë‹¤ë©´, ì´ì— ëŒ€í•œ ì´í•´ê°€ ì‰¬ìš¸ ê²ƒì´ë‹¤. ì‰½ê²Œ ì„¤ëª…í•˜ë©´, ë°ì´í„°ë¥¼ bit sequenceë¡œ mappingí•  ë•Œ ëª¨ë“  ë°ì´í„°ì—ê²Œ ë™ì¼í•œ bit sequenceì˜ ê¸¸ì´ë§Œí¼ì„ í• ë‹¹í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ë¹ˆë„ê°€ ë†’ì€ ë°ì´í„°ë¶€í„° ì§§ì€ bit sequence ê¸¸ì´ë¥¼ í• ë‹¹í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ë•Œ bit sequenceì˜ ê¸¸ì´ë¥¼ Entropyë¥¼ ì´ìš©í•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤. ì´ ê¸¸ì´ëŠ” í•­ìƒ í•´ë‹¹ ë°ì´í„°ì˜ Entropyë³´ë‹¤ëŠ” ì»¤ì•¼ í•œë‹¤. ë”°ë¼ì„œ, í•´ë‹¹ Entropyë³´ë‹¤ í° ê°€ì¥ ì‘ì€ ìì—°ìˆ˜ê°€ í•´ë‹¹ ë°ì´í„°ì˜ Bit Sequence ê¸¸ì´ì˜ ìµœì ê°’ì´ë‹¤.\n\n### KL divergence(Relative Entropy)\n\nKullback-Leibler Divergenceì˜ ì•½ìë¡œ, ìš°ë¦¬ê°€ êµ¬í•˜ê³ ìí•˜ëŠ” ì‹¤ì œ í™•ë¥ (p)ê³¼ ì¶”ì¸¡ í™•ë¥ (q) ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì§€í‘œì´ë‹¤. ë”°ë¼ì„œ, ë™ì¼í•œ Sample Spaceì™€ Random Variableì— ëŒ€í•œ ì„œë¡œ ë‹¤ë¥¸ í™•ë¥  ë¶„í¬ë¥¼ ë¹„êµí•œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.\n\n$$\nD(p||q) = KL(p||q) = \\sum_{x \\in \\Omega}p(x)\\log_{2}(p(x)/q(x)) = E_{p}[\\log_{2}({p(x) \\over q(x)})]\n$$\n\nì•„ì‰½ê²Œë„ KL divergenceëŠ” ê±°ë¦¬ì™€ ê°™ì€ ì—°ì‚°ì„ ì ìš©í•  ìˆ˜ ì—†ë‹¤. ì¦‰, ë‘˜ ì‚¬ì´ì˜ ì—­ì—°ì‚°ì€ ê°™ì§€ ì•Šì„ ë¿ë§Œ ì•„ë‹ˆë¼($D(p||q) \\neq D(q||p)$), ì„œë¡œ ë‹¤ë¥¸ Random Variableì˜ KL divergenceì˜ í•©ì´ Random Variableì˜ í•©ì˜ KL divergenceì™€ëŠ” ë‹¤ë¥´ë‹¤.\n\n### Mutual Information\n\nKL divergenceë¥¼ í™œìš©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ Random Variable X,Yì˜ ì—°ê´€ì„±ì„ ìœ ì¶”í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤.\n\n$$\nI(X, Y) = D(p(x,y) || p(x)p(y))\n$$\n\n$I$ê°’ì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ Yë¥¼ ì•„ëŠ” ê²ƒì´ Xì˜ ì¶”ì¸¡ì„ ì–¼ë§ˆë‚˜ ì‰½ê²Œí•˜ëŠ”ì§€ì— ëŒ€í•œ ì§€í‘œë¡œ ì‘ìš©í•œë‹¤.\n\nì¦ëª… ê³¼ì •ì€ ìƒëµí•˜ì§€ë§Œ, ìœ„ì˜ ì‹ì„ ì •ë¦¬í•˜ë©´ ê²°êµ­ì€ ì•„ë˜ì™€ ê°™ì•„ì§€ê¸° ë•Œë¬¸ì´ë‹¤.\n\n$$\n\\begin{align*}\n  I(X, Y) &= H(X) - H(X|Y) \\\\\n  &= H(Y) - H(Y|X)\n\\end{align*}\n$$\n\n### Cross Entropy\n\nìš°ë¦¬ê°€ íŠ¹ì • corpus(dataset)ë¥¼ í†µí•´ì„œ í™•ë¥ ì„ ì¶”ì •í–ˆë‹¤ê³  í•´ë³´ì. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ì´ ê°€ì„¤ì„ ì¦ëª…í•˜ê¸° ìœ„í•´ì„œ ë‹¤ë¥¸ dataì— ëŒ€í•´ì„œ í•´ë‹¹ ì¶”ì¸¡ì´ ì ì ˆí•œì§€ë¥¼ í™•ì¸í•˜ì—¬ ì •ë‹¹ì„±ì„ ì¦ëª…í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œ ìš°ë¦¬ê°€ ë§Œë“  í™•ë¥ ì—ì„œ ì •ë³´ëŸ‰ì„ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” dataì˜ ê³µê°„ì— ë„£ì—ˆì„ ë•Œì˜ í™•ë¥ ì„ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œ Cross Entropyë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n\n$$\nH_{q}(p) = \\sum_{x \\in \\Omega}q(x)\\log_{2}{1 \\over p(x)}\n$$\n\nê°„ë‹¨í•˜ê²Œ ìš”ì•½í•˜ë©´, ìœ„ ì‹ì€ í‹€ë¦´ ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ê°–ê³  êµ¬í•œ ìµœì ì˜ Entropyë¡œ, ì •ë³´ëŸ‰ì— ì¶”ì¸¡ ì •ë³´ëŸ‰ì„ ë„£ê³ , í™•ë¥ ì—ëŠ” ì‹¤ì œ í™•ë¥ ì„ ë„£ëŠ” ë°©ì‹ì´ë‹¤.\n\në˜í•œ, ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•ë˜ê¸°ë„ í•œë‹¤.\n\n$$\nH_{q}(p) = H(q) + D(q || p)\n$$\n\në˜í•œ, íŠ¹ì • ì¡°ê±´ì´ ì£¼ì–´ì¡Œì„ ë•Œì˜ Conditional Cross EntropyëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\nH_{q}(p) = - \\sum_{y \\in \\Omega_{Y}}\\sum_{x \\in \\Omega_{X}}q(y,x)\\log_{2}p(y|x)\n$$\n\ní•˜ì§€ë§Œ, ì‹¤ì œ êµ¬í˜„ levelì—ì„œëŠ” ì´ëŸ¬í•œ Cross Entropyë¥¼ ì •ì„ìœ¼ë¡œ êµ¬í•˜ëŠ” ê²ƒì€ ë¹„ìš©ì´ í¬ë‹¤. ë”°ë¼ì„œ, ì´ì™€ ê·¼ì‚¬í•˜ëŠ” ì‹ì„ ì‚¬ìš©í•œë‹¤.\n\n$$\n\\begin{align*}\n  H_{q}(p) &= - \\sum_{y \\in \\Omega_{Y}}\\sum_{x \\in \\Omega_{X}}q(y,x)\\log_{2}p(y|x) \\\\\n  &= {1\\over{|T_{Y}|}}\\sum_{i=1..|T_{Y}|}{\\log_{2}p(y_{i}|x_{i})}\n\\end{align*}\n$$\n\nìœ„ ì‹ì„ ì´ìš©í•  ë•Œì—ëŠ” ì‹¤ì œ ë°ì´í„°ì™€ ë¹„êµì— ì‚¬ìš©í•´ì„œëŠ” ì•ˆëœë‹¤. ëŒ€ì‹  ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ p,qê°€ ìˆì„ ë•Œ, së¼ëŠ” ì‹¤ì œ ë¶„í¬ì— ì–´ë–¤ ê²ƒì´ ë” ì ì ˆí•œì§€ë¥¼ íŒëª…í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n","slug":"ml-base-knowledge","date":"2022-10-14 19:28","title":"[ML] 0. Base Knowledge","category":"AI","tags":["ML","Probability","Calculus","InformationTheory"],"desc":"Machine Learningì€ dataë“¤ë¡œ ë¶€í„° íŠ¹ì • patternì„ ë‚˜íƒ€ë‚´ëŠ” functionì„ ë§Œë“œëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì¦‰, patternì€ dataì— ëŒ€í•œ ê°„ë‹¨í•œ ìš”ì•½ë³¸ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.í™•ë¥ /í†µê³„ ì´ë¡  ë° ì„ í˜•ëŒ€ìˆ˜, ë¯¸ì ë¶„, ì •ë³´ ì´ë¡  ê´€ë ¨ ê¸°ë³¸ ë‚´ìš©ì„ í•´ë‹¹ í¬ìŠ¤íŒ…ì— ì •ë¦¬í•œë‹¤. ì—¬ê¸°ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ëŒ€ê²Œ ë§ì´ ì¶”ìƒì ì¸ ë‚´ìš©ì´ë©°, í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ ë‚´ìš©ì´ë‹¤. ë§Œì•½, ì¶”ê°€ì ì¸ ì„¤ëª…ì´ í•„ìš”í•˜ë‹¤ë©´ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë” ê²€ìƒ‰ì„ í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"AI"}},"__N_SSG":true}