{"pageProps":{"post":{"content":"\n## Reference\n\n- [🔗 Docker Deep Dive](https://www.oreilly.com/library/view/docker-deep-dive/9781800565135/), Nigel Poulton\n\n## Intro\n\n여태까지 docker의 driver를 통한 networking 기술을 알아보았고, 이제 libnetwork로 1/3에서 제시했던 기본 routing과 같은 기능 외에 구현되어 있는 기능들에 대해서 알아봅니다.\n\n- service discovery\n- load balancing\n\n### Service discovery\n\n모든 container들과 swarm의 서비스들이 이름을 통해서 각 각을 찾을 수 있도록 하는 것이다. Docker는 자체적으로 내부의 DNS 서버를 이용하여 이를 수행한다. 과정을 요약하자면 다음과 같다.\n\n1. container가 이름을 통해서 특정 container를 찾아야 함을 인식한다.\n2. 먼저 Local 내부에서 이에 대한 정보를 갖고 있는지를 탐색한다. -> 있다면, 종료\n3. Docker DNS server에 이를 요청하는 query를 전송한다.\n4. Docker DNS server는 모든 container의 name과 network alias(별칭)를 알기 때문에 이를 찾을 수 있다.\n5. 이때, DNS server는 먼저 동일한 network에 해당 container가 존재하는지를 확인한다. -> 없다면, 외부 DNS server로\n6. 존재한다면, 이를 요청을 보낸 resolver에게 전달하고, 이게 다시 container로 전달된다.\n\n### Load balancing\n\ndocker swarm은 기본적인 load balancer를 지원하여, 아래 그림과 같이 구현되어진다.\n\n```bash\n$ docker service create \\\n  --name my-web \\\n  --publish published=8080,target=80 \\\n  --replicas 2 \\\n  nginx\n```\n\n![docker-ingress-network](/images/docker-ingress-network.png)\n\n즉, 어디로 요청을 보낸다고, 할지라도 load balancer는 어디에 해당 서비스가 존재하는지를 파악하고, 이를 전달하는 것이 가능해진다. 따라서, 어느 노드로 요청을 보내더라도 정상적으로 요청이 전달될 수 있는 것이다. 이를 Ingress load balancing이라고 부른다.\n\n만약, 특정 node로 전달된 요청은 해당 node에 있는 container로 전달되기를 바란다면, host모드를 이용하여 진행할 수도 있다.\n\n여기까지가 network에 대한 전반적이 내용입니다.\n","slug":"docker-network-3","date":"2021-07-11 00:40","title":"[Docker] Network(3)","category":"Tech","tags":["Docker","Container","Network"],"thumbnailSrc":"https://euidong.github.io/images/docker-picture.jpg"},"relatedPosts":[{"content":"\n## 출처\n\n- [🔗 just-containers/s6-overlay](https://github.com/just-containers/s6-overlay)\n\n## Intro\n\ndocker에 기본 이념은 하나의 container에는 하나의 process만 두어야 한다는 것이다. 하지만, 이에 대해서 반대를 하고, 하나의 container에 여러 개의 process를 심고 이를 이용하겠다는 생각으로 만들어진 open source이다. 사용 예시는 main process 내부에 cronjob을 끼워넣는 것과 같은 경우가 있을 것이다.\n\n이는 결론적으로 Dockerfile의 확장 버전이라고 볼 수도 있을 거 같다. 좀 더 복잡한 작업을 더 체계적으로 할 수 있는 틀을 제공한다. 아이디어 자체는 참신하나 남발하게 되면, stateless하던 container가 점점 stateful하게 되면서 시스템이 오염될 수도 있음을 유의하자.\n\n아래 내용은 해당 opensource의 README를 직접 번역한 내용이니 의역도 많이 포함된다. 주의해서 읽도록 하자. 또한, 모르는 용어는 아래 Terminology를 확인해보도록 하자.\n\n## Goals\n\n- 이미지 제작자가 쉽게 s6(s6는 process를 감독, 관리, logging, 초기화하는 기능들의 집합)를 활용할 수 있도록 지원\n- 다른 docker image들처럼 동일하게 작동\n\n## Features\n\n- cont-init.d →초기화 작업과 같은 end-user 실행 작업을 허용하는 간단한 초기 작업\n- cont-finish.d → 마무리 작업\n- fix-attrs.d → ownership 권한을 수정\n- s6-overlay는 적당한 PID 1 기능을 제공한다.\n  - container에 걸리는 zombie process를 가지지 않고, 적당한 절차에 따라 제거될 것이다.\n- **여러 개의 process를 하나의 container에서 작동시키는 것이 가능하다.**\n- **\"The Docker Way\"(아래에서 설명)에 따라 작동시키는 것이 가능하다.**\n- 모든 기반 이미지(Ubuntu, CentOS, Fedora, 심지어는 Busybox)를 사용하는 것이 가능하다.\n- 이미지의 layer의 수를 작게 유지하기 위하여 하나의 tar, gz 파일로 배포한다.\n- s6 와 s6-portable-utils 는 손쉽고 작성이 쉬운 유틸리티 전체를 포함한다.\n- 비밀리에 s6-log 를 사용하는 logutil-service는 오래된 로그의 순환을 수행한다.\n- 특정 유저로 전체 process tree를 동작시키기 위하여, Docker의 USER 지침에 대하여 지원한다. 모든 feature에 대하여 지원하는 것은 아니라는 것을 알아두어야 한다. 자세한 사항은 여기서 확인가능하다.([🔗 notes](https://github.com/just-containers/s6-overlay#notes))\n\n## The Docker Way\n\n자주 강조되어지는 Docker의 신념은 \"하나의 container에 하나의 process만 두어야 한다.\"는 것이다. 그러나, S6 overlay 팀은 이에 대하여 동의하지 않는다. 하나의 container에 여러 process를 동작시키는 것에 대한 본질적 문제는 없다. 더 추상적으로 \"하나의 container에 하나의 thing만 두어야 한다.\"는 것이 해당 프로젝트의 목적이다. → 하나의 container는 하나의 것만 수행할 수 있다. 예로써, chatting service 또는 gitlab의 동작을 들 수 있다. 이것들은 여러 개의 process를 포함하지만, 하나의 thing이다. 따라서, 올바르다는 것이다.\n\n이미지 제작자가 process supervisor를 피하는 이유는 하나의 process supervisor가 실패한 서비스를 다시 시작해야 한다고 생각하기 때문입니다. 이로 인해서 결국 Docker container는 절대 죽지 않을 것이다. 이러한 container가 죽지 않는 현상은 docker 생태계를 파괴할 것이다. 대부분의 이미지는 에러가 발생했을 때, 중단을 요청하는 하나의 process를 동작시킨다. 에러에 대해서 종료를 수행함으로써, 시스템 관리자는 실패를 원하는대로 다루는 것을 허락받는다. 만약 이미지를 절대 종료시키지 않는다면, 에러 회복과 실패 알림에 대한 대안책이 필요하다. 즉, container를 실패에도, 종료하지 않는 것은 매우 위험하다.\n\nS6 overlay 팀의 정책은 만약 thing이 실패한다면, 그때 container가 반드시 실패햐야 한다는 것이다. 우리는 어떤 process들을 다시 동작시킬 수 있을지와 어떤 container를 끌어내릴지를 결정한다. 예를 들어, cron 이나 syslog에서 실패가 발생했을 때, container는 부정적 영향 없이 이것을 재시작하는 것이 가능할 것이다. 그러나 만약, ejabberd 가 실패했을 경우에는 container는 종료될 것이고, 이를 통해서 시스템 관리자는 이에 대한 대책을 수행할 수 있을 것이다.\n\n따라서, S6 team에서 생각하는 \"The Docker Way\"란 다음과 같다.\n\n- Container는 반드시 하나의 thing만 수행해야 한다.\n- Container는 thing이 멈춘다면, 반드시 멈춰야 한다.\n\n그리고, 우리의 초기 시스템은 이를 위해서 설계되었다. 이미지는 여전히 다른 Docker 이미지처럼 동작할 것이고, 이미 존재하는 이미지들의 생태계와 함께 어우러질 것이다.\n\n---\n\n## Init stages\n\nS6 overlay init은 container화된 환경에 적당하게 동작하기 위해 적당하게 맞춤화한 프로그램이다. 해당 section에서는 어떻게 stage들이 동작하는지 간단히 설명한다. 만약 더 자세한 사항이 궁금하다면, 다음 article을 읽기를 추천한다. ([How to run s6-svscan as process 1](http://skarnet.org/software/s6/s6-svscan-1.html))\n\n- **stage 1 :** 해당 단계의 목적은 두 번째 단계에 진입하기 위해서 이미지를 준비하는 것이다. 다른 것들 사이에서, 이것은 container 환경변수들을 준비하는 것과 s6 가 효과적으로 시작될 때까지 두 번째 stage의 시작을 막는 것에 대한 책임이 있다.\n- **stage 2 :** end-user가 제공한 대부분의 파일들이 수행되어지는 단계이다.\n  1. /etc/fix-attrs.d를 사용하여, 소유권과 권한을 고정한다.\n  2. /etc/cont-init.d에 기술된 초기화 script를 실행시킨다.\n  3. /etc/services.d에 적힌 user service들을 s6가 supervision을 동작 중인 폴더에 복사하고, signal을 보냄으로써 적절하게 supervising을 시작할 수 있다.\n- **stage 3 :** 해당 단계는 종료 단계이다. 이는 다음과 같은 동작을 수행한다.\n  1. TERM signal을 모든 관리 중인 service에게 전송한다.\n  2. /etc/cont-init.d에 포함된 종료 scripts를 수행한다. 이는 서비스가 여전히 종료되어지는 중에도 종료되어질 수 있다.\n  3. 모든 service가 종료되기를 기다린다.(S6\\_SERVICES\\_GRACETIME milliseconds를 넘지 않는 선에서 - default 3000)\n  4. 모든 process에게 TERM signal을 보낸다.\n  5. S6\\_KILL\\_GRACETIME milliseconds(default 3000)만큼 sleep을 수행한다.\n  6. 모든 process에게 KILL signal을 전송한다.\n\n## Usage\n\n해당 project는 표준 tar, gz으로 배포되어졌다. 이를 사용하기 위해서는 image의 root에 이를 추출하고, ENTRYPOINT에 /init 을 기입해주면 된다. (만약, 기존 ENTRYPOINT가 있다면, 이를 cont-init으로 옮겨주어야 할 것이다. 또는 S6\\_CMD\\_ARG0를 이용하면 된다.)\n\n여기서, 해당 project는 wget 또는 curl을 수행할 때, Docker의 RUN보다 ADD 지시어를 사용할 것을 추천한다. (왜냐하면, 이를 이용하면, Docker가 https URL을 다룰 수 있다.)\n\n여기서부터, 서비스를 작성할 때에는 두 쌍의 선택지를 갖게 된다.\n\n> **1. image의 CMD 를 이용하여 service/program을 실행시킨다.**\n\n```Dockerfile\nFROM busybox\nADD https://github.com/just-containers/s6-overlay/releases/download/v1.21.8.0/s6-overlay-amd64.tar.gz /tmp/\nRUN gunzip -c /tmp/s6-overlay-amd64.tar.gz | tar -xf - -C /\nENTRYPOINT [\"/init\"]\n```\n\n```bash\n# run\ndocker-host $ docker build -t s6demo .\ndocker-host $ docker run -ti s6demo /bin/sh\n[fix-attrs.d] applying owners & permissions fixes...\n[fix-attrs.d] 00-runscripts: applying... \n[fix-attrs.d] 00-runscripts: exited 0.\n[fix-attrs.d] done.\n[cont-init.d] executing container initialization scripts...\n[cont-init.d] done.\n[services.d] starting services\n[services.d] done.\n\n# ps\ndocker-host $ docker ps\nPID   USER     COMMAND\n    1 root     s6-svscan -t0 /var/run/s6/services\n    21 root     foreground  if   /etc/s6/init/init-stage2-redirfd   foreground    if     s6-echo     [fix-attrs.d] applying owners & permissions fixes.\n    22 root     s6-supervise s6-fdholderd\n    23 root     s6-supervise s6-svscan-log\n    24 nobody   s6-log -bp -- t /var/log/s6-uncaught-logs\n    28 root     foreground  s6-setsid  -gq  --  with-contenv  /bin/sh  import -u ? if  s6-echo  --  /bin/sh exited ${?}  foreground  s6-svscanctl  -t\n    73 root     /bin/sh\n    76 root     ps\n\n# exit\n/bin/sh exited 0\ndocker-host $\n```\n\n> **2. s6-overlay를 활용하는 쉬운 방법이라고 할 수 있다.**\n\nDockerfile이 build할 때, 작성하거나 runtime에 command line으로 입력이 가능하다. 이것은 s6 supervisor에 의해서 실행되어질 것이다. 그리고 실패나 종료 시에 해당 container는 종료되어질 것이다. interactive program도 s6 supervisor 하위에서도 동작시킬 수 있다. service script를 작성하여 실행시킨다.\n\n`/etc/services.d/myapp/run`\n\n```shell\n  !/usr/bin/execlineb -P\n  nginx -g \"daemon off;\"\n```\n\n관리되는 서비스를 제작하는 것은 단지 /etc/services.d에 service directory를 만들고, 장기간 존재할 process 실행에 대한 내용을 적은 run 파일을 이 안에 만드는 것보다 더 쉬워질 수 없다. 이거면 다다. 만약 s6 supervision에 대한 더 많은 내용을 알기를 원한다면 다음 문서를 살펴보아라. (\\[servicedir\\](<[http://skarnet.org/software/s6/servicedir.html](http://skarnet.org/software/s6/servicedir.html)\\>))\n\n## 소유권 및 권한 고정\n\n때때로, 진행 전에 소유권과 권한을 고정하는 것이 필요할 때가 있다. 대표적인 예시가 container 내부에 host folder와 mount된 folder가 있을 때이다. overlay는 /etc/fix-attrs.d의 파일을 사용하여 이를 헤쳐나갈 방법을 제공한다.\n\n- Format\n  - path : File 또는 Directory의 경로\n  - recurse : folder가 발견되었다면, 해당 folder 내부의 내용도 포함할지를 결정합니다. (true or false)\n  - account : target의 account이다. account를 찾을 수 없을 경우 default로 예비 uid:gid(user id, group id)를 사용할 수 있다. 예를들어, nobody, 32768:32768 이라고 입력할 경우, nobody account를 첫번째로 사용하기 위한 시도를 하고, 예비로 uid가 32768인 대상을 예비로 찾는다. 예를들어, daemon 의 계정이 UID=2이고, GID=2인 경우 다음과 같은 account 도 사용할 수 있다.\n    - daemon: UID=2 GID=2\n    - daemon,3:4: UID=2 GID=2\n    - 2:2,3:4: UID=2 GID=2\n    - daemon:11111,3:4: UID=2 GID=11111\n    - 11111:daemon,3:4: UID=11111 GID=2\n    - daemon:daemon,3:4: UID=2 GID=2\n    - daemon:unexisting,3:4: UID=2 GID=4\n    - unexisting:daemon,3:4: UID=3 GID=2\n    - 11111:11111,3:4: UID=11111 GID=11111\n  - fmode : target file의 mode → example 0644\n  - dmode : target directory의 mode → example 0755\n- path recurse account fmode dmode\n\n> **Example**\n\n`/etc/fix-attrs.d/01-mysql-data-dir`\n\n```shell\n/var/lib/mysql true mysql 0600 0700\n```\n\n`/etc/fix-attrs.d/02-mysql-log-dirs`\n\n```shell\n/var/log/mysql-error-logs true nobody,32768:32768 0644 2700\n/var/log/mysql-general-logs true nobody,32768:32768 0644 2700\n/var/log/mysql-slow-query-logs true nobody,32768:32768 0644 2700\n```\n\n## 초기화 작업 실행하기\n\n`/etc/fix-attrs.d`에 따라 속성을 고정하는 작업을 수행한 후, `/etc/services.d`에 적힌 user에게 제공되는 서비스를 시작하기 전에, overlay는 /etc/cont-init.d 에서 발견된 모든 script를 실행시킨다.\n\n`/etc/cont-init.d/02-confd-onetime`\n\n```shell\n#!/usr/bin/execlineb -P\n\nwith-contenv\ns6-envuidgid nginx\nmultisubstitute\n{\n  import -u -D0 UID\n  import -u -D0 GID\n  import -u CONFD_PREFIX\n  define CONFD_CHECK_CMD \"/usr/sbin/nginx -t -c {{ .src }}\"\n}\nconfd --onetime --prefix=\"${CONFD_PREFIX}\" --tmpl-uid=\"${UID}\" --tmpl-gid=\"${GID}\" --tmpl-src=\"/etc/nginx/nginx.conf.tmpl\" --tmpl-dest=\"/etc/nginx/nginx.conf\" --tmpl-check-cmd=\"${CONFD_CHECK_CMD}\" etcd\n```\n\n## 부가적인 종료 작업 작성하기\n\n기본적으로, /etc/services.d 에 의해 생성된 서비스는 자동적으로 재시작된다. 만약, 서비스가 container를 down 시켜야한다면, finish script를 통해서 이를 수행할 수 있다.\n\n`/etc/services.d/myapp/finish`\n\n```shell\n#!/usr/bin/execlineb -S0\n\ns6-svscanctl -t /var/run/s6/services\n```\n\n더 발전된 기능을 사용할 수도 있다.\n\n`/etc/services.d/myapp/finish`\n\n```shell\n#!/usr/bin/execlineb -S1\nif { s6-test ${1} -ne 0 }\nif { s6-test ${1} -ne 256 }\n\ns6-svscanctl -t /var/run/s6/services\n```\n\n## Logging\n\nS6 overlay는 즉시 \\[s6-log\\](<[http://skarnet.org/software/s6/s6-log.html](http://skarnet.org/software/s6/s6-log.html)\\>)를 통한 logging mechnism으로 쉽게 logging을 관리하는 방법을 제공한다.\n\n또한, logging을 하나의 바이너리 호출로 만들 수 있도록 logutil-service라는 도움 장치를 제공한다.\n\n이는 다음 순서에 따라 진행된다.\n\n- s6-log가 어떻게 S6\\_LOGGING\\_SCRIPT에 적힌 logging script를 읽을지를 조회합니다.\n- nobody user의 root 권한을 삭제한다.(만약, 존재하지 않는다면, 기본적으로 32768:32768 에게 넘겨집니다.)\n- 모든 환경 변수를 지웁니다.\n- s6-log를 실행함으로써 logging을 시작합니다.\n\n> **주의사항**\n\n- 권한이 자동적으로 삭제된 이후로, s6-setuidgid로 user를 변경할 필요가 없다.\n- 둘 중 하나의 내용을 log folder에서 보장해야 한다.\n  1. 존재한다면, nobody user에 의해 작성이 가능해야 한다.\n  2. 존재하지 않는다면, 상위 폴더가 nobody user에 의해 작성이 가능해야 한다.\n\nlog foder는 cont-init.d script에서 또는 run script 안에서 생성하는 것이 가능하다.\n\n### Example\n\n> **1. cont-inid.d를 활용한 방법**\n\n`/etc/cont-init.d/myapp-logfolder`\n\n```shell\n#!/bin/sh\nmkdir -p /var/log/myapp\nchown nobody:nogroup /var/log/myapp\n```\n\n> **2.  run script를 활용한 방법**\n\n`/etc/services.d/myapp/log/run`\n\n```shell\n#!/bin/sh\n# input stdin을 기반으로 하는 logging\nexec logutil-service /var/log/myapp\n\n#!/bin/sh\n# fifo에 따른 log를 쌓기를 원한다면, 다음과 같이 수행하는 것도 가능하다.\nexec logutil-service -f /var/run/myfifo /var/log/myapp\n```\n\n## 권한 삭제\n\n서비스 실행이 다가오면, 실행 전에 권한을 부여하는 것은 서비스이건 logging 서비스이건 매우 중요한 작업이다. s6는 이미 이러한 작업을 위한 기능을 포함하고 있다.\n\n> **In execline**\n\n```shell\n#!/usr/bin/execlineb -P\ns6-setuidgid daemon\nmyservice\n```\n\n> **In sh**\n\n```shell\n#!/bin/sh\nexec s6-setuidgid daemon myservice\n```\n\n만약 이러한 기능에 대하여 더 알고 싶다면, 다음 문서들을 살펴 보아라. [s6-setuidgid](http://skarnet.org/software/s6/s6-setuidgid.html),  [s6-envuidgid](http://skarnet.org/software/s6/s6-envuidgid.html), [s6-applyuidgid](http://skarnet.org/software/s6/s6-applyuidgid.html)\n\n## Container 환경\n\n만약 container 환경을 제공하기 위해서 직접 만든 script를 원한다면, with-contenv를 통해서 이를 수행하는 것이 가능하다.\n\n> **/etc/cont-init.d/01-contenv-example**\n\n```shell\n#!/usr/bin/with-contenv sh\necho $MYENV\n```\n\n## Read-Only Root 파일 시스템\n\n최근 dokcer의 버전에서 read-only 파일 시스템으로 container를 동작시키는 것을 허용하였다. 2단계 과정에서, overlay는 사용자가 제공하는 cont-init.d 의 권한을 변경하는 부가작업을 수행한다. 만약, root 파일 시스템이 read-only라면, S6\\_READ\\_ONLY\\_ROOT=1 라는 설정을 해주어 stage 2에서 이를 알 수 있도록 해야 한다. 이를 통해서 permission을 변경하기 이전에, /var/run/s6 에 사용자의 파일을 복사하게 된다.\n\n이는 /var 가 수정이 가능한 권한을 갖게된다는 것이고, 이는 tmpfs 라는 파일시스템에 의해서 가능하다.\n\n→ 다음과 같이 사용하면, 수행이 가능하다.\n\n```bash\n$ docker run -e S6_READ_ONLY_ROOT=1 --read-only --tmpfs /var:rw,exec [image name]\n```\n\n> **주의사항**\n\n만약 S6\\_READ\\_ONLY\\_ROOT=1 를 사용할 때, fix-attrs.d, cont-init.d, cont-finish.d, services.d의 symbol link를 유의해야 한다. s6의 제한사항 때문에, 앞 선 디렉토리가 /var/run/s6에 복사되며 symbol link가 실행되어 예기치 않은 중복이 발생한다.\n\n### **s6 동작 사용자화**\n\ns6의 동작을 이미 정의된 환경 변수를 설정함으로써 실행단계에서 조정하는 것이 가능하다.\n\n- S6\\_KEEP\\_ENV (default = 0): 만약 설정이 되면, 환경과 전체 관리 과정이 바라보는 원본 환경변수는 reset되지 않는다. 이는 with-contenv를 무의미하게 바꿔버린다.\n- S6\\_LOGGING (default = 0):\n  - **0**: 모든 Output이 stdout/stderr로 전달된다.\n  - **1**: 내부의 catch-all logger를 사용하여, 지속적으로 이것에 전송한다. 이는 /var/log/s6-uncaught-logs에 위치한다. 그래도 CMD에서는 stdout/stderr를 통해 전달한다.\n  - **2**: 내부의 catch-all logger를 사용하여, 지속적으로 이것에 전송한다. 이 과정에 CMD도 포함된다. 따라서, stdout/stderr로 쓰여지는 것은 아무것도 없다.\n- S6\\_BEHAVIOUR\\_IF\\_STAGE2\\_FAILS (default = 0):\n  - **0**: script(fix-attrs or cont-init)에서 에러가 발생했더라도 조용하게 지속한다.\n  - **1**: 에러 메세지에 대한 경고를 제공한 후에 지속한다.\n  - **2**: 관리 시스템에 종료 signal을 전송하면서 종료한다.\n- S6\\_KILL\\_FINISH\\_MAXTIME (default = 5000): /etc/cont-finish.d 의 script가 종료 signal을 받을 때까지 가질 수 있는 최대 대기 시간을 의미한다. 각 script가 수행될 때마다 수행된다.\n- S6\\_SERVICES\\_GRACETIME (default = 3000): 얼마나 s6 가 서비스가 TERM signal을 보낼 때까지 기달릴지를 의미합니다.\n- S6\\_KILL\\_GRACETIME (default = 3000): 얼마나 s6 가 zombie를 거두는데 기다릴지를 의미합니다. 시간이 지나면 KILL signal을 전송합니다.\n- S6\\_LOGGING\\_SCRIPT (default = \"n20 s1000000 T\"): 해당 변수는 어떻게 무엇을 logging할지를 결정한다. 기본적으로 ISO8601를 모든 line에 덧붙이고, 1mb에 도달하거나 20개 이상의 파일이 생성되면 rotation을 수행한다.\n- S6\\_CMD\\_ARG0 (default = not set): 해당 환경 변수의 값은 docker에 의해 전달된 CMD 인자에 덧붙여진다. 존재하는 이미지를 s6-overlay로 변경할 때, 이전에 사용했던 ENTRYPOINT의 값을 이곳에 전달함으로써 이를 사용하는 것이 가능하다.\n- S6\\_FIX\\_ATTRS\\_HIDDEN (default = 0): 어떻게 fix-attrs.d script가 파일과 directory를 처리할지를 제어한다.\n  - **0**: 숨겨진 파일과 directory를 제외한다.\n  - **1**: 모든 파일과 directory를 포함한다.\n- S6\\_CMD\\_WAIT\\_FOR\\_SERVICES\\_MAXTIME (default = 5000): CMD 실행을 지속하기 전에 기다리는 최대 시간을 의미한다.\n- S6\\_READ\\_ONLY\\_ROOT (default = 0): read-only 파일 시스템을 사용하는 container 내부에서 동작할 때, 1로 설정하여 초기화 scripts를 권한 설정이전에 /etc에서 /var/run/s6/etc 로 복사하도록 하는 방식이다. 자세한 사항은 다음을 참조([Read-Only Root Filesystem](https://github.com/just-containers/s6-overlay#read-only-root-filesystem))\n- S6\\_SYNC\\_DISKS (default = 0): 1로 설정하여 stage 3에서 container 종료 이전에 file 시스템의 sync를 맞추어야 함을 알린다.\n\n### **Terminology**\n\n- PID 1 : linux kernel에서 첫번째로 시작되어진 process에게 PID 1이 부여된다. PID 1은 다른 process들과는 달리 다음과 같은 특징일 갖는다.\n  - PID 1 process가 종료된다면, 모든 다른 process는 KILL signal로 종료된다.\n  - 자식 process를 가진 어떤 process라도 무슨 이유에서건 죽는다면, 자식들은 PID 1 process의 자식으로 다시 태어난다.\n- Zombie process : 실행이 종료되었지만, 아직 삭제되지 않은 process를 의미합니다. 이를 실제로 유지하는 것이 일반적인데, 그 이유는 부모 process가 자식 process의 종료 상태를 파악하기 위해서이다. 이러한 데이터는 실행한 명령의 결과에 따라서 분기를 하고 싶을 때, 종료 값은 유용하게 사용된다. 이렇게 zombie 상태로 진입한 process는 부모 process가 종료되거나 wait() 계열의 함수를 이용해서 process가 정리될 대까지 남아있게 된다. 만약, 이러한 zombie process가 다수 남아 있게 된다면 시스템에도 악영향을 미칠 수 있다.\n- process supervisor : 여러 process를 모니터링하고, 제어하는 program을 의미한다.\n- ejabberd : Robust, Scalable and Extensible Realtime PlatformXMPP Server + MQTT Broker + SIP Service\n- symbol link : soft link라고 불리기도 하며, 다른 파일을 가르키는 특별한 파일로 여길 수 있다. target 파일의 데이터를 포함하는 것이 아닌 단순히 파일 시스템의 다른 파일을 가르키고, 이를 통해 실행하는 것이 가능한 방법이다.\n- nobody user: 대다수의 Unix 시스템에서, \"nobody\"는 전통적으로 파일을 가지지 않고, 어느 권한 그룹에도 속하지 않으며, 다른 모든 유저들이 가지는 기능을 제외하면 기능이 없는 유저를 의미한다.\n","slug":"s6-overlay","date":"2021-07-11 12:13","title":"S6 Overlay","category":"Tech","tags":["Docker"],"thumbnailSrc":"https://euidong.github.io/images/docker-picture.jpg"},{"content":"\n## Reference\n\n- [🔗 Docker Deep Dive](https://www.oreilly.com/library/view/docker-deep-dive/9781800565135/), Nigel Poulton\n- Tumbnail : Photo by [Michael](https://unsplash.com/@michael75?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/cargo-ships?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n\n## Intro\n\nDocker Swarm을 docker stack을 이용하여 실행시키게 된다면, 무엇이 생성되는가? 우리는 서비스가 생성되기도 전에 network가 생성되는 것을 볼 수가 있다. container와 container간 그리고, host를 통해 외부 internet환경에 container를 연결 시키는 모든 과정을 알아보자.\n\nDocker를 사용하다보면, host와 통신을 위해 외부로 port를 열어주는 것과 container 간의 통신을 헷갈려 하는 사람들이 생각보다 많은 것 같다. 심지어는 container간 통신을 위해서 localhost로 정보를 주고받을려고 하는 몹쓸 시도를 하는 관경도 몇몇 봐왔다.\n\n따라서, 우리는 한 번 Docker의 network에 대해서 한 번 공부해보는 것이 좋을 것이다.\n\n해당 차시에서는 우선 전체적인 docker network를 설명하는 기본적인 키워드를 알아볼 것이고,\n\n2 차시에서는 주로 사용되는 docker network driver를 알아볼 것이고,\n\n3 차시에서는 libnetwork의 핵심 기능 중 service discovery, load balancing에 대해서 알아보겠다.\n\n## Docker Networking Base\n\n우리가 기억해야 할 것은 CNM, libnetwork, Driver 이렇게 3가지다. 각 각이 무엇인지는 차례차례 알아보자.\n\n### Container Network Model (CNM)\n\ncontainer간의 network를 구현하기 위한 design을 제시한 내용입니다. 따라서, idea일 뿐입니다. 자세한 내용은 하위 링크를 통해서 확인 가능합니다.\n\n[🔗 Github - moby/libnetwork](https://github.com/moby/libnetwork/blob/master/docs/design.md)\n\n하지만, 이를 좀 더 요약해봅시다. 일단 핵심 요소 3가지를 먼저 이해해봅시다.\n\n- **Sandbox** : 고립된 하나의 Network 공간을 의미합니다. 해당 공간에는 ehternet interface나 port 그리고 routing table같은 구현이 포함됩니다.\n- **Endpoints** : Virtual Network를 서로 연결하는 interface의 역할입니다. (veth라고도 불립니다.) CNM에서는 Sandbox 내부에서 이와 Network를 연결하는 역할을 합니다.\n- **Networks** : Virtual Switch로 여기면 됩니다. 이를 통해서 여러 개의 endpoints를 연결할 수 있습니다.\n\n자 이제 이렇게 3개의 네트워크를 정리하면, 이제 Container 내부에 Sandbox가 존재하고, 그 Sandbox 내부의 endpoints를 연결하는 Network를 통해서 결론적으로 Container 간의 연결을 수행하게 됩니다.\n\n![cnm](/images/cnm.jpeg)\n\n### libnetwork\n\n위에서 이야기한 것처럼 CNM은 단순히 idea일 뿐입니다. 이를 구현허여 표준화된 것이 바로 libnetwork라고 생각하면 됩니다. 이는 Go를 이용하여 작성된 open source로 위에서 제시한 링크를 통해서 해당 open source에 접근할 수 있습니다. 위에서 언급한 CNM을 구현하였고, 추가적으로 service discovery, ingress-based container load balancing, network control plane 및 management plane 기능을 구현하였다. 현재에는 docker에서 network 구현에 사용된다.\n\n\\* control & management plane : 직접적으로 network의 흐름을 제어하는 단계로, routing과 같은 제어를 수행한다.\n\n### Drivers\n\n즉, libnetwork가 전체적인 network의 control plane과 management plane 기능을 구현하였다면, driver는 data plane을 구현한다. 즉, 직접적으로 데이터를 전달하는 역할을 수행한다. 이러한 기능들은 docker에서 여러 개의 driver라는 submodule을 통해서 구현하였다. docker pub를 통해서 default보다 나아간 driver 역시 설치가 가능하다. 하지만 기본적으로, host, bridge, overlay, ipvlan, macvlan 등을 포함하고 있다.\n\n여기까지가 docker network에 대한 overview이다. 다음 차시에 계속...\n","slug":"docker-network-1","date":"2021-07-10 21:21","title":"[Docker] Network(1)","category":"Tech","tags":["Docker","Container","Network"],"thumbnailSrc":"https://euidong.github.io/images/docker-picture.jpg"},{"content":"\n## Reference\n\n- [🔗 Docker Deep Dive](https://www.oreilly.com/library/view/docker-deep-dive/9781800565135/), Nigel Poulton\n\n## Intro\n\n저번 글에 이어서 이번에는 docker network의 driver들에 대한 자세한 내용을 다루겠다.\n\n- bridge networks\n- overlay networks\n- host networking\n- IPVlan networks\n- MacVlan networks\n\n### Bridge Network\n\ncontainer간의 통신을 위해서 필요한 것이 bridge 네트워크이다. 하지만, 여기서 주의해야할 것은 오직, single host에서만 동작한다는 점이다. 즉, 다른 docker host에 존재하는 container와는 연결이 불가능하다.\n\n그렇다면, bridge가 무엇인가? 이는 두 개의 network 장치를 연결하는 L2 switch를 말한다. 즉, container를 연결하는 도구라고 보면 되겠다. 이를 통해서 연결된 container는 해당 container의 모든 port에 접근이 가능해진다.\n\n![docker-bridge-network](/images/docker-bridge-network.png)\n\n위는 `$ docker network ls`를 입력하면 기본적으로 볼 수 있는 내용이다. 위에 세개는 처음부터 끝까지 docker에 존재하는 default network입니다. host는 직접적으로 host에 연결하는 경우의 network이고,(후에 설명합니다.) none은 아무 네트워크에도 연결되지 않아 외부로 어떤 traffic도 보내지 않을 container들이 속하게 된다. 여기서 bridge는 default bridge라고 불리며, network를 설정하지 않고, container를 생성하게 되면 기본적으로 해당 bridge로 연결되게 된다. 이를 통해서 container 간의 연결도 구현하는 것이 가능하다.\n\n하지만, 일반적으로 단일 기기에서 container 간의 연결을 수행할 때에는 bridge를 직접 생성하여 연결하는 것이 일반적이다. (그 이유는 도메인 네임 설정을 자동으로 해준다는 점에서 이점이 있기 때문 -> [🔗 참고](https://docs.docker.com/network/bridge/#differences-between-user-defined-bridges-and-the-default-bridge))\n\n아래는 이를 이용한 간단한 예시이다.\n\n```bash\n# bridge 생성\n$ docker network create -d bridge eui_bridge\n\n# container 생성\n$ docker container run -d --name c1 \\\n  -network eui_bridge \\\n  alpine sleep 1d\n  \n# container2 생성\n$ docker container run -it --name c2 \\\n  -network eui_bridge \\\n  alpine sh\n   \n# ping을 통해 c1과 연결 여부 확인\n$ ping c1\n```\n\n위의 과정을 처음부터 설명하자면,\n\n1. eui\\_bridge라는 network를 bridge로 생성한다.\n2. container에 eui\\_bridge를 연결하고, alpine 이미지를 기반으로 생성한다. 이때 시작 시에 sleep을 하루 동안 시행한다.(sleep 하는 이유는 꺼지지 않게 하기 위함)\n3. 마찬가지로 eui\\_bridge에 연결하고, alpine 이미지로 container를 생성한 후에 shell을 실행시킨다.\n4. c2에서 실행된 shell에서 c1으로 ping을 전송한다. (이때 같은 network bridge끼리는 container name으로 domain이 생성된다.)\n\n참고로 여기서 기억해야할 것이 있다면, bridge는 container간의 연결을 위한 것이고, container의 특정 port를 host와 mapping하고자 할 때에는 `--publish` 를 활용해야 한다.\n\n```bash\n$ docker run -p 5000:80 nginx\n```\n\n이를 통해서 host의 5000번과 container의 80번 port를 연결할 수 있다.\n\n### Overlay Network\n\n위에서 설명한 것이 단일 호스트 내부에서 container 간의 연결이었다면, 여러 host가 존재하는 cluster 환경에서 docker의 container간 통신을 위한 driver가 overlay이다. 현재에는 docker swarm을 통해서 application을 여러 host에서 제공하는 경우에 사용하게 된다.\n\n먼저 원리를 알아보자면, VXLAN을 활용한다는 것이다. 이는 L3 network 상위에서 다른 두 기기 간에 L2 통신을 지원하는 것인데, 이를 통해서 우리는 다른 node간에 존재하는 container 끼리도 통신할 수 있도록 할 수 있다. docker swarm에 의해서 관리되어 L3로 연결된 두 node의 위에서는 VXLAN Tunnel EndPoint(VTEP)이 각 각 존재한다. 이들을 통해서, tunnel이 형성되고 통신이 가능해지는데, 기존에 container에 존재하고 있던 CNM에서 정의한 Sandbox 속에 virtual switch가 생성되고 이와 VTEP이 연결되어 다른 기기에 있는 container간에도 통신이 가능해지는 것이다.\n\n예시를 든다면, docker stack을 통해서 시스템을 구성해본 적이 있다면, container를 생성하는 과정에서 network가 먼저 생성되는 것을 확인할 수 있을 것이다. 이때 생성되는 것이 overlay 네트워크로 이를 통해서 여러 container가 replica가 어느 node에 생길지 확정할 수 없음에도 통신을 자유롭게 하는 것을 볼 수 있다.\n\n### Host Networking\n\n해당 방식은 docker를 한 번이라도 써본 사람이라면 다음 명령어는 익숙할 것이다.\n\n```bash\n$ docker run -p 80:80 nginx\n```\n\nnginx image를 기반으로 container를 실행시키고, container 내부의 80번과 host의 80번 port를 mapping하겠다는 것이다. 이를 통해서 container는 host의 network에 관여하는 것이 가능하다.\n\n하지만, host networking을 이용하게 되면 container 내부에 network stack이 생성되지 않고, 해당 container의 모든 network 설정이 해당 host의 설정에 그대로 mapping되는 것이다. 이를 이용하면 성능상의 이점은 있겠지만, 상당히 설정이 난잡해질 수 있다.\n\n### IPVlan Network\n\nMAC address와 IP adress를 부여하여, 실제 네트워크에 container를 직접 연결하는 방식이다.\n\n장점은 별도의 port forwarding이나 bridge를 사용하지 않으므로 당연히 빠르지만, NIC를 이용하기에 promiscuous mode를 open해야 한다는 단점이 있다. 이는 switch가 데이터를 전송할 대상을 찾지 않고, 연결된 모든 대상에게 보내는 모드로, sniffing에 취약하고 이 때문에 public cloud system에서는 이를 막아 놓기에 사용할 수 없다.\n\n![docker-ip-vlan](/images/docker-ip-vlan.png)\n여기까지 말했을 때, 이해했다면, 이미 설정하는 것을 알아보러 떠나면 될 것이고, 이해하지 못했다면, 아마 쓸 일이 없을 것이니 넘어가시면 될 것이다.\n\n자세한 사항은 공식 페이지를 참고하자.\n\n[🔗 IPvlan networks](https://docs.docker.com/network/ipvlan/)\n\n### MacVlan Network\n\nipvlan과 동일하지만 차이점은 MAC 주소를 할당한다는 점이다. 그 외에는 다를 것이 없다.\n\n[🔗 macvlan networks](https://docs.docker.com/network/macvlan/)\n","slug":"docker-network-2","date":"2021-07-11 00:04","title":"[Docker] Network(2)","category":"Tech","tags":["Docker","Container","Network"],"thumbnailSrc":"https://euidong.github.io/images/docker-picture.jpg"},{"content":"\n## Reference\n\n- [🔗 Docker Deep Dive](https://www.oreilly.com/library/view/docker-deep-dive/9781800565135/), Nigel Poulton\n\n## Intro\n\n해당 글은 Linux에서 docker를 동작시킨다는 가정하에 작성하였다. (Window도 대부분 동일하다고 한다.)\n\ndocker는 여러 개의 보안 정책을 포함한다.\n\n이를 크게 누가 관리하느냐에 따라서 두 개의 부류로 나눌 수 있다.\n\n1. OS system (Linux)\n2. Docker\n\n이전 가상화와 반가상화를 비교한 글에서 보았듯이 Container 기술을 결과적으로 반가상화에 해당하며, 이를 위해서 OS의 지원이 필요하다. 따라서, 이를 Linux 자체에서 구현해주는 것이 존재하고, Docker에서 Application 단에서 구현한 부분으로 나뉘어지는 것이다.\n\n먼저, Linux에서 지원하는 각종 security에 대해서 알아봅시다.\n\n## Linux's Security for Docker\n\n전체적인 디테일 사항은 정리하지 않는다. 해당 내용은 간단히 살펴보는 정도이다.\n\n### Namespaces\n\nnamespace는 container 기술에서 매우 핵심적인 위치에 존재한다고 할 수 있다. 이를 통해서, OS를 여러 개로 나누고, 마치 완전히 고립된 형태의 OS처럼 느끼도록 만든다. (키워드는 isolation) 그렇다면 하나의 host 내에서 어떻게 여러 개의 container가 완벽하게 독립되어 있다고 느낄 수 있게 할 수 있을까? 이는 다음과 같은 종류의 namespace를 분리함으로서 가능하다.\n\n- **process ID (pid)** : process는 tree 형태로 이루어지게 된다. 따라서, 하나의 process (즉, PID 1)에 의해서 여러 개의 process가 동작을 시작하는 것이다. 그런데, namespace를 통해서 우리는 여러 개의 완벽하게 독립적인 process tree를 구축하게 된다.\n- **Network (net)** : 각 각의 container마다 network stack을 구현한다. 즉, network interface 부터 시작해서, IP Address, port, routing table 등을 구축하게 되는 것이다.\n- **Filesystem / mount (mnt)** :모든 container가 각자의 root filesystem을 가지고, 다른 모든 container들은 이것에 접근할 수 없다.\n- **Inter process Communication(ipc)** : process간의 통신을 위해서 우리는 shared memory를 사용하게 되는데 이 또한 고립적으로 구현되도록 한다.\n- **User** : 각 container마다 다른 user group을 구축하고 사용할 수 있도록 한다.\n- **Unix Time sharing (uts)** : hostname을 container마다 제공하는 것으로, 이를 통해서 network 상에서 ip가 아닌 hostname으로 접근하는 것이 가능해진다.\n\n즉, 해당 절에서는 이 한 마디를 기억하면 편해집니다. \"하나의 Docker의 container는 namespace들의 집합으로 이루어져있다.\"\n\n### Control Groups(C group)\n\nnamespace가 각 container간의 isolation을 보장한다면, cgroup은 한계를 설정하는 것이 역할이다. container들이 하나의 machine에서 동작한다면 어쩔 수 없이 그들이 사용할 수 있는 총 자원의 양은 한정될 수 밖에 없다. 그리고, 자칫 잘못하면 하나의 container가 너무 많은 자원(CPU, Memory, Storage, ...)을 소모하여 다른 container의 동작을 방해할 수 있다. 이를 막기 위한 것이 바로 cgroup이다. 이를 통해서 우리는 각 container에게 자원을 나누어 할당하는 것이 가능하다.\n\n### Capabilities\n\n어떤 작업을 하더라도, Machine을 root 권한으로 작업을 하는 것은 굉장히 위험하다. 따라서, container에서 application을 동작시키기 위한 최소한의 권한만을 부여하여 사용하는 것이 올바르다. 이를 수행할 수 있도록, 권한을 지정하는 것이 가능하다.\n\n### Mandatory Access Control(MAC) system\n\nMAC은 파일이나 특정 데이터에 대한 접근 제어를 수행하는 것을 의미한데, 이는 AppArmor나 SELinux 등에 의해서 구현되어지는데, 기본적으로 Docker는 container에 AppArmor를 각 container에 적용하여 이를 구현한다. customizing이 가능하지만, 이에 대한 이해를 충분히 하기를 권한다.\n\n### seccomp\n\nseccomp의 filter mode를 활용하면, container에서 발생하는 syscall을 제한하는 것이 가능하다. 이는 MAC 처럼 직접 customizing도 가능하지만 이에 대한 깊은 이해가 뒷받침되어야 한다.\n\n## Docker Engine's Security for Docker\n\n### Secure Swarm Mode\n\n기본적으로 Docker Swarm은 manager와 worker로 구분되어 동작한다. manager는 기본적으로 control plane을 제어하고, 전체 적인 cluster 환경을 구성하며, 작업을 적절하게 전달한다. 그리고, 전체적인 application code를 동작시키는 것이 worker들이 수행하는 역할이다. 기본적으로 manager와 worker들은 모두 다른 Node이다. 따라서, 이들간의 통신을 수행할 때에 인증과 같은 작업을 필수적이다. 따라서, Docker Swarm에서는 이를 지원하기 위해서 manager로 임명된 node를 CA로 하여 TLS 인증을 수행한다. 이를 통해서, 서로를 인증하고, 전송 데이터 암호화를 수행한다.  \n  \n\\* control plane vs data plane : 통신을 일상에서의 교통흐름이라고 본다면, control plane은 신호등과 같은 규칙을 의미하고, data plane은 실제로 이동하는 차량들로 비유할 수 있다. 즉, control plane은 cluster 환경에서의 제어를 위한 데이터이고, data plane은 실제로 주고 받는 데이터라고 볼 수 있다.\n\n### Image Scanning\n\nDocker는 이미지에서 보안상의 취약점 여부를 scan하는 기능을 기본적으로 탑재하고 있다. 이를 통해서, 이미지가 가진 취약점 등을 파악하는 것이 가능하다.\n\n### Docker Content Trust\n\nDocker는 download 또는 실행할 이미지의 제공자를 식별하고 무결성을 쉽게 체크할 수 있도록 하기 위해서 Docker Content Trust를 제공한다. registry에 이미지를 업로드할 때, 직접 서명이 가능하고, 이를 통해서 특정 사용자에 의해서 생성되었음을 확정할 수 있다. 이렇게 서명이 존재해야만 pull이 가능하도록 설정하는 것 역시 가능하다.\n\n### Docker Secrets\n\nDocker에서 보안 정보를 안전하게 보관하기 위해서 고안된 것으로, 특정 타겟에서 안전하게 SSH key와 같은 정보를 안전하게 전달하는 것 이 가능합니다.\n","slug":"docker-security","date":"2021-07-10 19:52","title":"[Docker] Security","category":"Tech","tags":["Docker","Container","Security"],"thumbnailSrc":"https://euidong.github.io/images/docker-picture.jpg"}]},"__N_SSG":true}