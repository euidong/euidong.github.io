<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Network ë¶„ì•¼ì— ê´€ì‹¬ì´ ë§ì€ ê°œë°œìë¡œ Computer Engineering ê´€ë ¨ Postingì„ ì£¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤."/><meta property="og:description" content="Network ë¶„ì•¼ì— ê´€ì‹¬ì´ ë§ì€ ê°œë°œìë¡œ Computer Engineering ê´€ë ¨ Postingì„ ì£¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤."/><meta property="og:type" content="blog"/><meta property="og:site_name" content="JustLog"/><meta property="og:image" content="https://euidong.github.io/logo192.png"/><title>#NLP | JustLog</title><meta property="og:title" content="#NLP | JustLog"/><link rel="canonical" href="https://euidong.github.io/tags/NLP"/><meta property="og:url" content="https://euidong.github.io/tags/NLP"/><meta name="next-head-count" content="11"/><link rel="icon" href="https://euidong.github.io/favicon.png"/><link rel="apple-touch-icon" href="https://euidong.github.io/logo192.png"/><link rel="preload" href="/_next/static/css/d4ec5c8b3df09443.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d4ec5c8b3df09443.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6dc16d084a5153e5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6dc16d084a5153e5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" id="Adsense-id" data-ad-client="ca-pub-7452732177557701" async="" defer="" data-nscript="beforeInteractive"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-81da43a8dcd978d9.js" defer=""></script><script src="/_next/static/chunks/main-7b6c38cbad60dfcf.js" defer=""></script><script src="/_next/static/chunks/pages/_app-490b2697367d9c62.js" defer=""></script><script src="/_next/static/chunks/675-ae8e8a351ce30ae2.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Bsubject%5D-8f5ae22995c00eb4.js" defer=""></script><script src="/_next/static/Zq6IxrKlCwpKTIcL6SE7Z/_buildManifest.js" defer=""></script><script src="/_next/static/Zq6IxrKlCwpKTIcL6SE7Z/_ssgManifest.js" defer=""></script><script src="/_next/static/Zq6IxrKlCwpKTIcL6SE7Z/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_wrapper__dKJSz root"><header class="Layout_header__XosLl" style="position:static"><div><button tabindex="1" class="SideBarToggler_search_bar_toggler__CEuUg"><svg stroke="currentColor" fill="none" stroke-width="0" viewBox="0 0 24 24" height="35px" width="35px" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></button><nav class="SideBar_side_bar__wrapper--close__8Nwnr"><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/">Home</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/tags">Tags</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Algorithm">Algorithm<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Computer%20Architecture">Computer Architecture<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->7<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Tech">Tech<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->17<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Web">Web<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Paper">Paper<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->2<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Network">Network<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/AI">AI<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->14<!-- -->)<!-- --></span></a></nav></div><a class="Logo_logo___yD0t" tabindex="1" href="/"></a><div><button class="SearchBarToggler_search_bar_toggler__3dHbA" tabindex="2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg></button></div></header><section><div class="RowCard_row_card__list__background___xFj5"><h1 class="RowCard_row_card__list__title__t4a2h">NLP</h1><label class="RowCard_row_card__list__select__wrapper__TZ4_9"><select class="RowCard_row_card__list__select__dxkxA"><option class="RowCard_row_card__list__select__option__GRKZU">ìµœì‹ ìˆœ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">AtoZ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">ZtoA<!-- --></option></select></label><ul class="RowCard_row_card__list__wrapper__5Gtgi"><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-maxent"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 7. MaxEnt" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 7. MaxEnt" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-maxent">[NLP] 7. MaxEnt</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 11ì›” 7ì¼ 10ì‹œ 02ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/MaximumEntropyModel"># <!-- -->MaximumEntropyModel<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/softmax"># <!-- -->softmax<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-hmm"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 6. Hidden Markov Model" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 6. Hidden Markov Model" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-hmm">[NLP] 6. Hidden Markov Model</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 10ì›” 21ì¼ 21ì‹œ 55ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/MarkovModel"># <!-- -->MarkovModel<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/HMM"># <!-- -->HMM<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/HiddenMarkovModel"># <!-- -->HiddenMarkovModel<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-naive-bayes"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 5. Naive Bayes" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 5. Naive Bayes" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-naive-bayes">[NLP] 5. Naive Bayes</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 10ì›” 21ì¼ 15ì‹œ 37ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-classification"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 4. Classification" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 4. Classification" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-classification">[NLP] 4. Classification</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 10ì›” 21ì¼ 13ì‹œ 37ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Classification"># <!-- -->Classification<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Generative"># <!-- -->Generative<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Discriminative"># <!-- -->Discriminative<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ModelEvaluation"># <!-- -->ModelEvaluation<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-language-modeling"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 3. Language Modeling" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 3. Language Modeling" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-language-modeling">[NLP] 3. Language Modeling</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 10ì›” 21ì¼ 12ì‹œ 15ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NoisyChannel"># <!-- -->NoisyChannel<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Ngram"># <!-- -->Ngram<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/LanguageModeling"># <!-- -->LanguageModeling<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Smoothing"># <!-- -->Smoothing<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/WordClass"># <!-- -->WordClass<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-text-processing"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 2. Text Processing" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 2. Text Processing" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-text-processing">[NLP] 2. Text Processing</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 10ì›” 19ì¼ 21ì‹œ 59ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Regex"># <!-- -->Regex<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Tokenization"># <!-- -->Tokenization<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Collocation"># <!-- -->Collocation<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/MinimumEditDistance"># <!-- -->MinimumEditDistance<!-- --></a></ul></div></div><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-linguistics"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 1. Linguistics" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 1. Linguistics" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-linguistics">[NLP] 1. Linguistics</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 10ì›” 19ì¼ 09ì‹œ 03ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Languagistics"># <!-- -->Languagistics<!-- --></a></ul></div></div></ul></div></section><footer class="Layout_footer__EL5v8"><div class="Layout_footer__copyright__r5baC"><span>Copyright Â© euidong</span><br/><span>ëª¨ë“  ì»¨í…ì¸ ì— ëŒ€í•œ ì €ì‘ê¶Œì€ ì‘ì„±ìì—ê²Œ ì¡´ì¬í•©ë‹ˆë‹¤. <!-- --><br/>ë¶ˆë²• ë³µì œë¥¼ í†µí•œ ìƒì—…ì  ì‚¬ìš©ì„ ì ˆëŒ€ì ìœ¼ë¡œ ê¸ˆì§€í•©ë‹ˆë‹¤. <!-- --><br/>ë‹¨, ë¹„ìƒì—…ì  ì´ìš©ì˜ ê²½ìš° ì¶œì²˜ ë° ë§í¬ë¥¼ ì ìš©í•œë‹¤ë©´ ììœ ë¡­ê²Œ ì‚¬ìš©ê°€ëŠ¥ í•©ë‹ˆë‹¤.<!-- --></span><span>Also I use photos by<!-- --> <!-- --><a href="https://unsplash.com/@lorenzoherrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Lorenzo Herrera</a> <!-- -->on<!-- --> <!-- --><a href="https://unsplash.com/s/photos/tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Unsplash</a></span></div><div class="Layout_footer__contents__YZWSm"><a class="Layout_footer__contents__link__K_TKH" href="https://github.com/euidong" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>github</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://euidong.github.io/portfolio" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M6 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm-5 6s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H1zM11 3.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 0 1h-4a.5.5 0 0 1-.5-.5zm.5 2.5a.5.5 0 0 0 0 1h4a.5.5 0 0 0 0-1h-4zm2 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2zm0 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2z"></path></svg><span>portfolio</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://chrome.google.com/webstore/detail/bonfire/nkooidijgbppkojdgkoafcoppnohdfka?hl=ko" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 16c-1.066-2.219-0.498-3.49 0.321-4.688 0.897-1.312 1.129-2.61 1.129-2.61s0.706 0.917 0.423 2.352c1.246-1.387 1.482-3.598 1.293-4.445 2.817 1.969 4.021 6.232 2.399 9.392 8.631-4.883 2.147-12.19 1.018-13.013 0.376 0.823 0.448 2.216-0.313 2.893-1.287-4.879-4.468-5.879-4.468-5.879 0.376 2.516-1.364 5.268-3.042 7.324-0.059-1.003-0.122-1.696-0.649-2.656-0.118 1.823-1.511 3.309-1.889 5.135-0.511 2.473 0.383 4.284 3.777 6.197z"></path></svg><span>chat</span></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"content":"\n## Intro\n\ní•´ë‹¹ Postingì—ì„œëŠ” Maximum Entropyë¥¼ ì´ìš©í•˜ì—¬ ìµœì ì˜ parameterë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” Machine Learning ì ‘ê·¼ë²•ì— ê¸°ë°˜í•œ NLP ë°©ì‹ì„ ì œì•ˆí•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ NLë¥¼ ìˆ˜í•™ì ì¸ í˜•íƒœë¡œ ë³€í˜•í•˜ê¸° ìœ„í•œ ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì¸ word2vecì— ëŒ€í•œ ì„¤ëª…ë„ ê°™ì´ ì§„í–‰í•œë‹¤.\n\n## MaxEnt Model\n\nMaximum Entropy Model(MEM)ì˜ ì•½ìë¡œ, ì´ê²ƒì˜ ì˜ë¯¸ëŠ” ì£¼ì–´ì§„ datasetì„ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì ì ˆí•œ ë¶„í¬ëŠ” Prior Knowledgeë¥¼ ë§Œì¡±í•˜ëŠ” ë¶„í¬ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ë†’ì€ Entropyë¥¼ ê°€ì§€ëŠ” ë¶„í¬ë¼ëŠ” ê²ƒì´ë‹¤.  \n\nì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê´€ì¸¡ì— ì˜í•´ì„œ ì •ì˜ëœ ê²ƒì´ë‹¤.\n\n1. ë‹¤ì–‘í•œ ë¬¼ë¦¬í˜„ìƒë“¤ì€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ Entropyë¥¼ ìµœëŒ€í™”í•˜ë ¤ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ë ¤ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n2. ë” ì ì€ ìˆ˜ì˜ ë…¼ë¦¬ë¡œ ì„¤ëª…ì´ ê°€ëŠ¥í•œ ê²½ìš°, ë§ì€ ìˆ˜ì˜ ë…¼ë¦¬ë¥¼ ì„¸ìš°ì§€ ë§ë¼ (ì˜¤ì»´ì˜ ë©´ë„ë‚ )\n\në‹¤ì†Œ ì–µì§€ê°™ì•„ ë³´ì´ëŠ” ë…¼ë¦¬ì¼ì§€ë¼ë„ í›„ì— ê°€ì„œ ì‚´í´ë³´ë©´, Machine Learningì˜ Logistic Regressionì— ì—°ê²°ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ìš°ì„ ì€ ì´ ì •ë„ ë…¼ë¦¬ë¡œ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ì •ë„ë¡œ ì´í•´í•´ë³´ì.\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ í’€ì–´ì•¼í•  ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 H(p) = - \\sum_{i=1}^{N}p_i\\log p_i \u0026\\\\\n  \\text{subject to} \\quad \u0026 p_i \\geq 0, \u0026 i = 1, ..., N \\\\\n                          \u0026 \\sum_{i=1}^{N}p_i = 1 \u0026\\\\\n                          \u0026 \\text{Other Prior Knowledge}\n\\end{align*}\n$$\n\nì´ë¥¼ ì´ìš©í•´ì„œ ë¬¸ì œë¥¼ ì„¸ ê°œ ì •ë„ í’€ì–´ë³´ë©´ ê°ì´ ì¡ì„ ìˆ˜ ìˆëŠ”ë° í•œ ë²ˆ ë”°ë¼ì™€ë³´ë„ë¡ í•˜ì.\n\n### Example\n\n\u003e \u003cmark\u003e**1. ì£¼ì‚¬ìœ„ ë˜ì§€ê¸°**\u003c/mark\u003e\n\n1ë¶€í„° 6ê¹Œì§€ì˜ ëˆˆì´ ìˆëŠ” ì£¼ì‚¬ìœ„ê°€ ìˆë‹¤ê³  í•  ë•Œ, ì£¼ì‚¬ìœ„ì˜ ê° ëˆˆì´ ë‚˜ì˜¬ í™•ë¥ ì„ ì•Œê³  ì‹¶ë‹¤ê³  í•˜ì. ì´ë•Œ ìš°ë¦¬ëŠ” ê°„ë‹¨í•˜ê²Œ $1\\over6$ì´ë¼ê³  ë§í•  ê²ƒì´ë‹¤. ì´ê²ƒë„ Maximum Entropyì— ê¸°ë°˜í•œ ì¶”ë¡  ë°©ë²• ì¤‘ì— í•˜ë‚˜ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒ ì‹ì„ ë³´ì.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 H(p) = - \\sum_{i=1}^{N}p_i\\log p_i \u0026\\\\\n  \\text{subject to} \\quad \u0026 p_i \\geq 0, \u0026 i = 1, ..., N \\\\\n                          \u0026 \\sum_{i=1}^{N}p_i = 1 \u0026\n\\end{align*}\n$$\n\nì •ë§ ì•„ë¬´ëŸ° ì •ë³´ê°€ ì—†ì„ ë•Œì—ëŠ” ìœ„ì˜ ì‹ì„ Lagrangianì„ ì“°ì§€ ì•Šê³ ë„ uniform distributionì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ëŠ” [ğŸ”— [ML] Base Information Theory](/posts/ml-base-knowledge#Information-Theory)ì—ì„œ ì‚´í´ë³´ì•˜ì—ˆë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ì¢€ ë” ë³µì¡í•œ ê²½ìš°ë¥¼ ê³ ë ¤í•´ë³´ì. ì•„ë˜ëŠ” Duke University ECE587 ìˆ˜ì—… PPTì˜ ì˜ˆì œì´ë‹¤.\n\n\u003e \u003cmark\u003e**2. í‰ê· ì´ ì£¼ì–´ì¡Œì„ ë•Œì˜ ì¶”ë¡ **\u003c/mark\u003e\n\nìš°ë¦¬ê°€ ë§Œì•½ í‰ê·  ë°ì´í„°ë¥¼ ì•Œê³  ìˆë‹¤ë©´, ì´ë¥¼ Maximum Entropyë¡œ ì–´ë–»ê²Œ ì¶”ì •í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ì•„ë˜ëŠ” ì–´ëŠ fastfoodì ì˜ ë©”ë‰´ë¼ê³  í•˜ì.\n\n| Item    | Price | Calories |\n| :------ | :---- | :------- |\n| Burger  | $1    | 1000     |\n| Chicken | $2    | 600      |\n| Fish    | $3    | 400      |\n| Tofu    | $8    | 200      |\n\nê·¸ë¦¬ê³  íŠ¹ì • í•™ìƒì´ ì´ ê°€ê²Œì—ì„œ í•˜ë£¨ì— í•˜ë‚˜ì”© ë¨¹ëŠ”ë‹¤ê³  í•  ë•Œ, í‰ê·  ì†Œë¹„ ê°€ê²©ì´ $2.5ë¼ê³  í•˜ì. ê·¸ë ‡ë‹¤ë©´, ì´ í•™ìƒì´ ê°€ì¥ ë§ì´ ë¨¹ëŠ” ë©”ë‰´ëŠ” ë¬´ì—‡ì¼ì§€ë¥¼ ì¶”ë¡ í•´ë³´ëŠ” ê²ƒì´ë‹¤.  \nì¦‰, ì´ë¥¼ ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n  \\text{maximize}   \\quad \u0026 H(p) = - \\sum_{i=1}^{N}p_i\\log p_i \u0026\\\\\n  \\text{subject to} \\quad \u0026 p_i \\geq 0, \u0026 i = 1, ..., N \\\\\n                          \u0026 \\sum_{i=1}^{N}p_i = 1 \u0026\\\\\n                          \u0026 E[\\text{price}] = 2.5 \u0026\n\\end{align*}\n$$\n\nì´ë¥¼ Lagrangian ë°©ì‹ì„ ì´ìš©í•´ì„œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = - \\sum_{i}^{N}p_{i}\\log{p_{i}} + \\lambda_{0}(\\sum_{i=1}^{N}p_{i} - 1) + \\lambda_{1}(\\sum_{i=1}^{N}\\text{price}_{i}\\times{p_{i}} -2.5)\n$$\n\nìœ„ ì‹ì„ ê° ê°ì˜ $p_{i}$ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n{\\partial \\mathcal{L}\\over\\partial p_{i}} = -\\log{p_{i}} -1 + \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i}\n$$\n\në”°ë¼ì„œ, $p_{i}$ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n0 \u0026= -\\log{p_{i}} -1 + \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} \\\\\n\\log{p_{i}} \u0026= \\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} - 1 \\\\\np_{i} \u0026= e^{\\lambda_{0} + \\lambda_{1}\\times\\text{price}_{i} - 1}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ë‚˜ì˜¤ëŠ” ëª¨ë“  ì‹ê³¼ ì œí•œ ì¡°ê±´ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n- $p(Burger) = e^{\\lambda_{0} + \\lambda_{1} - 1}$, $p(Chicken) = e^{\\lambda_{0} + 2\\lambda_{1} - 1}$, $p(Fish) = e^{\\lambda_{0} + 3\\lambda_{1} - 1}$, $p(Tofu) = e^{\\lambda_{0} + 8\\lambda_{1} - 1}$\n- $p(Burger) + p(Chicken) + p(Fish) + p(Tofu) = 1$\n- $p(Burger) + 2p(Chicken) + 3p(Fish) + 8p(Tofu) = 2.5$\n\nìœ„ì˜ ì‹ì„ ì—°ë¦½í•´ì„œ í’€ë©´, $\\lambda_{0} = 1.2371$, $\\lambda_{1}=0.2586$ì´ê³ , ì „ì²´ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n| Item    | p      |\n| :------ | :----- |\n| Burger  | 0.3546 |\n| Chicken | 0.2964 |\n| Fish    | 0.2478 |\n| Tofu    | 0.1011 |\n\n\u003e \u003cmark\u003e**3. ì£¼ì‚¬ìœ„ì˜ ëˆˆì˜ í•©**\u003c/mark\u003e\n\n1ë²ˆì—ì„œ ë³´ì•˜ë˜ ì£¼ì‚¬ìœ„ë¥¼ nê°œ ë˜ì ¸ì„œ ë‚˜ì˜¨ ëˆˆì˜ í•©ì„ ì•Œ ë•Œ, ì£¼ì‚¬ìœ„ì˜ ë¹„ìœ¨ì„ ì¶”ì •í•œë‹¤ê³  í•´ë³´ì.\n\nì´ë•Œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë³€ìˆ˜ë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n- ì£¼ì‚¬ìœ„ì˜ ê°¯ìˆ˜ : $n$\n- iê°œì˜ ëˆˆì„ ê°€ì§„ ì£¼ì‚¬ìœ„ì˜ ê°¯ìˆ˜ : $n_{i}$\n- ì „ì²´ ëˆˆì˜ ìˆ˜ì˜ í•© : $n\\alpha$\n- ì¶”ê°€ë˜ëŠ” Prior Knowledge : $\\sum_{i=1}^{6}{i n_{i}} = n\\alpha$\n\nì´ë¥¼ Maximum Entropyë¥¼ ì´ìš©í•´ì„œ í’€ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ë¡ ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\np_{i} = {e^{\\lambda_{i}}\\over{\\sum_{i=1}^{6}{e^{\\lambda_{i}}}}}\n$$\n\n## Generalization\n\nMaximum Entropyë¥¼ ìœ„ì˜ ì‹ì„ í†µí•´ì„œ êµ¬í•˜ëŠ” ê²ƒë„ ë¬¸ì œëŠ” ì—†ì§€ë§Œ ìš°ë¦¬ëŠ” ì¢€ ë” ì¼ë°˜í™”ëœ ì‹ì„ ì›í•œë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì„ ê³ ë ¤í•´ë³´ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ê°€ ë§ˆì§€ë§‰ ë³´ì•˜ë˜ ì˜ˆì‹œê°€ ì‚¬ì‹¤ì€ ìš°ë¦¬ê°€ í•˜ê³ ì í•˜ëŠ” ê³¼ì •ì„ ëŒ€í‘œí•˜ëŠ” í•˜ë‚˜ì˜ ì˜ˆì‹œì´ë‹¤. ìš°ë¦¬ê°€ ê°€ì§„ ì‚¬ì „ ì§€ì‹ì€ ì´ì „ì— ê´€ì¸¡í•œ ë°ì´í„°ì™€ ì´ê²ƒì˜ classì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê´€ì¸¡ ê²°ê³¼ì˜ ê°€ì§“ìˆ˜(class)ê°€ $K$ê°œì´ê³ , ë°ì´í„°ì˜ inputê³¼ ê²°ê³¼ë¥¼ $(X, Y)$ ìŒì´ ë¼ê³  í•  ë•Œ, íŠ¹ì • input data($X_{i}$)ê°€ class kì¼ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n$$\np(Y_{i} = k) = {e^{w^{\\top}_{k}X_{i}}\\over{\\sum_{k^{\\prime}=1}^{K}{e^{w_{k^{\\prime}}^{\\top}X_{i}}}}}\n$$\n\nì—¬ê¸°ì„œ ê°€ì¥ ì¤‘ìš”í•œ Pointê°€ ë°œê²¬ëœë‹¤. ë°”ë¡œ ì´ ì‹ì´ **softmax** í•¨ìˆ˜ë¼ëŠ” ê²ƒì´ë‹¤. \u003cmark\u003eì¦‰, Maximum Entropyë¥¼ í†µí•œ classificationì˜ ì˜ë¯¸ëŠ” ì‚¬ì‹¤ìƒ multinomial logistic regressionì˜ ë‹¤ë¥¸ ì´ë¦„ì¼ ë¿ì´ë‹¤.\u003c/mark\u003e (logistic regressionì— ëŒ€í•œ ë‚´ìš©ì€ [ğŸ”— [ML] 3. Logistic Regression](/posts/ml-logistic-regression)ì—ì„œ ë‹¤ë£¨ì—ˆë‹¤.)\n\në”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” ë³„ë„ë¡œ Modeling, Estimation, Smoothing ì ˆì°¨ë¥¼ ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤. machine learningì˜ ë°©ë²•ê³¼ ë™ì¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n\n## Features\n\nNLì˜ ê°€ì¥ í° íŠ¹ì§•ì€ dataê°€ sparseí•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. domainë§ˆë‹¤ ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ì™€ ë¹ˆë„ê°€ ë„ˆë¬´ë‚˜ ì²œì°¨ë§Œë³„ì´ê¸° ë•Œë¬¸ì— sparse í˜„ìƒì´ í•„ì—°ì ìœ¼ë¡œ ë°œìƒí•œë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œ ëŒ€ê²Œì˜ dataëŠ” domain ë³„ë¡œ ë”°ë¡œë”°ë¡œ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ë˜í•œ, dataì—ì„œ ì˜¬ë°”ë¥¸ featureë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ êµ‰ì¥íˆ ì¤‘ìš”í•˜ë‹¤.  \nì´ë¥¼ ìœ„í•´ NLì—ì„œ ì „í†µì ìœ¼ë¡œ ì“°ë˜ ë°©ì‹ì€ ëŒ€ì†Œë¬¸ì ì—¬ë¶€, ì–µì–‘ í‘œê¸°, í’ˆì‚¬, ë¬¸ì¥êµ¬ì¡°, ëœ» ë“±ì„ ë‹¨ì–´ì— ë¯¸ë¦¬ ì ìš©í•˜ê¸°ë„ í•˜ì—¬ ì´ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤. ê·¸ëŸ°ë° ì´ëŸ¬í•œ í’ˆì‚¬, ëœ» ë“±ì„ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ë„ Statistical Inferenceê°€ í•„ìš”í•˜ë‹¤. ë”°ë¼ì„œ, ì•ìœ¼ë¡œ chapterì—ì„œëŠ” í’ˆì‚¬ì™€ ë¬¸ì¥êµ¬ì¡° ëœ»ì„ ì •ì˜í•˜ê¸° ìœ„í•œ ê¸°ìˆ ë“¤ê³¼ ì´ë¥¼ ì–´ë–»ê²Œ ì°¾ì„ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì•Œì•„ë³¼ ê²ƒì´ë‹¤.\n\në˜í•œ, Wordìì²´ë¥¼ Vectorë¡œ ì¹˜í™˜í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” Word2Vecë°©ì‹ì— ëŒ€í•´ì„œë„ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- Maximum Entropy ìë£Œ ì°¸ê³ , \u003chttps://www2.isye.gatech.edu/~yxie77/ece587/Lecture11.pdf\u003e\n","slug":"nlp-maxent","date":"2022-11-07 10:02","title":"[NLP] 7. MaxEnt","category":"AI","tags":["NLP","MaximumEntropyModel","softmax"],"desc":"í•´ë‹¹ Postingì—ì„œëŠ” Maximum Entropyë¥¼ ì´ìš©í•˜ì—¬ ìµœì ì˜ parameterë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” Machine Learning ì ‘ê·¼ë²•ì— ê¸°ë°˜í•œ NLP ë°©ì‹ì„ ì œì•ˆí•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ NLë¥¼ ìˆ˜í•™ì ì¸ í˜•íƒœë¡œ ë³€í˜•í•˜ê¸° ìœ„í•œ ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì¸ word2vecì— ëŒ€í•œ ì„¤ëª…ë„ ê°™ì´ ì§„í–‰í•œë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nì´ì „ê¹Œì§€ íŠ¹ì • wordë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ modelingì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ íŠ¹ì • wordì˜ sequenceë¥¼ í†µí•´ì„œ ê° wordì— ëŒ€í•œ classificationì„ í•œ ë²ˆì— í•˜ê³  ì‹¶ì€ ê²½ìš°ëŠ” ì–´ë–»ê²Œ í• ê¹Œ?(ì˜ˆë¥¼ ë“¤ì–´, ê° ë‹¨ì–´ì˜ í’ˆì‚¬ë¥¼ ì§€ì •í•˜ëŠ” ì¼) ì¼ë°˜ì ìœ¼ë¡œ ê° ë‹¨ì–´ê°€ íŠ¹ì • í•´ë‹¹ classì¼ í™•ë¥ ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì´ ì¼ë°˜ì ì¼ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ í™•ë¥ ì„ êµ¬í•  ë°©ë²•ì€ ì—†ì„ê¹Œ? ê·¸ ë°©ë²•ì€ ë°”ë¡œ bigramì„ ì´ìš©í•˜ë©´ ë  ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì‚¬ì‹¤ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ë¬¸ë§¥ì´ ë‹¨ì–´ ìì²´ë³´ë‹¤ëŠ” ì´ì „ classê°€ ë” ì˜í–¥ì´ í¬ë‹¤ë©´, ì´ëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ? ì´ë¥¼ ìœ„í•œ í•´ê²°ì±…ì´ HMMì´ë‹¤. NLP ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ ë„“ê²Œ ì‚¬ìš©ë˜ê³  ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” NLP ë¶„ì•¼ì—ì„œ ì–´ë–»ê²Œ ì´ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³¼ ê²ƒì´ë‹¤.\n\n## Markov Model\n\nHMMì„ ì•Œì•„ë³´ê¸°ì „ì— Markov Modelì„ ì•Œì•„ì•¼ í•œë‹¤. ì´ëŠ” íŠ¹ì • sequenceì˜ í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰ ìš°ë¦¬ì—ê²Œ state sequence ($S= {s_{0}, s_{1}, ..., s_{N}}$)ê°€ ì£¼ì–´ì§ˆ ë•Œ, ê° stateì—ì„œ ë‹¤ìŒ stateë¡œ ì „ì´(ì´ë™)í•  í™•ë¥ ì„ ì´ìš©í•´ì„œ state sequenceì˜ í™•ë¥ ì„ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\n![nlp-markov-model-1](/images/nlp-markov-model-1.jpg)\n\nìœ„ì˜ ê·¸ë¦¼ì´ state ê° ê°ì—ì„œ ë‹¤ìŒ stateë¡œ ì „ì´í•  í™•ë¥ ì„ ë‚˜íƒ€ë‚¸ ê²ƒì´ë¼ë©´, ìš°ë¦¬ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì€ ê·¸ë¦¼ìœ¼ë¡œ sequenceì˜ í™•ë¥ ì„ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n![nlp-markov-model-2](/images/nlp-markov-model-2.jpg)\n\në”°ë¼ì„œ, ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ìš°ë¦¬ê°€ ë§Œì•½ $(s_{0}, s_{1}, s_{0}, s_{2})$ìœ¼ë¡œ ì´ë£¨ì–´ì§„ sequenceì˜ í™•ë¥ ì„ ì–»ê¸°ë¥¼ ë°”ë€ë‹¤ë©´, ê·¸ í™•ë¥ ì€ ì•„ë˜ì™€ ê°™ì•„ì§„ë‹¤.\n$$\n\\begin{align*}\np(s_{0}, s_{1}, s_{0}, s_{2}) \u0026= p(s_{0}| \\text{start}) \\times p(s_{1}|s_{0}) \\times p(s_{0}|s_{1}) \\times p(s_{2}|s_{1}) \\times p(end|s_{2}) \\\\\n\u0026= \\pi_{0} \\times p_{01} \\times p_{10} \\times p_{12} \\times 1\n\\end{align*}\n$$\n\nì´ë¥¼ ì˜ ì‚´í´ë³´ë‹ˆ bigramì—ì„œì˜ Likelihoodë¥¼ êµ¬í•˜ëŠ” ê³µì‹ê³¼ ë˜‘ê°™ë‹¤. ì¦‰, state ê° ê°ì„ wordë¼ê³  ë³¸ë‹¤ë©´, Markov Modelì„ í†µí•´ì„œ êµ¬í•  ìˆ˜ ìˆëŠ” í™•ë¥ ì€ bigramì˜ Likelihoodì¸ ê²ƒì´ë‹¤.\n\nê·¸ë¦¬ê³  ì´ë¥¼ ì¼ë°˜í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\np(seq) = \\prod_{i=1}^{N}p(seq_{i}|seq_{i-1})\n$$\n\nê·¸ëŸ°ë°, ì—¬ê¸°ì„œ nì´ 3 ì´ìƒì¸ ngramì„ ì ìš©í•˜ê³  ì‹¶ë‹¤ë©´, ê° stateë¥¼ n-1 gramìœ¼ë¡œ ì„¤ì •í•˜ë©´ ëœë‹¤.\n\n$$\n\\begin{align*}\nX_{i} \u0026= (Q_{i-1}, Q_{i}) \\text{ë¼ë©´, }\\\\\nP(X_{i} | X_{i-1}) \u0026= P(Q_{i-1}, Q_{i} | Q_{i-2}, Q_{i-1}) \\\\\n\u0026= P(Q_{i} | Q_{i-2}, Q_{i-1})\n\\end{align*}\n$$\n\në”°ë¼ì„œ, trigramì„ ì ìš©í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\n$$\n\\begin{align*}\np((start, w_{0}), (w_{0}, w_{1}), (w_{1}, w_{0}), (w_{0}, w_{2})) \u0026= p(w_{0}| \\text{start}, \\text{start}) \\times p(w_{1}|\\text{start}, w_{0}) \\times p(w_{0}|w_{0}, w_{1}) \\times p(w_{2}|w_{1}, w_{0}) \\times p(end|w_{0}, w_{2}) \\\\\n\u0026= \\pi_{0} \\times p_{01} \\times p_{10} \\times p_{12} \\times 1\n\\end{align*}\n$$\n\n## Hidden Markov Model\n\nHidden Markov Modelì€ stateë¥¼ í•˜ë‚˜ ë” ë§Œë“ ë‹¤ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤. ê·¸ë˜ì„œ, ìš°ë¦¬ê°€ ì§ì ‘ ê´€ì¸¡í•˜ëŠ” state(**observed state**)ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ì¸¡í•˜ì§€ ì•Šì§€ë§Œ, ê´€ì¸¡í•œ stateë“¤ì— ì˜ì¡´í•˜ëŠ” state(**hidden state**) ì´ ë‘ ê°œì˜ stateë¥¼ ì‚¬ìš©í•œë‹¤. ì¼ë°˜ì ì¸ ì˜ˆì‹œê°€ textê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ ìš°ë¦¬ëŠ” ê° ë‹¨ì–´ë¥¼ observed stateë¼ê³  í•œë‹¤ë©´, ê° ë‹¨ì–´ì˜ í’ˆì‚¬ë¥¼ hidden stateë¼ê³  ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n![nlp-markov-model-3](/images/nlp-markov-model-3.jpg)\n\nìœ„ì˜ ì˜ˆì‹œëŠ” ìš°ë¦¬ê°€ ê´€ì¸¡í•˜ëŠ” ë°ì´í„°($O$)ê°€ 3ê°œì˜ stateë¥¼ ê°€ì§€ê³ , ì´ ì‚¬ê±´ì— ì˜ì¡´ì ì¸ ë˜ ë‹¤ë¥¸ ì‚¬ê±´($H$)ì´ 3ê°œì˜ stateë¥¼ ê°€ì§€ëŠ” ê²½ìš°ì´ë‹¤. ì´ë¥¼ ì´ìš©í•´ì„œ ê¸°ì¡´ Markov Modelë³´ë‹¤ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.\n\n### Estimation\n\nìš°ë¦¬ê°€ í•  ìˆ˜ ìˆëŠ” ì‘ì—…ì€ í¬ê²Œ ë‘ ê°€ì§€ì´ë‹¤. ì¼ë°˜ì ì¸ Markov Modelì—ì„œ í•  ìˆ˜ ìˆë˜ ë°©ì‹ì´ **Trellis** ë°©ì‹ì´ê³ , ë˜ ë‹¤ë¥¸ ë°©ì‹ì´ **Viterbi** ë°©ì‹ì´ë‹¤.\n\n1. $(o_{0}, o_{1}, o_{0}, o_{2})$ì˜ í™•ë¥ ì´ ê¶ê¸ˆí•  ë•Œ(**Trellis**)\n2. $(o_{0}, o_{1}, o_{0}, o_{2})$ê°€ ì£¼ì–´ì§ˆ ë•Œ, ì´ê²ƒì˜ hidden stateì˜ sequence ì¤‘ ê°€ì¥ ìœ ë ¥í•œ sequenceë¥¼ ì°¾ê³ ìí•  ë•Œ(**Viterbi**)\n\nìœ„ì˜ ê²½ìš°ë¥¼ ê°ê° í’€ì–´ë³´ë„ë¡ í•˜ì.\n\n\u003e \u003cmark\u003e**1. Trellis**\u003c/mark\u003e\n\nìš°ë¦¬ê°€ ì§ì ‘ ê´€ì¸¡í•œ ë°ì´í„°ì˜ sequence ìì²´ì˜ í™•ë¥ ì´ ê¶ê¸ˆí•  ë•Œì´ë‹¤. ë”°ë¼ì„œ, ì´ì— ëŒ€í•œ ë¶„ì„ì€ $(o_{0}, o_{1}, o_{0}, o_{2})$ì˜ í™•ë¥ ì„ ë¶„ì„í•´ë³´ë©´ì„œ ì„¤ëª…í•˜ê² ë‹¤.\n\n$$\n\\begin{align*}\np(o_{0}, o_{1}, o_{0}, o_{2}) \u0026= p(o_{0}, o_{1}, o_{0}) \\times p(o_{2} | o_{0}, o_{1}, o_{0}) \\\\\n\u0026= p(o_{0}, o_{1}, o_{0}) \\times \\{p(o_{2} | h_{0})p(h_{0} | o_{0}, o_{1}, o_{0}) + p(o_{2} | h_{1})p(h_{1} | o_{0}, o_{1}, o_{0}) + p(o_{2} | h_{2})p(h_{2} | o_{0}, o_{1}, o_{0})\\} \\\\\n\u0026= p(o_{0}, o_{1}, o_{0}) \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n\u0026= p(o_{0}, o_{1}) \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n\u0026= p(o_{0}) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n\u0026= \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) }\n\\end{align*}\n$$\n\nì´ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n![nlp-hidden-markov-model-1](/images/nlp-hidden-markov-model-1.jpg)\n\në˜í•œ, ì´ ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ì¶•ì†Œê°€ ê°€ëŠ¥í•˜ë‹¤.\n\n$$\n\\begin{align*}\n  \u0026\\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =\u0026 \\sum_{i=0}^{2}\\alpha_{0 i} \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =\u0026 \\sum_{i=0}^{2}{\\alpha_{1 i} } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =\u0026 \\sum_{i=0}^{2}{\\alpha_{2 i} } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =\u0026 \\sum_{i=0}^{2}{\\alpha_{3 i} }\n\\end{align*}\n$$\n\nìš°ë¦¬ëŠ” ì´ë¥¼ í†µí•´ì„œ, Markov Modelì˜ íŠ¹ì§•ì„ í•˜ë‚˜ ë°°ìš¸ ìˆ˜ ìˆë‹¤. ê·¸ê²ƒì€ ë°”ë¡œ ë³µì¡í•œ sequence ì „ì²´ì˜ í™•ë¥ ì—ì„œ ë²—ì–´ë‚˜ì„œ ë°”ë¡œ ì§ì „ì˜ í™•ë¥ ê°’ë§Œ ìœ¼ë¡œ ë‹¤ìŒ í™•ë¥ ì„ ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì´ Markov Chainì´ë¼ëŠ” ì´ë¡ ì´ê³ , ì´ë¥¼ ì´ìš©í–ˆê¸° ë•Œë¬¸ì— Markov Modelë¼ê³  ë¶€ë¥´ëŠ” ê²ƒì´ê¸°ë„ í•˜ë‹¤.\n\në”°ë¼ì„œ, $\\alpha$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\alpha(t, i) = \\sum_{k=1}^{N}{\\alpha(t-1, k)p(h_{i}|h_{k})p(o = s_{t}|h_{i})} \\quad (s_{t} = \\text{inputìœ¼ë¡œ ë“¤ì–´ì˜¨ sequenceì˜ të²ˆì§¸ ê°’})\n$$\n\në˜, ì´ë¥¼ ë°˜ëŒ€ë¡œ í•  ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n![nlp-hidden-markov-model-2](/images/nlp-hidden-markov-model-2.jpg)\n\n$$\n\\begin{align*}\n  \u0026\\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{p(o_{2} | h_{i})p(h_{i} | o_{0}, o_{1}, o_{0}) } \\\\\n  =\u0026 \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{p(o_{0} | h_{i})p(h_{i} | o_{0}, o_{1}) } \\times \\sum_{i=0}^{2}{\\beta_{3i}} \\\\\n  =\u0026 \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{p(o_{1} | h_{i})p(h_{i} | o_{0}) } \\times \\sum_{i=0}^{2}{\\beta_{2i}} \\\\\n  =\u0026 \\sum_{i=0}^{2}p(o_{0}|h_{i})p(h_{i}|start) \\times \\sum_{i=0}^{2}{\\beta_{1i}} \\\\\n  =\u0026 \\sum_{i=0}^{2}{\\beta_{0i}} \\\\\n\\end{align*}\n$$\n\n$$\n\\beta(t, i) = \\sum_{k=1}^{N}{\\beta(t+1, k)p(h_{k}|h_{i})p(o = s_{t}|h_{i})} \\quad (s_{t} = \\text{inputìœ¼ë¡œ ë“¤ì–´ì˜¨ sequenceì˜ të²ˆì§¸ ê°’})\n$$\n\nìœ„ì˜ ì²˜ëŸ¼ ì•ì—ì„œë¶€í„° í’€ì´ë¥¼ í•´ë‚˜ê°€ë©´ì„œ, $\\alpha$ì˜ í•©ìœ¼ë¡œ ëì´ ë‚˜ë„ë¡ í‘¸ëŠ” ë°©ë²•ì„ forwarding ë°©ì‹ì´ë¼í•˜ê³ , ë°˜ëŒ€ë¡œ ë’¤ì—ì„œë¶€í„° í’€ì´í•˜ë©´ì„œ $\\beta$ì˜ í•©ìœ¼ë¡œ í‘¸ëŠ” ë°©ë²•ì„ backwarding ë°©ì‹ì´ë¼ê³  í•œë‹¤. ì‚¬ì‹¤ ì´ ê²½ìš°ëŠ” HMMì´ êµ³ì´ ì•„ë‹ˆë”ë¼ë„, MMìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆìœ¼ë‹ˆ êµ³ì´ í•„ìš”ëŠ” ì—†ë‹¤. í•˜ì§€ë§Œ, ì´ê²ƒì€ í›„ì— modeling ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì•Œì•„ë‘ì–´ì•¼ í•œë‹¤.\n\n\u003e \u003cmark\u003e**2. Viterbi**\u003c/mark\u003e\n\nì´ëŠ” observed stateì˜ sequenceì— ì˜í•´ì„œ íŒŒìƒë˜ëŠ” ê°€ì¥ ì ì ˆí•œ hidden sequenceë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. ì´ë¥¼ í†µí•´ì„œ í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ ê²ƒì´ sequence classificationì´ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´ ê°€ì¥ ìœ ë ¥í•œ hidden stateì˜ sequenceë¥¼ $\\hat{s}^{(H)}$ë¼ê³  í•˜ì. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n\\hat{s}^{(H)} \u0026= \\argmax_{s^{(H)} \\in S^{(H)}}P(s^{(H)}|s^{(O)}) \\\\\n\u0026= \\argmax_{s^{(H)} \\in S^{(H)}}P(s^{(O)}|s^{(H)})P(s^{(H)}) \\\\\n\u0026= \\argmax_{{h_{1}, h_{2}, ..., h_{N}} \\in S^{(H)}}\\underbrace{P(o_{1}, o_{2}, ... , o_{N}|h_{1}, h_{2}, ... , h_{N})}_{\\text{Markov Model}}\\underbrace{P(h_{1}, h_{2}, ... , h_{N})}_{\\text{Markov Model}} \\\\\n\u0026= \\argmax_{{h_{1}, h_{2}, ..., h_{N}} \\in S^{(H)}}\\prod_{i=1}^{N}p(o_{i}|h_{i})p(h_{i}|h_{i-1})\n\\end{align*}\n$$\n\n![nlp-hidden-markov-model-3](/images/nlp-hidden-markov-model-3.jpg)\n\nì¦‰, ê° layerì—ì„œ ë‹¨ í•˜ë‚˜ì˜ ê°€ì¥ í° outputë§Œ ì‚´ì•„ë‚¨ì„ ìˆ˜ ìˆê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. ì´ ê³¼ì •ì´ ì‚¬ì‹¤ìƒ HMMì˜ ë³¸ì§ˆì ì¸ ëª©í‘œì´ë‹¤. sequenceë¥¼ ì…ë ¥í•´ì„œ sequence í˜•íƒœì˜ classification ê²°ê³¼ë¥¼ ì–»ëŠ” ê²ƒì´ë‹¤.\n\n### Modeling\n\nì—¬íƒœê¹Œì§€ HMMì„ í™œìš©í•˜ì—¬ sequential classë¥¼ ì–´ë–»ê²Œ estimation í•˜ëŠ”ì§€ ì•Œì•„ë³´ì•˜ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ì œëŠ” ì´ë¥¼ ìœ„í•´ì„œ ì‚¬ìš©ë˜ëŠ” í™•ë¥ ê°’ì„ êµ¬í•´ì•¼í•œë‹¤. í•„ìš”í•œ í™•ë¥ ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n- $p(h_{i}|h_{i-1})$ : Hidden Stateì—ì„œ Hidden Stateë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•œ í™•ë¥ ì´ë‹¤.\n- $p(o_{i}|h_{i})$ : ë°©ì¶œ í™•ë¥ ë¡œ íŠ¹ì • Hidden Stateì—ì„œ ë‹¤ìŒ Stateì˜ Observed Stateë¡œ ë„˜ì–´ê°€ëŠ” ë°©ë²•ì´ë‹¤.\n- $\\pi_{i}$\n\nTrelli ë°©ì‹ì—ì„œ ë§Œë“¤ì—ˆë˜, $\\alpha$ì™€ $\\beta$ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•´ì•¼ í•œë‹¤. ê° ê°ì€ í•´ë‹¹ ê³¼ì •ê¹Œì§€ ì˜¤ë©´ì„œ ëˆ„ì í•´ì˜¨ í™•ë¥ ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì˜ ë°˜ì˜í•  ìˆ˜ ìˆëŠ” í™•ë¥  ê°’ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ìƒê°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ í‰ê· ì„ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ë¨¼ì € ì‚´í´ë³´ì.\n\n$$\n\\begin{align*}\n  c(i, j, k) \u0026= h_{i}\\text{ì—ì„œ } h_{j}\\text{ë¡œ ë„˜ì–´ê°€ê³ , } o_{k}\\text{ê°€ ê´€ì¸¡ë  í™•ë¥ ì˜ í•©} \\\\\n  \u0026= \\sum_{t=2}^{T} \\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j) \\\\\n  \\\\\n  c(i,j) \u0026= h_{i}\\text{ì—ì„œ } h_{j}\\text{ë¡œ ë„˜ì–´ê°ˆ í™•ë¥ ì˜ í•©} \\\\\n  \u0026= \\sum_{k=1}^{K}\\sum_{t=2}^{T}{\\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j)} \\\\\n  \\\\\n  c(i) \u0026= h_{i}\\text{ì—ì„œ ìƒíƒœë¥¼ ë³€ê²½í•˜ëŠ” í™•ë¥ ì˜ í•©} \\\\\n  \u0026= \\sum_{j=1}^{N}\\sum_{k=1}^{K}\\sum_{t=2}^{T}{\\alpha(t-1, i)p(h_{j}|h_{i})p(o_{k}|h_{j}) \\beta(t, j)} \\\\\n\\end{align*}\n$$\n\nìœ„ì˜ ê°’ì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆë˜ í™•ë¥ ì„ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\np(h_{j}|h_{i}) \u0026= {c(i,j)\\over c(i)} \\\\\np(o_{k}|h_{i}) \u0026= {c(i,j,k)\\over c(i,j)}\n\\end{align*}\n$$\n\nì¦‰, ìš°ë¦¬ëŠ” ë‹¤ìŒ ê³¼ì •ì„ ìˆ˜í–‰í•˜ì—¬ Modelingì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n1. ì´ˆê¸°ê°’ ($p(h_{i}|h_{i-1})$, $p(o_{i}|h_{i})$, $\\pi_{i}$)ì„ ì´ˆê¸°í™” í•œë‹¤.  \n2. Trellië¥¼ í†µí•´ì„œ $\\alpha$, $\\beta$ë¥¼ ê³„ì‚°í•œë‹¤.\n3. $p(h_{i}|h_{i-1})$, $p(o_{i}|h_{i})$ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤.  \n   ($pi_{i}$ê°™ì€ ê²½ìš°ëŠ” ë°œìƒ ë¹ˆë„ë¡œ ì—…ë°ì´íŠ¸ í•œë‹¤.)\n4. ì„ê³„ì¹˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ 2,3ë²ˆì„ ë°˜ë³µí•œë‹¤.\n\nì´ ê³¼ì •ì„ ëŒ€ê²Œ 10ë²ˆ ì •ë„ë§Œ í•˜ë©´ ìˆ˜ë ´í•˜ê²Œ ë˜ê³ , ì´ë¥¼ í™•ë¥ ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"nlp-hmm","date":"2022-10-21 21:55","title":"[NLP] 6. Hidden Markov Model","category":"AI","tags":["NLP","MarkovModel","HMM","HiddenMarkovModel"],"desc":"ì´ì „ê¹Œì§€ íŠ¹ì • wordë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ modelingì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ íŠ¹ì • wordì˜ sequenceë¥¼ í†µí•´ì„œ ê° wordì— ëŒ€í•œ classificationì„ í•œ ë²ˆì— í•˜ê³  ì‹¶ì€ ê²½ìš°ëŠ” ì–´ë–»ê²Œ í• ê¹Œ?(ì˜ˆë¥¼ ë“¤ì–´, ê° ë‹¨ì–´ì˜ í’ˆì‚¬ë¥¼ ì§€ì •í•˜ëŠ” ì¼) ì¼ë°˜ì ìœ¼ë¡œ ê° ë‹¨ì–´ê°€ íŠ¹ì • í•´ë‹¹ classì¼ í™•ë¥ ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì´ ì¼ë°˜ì ì¼ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ í™•ë¥ ì„ êµ¬í•  ë°©ë²•ì€ ì—†ì„ê¹Œ? ê·¸ ë°©ë²•ì€ ë°”ë¡œ bigramì„ ì´ìš©í•˜ë©´ ë  ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì‚¬ì‹¤ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ë¬¸ë§¥ì´ ë‹¨ì–´ ìì²´ë³´ë‹¤ëŠ” ì´ì „ classê°€ ë” ì˜í–¥ì´ í¬ë‹¤ë©´, ì´ëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ? ì´ë¥¼ ìœ„í•œ í•´ê²°ì±…ì´ HMMì´ë‹¤. NLP ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ ë„“ê²Œ ì‚¬ìš©ë˜ê³  ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” NLP ë¶„ì•¼ì—ì„œ ì–´ë–»ê²Œ ì´ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³¼ ê²ƒì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nNaive Bayes Modelì€ ê°€ì¥ ì‰½ê²Œ Classificationì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” Modelì´ì§€ë§Œ, ì„±ëŠ¥ì´ ë‹¤ë¥¸ Modelì— ë¹„í•´ ë›°ì–´ë‚˜ì§€ëŠ” ì•Šë‹¤. ê·¸ëŸ¼ì—ë„ Naive BayesëŠ” ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” Modelì´ê¸°ì— ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ê³ , Classificationì˜ insightë¥¼ í‚¤ìš°ëŠ”ë° ë§ì€ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œëŠ”, ì „ë°˜ì ì¸ ê°œë…ê³¼ ì´ë¥¼ ì§ì ‘ Spam Filteringì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì‚´í´ë³¸ë‹¤.\n\n## Naive Bayes Model\n\níŠ¹ì • classì—ì„œ í•´ë‹¹ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë°œìƒë˜ëŠ”ì§€ì™€ ì‹¤ì œë¡œ í•´ë‹¹ classì˜ ë¹ˆë„ë¥¼ í™œìš©í•˜ì—¬, classificationì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ìš°ì„  ì´ë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ë¨¼ì € ì •ì˜í•´ë³´ì.\n\n- **documents($D$)**: ì—¬ëŸ¬ ê°œì˜ Documentë¥¼ ì˜ë¯¸í•˜ë©°, í•˜ë‚˜ì˜ DocumentëŠ” ëŒ€ê²Œ ì—¬ëŸ¬ ê°œì˜ wordsë¥¼ í¬í•¨í•œë‹¤. ê° documentëŠ” $d_{i} \\in D$ì˜ í˜•íƒœë¡œ í‘œí˜„í•œë‹¤.\n- **classes($C$)**: classëŠ” ë‘ ê°œ ì´ìƒì„ ê°€ì§„ë‹¤. ê° í´ë˜ìŠ¤ëŠ” $c_{i} \\in C$ì˜ í˜•íƒœë¡œ í‘œí˜„ëœë‹¤.\n- **labeled dataset**: ì´ëŠ” (document($d_{i}$), class($c_{i}$))ê°€ í•˜ë‚˜ì”© mappingëœ í˜•íƒœë¡œ ì¡´ì¬í•œë‹¤. ìš°ë¦¬ê°€ ê°€ì§€ëŠ” datasetìœ¼ë¡œ í•™ìŠµ, í‰ê°€ ì‹œì— ì‚¬ìš©í•œë‹¤. ëŒ€ê²Œ í‰ê°€ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ëŠ” í•™ìŠµ ì‹œì— ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¸ˆì§€í•˜ê¸° ë•Œë¬¸ì— ë³„ë„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì‚¬ìš©í•œë‹¤.\n- **word($w$)**: í•˜ë‚˜ì˜ wordë¥¼ ì˜ë¯¸í•˜ë©° NLP í•™ìŠµ ì‹œì— ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ì‘ì€ ë‹¨ìœ„ì´ë‹¤. ëŒ€ê²Œ document í•˜ë‚˜ì— ìˆëŠ” ë‹¨ì–´ì˜ ìˆ˜ëŠ” Nìœ¼ë¡œ í‘œê¸°í•˜ê³ , uniqueí•œ ë‹¨ì–´ì˜ ìˆ˜ëŠ” V(size of vocabulary)ë¡œ í‘œì‹œí•œë‹¤.\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ ì°¾ê³ ì í•˜ëŠ” ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ classëŠ” ë‹¤ìŒì„ í†µí•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\nc_{MAP} \u0026= \\argmax_{c \\in C}{P(c|d)} \\\\\n\u0026= \\argmax_{c \\in C}{p(d|c)p(c)\\over p(d)} \\\\\n\u0026= \\argmax_{c \\in C}{p(d|c)p(c)} \\\\\n\u0026= \\argmax_{c \\in C}{p(w_{1}, w_{2}, ... , w_{N} | c)p(c)} \\\\\n\u0026= \\argmax_{c \\in C}{\\prod_{i=1}^{N}p(w_{i}|c)p(c)} \\\\\n\u0026= \\argmax_{c \\in C}{\\log(\\prod_{i=1}^{N}p(w_{i}|c)p(c))} \\\\\n\u0026= \\argmax_{c \\in C}{\\sum_{i=1}^{N}\\log p(w_{i}|c) + \\log{p(c)}} \\\\\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ìš°ë¦¬ê°€ language modelì„ ë¬´ì—‡ìœ¼ë¡œ ì •í–ˆëŠ”ì§€ê°€ ì¤‘ìš”í•˜ë‹¤. ìœ„ì—ì„œëŠ” uni-gramì´ë¼ê³  ê°€ì •í•´ì„œ í’€ì´í–ˆì§€ë§Œ, bi-gramì¸ ê²½ìš° documentì˜ í˜•íƒœê°€ $d={(w_{1}, w_{2}), (w_{2}, w_{3}), ... , (w_{N-1}, w_{N})}$ì´ë‹¤. ë”°ë¼ì„œ, ì „ì²´ì ì¸ í¬ê¸°ì™€ vocabularyìì²´ë„ ë°”ë€Œê²Œ ëœë‹¤.\n\nì¦‰, ìš°ë¦¬ëŠ” train setì„ í†µí•´ì„œ vocabularyë¥¼ ì™„ì„±í•œë‹¤. ê·¸ë¦¬ê³ , ê° wordì˜ count ë° í•„ìš”ì— ë”°ë¼ í•„ìš”í•œ word sequenceì˜ countë¥¼ ìˆ˜ì§‘í•˜ì—¬ $p(w_i)$ë¥¼ êµ¬í•œ í›„ ìœ„ì— ë°©ë²•ì„ í†µí•´ì„œ íŠ¹ì • classë¥¼ ì¶”ì¸¡í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\nì´ì œ êµ¬ì²´ì ì¸ Naive Bayesì˜ ë™ì‘ ì ˆì°¨ëŠ” Spam Filteringì´ë¼ëŠ” Case Studyë¥¼ í†µí•´ì„œ ìì„¸íˆ ì‚´í´ë³´ë„ë¡ í•˜ì.\n\n## Case Study. Spam Filtering\n\nì´ˆê¸° NLPê°€ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ì—ˆë˜ ì˜ˆì‹œ ì¤‘ì— í•˜ë‚˜ì´ë‹¤. ì—¬ëŸ¬ ê°œì˜ ë©”ì¼ì— spamì¸ì§€ hamì¸ì§€ë¥¼ labelingí•œ ë°ì´í„°ë¥¼ ê°–ê³  í›„ì— inputìœ¼ë¡œ mail ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ filteringí•˜ëŠ” ê²ƒì´ë‹¤. ìœ„ì—ì„œ ì‚´í´ë³´ì•˜ë˜ í™•ë¥ ì„ ê·¸ëŒ€ë¡œ ì ìš©í•˜ë©´ ëœë‹¤. ì˜ˆì¸¡ì— í•„ìš”í•œ í™•ë¥ ì„ ìŠµë“í•˜ê³ , ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ê³¼ ì´ë¥¼ í‰ê°€í•˜ëŠ” ë°©ë²•ì˜ ìˆœìœ¼ë¡œ ì„¤ëª…í•˜ê² ë‹¤.\n\n### 0. Preprocessing\n\nì‚¬ì‹¤ mail dataì˜ í˜•íƒœê°€ ì´ìƒí•  ìˆ˜ë„ ìˆë‹¤. Subjectë¶€í„° ì‹œì‘í•˜ì—¬ ë‚ ì§œ ë°ì´í„° ê·¸ë¦¬ê³  íŠ¹ìˆ˜ ë¬¸ì ë“±ì´ ì¡´ì¬í•  ìˆ˜ ìˆëŠ”ë°, ì´ë¥¼ ë¨¼ì € ì²˜ë¦¬í•´ì„œ í›„ì— ìˆì„ Modeling ë‹¨ê³„ì—ì„œ ì˜ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í˜•íƒœë¥¼ ë³€í˜•í•´ì£¼ì–´ì•¼ í•œë‹¤.\n\n[ğŸ”— ì´ì „ Posting(Text Processing)](/posts/nlp-text-processing)ì—ì„œ ë°°ì› ë˜ ê¸°ìˆ ë“¤ì„ í™œìš©í•˜ì—¬ ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n\nëŒ€í‘œì ìœ¼ë¡œ í•´ì¤„ ìˆ˜ ìˆëŠ” ì‘ì—…ë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. ëŒ€ì†Œë¬¸ì í†µì¼\n2. alphabetì´ í•˜ë‚˜ë¼ë„ ë“¤ì–´ìˆì§€ ì•Šì€ ë°ì´í„°ëŠ” ì‚­ì œ\n3. date, ì°¸ì¡° ë“±ì„ ì˜ë¯¸í•˜ëŠ” ë°ì´í„° ì‚­ì œ\n\n### 1. Modeling\n\nParameter Estimation / Learning / Modeling ë“±ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” ë‹¨ê³„ì´ë‹¤. ì¼ë‹¨ ìš°ë¦¬ëŠ” train setìœ¼ë¡œë¶€í„° ìš°ë¦¬ê°€ ì›í•˜ëŠ” í™•ë¥ ì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. ê·¸ ì „ì— ìš°ë¦¬ê°€ ì–´ë–¤ language modelì„ ì´ìš©í• ì§€ ì„ íƒí•´ì•¼ í•œë‹¤. ë¨¼ì € uni-gramì¸ ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ train setì´ ì •ì˜ëœë‹¤.\n$$\n\\text{TrainSet} = {(d_{1}, c_{1}),  (d_{2}, c_{2}), ..., (d_{N}, c_{N})}\n$$\n$$\nd_{i} = \\begin{cases}\n  {w_{1}, w_{2}, ... , w_{M_{i}}} \\quad\u0026\\text{unigram} \\\\\n  {(\u003cs\u003e, w_{1}), (w_{1}, w_{2}), ... , (w_{M_{i}}, \u003c/s\u003e)} \\qquad\u0026\\text{bigram}\n\\end{cases}\n$$\n\nì´ì œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” parameter, ì¦‰ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ì´ë‹¤.\n\n\u003e **unigram**\n\n$$\n\\begin{align*}\np(w_{i}|c_{j}) \u0026= {\\text{count}(w_{i}, c_{j}) \\over \\sum_{w \\in V} \\text{count}(w, c_{j})} \\\\\np(c_{j}) \u0026= {\\sum_{i = 1}^{N}{1[c_{i} = c_{j}]} \\over N}\n\\end{align*}\n$$\n\n\u003e **bigram**\n\n$$\n\\begin{align*}\np(w_{i}|w_{i-1},c_{j}) \u0026= {\\text{count}((w_{i-1}, w_{i}), c_{j}) \\over \\sum_{(w^{(1)}, w^{(2)}) \\in V} \\text{count}((w^{(1)}, w^{(2)}), c_{j})} \\\\\np(c_{j}) \u0026= {\\sum_{i = 1}^{N}{1[c_{i} = c_{j}]} \\over N}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ë°˜ë“œì‹œ Smoothingì„ í•´ì£¼ì–´ì•¼ í•œë‹¤. ì™œëƒí•˜ë©´, spam mailì—ì„œ ì•ˆ ë³¸ ë‹¨ì–´ê°€ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë„ˆë¬´ë‚˜ ë†’ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, ì‹¤ì œ $p(w_{i}|c_{j})$ëŠ” ì•„ë˜ì™€ ê°™ì´ ë³€ê²½ëœë‹¤. (ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ë“¤ê¸° ìœ„í•´ì„œ Add-1 ë°©ì‹ì„ ì‚¬ìš©í–ˆë‹¤. - í•´ë‹¹ ë‚´ìš©ì´ ê¸°ì–µì´ ë‚˜ì§€ ì•ŠëŠ”ë‹¤ë©´, [ğŸ”— ì´ì „ í¬ìŠ¤íŒ…](/posts/nlp-language-modeling)ì„ ë‹¤ì‹œ ë³´ê³  ì˜¤ì.)\n\n$$\np(w_{i}|c_{j}) = {\\text{count}(w_{i}, c_{j}) + 1 \\over \\sum_{w \\in V} \\text{count}(w, c_{j}) + |V|}\n$$\n\nì£¼ì˜í•  ì ì€ ë‹¤ì‹œ í•œ ë²ˆ ê°•ì¡°í•˜ì§€ë§Œ, $V$ëŠ” í›„ì— Estimationì—ì„œ inputìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë‹¨ì¼ documentê¹Œì§€ í¬í•¨í•œ Vocabularyì´ë‹¤.\n\n### 2. Estimation\n\nì´ì œ ìš°ë¦¬ê°€ ì–»ì€ parameterë¥¼ ì´ìš©í•´ì„œ ì‹¤ì œ input dataì— ëŒ€í•œ estimationì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\nì´ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\hat{c} = \\argmax_{c \\in C} p(c)\\prod_{w \\in d_{\\text{input}}}p(w|c)\n$$\n\në¬¼ë¡  ì–´ë–¤ n-gramì„ ì“°ëƒì— ë”°ë¼ $d_{\\text{input}}$ë„ í˜•íƒœê°€ ë‹¬ë¼ì§ˆ ê²ƒì´ë‹¤.\n\n### 3. Evaluation\n\nì´ì œ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ê²ƒì´ë‹¤. í‰ê°€ëŠ” ìš°ë¦¬ê°€ ì•Œì•„ë´¤ë˜ Accuracyì™€ F1 Scoreë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤. Binary Classificationì´ê¸° ë•Œë¬¸ì— ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n| prediction\\answer | True                                                                       | False                                                                     |\n| :---------------- | :------------------------------------------------------------------------- | :------------------------------------------------------------------------ |\n| Positive          | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} = c, c = \\text{spam}]$    | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} \\neq c, c = \\text{ham}]$ |\n| Negative          | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} \\neq c, c = \\text{spam}]$ | $\\sum_{(d, c) \\in D_{\\text{test}}} 1[\\hat{c}_{d} = c, c = \\text{ham}]$    |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"nlp-naive-bayes","date":"2022-10-21 15:37","title":"[NLP] 5. Naive Bayes","category":"AI","tags":["NLP"],"desc":"Naive Bayes Modelì€ ê°€ì¥ ì‰½ê²Œ Classificationì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” Modelì´ì§€ë§Œ, ì„±ëŠ¥ì´ ë‹¤ë¥¸ Modelì— ë¹„í•´ ë›°ì–´ë‚˜ì§€ëŠ” ì•Šë‹¤. ê·¸ëŸ¼ì—ë„ Naive BayesëŠ” ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” Modelì´ê¸°ì— ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ê³ , Classificationì˜ insightë¥¼ í‚¤ìš°ëŠ”ë° ë§ì€ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œëŠ”, ì „ë°˜ì ì¸ ê°œë…ê³¼ ì´ë¥¼ ì§ì ‘ Spam Filteringì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì‚´í´ë³¸ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nì´ì „ Postingì—ì„œëŠ” sentenceì˜ ì ì ˆì„±ì„ í™•ì¸í•œë‹¤ë“ ì§€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìœ ì¶”í•œë‹¤ë“ ì§€ ì˜¤íƒ€ë¥¼ ì •ì •í•˜ëŠ” ë“±ì— í•„ìš”í•œ ê¸°ë³¸ì ì¸ Language Modeling ë°©ì‹ì„ ì‚´í´ë³´ì•˜ë‹¤. ì´ë²ˆì—ëŠ” ì‹¤ì œë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì˜ˆì œì¸ Classificationì„ Language Modelì„ ì´ìš©í•˜ì—¬ ì–´ë–»ê²Œ êµ¬í˜„í•˜ëŠ”ì§€ë¥¼ ë‹¤ë£¬ë‹¤.\n\n## Classification\n\nClassificationì€ inputì´ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ ì•Œë§ì€ ë¶„ë¥˜ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. ë‹¨ìˆœíˆ Ruleì— ê¸°ë°˜í•˜ì—¬ ì´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ë„ ìˆì§€ë§Œ, Statisticí•œ Language Modelingì„ ì´ìš©í•˜ë©´, ë” ì •í™•ë„ê°€ ë†’ì€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ê²°êµ­ Statistic Predictionì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” 3ê°œ(Estimation, Modeling, Evaluation)ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ë´ì•¼ í•˜ëŠ” ê²ƒì€ Classificationë„ ë™ì¼í•˜ë‹¤. ë”°ë¼ì„œ, ì´ì— ëŒ€í•´ì„œ ì‚´í´ë³¼ ê²ƒì´ê³ , ê·¸ ì „ì— ë¨¼ì € Classification Model ì˜ ì¢…ë¥˜ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## Generative Model vs Discriminative Model\n\nClassificationì—ì„œ ì´ìš©ë˜ëŠ” Modelì„ í¬ê²Œ ë‘ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ”ë° ì´ì— ëŒ€í•´ì„œ ë¨¼ì € ì•Œì•„ë³´ë„ë¡ í•˜ì.\n\n1. **Generative Model(ìƒì„± Model)**\n   1. Naive Bayes\n   2. Hidden Markov Model(HMM)\n2. **Discriminative Model(íŒë³„ Model)**\n   1. Logistic Regression\n   2. K Nearest Neighbors\n   3. Support Vector Machine\n   4. Maximum Entropy Model(MaxEnt)\n   5. Neural Network(Deep Learning)\n\në‘ Modelì˜ ê°€ì¥ í° ì°¨ì´ì ì€ ì¶”ë¡ ì˜ ê³¼ì •ì´ë‹¤. ìš°ë¦¬ê°€ ì›í•˜ëŠ” ë°ì´í„° $P(\\text{class}=c | \\text{input} = \\text{data})$(íŠ¹ì • dataê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê° classì˜ ì†í•  í™•ë¥ )ë¥¼ ì–»ëŠ” ê³¼ì •ì´ ì„œë¡œ ë‹¤ë¥´ë‹¤.\n\n**ì²« ë²ˆì§¸**ë¡œ, $P(\\text{class}=c, \\text{input} = \\text{data})$ì¼ í™•ë¥ ì„ êµ¬í•˜ì—¬ **ê°„ì ‘ì **ìœ¼ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\n$$\n\\begin{align*}\nP(\\text{class}=c | \\text{input} = \\text{data}) \u0026= {{P(\\text{class}=c, \\text{input} = \\text{data})}\\over{P(\\text{input} = \\text{data})}} \\\\\n\u0026\\propto {P(\\text{class}=c, \\text{input} = \\text{data})}\n\\end{align*}\n$$\n\nì´ëŸ° ì‹ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ì¶”ë¡ í•˜ëŠ” ë°©ì‹ì„ \u003cmark\u003eGenerative Model\u003c/mark\u003eì´ë¼ê³  í•œë‹¤. ì´ ë°©ì‹ì€ ê²°êµ­ Conditional Probabilityë¥¼ ì¶”ë¡ í•˜ê¸° ìœ„í•´ì„œ Joint Probabilityë¥¼ ì´ìš©í•˜ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì— ì–´ëŠì •ë„ í•œê³„ê°€ ì¡´ì¬í•œë‹¤ëŠ” ì ì„ ìœ ì˜í•˜ì.\n\n**ë‘ ë²ˆì§¸**ë¡œëŠ”, $P(\\text{class}=c | \\text{input} = \\text{data})$ë¥¼ **ì§ì ‘ì **ìœ¼ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œ, ë§ˆì¹œ Conditional Probabilityë¥¼ êµ¬í•œ ê²ƒê³¼ ìœ ì‚¬í•œ íš¨ê³¼ë¥¼ ë‚´ëŠ” **Discriminant Function(íŒë³„ í•¨ìˆ˜)**ì´ë¼ëŠ” íŠ¹ë³„í•œ í•¨ìˆ˜ë¥¼ inputì— ì ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ í•¨ìˆ˜ ì¤‘ì—ì„œ ê°€ì¥ ëŒ€í‘œì ì¸ ê²ƒì´ Softmax functionì´ë‹¤. ìš°ë¦¬ê°€ ë§Œì•½ inputì„ softmax functionì— ì…ë ¥í•˜ê²Œ ë˜ë©´, ì´ ê°’ì€ [0, 1] ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ í‘œí˜„ëœë‹¤. ì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” í•´ë‹¹ inputì´ classì¸ ê²½ìš° 1ì— ê°€ê¹ê²Œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° 0ì— ê°€ê¹ê²Œ í‘œí˜„í•˜ì—¬ ì—¬ëŸ¬ ë°ì´í„°ì— ì ìš©í•˜ë©´, classì˜ inpuutì— ë”°ë¥¸ ë¶„í¬ ì–‘ìƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , ì´ ë¶„í¬ ì–‘ìƒì„ í™•ë¥ ë¡œ ì¦‰ê°ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— softmax functionì„ ì·¨í•œ ê²°ê³¼ê°€ $P(\\text{class}=c | \\text{input} = \\text{data})$ê³¼ ë¹„ë¡€í•œë‹¤ëŠ” ê²°ë¡ ì„ ë‚¼ ìˆ˜ ìˆë‹¤. ìì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•˜ë‹¤ë©´, [ğŸ”— Logistic Regression](/posts/ml-logistic-regression#Logistic-Regression)ì„ ì°¸ê³ í•˜ë„ë¡ í•˜ì. ì´ëŸ¬í•œ ë°©ì‹ì„ ìš°ë¦¬ëŠ” \u003cmark\u003eDiscriminative Model\u003c/mark\u003eì´ë¼ê³  í•œë‹¤.\n\nìœ„ì—ì„œ ì œì‹œí•œ ë°©ë²•ë“¤ ì¤‘ ëŒ€í‘œì ì¸ ë°©ë²•ë“¤ì€ ë³„ë„ì˜ Postingì„ í†µí•´ì„œ ì •ë¦¬í•˜ì˜€ë‹¤. í•´ë‹¹ ë§í¬ë¥¼ ì°¸ì¡°í•˜ì—¬ í™•ì¸í•´ë³´ë„ë¡ í•˜ì.\n\n- **Generative Model(ìƒì„± Model)**\n  - [ğŸ”— Naive Bayes](/posts/nlp-naive-bayes)\n  - [ğŸ”— Hidden Markov Model(HMM)](/posts/nlp-hmm)\n- **Discriminative Model(íŒë³„ Model)**\n  - [ğŸ”— Maximum Entropy Model(MaxEnt)](/posts/nlp-maxent)\n  - [ğŸ”— Logistic Regression](/posts/ml-logistic-regression)\n\n## Estimation\n\nì–´ë–¤ Modelì„ ì„ íƒí–ˆë‹¤ê³  í•˜ë”ë¼ë„ ê²°êµ­ ìš°ë¦¬ê°€ Classë¥¼ ê²°ì •í•˜ëŠ” ê³¼ì •ì„ ë™ì¼í•˜ë‹¤. ìœ„ì˜ ê³¼ì •ì„ í†µí•´ì„œ ì–´ì°Œë˜ì—ˆë“  ë‹¤ìŒ ê°’ì„ ì°¾ìœ¼ë©´ ëœë‹¤.\n\n$$\n\\begin{align*}\nc^{\\prime} \u0026= \\argmax_{c \\in C}{P(\\text{class}=c | \\text{input} = \\text{data})}\n\\end{align*}\n$$\n\n## Modeling\n\nModelì„ ë§Œë“œëŠ” ê³¼ì •, ì¦‰ í•™ìŠµí•˜ëŠ” ê³¼ì •ì€ ê²°êµ­ Modelì˜ êµ¬í˜„ë§ˆë‹¤ ì²œì°¨ ë§Œë³„ì´ë‹¤. Naive BayesëŠ” ë‹¨ìˆœí•˜ê²Œ dataì˜ wordì™€ countë¥¼ í™œìš©í•˜ê³ , HMMì€ EM algorithmì„ í™œìš©í•˜ë©°, Linear Regressionì€ Gradient Descentë¥¼ í™œìš©í•œë‹¤. ë”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•Šê³  ìœ„ì—ì„œ ì œì‹œí•œ ë§í¬ë¥¼ ë”°ë¼ê°€ì„œ ê° Modelë§ˆë‹¤ì˜ í•™ìŠµë²•ì„ í™•ì¸í•´ë³´ë„ë¡ í•˜ì.\n\n## Evaluation\n\nClassificationì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ê²ƒ ì—­ì‹œ ì¤‘ìš”í•œ ì¼ì´ë‹¤. ê°€ì¥ ì‰¬ìš´ Binary Classificationë¶€í„° ì•Œì•„ë³´ì.\n\nbinary classificaitonì˜ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì´ 4ê°œ ì¤‘ í•˜ë‚˜ë¡œ ê²°ì •ëœë‹¤.\n\n| prediction\\answer | True           | False          |\n| :---------------- | :------------- | :------------- |\n| Positive          | true positive  | false positive |\n| Negative          | false negative | true negative  |\n\nì´ë¥¼ ì‰½ê²Œ ì´í•´í• ë ¤ë©´, ë³‘(ì½”ë¡œë‚˜)ì˜ ì–‘ì„±/ìŒì„± íŒì •ì´ rowì— í•´ë‹¹í•˜ê³ , ì‹¤ì œ ë³‘ì˜ ì—¬ë¶€ë¥¼ columnìœ¼ë¡œ ìƒê°í•˜ë©´ ì‰½ë‹¤. ë˜í•œ, ê° cellì˜ ê°’ì´ í—·ê°ˆë¦´ ìˆ˜ ìˆëŠ”ë°, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì´ ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ì˜ˆì¸¡ ê²°ê³¼ëŠ” ê·¸ëŒ€ë¡œ ë³´ì—¬ì£¼ë©´ì„œ, ì´ê²ƒì´ í‹€ë ¸ëŠ”ì§€ ë§ì•˜ëŠ”ì§€ë¥¼ ì•ì— true/falseë¡œ í‘œí˜„í–ˆë‹¤ê³  ìƒê°í•˜ë©´ ì‰½ë‹¤.\n\nclassificationì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œëŠ” ëŒ€í‘œì ìœ¼ë¡œ 4 ê°€ì§€ê°€ ìˆë‹¤.\n\n1. **Accuracy(ì •í™•ë„)**  \n   ê°€ì¥ ì‰½ê²Œ ê·¸ë¦¬ê³  ì¼ë°˜ì ìœ¼ë¡œ ìƒê°í•˜ëŠ” ì§€í‘œë‹¤. ìœ„ì˜ í‘œì—ì„œëŠ” ì „ì²´ ê²½ìš°ì˜ ìˆ˜ë¥¼ ë”í•˜ì—¬ ì˜³ê²Œ ì˜ˆì¸¡í•œ ê²ƒ(true postive, true negative)ì˜ í•©ì„ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.\n   $tp + fn \\over tp + fp + fn + tn$  \n   í•˜ì§€ë§Œ, ì´ ë°©ì‹ì€ í•œê³„ê°€ ìˆë‹¤. ë°”ë¡œ, ë°ì´í„°ê°€ í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì³ì ¸ìˆì„ ë•Œì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìš°ë¦¬ê°€ ì§„ì§œë¥¼ ì§„ì§œë¼ê³  ë§ì¶œí™•ë¥ ì€ ë†’ì§€ë§Œ, ê°€ì§œë¥¼ ê°€ì§œë¼ê³  ë§ì¶œ í™•ë¥ ì´ ë‚®ë‹¤ê³  í•  ë•Œ, ì´ë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ê¸°ê°€ ì–´ë µë‹¤. ê·¸ëŸ°ë° ë°ì´í„°ì—ì„œ ì§„ì§œê°€ ê°€ì§œë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ì„ ê²½ìš° ì •í™•ë„ëŠ” ì¢‹ì€ ì§€í‘œë¡œ ì“°ê¸° ì–´ë µë‹¤ëŠ” ê²ƒì´ë‹¤.\n2. **Precision(ì •ë°€ë„, ì •ë‹µë¥ )**  \n   ì‰½ê²Œ ì •ë‹µ ìì²´ë¥¼ ë§í í™•ë¥ ì…ë‹ˆë‹¤.  \n   $tp \\over tp + fn$\n3. **Recall(ì¬í˜„ìœ¨)**  \n   ì˜ˆì¸¡ì´ ë§ì„ í™•ë¥ ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  \n   $tp \\over tp + fp$\n4. **F1 Score**  \n   ì¢€ ë” ì„¸ë¶„í™”ëœ í‰ê°€ì§€í‘œì´ë‹¤. ì¡°í™” í‰ê· ì— ê¸°ë°˜í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì •í™•í•˜ê²Œ í‰ê°€í•  ë•Œ ì‚¬ìš©í•œë‹¤.  \n   ${2\\over{{1\\over\\text{Precision}} + {1\\over\\text{Recall}}}} = 2 \\times {\\text{Precision} \\times \\text{Recall} \\over \\text{Precision} + \\text{Recall}}$\n\nì—¬ê¸°ê¹Œì§€ ë´¤ìœ¼ë©´, ìŠ¬ìŠ¬ multi classì˜ ê²½ìš°ì—ëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ì§€ ê¶ê¸ˆí•  ê²ƒì´ë‹¤. ëŒ€ê²Œ ë‘ ê°€ì§€ ë°©ë²•ì„ í†µí•´ì„œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\n\u003e **1. Micro Average**\n\nì „ì²´ classë¥¼ í•˜ë‚˜ì˜ binary tableë¡œ í•©ì¹˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, í´ë˜ìŠ¤ê°€ A, B, C 3ê°œê°€ ìˆë‹¤ë©´, ê° í´ë˜ìŠ¤ ë³„ë¡œ ì˜ˆì¸¡ ì„±ê³µë„ë¥¼ binaryë¡œ í‘œì‹œí•˜ê³ , ì´ë¥¼ í•˜ë‚˜ì˜ í…Œì´ë¸”ë¡œ í•©ì¹˜ëŠ” ê²ƒì´ë‹¤. ê·¸ í›„ì—ëŠ” binaryì—ì„œ ê³„ì‚°í•˜ëŠ” ì‹ì„ ê·¸ëŒ€ë¡œì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  \n\n\u003e **2. Macro Average**\n\nmulti classì˜ ê²½ìš°ì—ë„ ë³„ë¡œ ë‹¤ë¥¼ ê²ƒì€ ì—†ë‹¤. ë‹¨ì§€ Precisionê³¼ Recall ê·¸ë¦¬ê³  Accuracyê°€ ì–´ë–»ê²Œ ë°”ë€ŒëŠ”ì§€ë§Œ ì•Œë©´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.  \n\n| prediction\\answer | c1            | c2            | c3            | c4            |\n| :---------------- | :------------ | :------------ | :------------ | :------------ |\n| c1                | true positive | x             | x             | x             |\n| c2                | x             | true positive | x             | x             |\n| c3                | x             | x             | true positive | x             |\n| c4                | x             | x             | x             | true positive |\n\n- Precision: $c_{ii} \\over \\sum_{j}c_{ij}$\n- Recall: $c_{ii} \\over \\sum_{j}c_{ji}$\n- Accuracy: $c_{ii} \\over \\sum_{i}\\sum_{j}c_{ij}$\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"nlp-classification","date":"2022-10-21 13:37","title":"[NLP] 4. Classification","category":"AI","tags":["NLP","Classification","Generative","Discriminative","ModelEvaluation"],"desc":"ì´ì „ Postingì—ì„œëŠ” sentenceì˜ ì ì ˆì„±ì„ í™•ì¸í•œë‹¤ë“ ì§€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìœ ì¶”í•œë‹¤ë“ ì§€ ì˜¤íƒ€ë¥¼ ì •ì •í•˜ëŠ” ë“±ì— í•„ìš”í•œ ê¸°ë³¸ì ì¸ Language Modeling ë°©ì‹ì„ ì‚´í´ë³´ì•˜ë‹¤. ì´ë²ˆì—ëŠ” ì‹¤ì œë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì˜ˆì œì¸ Classificationì„ Language Modelì„ ì´ìš©í•˜ì—¬ ì–´ë–»ê²Œ êµ¬í˜„í•˜ëŠ”ì§€ë¥¼ ë‹¤ë£¬ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nì´ì œ í†µê³„ì ì¸ ê´€ì ì—ì„œ NLì„ inputìœ¼ë¡œ í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ë°©ë²•ì„ ì°¾ì„ ê²ƒì´ë‹¤. ì´ë¥¼ Language Modelingì´ë¼ê³  í•˜ë©°, ì´ë¥¼ ìœ„í•´ì„œ ë˜ëŠ” ì´ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•œ ë°©ë²•ë“¤ì„ ì†Œê°œí•  ê²ƒì´ë‹¤.\n\n## Noisy Channel\n\nì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ì›í•˜ëŠ” ê²°ê³¼ê°€ ìˆë‹¤. ê·¸ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” ë§ì„ í•˜ê±°ë‚˜ í–‰ë™ì„ í•˜ê±°ë‚˜ ê¸€ì„ ì“´ë‹¤. ê·¸ ê³¼ì •ì€ ìš°ë¦¬ê°€ ê°–ê³  ì‹¶ì€ Aë¼ëŠ” ê²ƒì„ ì–»ê¸° ìœ„í•´ì„œ Bë¼ëŠ” í–‰ë™ì„ ëŒ€ì‹ í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. ì¦‰, ìš°ë¦¬ëŠ” ì´ë¥¼ Aì— noiseê°€ ê»´ì„œ Bë¼ëŠ” ê²ƒì´ ìƒì„±ë˜ì—ˆë‹¤ê³  ìƒê°í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ìš°ë¦¬ê°€ ê´€ì¸¡í•  ìˆ˜ ìˆëŠ” ê²ƒì€ Bë°–ì— ì—†ëŠ” ê²ƒì´ë‹¤.\n\nì´ëŠ” ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” NLì—ì„œë„ ë™ì¼í•˜ë‹¤. ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²°ê³¼ê°’ Aë¥¼ ì–»ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” Bë¼ëŠ” ë¬¸ì¥, ìŒì„±ì„ ì œì‹œí•œë‹¤. ê·¸ ê²°ê³¼ê°€ ì›í•˜ëŠ” ê²°ê³¼ë¡œ ë  ìˆ˜ ìˆëŠ” í™•ë¥ ì„ ì–»ì–´ì„œ ìµœì¢… ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ ëª©í‘œì¸ ê²ƒì´ë‹¤.\n\nì´ ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤.\n\n$$\nP(A|B) = {P(B|A)P(A)\\over{P(B)}}\\quad(\\text{Bayes Rule})\n$$\n\nìš°ë¦¬ê°€ ì–»ê³  ì‹¶ì€ $P(A|B)$ ë¥¼ ì–»ê¸° ìœ„í•´ì„œ, $P(A)$ì™€ $P(B|A)$ ë¥¼ í†µí•´ì„œ êµ¬í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. ì´ì— ëŒ€í•œ ë” ìì„¸í•œ ë‚´ìš©ì€ MLì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì´ë‹¤. ì¶”ì²œí•˜ëŠ” Postingì€ [ğŸ”— [ML] Parametric Estimation](/posts/ml-parametric-estimation)ì´ë‹¤.\n\n## Language Modeling\n\nê²°êµ­ ìš°ë¦¬ê°€ ì–»ê³  ì‹¶ì€ ê²ƒì€ íŠ¹ì • ë¬¸ì¥ì˜ í• ë‹¹ëœ í™•ë¥ ì¸ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, Language Modelì€ inputìœ¼ë¡œ word sequenceê³¼ ë“¤ì–´ì™”ì„ ë•Œ, í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë‹¤.\nê·¸ë¦¬ê³ , ì´ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ parameterë¥¼ ì°¾ëŠ” ê³¼ì •ì„ Language Modelingì´ë¼ê³  í•œë‹¤.\n\n## Input(N-gram)\n\nëŒ€ê²Œ ì´ëŸ¬í•œ ëª¨ë¸ì€ ë¬¸ì¥ ë˜ëŠ” wordì˜ ë°°ì—´ì´ ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì§ˆ ë•Œ, $W = w_{1}\\ w_{2}\\ w_{3}\\ ...\\ w_{n}$ ì•„ë˜ì™€ ê°™ì€ í˜•íƒœë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\n\n- **Single word probability**  \n  í•˜ë‚˜ì˜ ë‹¨ì–´ì˜ í™•ë¥ ì„ ë‚˜íƒ€ë‚¼ ë•Œ ë‹¨ìˆœí•˜ê²Œ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•œë‹¤.  \n  $P(w_{i})\\quad(w_{i} \\in W)$\n- **Sequence of Words probability**  \n  ì¼ë°˜ì ìœ¼ë¡œ sentenceì˜ í™•ë¥ ì„ ë‚˜íƒ€ë‚¼ ë•Œ, ì—¬ëŸ¬ ë¬¸ì¥ì„ í•œêº¼ë²ˆì— ê°€ì§€ëŠ” í™•ë¥ ì´ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.  \n  $P(W) = P(w_{1}, w_{2}, w_{3}, ..., w_{n})$\n- **single word probability with context**  \n  ì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ì´ì „ì— ì‚¬ìš©í•œ ë‹¨ì–´ê°€ ë¬¸ë§¥ì´ë¼ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, êµ¬ì²´ì ì¸ ë‹¨ì–´ë“¤ ì´í›„ì— íŠ¹ì • ë‹¨ì–´ê°€ ë‚˜ì˜¤ëŠ” ê²ƒì€ ë¬¸ë§¥ì„ ë°˜ì˜í•œ í™•ë¥ ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.  \n  $P(W) = P(w_{5}| w_{1}, w_{2}, w_{3}, w_{4})$\n\nìœ„ì˜ ì‹ì„ ë³´ê²Œ ë˜ë©´, ìš°ë¦¬ëŠ” ë‹¤ì‹œ í•œë²ˆ sentenceì˜ í™•ë¥ ì„ ë‹¤ì‹œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\nP(W) = P(w_{1}) \\times P(w_{2}|w_{1}) \\times P(w_{3}|w_{1},w_{2}) \\times ... \\times P(w_{n}| w_{1},w_{2},..., w_{n-1})\n$$\n\nìœ„ì˜ ì‹ì„ ë³´ê²Œë˜ë©´, Wê°€ ì§§ë‹¤ê³  í•˜ë”ë¼ë„ êµ‰ì¥íˆ ë§ì€ ì²˜ë¦¬ê°€ í•„ìš”í•˜ê³ , ì €ì¥ì„ ìœ„í•´ ë§ì€ ê³µê°„ì´ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” í˜„ì¬ ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë„ˆë¬´ ì˜¤ë˜ëœ ë‹¨ì–´ì— ëŒ€í•´ì„œëŠ” ë¬´ì‹œë¥¼ í•˜ë„ë¡ í•˜ëŠ” ë°©ë²•ì„ ì·¨í•˜ëŠ” ê²ƒì´ë‹¤.(**Markov Chain**) ì´ë¥¼ \"n ë²ˆì§¸ê¹Œì§€ í—ˆë½\"í–ˆì„ ë•Œ, ì´ë¥¼ n-gram ì´ë¼ê³  ë¶€ë¥¸ë‹¤.\n\n$$\np(W) = \\prod_{i=1}^{n}{p(w_{i}|w_{i-n+1},w_{i-n+2},...,w_{i-1})}\n$$\n\nê·¸ë ‡ë‹¤ë©´, n-gramì—ì„œ ì ì ˆí•œ nì´ë€ ë¬´ì—‡ì¼ê¹Œ? ì¼ë°˜ì ìœ¼ë¡œëŠ” nì´ í¬ë‹¤ëŠ” ê²ƒì€ contextë¥¼ ë§ì´ ë°›ì•„ë“¤ì¼ ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ë¡œ ë°›ì•„ë“¤ì—¬ì§ˆ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, nì´ í´ìˆ˜ë¡ ì„±ëŠ¥ì˜ ìµœì í™” ê°€ëŠ¥ì„±ì´ ë” ë†’ë‹¤. í•˜ì§€ë§Œ, Vocabularyì˜ ì‚¬ì´ì¦ˆê°€ ì»¤ì§€ëŠ” ê²½ìš°ë¥¼ ì˜ˆë¥¼ ë“¤ì–´ë³´ì. ì—¬ê¸°ì„œëŠ” $|V| = 60$k ë¼ê³  í•´ë³´ì.\n\n| n-gram          | p(w_{i})                         | # of parameters   |\n| :-------------- | :------------------------------- | :---------------- |\n| 0-gram(uniform) | ${1\\over\\vert V\\vert}$           | 1                 |\n| 1-gram(unigram) | $p(w_{i})$                       | $6\\times10^4$     |\n| 2-gram(bigram)  | $p(w_{i}\\vert w_{i-1})$          | $3.6\\times10^9$   |\n| 3-gram(trigram) | $p(w_{i}\\vert w_{i-2}, w_{i-1})$ | $2.16\\times10^14$ |\n\nnì´ ì»¤ì§ˆ ìˆ˜ë¡ ê°€ëŠ¥í•œ ì¡°í•©ì˜ ìˆ˜ëŠ” êµ‰ì¥íˆ ì»¤ì§€ê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ ë³´ì§€ ëª»í•˜ëŠ” ê²½ìš°ì˜ ìˆ˜ë„ êµ‰ì¥íˆ ì¦ê°€í•˜ê²Œ ë˜ì–´ dataìì²´ì˜ ë¹ˆë„ê°€ ì ì–´ì§€ëŠ” í˜„ìƒ(sparse)ì´ ë°œìƒí•œë‹¤. ë”°ë¼ì„œ, ëŒ€ê²Œì˜ ê²½ìš° ìµœëŒ€ nì˜ í¬ê¸°ëŠ” 3ì •ë„ë¡œ í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\n\n```plaintext\n ğŸ¤” ì£¼ì˜\n\n ì‹¤ì œ ë°ì´í„°ë¥¼ ê°€ê³µí•  ë•Œì—ëŠ” bigramë¶€í„°ëŠ” ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ í‘œì‹œí•´ì£¼ì–´ì•¼ í•œë‹¤. \n ê·¸ë ‡ì§€ ì•Šìœ¼ë©´, ì²«ë²ˆì§¸ ë¬¸ìì˜ í™•ë¥ ì„ êµ¬í•  ë•Œ, ì´ì „ ë‹¨ì–´ì˜ ì˜í–¥ì„ ë°›ì„ ìˆ˜ ì—†ë‹¤.\n ì •í•´ì§„ ê·œì¹™ì€ ì—†ì§€ë§Œ, ëŒ€ê²Œ \u003cs\u003e\u003c/s\u003eë¥¼ ì´ìš©í•œë‹¤.\n ex.  bigram : \u003cs\u003e w1 w2 w3 w4 \u003c/s\u003e\n     trigram : \u003cs\u003e \u003cs\u003e w1 w2 w3 w4 \u003c/s\u003e \u003c/s\u003e\n```\n\n```plaintext\n ğŸ¤” Google N-gram\n\n êµ¬ê¸€ì—ì„œ 2006ë…„ì— N-gramì„ ì§ì ‘ êµ¬ì„±í•œ ê²ƒì´ ìˆë‹¤. \n ì´ 1,024,908,257,229ê°œì˜ ë‹¨ì–´ê°€ ì¡´ì¬í•˜ê³ , 40íšŒ ì´ìƒ ë“±ì¥í•˜ëŠ” 5-gramì´ 1,176,470,663ê°œ ì¡´ì¬í•œë‹¤. \n ì´ Vocabularyì˜ sizeëŠ” 200ë²ˆ ì´í•˜ë¡œ ë“±ì¥í•˜ëŠ” ê²ƒì€ ì œì™¸í•˜ë©´, 13,588,391ê°œì´ë‹¤. \n```\n\n## Estimation\n\nMLì—ì„œëŠ” Estimationì„ ìˆ˜í–‰í•  ë•Œ, continuousí•˜ê²Œ ì¶”ì •í•˜ì˜€ë‹¤. ì¦‰, ë³´ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ continuousí•œ ë¶„í¬ì˜ parameterë§Œ ì¶”ì •í•˜ë©´ ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ, NLPì—ì„œëŠ” ë‹¤ë¥´ë‹¤. NLë¥¼ continuousí•˜ê²Œ í‘œí˜„í•  ë§ˆë•…í•œ ë°©ë²•ì´ ì—†ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê²°êµ­ ëª¨ë“  í™•ë¥ ì„ discreteí•˜ê²Œ êµ¬í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” íŠ¹ì • ë‹¨ì–´ì˜ í™•ë¥ ì„ êµ¬í•˜ëŠ” ë°©ë²•ì€ ë‹¨ í•˜ë‚˜ê°€ ëœë‹¤.\n\n$$\n|T| = \\text{count of observed tokens}\n$$\n$$\nc(w_{i}) = \\text{count of observed } w_{i}\n$$\n$$\n\\begin{align*}\n\u0026P(w_{i}) = {{c(w_{i})}\\over{|T|}} \\\\\n\u0026P(w_{i}| w_{i-1}) = {{c(w_{i-1}, w_{i})}\\over{c(w_{i-1})}} \\\\\n\u0026P(w_{i}| w_{i-2}, w_{i-1}) = {{c(w_{i-2}, w_{i-1}, w_{i})}\\over{c(w_{i-2}, w_{i-1})}} \\\\\n\\end{align*}\n$$\n\nì´ë•Œ, ë°˜ë“œì‹œ sequenceì˜ ìˆœì„œë¥¼ ìœ ì˜í•˜ë„ë¡ í•˜ì. ìˆœì„œê°€ ë°”ë€Œë©´ ë‹¤ë¥¸ ì¢…ë¥˜ì´ë‹¤.\n\n\u003e **Small Example**\n\në°ì´í„°ê°€ ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì§„ë‹¤ê³  í•˜ì.\n\n```plaintext\n He can buy the can of soda.\n```\n\nì´ë•Œ ê° n-gramì„ ì´ìš©í•œ modelì˜ í™•ë¥ ë“¤ì„ ì‚´í´ë³´ì.\n\n| model    | probability                                                                                                                                                     |\n| :------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| uni-gram | $p(He)=p(buy)=p(the)=p(of)=p(soda)=p(.) = 0.125$\u003cbr /\u003e $p(can)=0.25$                                                                                            |\n| bi-gram  | $p(He\\vert \u003cs\u003e)=p(can\\vert He)=p(the\\vert buy)=p(can\\vert the)=p(soda\\vert of)=p(.\\vert sode) =p(\u003c/s\u003e \\vert .) = 1$\u003cbr /\u003e $p(buy\\vert can)=p(of\\vert can)= 0.5$ |\n| tri-gram | $p(He\\vert \u003cs\u003e, \u003cs\u003e)=p(can\\vert \u003cs\u003e, He)=p(the\\vert He, buy)=...=p(\u003c/s\u003e\\vert ., \u003c/s\u003e) =1$                                                                       |\n\n## Evaluation\n\ní‰ê°€í•  ë•ŒëŠ” MLê³¼ ê²°êµ­ì€ ë™ì¼í•˜ë‹¤. ìš°ë¦¬ê°€ í™•ë¥ ë¶„í¬ë¥¼ êµ¬í•  ë•Œ, ì‚¬ìš©í•œ ë°ì´í„° ì™¸ì— ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ ì˜ ì ìš©ì´ ë˜ì—ˆëŠ”ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, wordì˜ ê°¯ìˆ˜ì™€ ë°ì´í„°ì˜ ìˆ˜ê°€ êµ‰ì¥íˆ ë§ì€ NLì˜ íŠ¹ì„±ìƒ ì´ Evaluation ë‹¨ê³„ì—ë§Œ êµ‰ì¥íˆ ë§ì€ ì‹œê°„ì„ ì†Œëª¨í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì¦‰ê°ì ì¸ í‰ê°€ë¥¼ ìœ„í•´ì„œ ì‚¬ìš©í•˜ëŠ” ì²™ë„ê°€ ìˆë‹¤.\n\n\u003e **Perplexity**\n\ntrain setì„ í†µí•´ í•™ìŠµì„ í•˜ê³ , test setì„ í†µí•´ì„œ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ë•Œ, train setì„ í†µí•´ êµ¬í•œ í™•ë¥ ì´ ì‹¤ì œ test setì—ì„œ ì–´ëŠì •ë„ì˜ Entropyë¥¼ ë°œìƒì‹œí‚¤ëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ë‹¤. ì›ë˜ì˜ ì‹ì€\n$PP = 2^{H}$ì´ì§€ë§Œ, ì´ë¥¼ ë³€í˜•í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\nPP(W) \u0026= \\sqrt[N]{1 \\over P(w_1, w_2, ..., w_N)} \\\\\n\u0026= \\sqrt[N]{\\prod_{i=1}^{N}{P(w_i|w_{i-n+1}, w_{i-n+2}, ..., w_{i-1})}}\n\\end{align*}\n$$\n\nì´ë¥¼ í†µí•´ì„œ, ì‹¤ì œë¡œ í•´ë‹¹ ë¬¸ì œê°€ ë„ˆë¬´ ì–´ë µì§€ëŠ” ì•Šì€ì§€, ì„ íƒí•œ modelì´ ì˜ëª»ë˜ì§€ëŠ” ì•Šì•˜ëŠ”ì§€ë¥¼ íŒë‹¨í•œë‹¤.\n\n| model    | probability                                                                                                                                                     | entropy |\n| :------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------ |\n| uni-gram | $p(He)=p(buy)=p(the)=p(of)=p(soda)=p(.) = 0.125$\u003cbr /\u003e $p(can)=0.25$                                                                                            | 2.75    |\n| bi-gram  | $p(He\\vert \u003cs\u003e)=p(can\\vert He)=p(the\\vert buy)=p(can\\vert the)=p(soda\\vert of)=p(.\\vert sode) =p(\u003c/s\u003e \\vert .) = 1$\u003cbr /\u003e $p(buy\\vert can)=p(of\\vert can)= 0.5$ | 0.25    |\n| tri-gram | $p(He\\vert \u003cs\u003e, \u003cs\u003e)=p(can\\vert \u003cs\u003e, He)=p(the\\vert He, buy)=...=p(\u003c/s\u003e\\vert ., \u003c/s\u003e) =1$                                                                       | 0       |\n\nìœ„ì˜ ì˜ˆì‹œë¥¼ ê°€ì ¸ì™€ì„œ ë´ë³´ì. ë¬¼ë¡  ë™ì¼í•œ datasetì—ì„œ perplexityë¥¼ ì¸¡ì •í•˜ê¸°ëŠ” í–ˆì§€ë§Œ, nì´ ì»¤ì§ˆ ìˆ˜ë¡ ì ì  entropyê°€ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ëŸ° dataê°€ ì¢‹ì€ ê±¸ê¹Œ? ì´ëŠ” ì¢‹ì€ ê²Œ ì•„ë‹ˆë‹¤. ì™œëƒí•˜ë©´, í•´ë‹¹ datasetì—ì„œë§Œ ì˜ ì‘ë™í•˜ë„ë¡ ë˜ì–´ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì¼ëª… **overfitting**ì´ë‹¤.\n\n## Smooting\n\nìœ„ì—ì„œ ë§í•œ overfittingì„ ì–´ëŠì •ë„ í•´ì†Œí•  ë¿ë§Œ ì•„ë‹ˆë¼ ì •ë§ í° ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” probabilityê°€ 0ì´ ë˜ëŠ” ë¬¸ì œ(ìš°ë¦¬ê°€ trainsetì„ í†µí•´ í•™ìŠµí•œ í™•ë¥  ë¶„í¬ì—ì„œ testsetì— ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” í™•ë¥ ê°’ì´ ì—†ì„ ë•Œ, ì¦‰ í•´ë‹¹ í™•ë¥ ì´ 0ì¼ë•Œ)ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” smoothingì´ í•„ìˆ˜ì ì´ë‹¤. probabilityê°€ 0ì´ ëœë‹¤ëŠ” ê²ƒì€ í›„ì— ìœ„ì—ì„œ êµ¬í•œ í™•ë¥ ë¡œ Predictionì„ í•  ë•Œ ëª¨ë“  ì˜ˆì¸¡ì„ ë§ì¹˜ëŠ” ìš”ì¸ì´ ëœë‹¤. ì™œëƒí•˜ë©´, ì¶”ì •í™•ë¥ ì˜ ìµœì  Entropyë¥¼ ì˜ë¯¸í•˜ëŠ” Cross Entropyë¥¼ $\\infin$ë¡œ ë§Œë“¤ê¸° ë•Œë¬¸ì´ë‹¤. (ìµœì  Entropyê°€ ë¬´í•œëŒ€ë¼ëŠ” ê²ƒì€ ì¶”ì •ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.)\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” probabilityê°€ 0ì´ ë˜ì§€ ì•Šê²Œ í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ê¸°ì¡´ì˜ í™•ë¥ ì˜ ì¼ë¶€ë¥¼ ë‚˜ëˆ„ì–´ì£¼ë„ë¡ í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤. ì´ê²ƒì´ smoothingì´ë‹¤.\n\nì´ë•Œ ë°˜ë“œì‹œ ìœ ì˜í•  ì ì€ smoothingì„ í•˜ê±´ ì•ˆí•˜ê±´ ê° í™•ë¥ ì˜ ì´í•©ì€ 1ì´ ë˜ë„ë¡ ë³´ì¥í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\sum_{w \\in \\Omega}p(w) = 1\n$$\n\nëŒ€ëµ 6ê°€ì§€ì˜ ëŒ€í‘œì ì¸ smoothing ë°©ì‹ë“¤ì„ ì†Œê°œí•˜ê² ë‹¤.\n\n\u003e \u003cmark\u003e**1. Add-1(Laplace)**\u003c/mark\u003e\n\nê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì˜ smoothing ë°©ë²•ì´ì§€ë§Œ, ì‹¤ìš©ì ì¸ë©´ì€ ë‹¤ì†Œ ë–¨ì–´ì§€ëŠ” ë°©ë²•ì´ë‹¤. ì•„ì´ë””ì–´ëŠ” ê°„ë‹¨í•˜ë‹¤. predictionì„ ìˆ˜í–‰í•  ë•Œ, í˜„ì¬ ë“¤ì–´ì˜¨ inputê¹Œì§€ í¬í•¨í•˜ì—¬ ë§Œë“  $|V|$ë¥¼ ë¶„ëª¨ì— ë”í•˜ê³ , ë¶„ìì— 1ì„ ë”í•´ì£¼ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´, ì„¤ì‚¬ countê°€ 0ì´ ë”ë¼ë„ í™•ë¥ ì´ 0ì´ ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.\n\n$$\np(w) = {X \\over Y} \\rArr p^{\\prime}(w) = {X + 1 \\over Y + |V|}\n$$\n\nì—¬ê¸°ì„œ, $|V|$ ê°’ì´ ì •ë§ í—·ê°ˆë ¸ëŠ”ë°, ì•„ë¬´ë„ ì˜ ì„¤ëª…ì„ ì•ˆí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ì§šê³  ë„˜ì–´ê°€ë©´, ìš°ë¦¬ê°€ í™•ë¥ ê°’ì„ ì–»ê¸° ìœ„í•´ì„œ ì‚¬ìš©í–ˆë˜ datasetê³¼ í˜„ì¬ predictionì„ í•˜ê¸° ìœ„í•´ì„œ ë“¤ì–´ì˜¨ input ë‘˜ì—ì„œ ë°œìƒí•œ ëª¨ë“  uniqueí•œ ë‹¨ì–´ì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. (# of vocabulary)\n\nê·¸ë ‡ê²Œ í•´ì•¼ë§Œ $\\sum_{w \\in \\Omega}p(w) = 1$ì„ ë§Œì¡±í•˜ëŠ” ê°’ì´ ë‚˜ì˜¨ë‹¤.\n\në”°ë¼ì„œ, ì´ë¥¼ ê° ê°ì˜ n-gramì— ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n| n    | $p(w_{i})$                                                 | $p^{\\prime}(w_{i})$                                                            |\n| :--- | :--------------------------------------------------------- | :----------------------------------------------------------------------------- |\n| $1$  | $c(w_{i}) \\over \\vert T \\vert$                             | $c(w_{i}) + 1 \\over \\vert T \\vert + \\vert V \\vert$                             |\n| $2$  | ${{c(w_{i-1}, w_{i})}\\over{c(w_{i-1})}}$                   | ${{c(w_{i-1}, w_{i}) + 1}\\over{c(w_{i-1})} + \\vert V \\vert} $                  |\n| $3$  | ${{c(w_{i-2}, w_{i-1}, w_{i})}\\over{c(w_{i-2}, w_{i-1})}}$ | ${{c(w_{i-2}, w_{i-1}, w_{i}) + 1}\\over{c(w_{i-2}, w_{i-1})} + \\vert V \\vert}$ |\n\n\u003e \u003cmark\u003e**2. Add-k**\u003c/mark\u003e\n\n1ì´ë¼ëŠ” ìˆ«ìê°€ ê²½ìš°ì— ë”°ë¼ì„œëŠ” êµ‰ì¥íˆ í° ì˜í–¥ì„ ì¤„ ë•Œê°€ ìˆë‹¤. íŠ¹íˆ ê¸°ì¡´ ë°ì´í„°ì˜ ìˆ˜ê°€ ì ì€ ê²½ìš°ì— ë”ìš± ê·¸ë ‡ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ 1ë³´ë‹¤ ì‘ì€ ì„ì˜ì˜ ê°’(k)ì„ ì“°ëŠ” ë°©ë²•ì´ë‹¤.\n\n$$\np(w) = {X \\over Y} \\rArr p^{\\prime}(w) = {X + 1 \\over Y + k|V|}\n$$\n\ní•˜ì§€ë§Œ, ìœ„ì™€ ê°™ì€ ë°©ì‹ì€ ê²°êµ­ ì–´ë–¤ í™•ë¥  ê°’ì´ë“ ì§€ ë¶„ìì— 1ì„ ë”í•˜ê¸° ë•Œë¬¸ì— ë¶ˆí‰ë“±í•˜ê²Œ ê°’ì„ ë‚˜ëˆ ì¤€ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´, ì• ì´ˆì— count(ë¶„ì)ê°€ í° ë°ì´í„°ì—ê²Œ 1ì€ ë³„ë¡œ ì˜í–¥ì„ ì•ˆì£¼ê² ì§€ë§Œ, ë¶„ìê°€ ì²˜ìŒë¶€í„° ì‘ì•˜ë˜ ê²½ìš°ì—ëŠ” ì´ë¡œ ì¸í•´ì„œ ë°›ëŠ” ì˜í–¥ì´ êµ‰ì¥íˆ í¬ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, ì´ëŸ¬í•œ í•œê³„ì ì„ ê·¹ë³µí•  ìˆ˜ ìˆëŠ” ë°©ë²•ë“¤ì´ ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ë“¤ì´ë‹¤.\n\n\u003e \u003cmark\u003e**3.Good Turing**\u003c/mark\u003e\n\nì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ìš°ë¦¬ëŠ” ìƒˆë¡œìš´ featureì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì•¼ í•œë‹¤. ë°”ë¡œ wordì˜ frequencyì˜ frequencyì´ë‹¤.\n\n$$\nN_{k} = \\sum_{i=1}^{n}1[c(w_{i}) = k]\n$$\n\nì•„ë§ˆ ì˜ˆì‹œë¥¼ ë´ì•¼ ì´í•´ê°€ ë¹ ë¥¼í…Œë‹ˆ í•˜ë‚˜ì˜ ì˜ˆì‹œë¥¼ ë³´ì.\n\n```plaintext\n sam I am I am sam I do not eat\n```\n\nì´ ê²½ìš° ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ countë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n  \u0026c(I) \u0026=\\ 3 \\\\\n  \u0026c(sam) \u0026=\\ 2\\\\\n  \u0026c(am) \u0026=\\ 2\\\\\n  \u0026c(do) \u0026=\\ 1\\\\\n  \u0026c(not) \u0026=\\ 1\\\\\n  \u0026c(eat) \u0026=\\ 1\\\\\n\\end{align*}\n\\quad\\rArr\\quad\n\\begin{align*}\n  \u0026 N_{1} = 3 \\\\\n  \u0026 N_{2} = 2 \\\\\n  \u0026 N_{3} = 1\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ Good Turingì€ í•œ ë²ˆë„ ì•ˆë³¸ ë°ì´í„°ì—ê²ŒëŠ” í•œ ë²ˆë§Œ ë³´ëŠ” ê²½ìš°ì˜ ìˆ˜ë¥¼ ì „ì²´ ê²½ìš°ì˜ ìˆ˜ë¡œ ë‚˜ëˆˆê°’ë§Œí¼ì˜ í™•ë¥ ì„ ë‚˜ëˆ„ì–´ì£¼ê³ , ê¸°ì¡´ ë°ì´í„°ë“¤ì—ê²ŒëŠ” laplaceì²˜ëŸ¼ 1ì„ ë”í•´ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë¹„ë¡€í•˜ëŠ” ë§Œí¼ì„ ê³±í•´ì£¼ì–´ ì ì ˆí•œ í™•ë¥ ì„ ê°€ì ¸ê°ˆ ìˆ˜ ìˆê²Œí•˜ì˜€ë‹¤.\n\në”°ë¼ì„œ, ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n$$\np(w_{i}) = {(c(w_{i})+1)N_{c(w_{i})+1}\\over{N_{c(w_{i})}}} \\times {1\\over |T|},\\quad (N_{0} = 1)\n$$\n\nëŒ€ê²Œ ${(c(w_{i})+1)N_{c(w_{i})+1}\\over{N_{c(w_{i})}}}$ì´ ë¶€ë¶„ì„ $c^{*}$ë¼ê³ ë„ ë¶€ë¥¸ë‹¤.\n\n\u003e \u003cmark\u003e**4. Kneser-Ney**\u003c/mark\u003e\n\nê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” Smoothing ë°©ì‹ìœ¼ë¡œ ê¸°ì–µí•´ë‘ëŠ” ê²ƒì´ ì¢‹ë‹¤. ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì €, Absolute Discountingì„ ë¨¼ì € ì´í•´í•´ì•¼ í•œë‹¤.  \nGood-turing ë°©ì‹ì„ ì‚¬ìš©í–ˆì„ ë•Œ $c$ì™€ $c^{*}$ì‚¬ì´ì— ì°¨ì´ê°€ ê²½í—˜ì ìœ¼ë¡œ íŠ¹ì • ìƒìˆ˜ë§Œí¼ì”© ì°¨ì´ê°€ ë‚œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ì—¬,\n\n$$\nc^{*} = c - d\n$$\n\nChurchê³¼ Galeì€ ì´ë¥¼ Absolute Discounting í™•ë¥ ì´ë¼ë©° ë‹¤ìŒ ì‹ì„ ì œì‹œí•œë‹¤.\n\n$$\nP(w_{i}|w_{i-1}) = {c(w_{i-1}, w_{i}) -d \\over c(w_{i-1})} + \\lambda(w_{i-1})P(w)\n$$\n\nì—¬ê¸°ì„œ ë’·ì— ë¶€ë¶„ $\\lambda(w_{i-1})P(w)$ì€ discountingìœ¼ë¡œ ë°œìƒí•œ ì˜¤ì°¨ë¥¼ ë§¤ê¾¸ê¸° ìœ„í•œ ê°’ì´ë‹¤.\n\nì—¬ê¸°ì„œ Kneser-Ney problemì€ ë” ë„“ì€ ë²”ìœ„ë¡œ í™•ì¥ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë²”ìœ„ë¡œ í™•ì¥ì‹œí‚¨ ê²ƒì´ë‹¤. ê¸°ì¡´ì—ëŠ” bigramìœ¼ë¡œ ì œí•œë˜ì–´ ìˆë˜ Absolute Discountingì˜ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•ëœë‹¤.\n\n$$\nP_{KN}(w_{i}|w_{i-n+1}^{i-1}) = {\\max(c(w_{i-1}, w_{i}) -d, 0) \\over c(w_{i-n+1}^{i-1})} + \\lambda(w_{i-n+1}^{i-1})P_{KN}(w_{i}|w_{i-n+2}^{i-1})\n$$\n\n(ìœ„ì˜ ì‹ì— ëŒ€í•´ì„œ ì •í™•í•˜ê²Œ ì´í•´ë¥¼ í•˜ì§€ ì•Šì•˜ì§€ë§Œ, ê·¸ë ‡êµ¬ë‚˜ í•˜ê³  ë„˜ì–´ê°€ë„ ì¶©ë¶„í•  ê²ƒ ê°™ë‹¤.)\n\n\u003e \u003cmark\u003e**5. Backoff \u0026 Interpolation**\u003c/mark\u003e\n\nìƒí™©ì— ë”°ë¼ì„œ unigram, bigram, trigramì„ ê°€ì¤‘ì¹˜ë§Œí¼ ë”í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤. ê²°êµ­ n-gramì—ì„œ nì´ ì‘ì•„ì§ˆ ìˆ˜ë¡ detailì„ ì‹ ê²½ì“¸ ìˆ˜ ì—†ì§€ë§Œ, ì‹ ë¢°ë„ ìì²´ëŠ” ëŠ˜ì–´ë‚  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ì ì ˆíˆ ì„ì–´ì“°ë©´ ì¢‹ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ì´ë¡ ì´ë‹¤. í•˜ì§€ë§Œ, ì–´ë–¤ ê²ƒì„ ë” ì¤‘ì ìœ¼ë¡œ ì§ì ‘ ì •í•´ì£¼ì–´ì•¼ í•œë‹¤.\n\n$$\np^{\\prime}(w_{i}|w_{i-2}, w_{i-1}) = \\lambda_{3}p(w_{i}|w_{i-2}, w_{i-1}) + \\lambda_{2}p(w_{i}|w_{i-1}) + \\lambda_{1}p(w_{i}) + {\\lambda_{0}\\over|V|}\n$$\n\nì´ë¥¼ ì •í•  ë•ŒëŠ” ëŒ€ê²Œ held-out dataë¥¼ í™œìš©í•´ì„œ êµ¬í•œë‹¤.(validation setì´ë¼ê³  ë¶€ë¥¸ë‹¤.) ì¦‰, ì „ì²´ corpusë¥¼ (train, validation, test)ë¡œ ì ì ˆíˆ ë‚˜ëˆ„ì–´ ì“°ë¼ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ì„±ëŠ¥ì„ ì¸¡ì •í•  ë•ŒëŠ” testsetì„ ì“°ê³ , $\\lambda$ë¥¼ ì¶”ì •í•  ë•Œì—ëŠ” validation(heldout)setì„ ì‚¬ìš©í•˜ë¼ëŠ” ê²ƒì´ë‹¤.\n\n## Word Class\n\nSmoothing ë°©ì‹ì„ ì´ìš©í•´ì„œ unseen dataë¥¼ ì²˜ë¦¬í•´ì£¼ì—ˆëŠ”ë° ì¢€ ë” íš¨ê³¼ì ìœ¼ë¡œ ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ê³ ë ¤í•œ ê²ƒì´ë‹¤. classë‹¨ìœ„ë¡œ wordë¥¼ groupingí•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ì˜€ë‹¤ê³  í•˜ë”ë¼ë„ íŠ¹ì • groupì— ì†í•œë‹¤ë©´, ì´ë¥¼ í™œìš©í•´ì„œ ì–´ëŠì •ë„ í™•ë¥ ì„ ë¶€ì—¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ ë°©ì‹ì„ í™œìš©í•˜ë©´, ì‹¤ì œë¡œ ë³´ì§€ ì•Šì€ ë°ì´í„°ì— ëŒ€í•´ì„œë„ í˜„ì‹¤ì ì¸ ì¶”ì •ì´ ê°€ëŠ¥í•˜ì§€ë§Œ detailì— ëŒ€í•œ ì„±ëŠ¥ì€ ê°ì†Œí•  ìˆ˜ ìˆë‹¤.\n\n$$\np(w_{i}|w_{i-1}) \\rArr p(w_{i}|c_{i-1}) ={ c(c_{i}, w_{i}) \\over c(c_{i-1}) }\n$$\n\nìœ„ì˜ ì‹ì„ ë³´ë©´, ì´ì „ ë‹¨ì–´ì˜ contextë¥¼ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì´ì œëŠ” classë¥¼ ë³´ê³  ë‹¤ìŒ ë‹¨ì–´ë¥¼ probabilityë¥¼ ê³„ì‚°í•˜ë„ë¡ ë°”ë€ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ í™•ë¥ ì€ classë‚´ë¶€ì—ì„œ í•´ë‹¹ ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ì´ì „ classì˜ ë¹ˆë„ë¡œ ë‚˜ëˆ„ì—ˆë‹¤ê³  ë³´ë©´ ë˜ê² ë‹¤.\n\n$$\np(w_{i}|c_{i-1}) = p(w_{i}|c_{i}) \\times p(c_{i}|h_{i})\n$$\n\nì¦‰, class ë‹¨ìœ„ë¡œ ë‹¨ì–´ë¥¼ ë¬¶ê³  classì—ì„œ ë‹¨ì–´ì˜ ë°œìƒ í™•ë¥ ì— classì—ì„œì˜ n-gramì„ ê³±í•œ ê°’ì´ ë˜ëŠ” ê²ƒì´ë‹¤. ì¼ë°˜ì ì¸ Bayes Decison Ruleì— ê¸°ë°˜í•˜ì—¬ ì„ íƒí•œë‹¤ê³  ë³´ë©´ ë˜ê² ë‹¤.\n\n## Example. Spelling Correction\n\nì—¬íƒœê¹Œì§€ ë°°ìš´ ë‚´ìš©ì„ í™œìš©í•˜ì—¬ Spelling ì˜¤ë¥˜ë¥¼ ì •ì •í•´ì£¼ëŠ” applicationì„ ì œì‘í•œë‹¤ê³  í•´ë³´ì. ë¨¼ì €, Spelling Errorì˜ ì¢…ë¥˜ë¶€í„° ì•Œì•„ë³´ë„ë¡ í•˜ì.\n\n- **Non word Error**  \n  ì˜ëª»ëœ spellingì— ì˜í•´ì„œ ì „í˜€ ëœ»ì´ ì—†ëŠ” ë‹¨ì–´ê°€ ë§Œë“¤ì–´ì§„ ê²½ìš°ì´ë‹¤.  \n  í•´ê²°ì„ ìœ„í•´ì„œëŠ” ì‚¬ì „ì—ì„œ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì°¾ì•„ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²ƒ ë˜ëŠ” ì´ì „ì— ë°°ì› ë˜ shortest edit distanceë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.\n- **Real word Error**  \n  ì˜ëª»ëœ spelling ë˜ëŠ” ìœ ì‚¬í•œ ë°œìŒ ë•Œë¬¸ì— ëœ»ì´ ìˆëŠ” ë‹¨ì–´ê°€ ë§Œë“¤ì–´ì¡Œì§€ë§Œ, ì˜¤ë¥˜ê°€ ì˜ì‹¬ë˜ëŠ” ê²½ìš°ì´ë‹¤.  \n  í•´ê²°ì±…ì€ ë¹„ìŠ·í•œ ë°œìŒ ë˜ëŠ” spellingì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ì°¾ì•„ì„œ í•´ë‹¹ ë‹¨ì–´ì™€ í•¨ê»˜ language modelì— ë„£ì–´ì„œ ê°€ì¥ ë†’ì€ ê°€ëŠ¥ì„±ì„ ê°€ì§€ëŠ” ê°’ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\në¨¼ì €, **Non Word Error** ê°™ì€ ê²½ìš°ëŠ” ì˜¤íƒ€ ë°ì´í„°ì— ì›ë˜ ì“°ë ¤ê³  í–ˆë˜ ê°’ì„ labelingí•´ì„œ ëª¨ì•„ë‘ê³  ë‹¤ìŒ ê°’ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.\n(*x=ì˜¤íƒ€ë°ì´í„°, w=ì‚¬ì „ì—ìˆëŠ”ë‹¨ì–´)\n\n- $P(x|w)$ = xê°€ wì¼ ê°€ëŠ¥ì„±\n- $p(w)$ = wì˜ í™•ë¥ \n\nê·¸ë¦¬ê³  ë‚˜ì„œ, ë‹¤ìŒì„ ì‹¤í–‰ì‹œì¼œì„œ ê°€ì¥ ì ì ˆí•œ $\\hat{w}$ë¥¼ ì°¾ìœ¼ë©´ ëœë‹¤.\n$$\n\\begin{align*}\n\\hat{w} \u0026= \\argmax_{w \\in V}P(w|x) \\\\\n\u0026= \\argmax_{w \\in V}{P(x|w)P(w)}\n\\end{align*}\n$$\n\n**Real Word Error**ì˜ ê²½ìš°ì—ëŠ” ê²°êµ­ ì´ì „ ë‹¨ì–´ sequenceë¥¼ í™œìš©í•´ì•¼ í•œë‹¤. ì „ì²´ corpusë¥¼ í•™ìŠµí•´ì„œ tri-gramì„ ì¶”ì¶œí•´ë†“ê³ , ë²ˆê°ˆì•„ê°€ë©´ì„œ í›„ë³´ ë‹¨ì–´ë“¤ì„ ì§‘ì–´ë„£ì–´ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í›„ë³´ ë‹¨ì–´ê°€ ë‹¤ìŒê³¼ ê°™ì´ ì •í•´ì¡Œë‹¤ê³  í•˜ì. ($\\bold{w}_{3} = {w_3^{(1)}, w_3^{(2)}, w_3^{(3)}, ...}$) ì´ë•Œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” wëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\hat{w}_{3} = \\argmax_{w_{3}^{(i)} \\in \\bold{w}_{3}} P(w_{3}^{(i)}| w_{1}, w_{2})\n$$\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"nlp-language-modeling","date":"2022-10-21 12:15","title":"[NLP] 3. Language Modeling","category":"AI","tags":["NLP","NoisyChannel","Ngram","LanguageModeling","Smoothing","WordClass"],"desc":"ì´ì œ í†µê³„ì ì¸ ê´€ì ì—ì„œ NLì„ inputìœ¼ë¡œ í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ë°©ë²•ì„ ì°¾ì„ ê²ƒì´ë‹¤. ì´ë¥¼ Language Modelingì´ë¼ê³  í•˜ë©°, ì´ë¥¼ ìœ„í•´ì„œ ë˜ëŠ” ì´ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•œ ë°©ë²•ë“¤ì„ ì†Œê°œí•  ê²ƒì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nNLPë¥¼ ìˆ˜í–‰í•  ë•Œ, ìš°ë¦¬ëŠ” sequence of characterë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ì œëŒ€ë¡œ ì•Œì•„ì•¼ ì œëŒ€ë¡œ ëœ ì „ì²˜ë¦¬ì™€ í›„ì²˜ë¦¬ ë“±ì´ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ í•´ë‹¹ chapterì—ì„œëŠ” ì–´ë–»ê²Œ ì›ë³¸ ë¬¸ì/ë‹¨ì–´/ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ë‹¤ë£° ê²ƒì´ë‹¤.\n\n## Regular Express\n\nì•„ì£¼ ê¸°ë³¸ì ì¸ ë¬¸ìì—´ ì²˜ë¦¬ ë°©ë²•ì´ë‹¤. ì´ë¥¼ ì•Œê³  ìˆì–´ì•¼ ì‹¤ì§ˆì ì¸ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ë³„ë„ì˜ Postingìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë‹¤ë£¨ì—ˆë‹¤. ([ğŸ”— Regex](/posts/regex))ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ì.\n\n## Text Normalization\n\nìš°ë¦¬ê°€ ì‚¬ìš©í•  NLì€ ì •ì œë˜ì–´ ìˆì§€ ì•Šì•„ì„œ ì—¬ëŸ¬ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. ê·¸ ì¤‘ì—ì„œ ëŒ€ì¤‘ì ìœ¼ë¡œ ì¢‹ë‹¤ê³  ì•Œë ¤ì§„ ë°©ë²•ë“¤ì„ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì•„ë˜ ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\n\n1. Word Tokenization  \n   ë§ ê·¸ëŒ€ë¡œ NL ë°ì´í„°ê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ, ì´ë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìª¼ê°œëŠ” ê²ƒì´ë‹¤.\n2. Word Reformating  \n   ë‹¨ì–´ë¥¼ ë‚˜ëˆ„ì—ˆë‹¤ë©´, ê° ë‹¨ì–´ì˜ í˜•íƒœë¥¼ ì²˜ë¦¬í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ Normalizingí•˜ëŠ” ê²ƒì´ë‹¤.  \n3. Sentence Segmentation  \n   ë¬¸ì¥ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•´ëŠ” ê³¼ì •ì´ë‹¤.\n\nì´ì œ ê° ë‹¨ê³„ë¥¼ ì„¸ë¶€ì ìœ¼ë¡œ ë‹¤ë¤„ë³´ê² ë‹¤.\n\n### 1. Word Tokenization\n\nìš°ì„  ì‰½ê²Œ ìƒê°í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ë‹¨ìˆœíˆ ë„ì–´ì“°ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê²Œ í•˜ë©´, ìš°ë¦¬ëŠ” ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ Corpusì—ì„œ tokenì„ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤.\n\ní•˜ì§€ë§Œ, \"San Francisco\"ì™€ ê°™ì€ ë‹¨ì–´ê°€ ë‘ ê°œì˜ tokenìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•˜ë‚˜ì˜ tokenìœ¼ë¡œ ì²˜ë¦¬ë˜ê¸°ë¥¼ ì›í•  ìˆ˜ ìˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼ ì¼ë¶€ ì–¸ì–´ë“¤(íŠ¹íˆ ì¤‘êµ­ì–´ì™€ ì¼ë³¸ì–´)ì˜ ê²½ìš° ë„ì–´ì“°ê¸° ì—†ì´ ì‘ì„±í•˜ëŠ” ì–¸ì–´ë“¤ì˜ ê²½ìš° ë¬¸ì œëŠ” ë” ì»¤ì§ˆ ìˆ˜ ìˆë‹¤. ì´ ê²½ìš°ì—ëŠ” **Word Segmenting**ì´ë¼ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•  ìˆ˜ë„ ìˆëŠ”ë°, ì›ë¦¬ëŠ” ë§¤ìš° ê°„ë‹¨í•˜ë‹¤. ì–¸ì–´ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ì‚¬ì „ì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì¥ì—ì„œ ì‚¬ì „ì— ì¼ì¹˜í•˜ëŠ” ê°€ì¥ ê¸´ ë¬¸ìì—´ì„ ì°¾ì„ ìˆ˜ ìˆì„ ë•Œê¹Œì§€ tokenì„ ì—°ì¥í•´ì„œ ë§Œë“œëŠ” ë°©ì‹ì´ë‹¤.\n\nê·¸ëŸ¬ë‚˜ ì´ ë°©ì‹ë„ ê²°êµ­ì€ íŠ¹ì • ì–¸ì–´(ì¤‘êµ­ì–´, ë“±)ì—ì„œëŠ” ì˜ ì‘ë™í•˜ì§€ë§Œ, ì¼ë¶€ ì–¸ì–´(ì˜ì–´ ë“±)ì—ì„œëŠ” ì˜ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ë‹¤. ë”°ë¼ì„œ, ìµœê·¼ì—ëŠ” í™•ë¥ ì— ê¸°ë°˜í•˜ì—¬ ê°™ì´ ë“±ì¥í•˜ëŠ” íšŸìˆ˜ê°€ ë§ì„ ê²½ìš° í•˜ë‚˜ì˜ tokenìœ¼ë¡œ ë¬¶ëŠ” í˜•íƒœì˜ tokenizationì„ ë” ì„ í˜¸í•œë‹¤.\n\nì´ ê³¼ì •ì—ì„œ ìš°ë¦¬ê°€ ì¶”ê°€ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ë°”ë¡œ wordì˜ ê°¯ìˆ˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ë‹¤. ëŒ€ê²Œ ìš°ë¦¬ê°€ ê´€ì‹¬ ìˆì–´í•˜ëŠ” ìˆ˜ëŠ” ì´ 3ê°€ì§€ì´ë‹¤.\n\n1. **number of tokens**  \n   ì¦‰, ë„ì–´ì“°ê¸°ë¡œ ë‚˜ë‰˜ì–´ì§€ëŠ” tokenë“¤ì˜ ì´ ê°¯ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.\n2. **number of types(Vocabulary)**  \n   ë„ì–´ì“°ê¸°ë¡œ ë‚˜ë‰˜ì–´ì§„ tokenë“¤ì˜ ì¤‘ë³µì„ ì œê±°í•œ ì¢…ë¥˜ë“¤ì˜ ê°¯ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ëŒ€ê²Œ ì´ëŸ¬í•œ typeë“¤ì˜ ëª¨ìŒì„ Vocabularyë¼ê³  í•œë‹¤.\n3. **number of each type's tokens**  \n   ê° ì¢…ë¥˜ì˜ tokenì´ ì–¼ë§ˆë‚˜ ë§ì´ ë“±ì¥í–ˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.\n\nì—¬ê¸°ì„œ ì´ëŸ¬í•œ tokenì´ë‚˜ typeì´ ì„œë¡œ ê°™ë‚˜ëŠ” ê²ƒì„ ì–´ë–»ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆì„ê¹Œ? ì´ë¥¼ ìœ„í•´ì„œ Word Reformatingì„ ìˆ˜í–‰í•˜ì—¬ ì¢€ ë” ì¼ë°˜ì ì¸ í˜•íƒœë¡œ ë³€í˜•í•˜ì—¬ ìœ„ì˜ ìˆ˜ë“¤ì„ íŒŒì•…í•˜ê¸°ë„ í•œë‹¤.\n\n### 2. Word Normalization and Stemming(Word Reformating)\n\nëŒ€ê²Œì˜ ì–¸ì–´ëŠ” wordì˜ í˜•íƒœê°€ ì—¬ëŸ¬ ê°œë¡œ ì¡´ì¬í•œë‹¤. ì´ ê³¼ì •ì—ì„œ ìš°ë¦¬ê°€ ê³ ë ¤í•´ì•¼ í•  ê²ƒì´ ì •ë§ ë§ë‹¤. ê·¸ ì¤‘ì—ì„œ ê°€ì¥ ê¸°ë³¸ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ì•¼ í•  ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ì˜ì–´ì— ì¤‘ì‹¬ì„ ë‘” ì„¤ëª…ì´ë‹¤.\n\n1. **Uppercase**  \n   ì˜ì–´ì—ì„œ ì²« ê¸€ìëŠ” ëŒ€ë¬¸ìë¡œ ì‹œì‘í•œë‹¤ëŠ” ê·œì¹™ì´ ìˆë‹¤. ë˜ëŠ” ê°•ì¡°í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë¥¼ ëŒ€ë¬¸ìë¡œ í‘œí˜„í•˜ê¸°ë„ í•œë‹¤. ê·¸ ê²°ê³¼ tokenì˜ ì¢…ë¥˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì—ì„œ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ê¸°ë„ í•œë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ëª¨ë‘ lowercaseë¡œ ë°”ê¿”ë²„ë¦¬ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ëª¨ë“  ê²½ìš°ì— ì´ë¥¼ ì ìš©í•  ìˆ˜ ì‡ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ê°€ì¥ ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ USì™€ usì˜ ì˜ë¯¸ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì´ë‹¤. ë˜, ê³ ìœ  ëª…ì‚¬ì¸ General Motorsì™€ ê°™ì€ ê²½ìš°ë„ ë‹¤ë¥´ê²Œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ê³ ë ¤í•´ì„œ ë¨¼ì € ì²˜ë¦¬í•œ ì´í›„ì— ì „ì²´ ë°ì´í„°ë¥¼ lowercaseë¡œ ë³€í™˜í•˜ëŠ” ë°©ì‹ì„ ìˆ˜í–‰í•œë‹¤.  \n2. **Lemmatization**  \n   Lemma(ê¸°ë³¸í˜•, ì‚¬ì „í˜•)ë¡œ ë‹¨ì–´ë¥¼ ë³€í™˜í•˜ëŠ” ê²ƒì´ë‹¤. ê°€ì¥ ê¸°ë³¸ì ì¸ ê²ƒì€ am, are, isì™€ ê°™ì€ beë™ì‚¬ë¥¼ ëª¨ë‘ beë¡œ ë³€í™˜í•˜ê±°ë‚˜ car, cars, car'së¥¼ ëª¨ë‘ ê¸°ë³¸í˜•íƒœì¸ carë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ë‹¤. ëŒ€ê²Œì˜ ê²½ìš°ì—ëŠ” ì´ ê³¼ì •ì—ì„œ ì˜ë¯¸ë¥¼ ì¼ë¶€ ìƒì–´ë²„ë¦¬ê¸° ë•Œë¬¸ì— lemma + tagë¡œ ê¸°ì¡´ tokenì„ ë³µêµ¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” tagë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n3. **Stemming**  \n   morpheme(í˜•íƒœì†Œ)ì€ ì¤‘ì‹¬ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” stemê³¼ í•µì‹¬ ì˜ë¯¸ëŠ” ì•„ë‹ˆì§€ë§Œ stemì— ì¶”ê°€ ì˜ë¯¸ë¥¼ ë”í•´ì£¼ëŠ” affixesë¡œ ë‚˜ëˆ„ì–´ wordë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ê° tokenì„ ê°€ì¥ coreì˜ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” stemìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ì‹ì´ë‹¤. ëŒ€í‘œì ì¸ ì˜ˆì‹œê°€ automate, automatic, automationì„ automatìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” lemmatizationë³´ë‹¤ ë„“ì€ ë²”ìœ„ì˜ wordë¥¼ í•˜ë‚˜ë¡œ ë¬¶ê¸° ë•Œë¬¸ì— ì„¸ë¶€ì˜ë¯¸ê°€ ë” ì†ì‹¤ë  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ê¸°ì¡´ ì˜ë¯¸ë¡œ ë³µêµ¬í•  ìˆ˜ ìˆëŠ” tagë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n\n### 3. Sentence Segmentation\n\në¬¸ì¥ì„ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¡œ ìš°ë¦¬ëŠ” \"?\", \"!\", \".\"ì„ í™œìš©í•œë‹¤. \"?\"ì™€ \"!\" ê°™ì€ ê²½ìš°ëŠ” ë¬¸ì¥ì˜ ëì„ ì˜ë¯¸í•˜ëŠ”ê²ƒì´ ëŒ€ê²Œ ìëª…í•˜ë‹¤. í•˜ì§€ë§Œ, \".\"ì€ ê½¤ë‚˜ ì• ë§¤í•  ìˆ˜ ìˆë‹¤. ì†Œìˆ˜ì , Abbreviation(Mr., Dr., Inc.)ì™€ ê°™ì€ ê²½ìš°ì— ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•´ì„œ Decision Treeë¥¼ ë§Œë“¤ì–´ì„œ ì´ë¥¼ ìˆ˜í–‰í•œë‹¤. ì•„ë˜ì™€ ê°™ì´ ì‚¬ëŒì´ ì§ì ‘ ê·œì¹™ì„ ì •í•  ìˆ˜ë„ ìˆì§€ë§Œ í˜„ì¬ëŠ” ëŒ€ê²Œ í†µê³„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰í•œë‹¤.\n\n![nlp-sentence-segmentation](/images/nlp-sentence-segmentation.jpg)\n\n## Collocation(ì—°ì–´) processing\n\nText Normalizationì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” sentenceë¥¼ êµ¬ë¶„í•˜ê³ , wordë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆì—ˆë‹¤. í•˜ì§€ë§Œ, ë‹¨ìˆœíˆ í•˜ë‚˜ì˜ wordë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì£¼ë³€ ë‹¨ì–´ë¥¼ í™œìš©í•˜ì—¬ ì²˜ë¦¬í•´ì•¼ë§Œ ì–»ì„ ìˆ˜ ìˆëŠ” ì •ë³´ë“¤ì´ ìˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¥¼ Collocation(ì—°ì–´)ë¥¼ í™œìš©í•˜ì—¬ ìˆ˜í–‰í•œë‹¤. ì´ëŠ” íŠ¹ì • ë‹¨ì–´ìŒì´ ë†’ì€ ë¹ˆë„ë¡œ ê°™ì´ ë¶™ì–´ ì‚¬ìš©ë˜ëŠ” í˜„ìƒì„ ë§í•œë‹¤. **\"ëª¨ë“  ë‹¨ì–´ëŠ” ì´ë¥¼ ë™ë°˜í•˜ëŠ” ì£¼ë³€ ë‹¨ì–´ì— ì˜í•´ íŠ¹ì„± ì§€ì–´ì§„ë‹¤.\"** ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ collocationì„ co-ocurrenceë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì„ í•  ìˆ˜ ìˆë‹¤.\n\n1. **lexicography(ì‚¬ì „ í¸ì°¬)** : ê°™ê°€ë‹ˆ ìœ ì‚¬í•œ ëœ»ì„ ê°€ì§€ëŠ” ë‹¨ì–´ëŠ” ë¹ˆë²ˆí•˜ê²Œ ë¶™ì–´ì„œ ì‚¬ìš©ë˜ëŠ”ë° ì´ë¥¼ ì´ìš©í•´ì„œ í•˜ë‚˜ì˜ ë‹¨ì–´ì˜ ëœ»ì„ ì•ˆë‹¤ë©´, ì´ë¥¼ í†µí•´ì„œ ë‹¤ë¥¸ ë‹¨ì–´ì˜ ëœ»ì„ ì¶”ë¡ í•˜ë©° í™•ì¥í•´ë‚˜ê°ˆ ìˆ˜ ìˆë‹¤.\n2. **language modeling** : NLë¥¼ í†µí•´ì„œ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ì„œ íŠ¹ì • parameterë¥¼ ì¶”ì •í•´ë‚´ëŠ” ê²ƒì„ language modelingì´ë¼ê³  í•˜ëŠ”ë° ì´ ê³¼ì •ì—ì„œ collocationì„ í™œìš©í•˜ëŠ” ê²ƒì´ ë‹¨ì¼ ë‹¨ì–´ë¥¼ í™œìš©í•˜ëŠ” ê²ƒë³´ë‹¤ contextë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ì¥ì ì„ ë°œíœ˜í•  ìˆ˜ ìˆë‹¤.\n3. **NL generation** : ìš°ë¦¬ëŠ” ë¬¸ë§¥ìƒ ë§¤ë„ëŸ¬ìš´ ë¬¸ì¥ì„ ì›í•œë‹¤. ì¦‰, \"ê°ì„ ì¡ë‹¤\"ë¥¼ \"ê°ì„ ë¶™ì¡ë‹¤\"ë¼ê³  í–ˆì„ ë•Œ, ëœ»ì„ ì´í•´í•  ìˆ˜ëŠ” ìˆì§€ë§Œ ì–´ìƒ‰í•˜ë‹¤ê³  ëŠë‚€ë‹¤. ë”°ë¼ì„œ, ì´ ê´€ê³„ë¥¼ í™œìš©í•´ì„œ NLì„ ìƒì„±í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— collocationì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ì´ëŸ¬í•œ Collocationì„ ì–´ë–»ê²Œ ì°¾ì„ ìˆ˜ ìˆì„ê¹Œ?\n\n1. **Frequency**  \n   ê°€ì¥ ê°„ë‹¨í•˜ê²Œ ë‹¨ìˆœíˆ ë™ì‹œ ë°œìƒ ë¹ˆë„ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ë‹¤. ì •í™•í•œ íŒŒì•…ì„ ìœ„í•´ì„œëŠ” ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ëŠ” ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ë¥¼ ë¨¼ì € filteringí•  í•„ìš”ê°€ ìˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ a, the, and ë“±ì´ ìˆë‹¤.\n2. **Hypothesis Testing**  \n   ê°€ì„¤ ê²€ì¦ìœ¼ë¡œ ìš°ë¦¬ê°€ ê°€ì •í•œ collocationì„ ì§€ì •í•˜ê³ , ì´ ì‚¬ê±´ì´ ì¼ì–´ë‚  ê°€ëŠ¥ì„±ì„ êµ‰ì¥íˆ ë‚®ê²Œ í•˜ëŠ” ê°€ì„¤ì„ ë°˜ëŒ€ë¡œ ê°€ì •í•œ í›„ì— ì´ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•˜ëŠ” Null Hypothesisë¥¼ ì´ìš©í•œ ì¦ëª…ìœ¼ë¡œ íƒ€ë‹¹ì„±ì„ í™•ë³´í•˜ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ê°€ ë³´ì´ê³ ì í•˜ëŠ” ê²ƒì€ word1, word2ê°€ ìˆì„ ë•Œ, ë‘ ë‹¨ì–´ê°€ ì„œë¡œ ì˜ì¡´ì ì´ë¼ëŠ” ê²ƒì„ ì¦ëª…í•˜ê³  ì‹¶ì€ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, Null Hypothesisë¡œ ë‘ ë‹¨ì–´ëŠ” ë…ë¦½ì´ë‹¤ë¼ê³  ì§€ì •í•˜ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.  \n   $p(w_{1}, w_{2}) = p(w_{1})p(w_{2})$  \n   ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ t-ê²€ì¦ì„ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.  \n   $t = {{p(w_{1}, w_{2}) - p(w_{1})p(w_{2})} \\over \\sqrt{p(w_{1}, w_{2})\\over{N}}} $  \n   tê°’ì´ ì´ì œ ì»¤ì§ˆ ìˆ˜ë¡ ìš°ë¦¬ëŠ” í•´ë‹¹ ê°€ì„¤ì´ í‹€ë ¸ìŒì„ ì¦ëª…í•˜ì—¬ collocationì„ì„ ì£¼ì¥í•  ìˆ˜ ìˆë‹¤.\n\n## Minimum Edit Distance\n\në‹¨ì–´ ë˜ëŠ” ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•  ë•Œ, ì‚¬ì „ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ë„ ìˆì§€ë§Œ ì°¸ê³ í•  corpusê°€ ë§ˆë•…í•˜ì§€ ì•Šê±°ë‚˜ ë” ì¶”ê°€ì ì¸ ìˆ˜ì¹˜ê°€ í•„ìš”í•˜ë‹¤ë©´, Minimum Edit Distanceë¡œ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê¸°ë„ í•œë‹¤. ì¦‰, ë‘ ë¬¸ìì—´ì´ ê°™ì•„ì§€ê¸° ìœ„í•´ì„œ ì–´ëŠì •ë„ì˜ ìˆ˜ì •ì´ í•„ìš”í•œì§€ë¥¼ ìˆ˜ì¹˜í™”í•œ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ì—°ì‚°ì€ ìƒˆë¡œìš´ ë¬¸ì ì¶”ê°€, ì‚­ì œ, ëŒ€ì²´ë§Œ ê°€ëŠ¥í•˜ë‹¤.\n\n```plaintext\nS - N O W Y  | - S N O W - Y\nS U N N - Y  | S U N - - N Y\ndistance : 3 | distance : 5\n```\n\në‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.\n\n1. ë¬¸ìì—´ x, yê°€ ìˆì„ ë•Œ, $E(i, j)$ëŠ” xì˜ 0\\~iê¹Œì§€ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ìì—´ê³¼ yì˜ 0~jê¹Œì§€ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ìì—´ì˜ distanceë¼ê³  í•˜ì.\n2. ì´ë ‡ê²Œ ë˜ë©´, $E(i,j)$ì—ì„œ ìš°ë¦¬ëŠ” ëë¬¸ìì˜ ê·œì¹™ì„ ë³¼ ìˆ˜ ìˆë‹¤.\n\n    ì˜¤ë¥¸ìª½ ë ë¬¸ìê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì¡°í•©ì€ 3ê°€ì§€ ë°–ì— ì—†ë‹¤.\n\n    ```plaintext\n    x[i]        | -           | x[i]\n    -           | y[j]        | y[j]\n    distance: 1 | distance: 1 | distance: 0 or 1\n    ```\n\n3. ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ ì‚¬ì‹¤ì„ ì•Œê²Œ ëœë‹¤.\n\n    $E(i,j)$ëŠ” ë‹¤ìŒ ê²½ìš°ì˜ ìˆ˜ ì¤‘ í•˜ë‚˜ì—¬ì•¼ë§Œ í•œë‹¤.\n\n    - $E(i-1, j) + 1$\n    - $E(i, j-1) + 1$\n    - $E(i-1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )$\n4. ë”°ë¼ì„œ, ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤.\n\n    $E(i, j) =\\min( \\\\\n      \\quad E(i-1, j) + 1, \\\\\n      \\quad E(i, j-1) + 1,  \\\\\n      \\quad E(i - 1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )\\\\\n    )$\n\në§Œì•½, ê° ì—°ì‚°ì˜ ë¹„ìš©ì´ ë‹¤ë¥¼ ê²½ìš°ë¼ë©´, 1 ëŒ€ì‹ ì— ê·¸ ê°’ì„ ë„£ì–´ì£¼ë©´ ì¶©ë¶„íˆ í’€ ìˆ˜ ìˆìœ¼ë©°, ì¶”ê°€ì ìœ¼ë¡œ ìµœì ì˜ ì´ë™í˜•íƒœë¥¼ ì•Œê³  ì‹¶ë‹¤ë©´, back pointer í•˜ë‚˜ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"nlp-text-processing","date":"2022-10-19 21:59","title":"[NLP] 2. Text Processing","category":"AI","tags":["NLP","Regex","Tokenization","Collocation","MinimumEditDistance"],"desc":"NLPë¥¼ ìˆ˜í–‰í•  ë•Œ, ìš°ë¦¬ëŠ” sequence of characterë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ì œëŒ€ë¡œ ì•Œì•„ì•¼ ì œëŒ€ë¡œ ëœ ì „ì²˜ë¦¬ì™€ í›„ì²˜ë¦¬ ë“±ì´ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ í•´ë‹¹ chapterì—ì„œëŠ” ì–´ë–»ê²Œ ì›ë³¸ ë¬¸ì/ë‹¨ì–´/ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ë‹¤ë£° ê²ƒì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"},{"content":"\n## Intro\n\nNatural Language(ìì—°ì–´, ì‚¬ëŒì´ ì‚¬ìš©í•˜ëŠ” í†µìƒ ì–¸ì–´)ë¥¼ inputìœ¼ë¡œ í™œìš©í•˜ê³ ì í•˜ëŠ” ë…¸ë ¥ì€ ì»´í“¨í„°ì˜ ë“±ì¥ë¶€í„° ì‹œì‘í•˜ì—¬ ì—¬ëŸ¬ ë²ˆ ì‹œë„ë˜ì–´ ì™”ë‹¤. ì§€ê¸ˆê¹Œì§€ë„ ì™„ë²½í•˜ê²Œ ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ í˜ë“¤ë‹¤. ì™œ Natural Languageë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì€ ì–´ë µê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ NLPì—ì„œëŠ” ì–´ë–¤ ë°©ì‹ì„ í™œìš©í• ì§€ì— ëŒ€í•œ ê°œëµì ì¸ overviewë¥¼ ì œì‹œí•œë‹¤. ë˜í•œ, Natural Languageì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ë‹¨ê³„ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œ Linguistics(ì–¸ì–´í•™)ì„ ê°„ëµí•˜ê²Œ ì •ë¦¬í•œë‹¤.\n\n## NLP\n\nNatural Language Processingì˜ ì•½ìë¡œ ì‚¬ëŒì´ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ë¥¼ inputìœ¼ë¡œ í•˜ì—¬ ì›í•˜ëŠ” ê°’ì„ ì¶”ì¶œí•´ë‚´ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. ì´ë¥¼ ìœ„í•´ì„œ ìš°ë¦¬ëŠ” ì‚¬ëŒì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê±°ë‚˜ ë‹¤ë£° ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì»´í“¨í„°ì—ê²Œ ë¶€ì—¬í•´ì•¼ í•œë‹¤.\n\në¨¼ì €, ì´ëŸ¬í•œ í•„ìš”ê°€ ìˆëŠ” ëŒ€í‘œì ì¸ usecaseë¥¼ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n### Usecase\n\n- **Spam Detection**  \n  ê°€ì¥ ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ mailì—ì„œ spam ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.\n- **POS tagging / NER**  \n  íŠ¹ì • ë‹¨ì–´ ë‹¨ìœ„ì˜ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ê²Œ ë˜ëŠ”ë° ë‹¨ì–´ì˜ í’ˆì‚¬ì™€ ëŒ€ëµì ì¸ ì˜ë¯¸ë¥¼ ê°€ì§„ categoryë¡œ ë¶„ë¥˜ë¡œ taggingí•˜ëŠ” ê³¼ì •ì´ë‹¤. ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ë‹¤ë¥¸ usecaseì—ì„œ í™œìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. í’ˆì‚¬ì™€ categoryëŠ” ë‹¨ì–´ì˜ ëœ»ì„ ì¶”ë¡ í•˜ëŠ”ë° í° ë„ì›€ì„ ì£¼ë©°, ì´ê²ƒì´ ë¬¸ì¥ì˜ ì´í•´ ë“±ì— ë„ì›€ì„ ì£¼ê¸° ë•Œë¬¸ì´ë‹¤.\n- **Sentiment Analysis**  \n  ê°ì •/ì—¬ë¡  ë¶„ì„ ë“±ì˜ ì˜ì—­ì„ ì˜ë¯¸í•˜ë©°, í…ìŠ¤íŠ¸ ë˜ëŠ” ëŒ€í™”ì—ì„œì˜ ê¸ì •/ë¶€ì • ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê±°ë‚˜ í‰ì  ë“±ì„ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.\n- **Conference Resolution**  \n  \"he\", \"she\" ë“± ëŒ€ëª…ì‚¬, ìƒëµ ë‹¨ì–´ ë“±ì„ ì›ë˜ì˜ ë‹¨ì–´ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜ ì±„ìš°ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤. ì´ ì—­ì‹œë„ ì—¬ëŸ¬ ì˜ì—­ì—ì„œ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ì ì¸ ì‘ì—…ì„ í•  ìˆ˜ ìˆë‹¤.  \n- **Word Sense Disambiguation(WSD)**  \n  íŠ¹ì • ë‹¨ì–´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë™ì˜ì–´, ë™ìŒì´ì˜ì–´ ë“±ì—ì„œ ê°€ë¥´í‚¤ëŠ” ì§„ì§œ ì˜ë¯¸ë¥¼ í—·ê°ˆë¦¬ì§€ ì•Šê²Œ ëª…í™•í•˜ê²Œ ë‹¤ì‹œ í•œ ë²ˆ ì²˜ë¦¬í•œë‹¤. ì´ ì—­ì‹œë„ ë‹¤ë¥¸ NLP usecaseì—ì„œ ë‘ë£¨ ì‚¬ìš©ëœë‹¤.\n- **Parsing**  \n  ë¬¸ì¥ì—ì„œ ë‹¨ì–´ë“¤ì„ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ë‹¨ìœ„(êµ¬, ì ˆ, ë¬¸ì¥)ë¡œ ë‹¤ì‹œ groupingí•œë‹¤. ì´ ê³¼ì •ì„ ì˜ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ WSDì™€ Conference Resolution, POS tagging, NERì´ ì´ë£¨ì–´ì§€ë©´ ì¢‹ë‹¤. ì´ ê³¼ì •ì„ í†µí•´ì„œ ë¬¸ì¥ì˜ ê°œëµì ì¸ ì˜ë¯¸ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.\n- **Machine Translation(MT)**  \n  íŠ¹ì • ì–¸ì–´ë¥¼ ë˜ ë‹¤ë¥¸ Natural Languageë¡œ ë³€ê²½í•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.\n- **Information Extraction(IE)**  \n  íŠ¹ì • ë¬¸ì¥ì—ì„œ ì‚¬ìš©ìì—ê²Œ ì˜ë¯¸ìˆì„ë§Œí•œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ë‹¤.\n- **Q\u0026A**  \n  íŠ¹ì • ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ì˜€ì„ ë•Œ, ì´ ëœ»ì„ ì´í•´í•˜ê³ , ì´ì— ì ì ˆí•œ ëŒ€ë‹µì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n- **Paraphrase**  \n  ë¬¸ì¥ì˜ ëœ»ì„ ì´í•´í•˜ê³ , ë” ì‰¬ìš´ í˜•íƒœì˜ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.\n- **Summarization**  \n  ì—¬ëŸ¬ ë¬¸ì¥ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê¸€ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ê³ , ì ì ˆí•œ ë‚´ìš©ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.\n- **Dialog**  \n  Natural Languageë¥¼ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒê³¼ 1:1ë¡œ ë‹´í™”ë¥¼ ì£¼ê³  ë°›ëŠ” ê²ƒì´ë‹¤. ì˜ë¯¸ë¥¼ ì´í•´í•  ë¿ë§Œ ì•„ë‹ˆë¼ ìì‹ ì´ ë‚´ë³´ë‚¼ outputì— ëŒ€í•´ì„œë„ ì ì ˆí•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ í•„ìš”í•˜ë‹¤.\n\nìœ„ì™€ ê°™ì´ ë§ì€ usecaseê°€ ìˆëŠ”ë° ì´ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ì§€ê¸ˆê¹Œì§€ë„ êµ‰ì¥íˆ challengeí•œ ë¶€ë¶„ì´ë‹¤. ê·¸ê²ƒì€ Natural Languageê°€ ê°€ì§€ëŠ” ëª‡ëª‡ íŠ¹ì§• ë•Œë¬¸ì´ë‹¤.\n\n### Why is NLP difficult?\n\nì—¬ê¸°ì„œëŠ” Natural Language ì¤‘ì—ì„œ ì˜ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì„¤ëª…ì´ì§€ë§Œ, í•œêµ­ì–´ë„ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤.\n\n- **non-standard** : Natural Languageë¥¼ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒë“¤ì´ í‘œì¤€ì„ í•­ìƒ ë”°ë¥´ì§€ëŠ” ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ì•½ì–´ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë¬¸ë²•ì— ë§ì§€ ì•ŠëŠ” ë¹„ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ì‚¬ì†Œí†µì„ í•˜ê¸° ë•Œë¬¸ì— ì´ê²ƒì„  ì‹œìŠ¤í…œì´ ì´í•´í•˜ê²Œ í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤.\n- **segmentation** issues : ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¬¶ëŠ” ê²ƒì´ ì–´ë µë‹¤ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ë¬¸ì¥ì˜ ë„ì–´ì“°ê¸°ë¥¼ ì–´ë””ë¡œ ë°›ì•„ë“¤ì´ëƒì— ë”°ë¼ì„œ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒì„ ë³¸ ê²½ìš°ê°€ ìˆì„ ê²ƒì´ë‹¤.\n- **idioms** : ê´€ìš©êµ¬ì˜ ì‚¬ìš©ì€ NLPì—ì„œ ì˜ˆì™¸ì²˜ë¦¬ë¡œ í•´ì£¼ì–´ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤. ë‹¨ì–´ ê·¸ëŒ€ë¡œì˜ ì˜ë¯¸ì™€ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì´ë‹¤.\n- **neologisms** : ì‹ ì¡°ì–´ëŠ” ê³„ì†í•´ì„œ ìƒê²¨ë‚˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ ê³„ì†í•´ì„œ ì—…ë°ì´íŠ¸ í•´ì£¼ëŠ” ê²ƒë„ ë¶€ë‹´ì´ ëœë‹¤.\n- **world knowledge** : ì‚¬ì „ ì§€ì‹ì„ ì•Œê³  ìˆì–´ì•¼ ì´í•´í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´, ë¬¸ì¥ì´ ì¡´ì¬í•œë‹¤. ì¦‰, ì–´ë–¤ ì§€ì‹ì„ ê°€ì§€ê³  ìˆëŠëƒì— ë”°ë¼ì„œ í•´ì„ì´ ë‹¬ë¼ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤.\n- **tricky entity names** : ê³ ìœ  ëª…ì‚¬ ì¤‘ì—ì„œ íŠ¹íˆ contents(ë…¸ë˜, ê·¸ë¦¼, ì†Œì„¤) ë“±ì˜ ì œëª©ì´ í•´ì„ ì‹œì— í—·ê°ˆë¦¬ê²Œ í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´, \"Let it be\"ë¼ëŠ” ë¹„í‹€ì¦ˆì˜ ë…¸ë˜ëŠ” ë¬¸ì¥ ì¤‘ê°„ì— ë“¤ì–´ê°€ë©´, í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë°›ì•„ë“¤ì—¬ì§€ê²Œ ë˜ëŠ”ë° ì´ë¥¼ ì˜ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ í•´ì•¼ í•œë‹¤.\n\nìœ„ì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ìë©´, ë‹¤ì–‘í•œ ë‹¨ì–´ê°€ ë‹¤ì–‘í•œ í˜„ìƒê³¼ ë‹¤ì–‘í•œ ë²•ì¹™(Grammer)ì˜ ì˜í–¥ì„ ë°›ê¸°ì— ì–´ë ¤ìš°ë©°, ë‹¨ì–´ê°€ ê°€ì§€ëŠ” ëª¨í˜¸ì„±ì´ ë¬¸ì œë¥¼ ì•¼ê¸°í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n### Solutions\n\nì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ í¬ê²Œ ë‘ ê°€ì§€ ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n\n- Rule based approach  \n  Gammerì™€ ê°™ì€ ë²•ì¹™ì„ ëª¨ë‘ ì ìš©í•´ì„œ prgorammingì„ í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì´ ë°©ì‹ì€ ë¹„ë¬¸ê³¼ ê°™ì€ ë¬¸ì¥ì„ ì œëŒ€ë¡œ ì²˜ë¦¬í•  ìˆ˜ ì—†ì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì •í™•í•œ í˜•íƒœì˜ ë¬¸ì¥ì´ë¼ë„ ì—¬ëŸ¬ ì˜ë¯¸ë¡œ í•´ì„ë˜ëŠ” ë¬¸ì¥ì—ì„œ ê²½í–¥ì„±ê³¼ ë¬¸ë§¥ì„ ì „í˜€ íŒŒì•…í•  ìˆ˜ ì—†ë‹¤.\n- Statistic based approach  \n  ê·¸ë˜ì„œ ìµœê·¼ì—ëŠ” ê²½í–¥ì„±ê³¼ ë¬¸ë§¥ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ AI ê¸°ìˆ , ML, Deep Learningì„ ì´ìš©í•˜ì—¬ NLPë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ í•˜ë‚˜ì˜ trendë¡œ ìë¦¬ ì¡ì•˜ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì–´ë–»ê²Œ í†µê³„ì ì¸ ì ‘ê·¼ë²•ì´ ê²½í–¥ì„±ê³¼ ë¬¸ë§¥ì„ í¬í•¨í•  ìˆ˜ ìˆì„ê¹Œ? ì´ëŠ” í†µê³„ê°€ ê°€ì§€ëŠ” ê²½í–¥ì„±ì´ë¼ëŠ” íŠ¹ì§•ê³¼ conditional probabilityë¥¼ ì‚¬ìš©í•  ë•Œì˜ ë¬¸ë§¥ì„ í¬í•¨í•œ ê²½í–¥ì„±ì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ í™œìš©í•´ì„œ ê°€ëŠ¥í•˜ë‹¤.\n\n## Linguistics\n\nê²°êµ­ ì•ìœ¼ë¡œ í†µê³„ì ì¸ ë°©ì‹ì„ í™œìš©í•˜ë”ë¼ë„ ìš°ë¦¬ëŠ” ìµœì†Œí•œì˜ ì–¸ì–´í•™ì ì¸ ê¸°ë³¸ì´ í•„ìš”í•˜ë‹¤. ì™œëƒí•˜ë©´, í†µê³„ì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. ìš°ë¦¬ê°€ ì‚¬ìš©í•  ë°ì´í„°ëŠ” text ë˜ëŠ” ìŒì„±ì´ë‹¤. ì´ë¥¼ ì ì ˆí•˜ê²Œ ì²˜ë¦¬í•˜ì—¬ í†µê³„ì— ì‚¬ìš©í•  ìœ ì˜ë¯¸í•œ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. ì´ë¥¼ ìœ„í•´ì„œ ì–¸ì–´í•™ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•œ ê²ƒì´ë‹¤.\n\nì¼ë°˜ì ìœ¼ë¡œ ì–¸ì–´ë¥¼ ë¶„ì„í•  ë•Œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬ëŠ” **Grammar**ì´ë‹¤. ì´ëŠ” íŠ¹ì • languageì—ì„œ í—ˆìš©ë˜ëŠ” ê·œì¹™ì˜ ì§‘í•©ì„ ì •ë¦¬í•œ ê²ƒì´ë‹¤. ì´ê²ƒì˜ ì¢…ë¥˜ëŠ” í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤.\n\n- **Classic Grammar**  \n  ì‚¬ëŒì´ ì‹¤ì œë¡œ ì–¸ì–´ë¥¼ ì‚¬ìš©í•¨ì— ìˆì–´ ë°œìƒí•˜ëŠ” ì´ìƒí•œ ìŠµê´€ê³¼ ê°™ì€ ì–¸ì–´ í‘œí˜„ì´ë‹¤. ì´ëŸ¬í•œ ë²•ì¹™ë“¤ì€ ëŒ€ê²Œ ì˜ˆì œë“¤ì„ í†µí•´ì„œ ì •ì˜ë˜ëŠ”ë° ì´ëŸ° ê²ƒì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ëª…ë°±í•œ ë„êµ¬ê°€ ì¡´ì¬í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ê°íƒ„ì‚¬ì™€ ê°™ì€ ê²ƒë“¤ì´ ì—¬ê¸°ì— í¬í•¨ë˜ê² ë‹¤. ì´ëŠ” ì´ëŸ¬í•œ ë³€ì¹™ì ì¸ í˜•íƒœ ë•Œë¬¸ì— programmingì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.\n- **Explicit Grammar**  \n  ëª…ë°±í•˜ê²Œ ì •ì˜ë˜ì–´ ìˆëŠ” ì–¸ì–´ ê·œì¹™ì„ ì˜ë¯¸í•œë‹¤. ì´ëŠ” Programmìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìœ¼ë©°, ì—¬ëŸ¬ Grammar ì •ë¦¬ ë‚´ìš©ì´ ì´ë¯¸ ì •ë¦¬ë˜ì–´ ìˆë‹¤. (CFG, LFG, GPSG, HPSG, ....)  \n  ì´ë¥¼ ë¬¸ë²•ì ìœ¼ë¡œ ë¶„ì„í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” 6ë‹¨ê³„ì˜ ìˆœì°¨ì ì¸ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤.\n\n### 6 Layers in Language\n\nê° ë‹¨ê³„ëŠ” inputê³¼ outputì„ ê°€ì§„ë‹¤. ë‹¨ê³„ì ìœ¼ë¡œ ì§„í–‰ë˜ê¸° ë•Œë¬¸ì— ì´ì „ ë‹¨ê³„ì˜ outputì´ ë‹¤ìŒ ë‹¨ê³„ì˜ inputì´ ë˜ë©°, ë•Œë•Œë¡œ ëª‡ ë‹¨ê³„ëŠ” ìƒëµë  ìˆ˜ ìˆê¸°ì— ìœ ì—°í•˜ê²Œ ìƒê°í•˜ë„ë¡ í•˜ì.\n\nê° ë‹¨ê³„ì—ì„œ ì‹¤ì œë¡œ íŠ¹ì • ë¬¸ì¥ì´ ì²˜ë¦¬ë˜ëŠ” ê³¼ì •ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œ \"Astronomers saw stars with telescope\"ë¼ëŠ” ë¬¸ì¥ì´ ìŒì„± ë˜ëŠ” textë¡œ ë“¤ì–´ì™”ì„ ë•Œë¥¼ ê°€ì •í•˜ì—¬ ê° ë‹¨ê³„ì—ëŠ” ë¬´ì—‡ì„ í•˜ê³  ì´ë¥¼ í†µí•´ì„œ ì–´ë–»ê²Œ ì´ ë¬¸ì¥ì„ ë°”ê¿€ ìˆ˜ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•´ë³´ê² ë‹¤.\n\n\u003e **1. Phonetics/Orthography(ìŒì„±í•™/ë§ì¶¤ë²•)**\n\në¨¼ì € OrthographyëŠ” ë§ì¶¤ë²• ê²€ì‚¬ë¥¼ ì˜ë¯¸í•˜ë©°, character sequenceë¡œ inputì´ ë“¤ì–´ì˜¤ë©´, ì´ë¥¼ ë§ì¶¤ë²•ì— ë§ëŠ”ì§€ë¥¼ í™•ì¸í•˜ì—¬ ì´ê²ƒì´ ìˆ˜ì •ëœ sequenceë¡œ ë°˜í™˜í•œë‹¤.  \nì˜ˆì‹œ ë¬¸ì¥ì— ìˆëŠ” \"telescope\"ëŠ” ë¬¸ë²•ì— ë§ì§€ ì•Šìœ¼ë¯€ë¡œ \"telescopes\"ë¡œ ë°”ë€Œì–´ì•¼ í•œë‹¤.\n\n| input                                 | output                                 |\n| :------------------------------------ | :------------------------------------- |\n| Astronomers saw stars with telescope. | Astronomers saw stars with telescopes. |\n\nPhoneticsëŠ” ìŒì„±í•™ì„ ì˜ë¯¸í•˜ë©°, í˜€ì™€ ìŒì„±ì˜ ì˜í–¥ì„ ì£¼ëŠ” ë‹¤ì–‘í•œ ê·¼ìœ¡ì˜ ìœ„ì¹˜ í˜•íƒœ, ëª¨ì–‘, ë¹ˆë„ë¥¼ í™œìš©í•˜ì—¬ ììŒê³¼ ëª¨ìŒì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤. Orthoographyì™€ëŠ” ë‹¬ë¦¬ ì–µì–‘ì´ë¼ëŠ” ê²ƒì„ ì¶”ê°€ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.\n\n| input                                                       | output                                 |\n| :---------------------------------------------------------- | :------------------------------------- |\n| Astronomers saw stars with telescopes.ë¥¼ ì˜ë¯¸í•˜ëŠ” ìŒì„± ì‹ í˜¸ | É™sËˆtrÉ’nÉ™mÉ™z sÉ”Ë stÉ‘Ëz wÉªÃ° ËˆtÉ›lÉªskÉ™ÊŠps. |\n\n*\u003chttps://tophonetics.com/\u003e ì„ í†µí•´ì„œ ë³€í™˜í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n\u003e **2. Phonology/Lexicalization(ìŒìš´ë¡ /ì–´íœ˜í™”)**\n\nPhonologyì€ ìŒìš´ë¡ ìœ¼ë¡œ ì†Œë¦¬ì™€ phonemes(ìŒì†Œ)ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬, ìŒì†Œë¥¼ íŠ¹ì • wordë¡œ ë³€í™˜í•˜ê³ , Lexicalizationì—ì„œëŠ” í•´ë‹¹ ë‹¨ì–´ë¥¼ ì‚¬ì „ì—ì„œì˜ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤.\n\n| input                                  | output                                 |\n| :------------------------------------- | :------------------------------------- |\n| É™sËˆtrÉ’nÉ™mÉ™z sÉ”Ë stÉ‘Ëz wÉªÃ° ËˆtÉ›lÉªskÉ™ÊŠps. | Astronomers saw stars with telescopes. |\n\n\u003e **3. Morphology(ì–´í˜•ë¡ )**\n\nMorphologyëŠ” ì–´í˜•ë¡ ìœ¼ë¡œ ìŒì†Œì˜ êµ¬ì„±ì„ ê¸°ë³¸í˜•(lemma)ì˜ í˜•íƒœë¡œ ë³€í™˜í•˜ë©°, ê° ë‹¨ì–´ë“¤ì„ í˜•íƒœí•™ì ì¸ ì˜ë¯¸ë¥¼ ê°–ëŠ” ì¹´í…Œê³ ë¦¬(category, tag)ë¡œ ë¶„ë¥˜í•œë‹¤.\nì—¬ê¸°ì„œ ì‚¬ìš©ë˜ëŠ” lemmaì™€ categoryê°€ ë¬´ì—‡ì¸ì§€ ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ì.\n\n- **lemma**  \n  - ì‚¬ì „ì— í‘œê¸°ë˜ëŠ” ë‹¨ì–´ì˜ ê¸°ë³¸í˜•ìœ¼ë¡œ, ì‚¬ì „ì—ì„œ wordë¥¼ ì°¾ëŠ” pointerê°€ ëœë‹¤.  \n  - ë™ìŒì´ì˜ì–´ì˜ ê²½ìš° íŠ¹ì • ëœ»ì„ ì§€ì¹­í•˜ê³  ì‹¶ì€ ê²½ìš°ì—ëŠ” numberingì„ ìˆ˜í–‰í•˜ê¸°ë„ í•œë‹¤.\n  - ë” ë‚˜ì•„ê°€ì„œëŠ” í˜•íƒœì†Œ(morpeheme)ê¹Œì§€ êµ¬ë¶„í•˜ê¸°ë„ í•œë‹¤. ì´ëŠ” í˜¼ìì„œ ì“°ì¼ ìˆ˜ ìˆëŠ” ìë¦½ í˜•íƒœì†Œ(root)ì™€ ì˜ì¡´ í˜•íƒœì†Œ(stem)ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.  \n    - ì˜ˆë¥¼ ë“¤ë©´, quotations -\u003e quote[root] + -ation[stem] + -s[stem]\n    - ìœ„ì™€ ê°™ì€ í˜•íƒœë¡œ ì„¸ë¶„í™”í•  ìˆ˜ë„ ìˆì§€ë§Œ, ëŒ€ê²ŒëŠ” lemma ë‹¨ìœ„ì—ì„œ ê·¸ì¹œë‹¤.\n- **categorizing**  \n  - categoryëŠ” ì •í•˜ê¸° ë‚˜ë¦„ì´ë©°, ì´ë¯¸ ì •í•´ì ¸ìˆëŠ” tagsetë“¤ë„(Brown, Penn, Multext) ë§ì´ ì¡´ì¬í•˜ê³ , ì–µì–‘ì´ë‚˜ ì‹¤ì œ ë¶„ë¥˜ ë“±ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤.\n  - **POS tagging**ì€ categoryë¥¼ ë¶„ë¥˜í•˜ëŠ” ë°©ë²• ì¤‘ì—ì„œ ê°€ì¥ ìœ ëª…í•œë°, ì´ëŠ” ì—¬ëŸ¬ ì–¸ì–´ì—ì„œ ê±°ì˜ í˜¸í™˜ë˜ê¸° ë•Œë¬¸ì— ì´ ë°©ì‹ì„ í™œìš©í•˜ì—¬ ë¶„ì„í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì•ˆì •ì ì¸ ë°©ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ë³„ë„ì˜ Postingì—ì„œ ë” ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.\n\në˜í•œ, ë‹¨ì–´ì˜ í˜•íƒœëŠ” ì–¸ì–´ë§ˆë‹¤ ë‹¤ì–‘í•˜ê¸° ë•Œë¬¸ì— ì–´ëŠì •ë„ ì–¸ì–´ë§ˆë‹¤ ë‹¤ë¥¸ ì‘ì—…ì„ í•´ì£¼ì–´ì•¼ í•œë‹¤. í¬ê²Œ êµ¬ë¶„ë˜ëŠ” í˜•íƒœë¡œ ì–¸ì–´ë¥¼ 3ê°œì˜ ì¢…ë¥˜ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.\n\n1. **Analytical Language(ê³ ë¦½ì–´)**  \n   í•˜ë‚˜ì˜ ë‹¨ì–´ê°€ ëŒ€ê²Œ í•˜ë‚˜ì˜ morphemeì„ ê°€ì§„ë‹¤. ë”°ë¼ì„œ, í•˜ë‚˜ ì´ìƒì˜ categoryë¡œ êµ¬ë¶„ë˜ì–´ì§ˆ ìˆ˜ ìˆë‹¤.  \n   ex. English, Chinese, Italian\n2. **Inflective Fusional Language(êµ´ì ˆì–´)**  \n   prefix/suffix/infixê°€ ëª¨ë‘ morphemeì— ì˜í–¥ì„ ë¯¸ì¹˜ë©°, morphemeì˜ ì •ì˜ ìì²´ê°€ ì• ë§¤í•´ì§€ëŠ” ì–¸ì–´ í˜•íƒœ  \n   ex. Czech, Russian, Polish, ...\n3. **Agglutinative Language(êµì°©ì–´)**  \n   í•˜ë‚˜ì˜ ë‹¨ì–´ì—ëŠ” morphemeì´ ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ê³ , prefix/suffix/infix ë˜í•œ ëª…í™•í•˜ê²Œ êµ¬ë¶„ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ, ê° morphemeì— ëª…í™•í•œ categoryë¥¼ mappingí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.  \n   ex. Korean, ...\n\n| input                                  | output(based on Brown tagset)                                         |\n| :------------------------------------- | :-------------------------------------------------------------------- |\n| Astronomers saw stars with telescopes. | (astronomer/NNS) (see/VBD) (star/NNS) (with/IN) (telescope/NNS) (./.) |\n\n\u003e **4. Syntax(í†µì‚¬ë¡ )**\n\nlemmaë‚˜ morphemeì„ êµ¬ë¬¸ì˜ ìš”ì†Œì¸ S(Subject, ì£¼ì–´), V(Verb, ë™ì‚¬), O(Object, ëª©ì ì–´)ì™€ ê°™ì€ ìš”ì†Œë¡œ ë¶„ë¥˜í•œë‹¤. ì´ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•  ë•Œì—ëŠ” ë¬¸ì¥ì˜ êµ¬ì„±ìš”ì†Œë¥¼ ì•Œì•„ì•¼ í•œë‹¤. ì´ë¥¼ bottom-upìœ¼ë¡œ ì‚´í´ë³´ì.\n\n- **Word(ë‹¨ì–´)**  \n  ì‚¬ì „ì— ëª…ì‹œëœ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” ê´€ìš©ì–´(dark horse)ë¥¼ í¬í•¨í•œë‹¤.\n- **Phrase(êµ¬)**  \n  ë‘˜ ì´ìƒì˜ ë‹¨ì–´ ë˜ëŠ” êµ¬ì˜ ê²°í•©ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤. ëŒ€ê²Œ í•˜ë‚˜ì˜ ë¬¸ë²•ì ì¸ ì˜ë¯¸ë¡œ ë³€í™˜ë˜ì–´ì§„ë‹¤.  \n  - ëŒ€í‘œì ì¸ ì˜ˆì‹œ\n    - Noun : a new book\n    - Adjective : brand new\n    - Adverbial : so much\n    - Prepositional : in a class\n    - Verb : catch a ball\n  - **Elipse(ìƒëµ)**  \n    ëŒ€ê²Œ ë‹¨ì–´ ë˜ëŠ” êµ¬ê°€ ìƒëµë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. íŠ¹íˆ ë‹´í™”ì˜ ê²½ìš° ë”ìš± ê·¸ë ‡ë‹¤.  \n    ì´ë¥¼ ì¶”ë¡ ì„ í†µí•´ì„œ ì¶”ê°€í•  ìˆ˜ë„ ìˆë‹¤.\n- **Clause(ì ˆ)**  \n  ì ˆì€ ì£¼ì–´ì™€ ì„œìˆ ì–´ë¥¼ ê°–ì¶˜ í•˜ë‚˜ì˜ ë¬¸ì¥ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, ë¬¸ì¥ ìš”ì†Œë¡œì„œ ë” ìƒìœ„ ë¬¸ì¥ì— ì†í•˜ëŠ” ê²½ìš°ì´ë‹¤.  \n  ë˜í•œ, ì˜ì–´ì—ì„œëŠ” íŠ¹íˆ ì ‘ì†ì‚¬ë¡œ ì—°ê²°ëœ ì ˆì´ ì•„ë‹Œ ê²½ìš°ì—ëŠ” í•´ë‹¹ ì ˆì´ ì§€ì¹­í•˜ëŠ” ëŒ€ìƒì´ ì ˆ ë‚´ë¶€ì—ì„œ ìƒëµëœë‹¤. ì´ë¥¼ gapì´ë¼ê³  í•œë‹¤.\n- **Sentense**  \n  í•˜ë‚˜ ì´ìƒì˜ ì ˆë¡œ ì´ë£¨ì–´ì§€ê³ , ì˜ì–´ì—ì„œëŠ” ì‹œì‘ ì‹œì— ëŒ€ë¬¸ìë¡œ í‘œê¸°í•˜ë©° ì¢…ë£Œ ì‹œì—ëŠ” êµ¬ë¶„ìë¡œ .?!ë¡œ ëë‚œë‹¤.\n\nê²°êµ­ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ìš”ì†Œë¥¼ ì ì ˆí•˜ê²Œ í‘œì‹œí•´ì•¼ í•˜ëŠ”ë°, ì´ë¥¼ ìœ„í•´ì„œ tree êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ëŒ€í‘œì ìœ¼ë¡œ ë‘ ê°€ì§€ì˜ êµ¬ì¡°ê°€ ìˆë‹¤.\n\n1. **phrase structure(derivation tree)**\n   ë¬¸ì¥ì„ ê¸°ì ìœ¼ë¡œ ì ˆ, êµ¬, ë‹¨ì–´ë¡œ top-downìœ¼ë¡œ ë‚´ë ¤ê°€ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.  \n   ê° ë‹¨ìœ„ë¥¼ ë¬¶ì„ ë•Œì—ëŠ” ()ë¥¼ ì´ìš©í•˜ê³ , ê·¸ ë’¤ì•  í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì´ ë¬´ìŠ¨ êµ¬, ì ˆì¸ì§€ë¥¼ í‘œê¸°í•œë‹¤.\n2. **dependency structure**  \n   ë‹¨ì–´ ê°„ì˜ ê´€ê³„ì— ë” ì§‘ì¤‘í•˜ì—¬ ë‚˜íƒ€ë‚¸ë‹¤. ë”°ë¼ì„œ, ì‚¬ëŒì´ ë³´ê¸°ì—ëŠ” ë¶ˆëª…í™•í•´ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ íŠ¹ì • usecaseì—ì„œëŠ” ìœ ìš©í•˜ë‹¤.\n\n| input                                                                 | output (phrase structure)                                                              |\n| :-------------------------------------------------------------------- | :------------------------------------------------------------------------------------- |\n| (astronomer/NNS) (see/VBD) (star/NNS) (with/IN) (telescope/NNS) (./.) | ((astronomer/NNS)NP ((see/VBD)V ((star/NNS)NP ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S |\n\n![nlp-phase-structure](/images/nlp-phase-structure.jpg)\n\n\u003e **5. Semantics(Meaning, ì˜ë¯¸ë¡ )**\n\nê°„ë‹¨í•˜ê²ŒëŠ” ì£¼ì–´, ëª©ì ì–´ì™€ ê°™ì€ tagë‚˜ \"Agent\"ë‚˜ \"Effect\"ì™€ ê°™ì€ tagë¥¼ ì ìš©í•˜ë©°, ì „ì²´ì ì¸ ì˜ë¯¸ë¥¼ ìœ ì¶”í•´ë‚¸ë‹¤.\n\n| input                                                                                  | output                                                                                                |\n| :------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------- |\n| ((astronomer/NNS)NP ((see/VBD)V ((star/NNS)NP ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S | ((astronomer/NNS)NP/agent ((see/VBD)V ((star/NNS)NP/affected ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S |\n\n\u003e **6. Discourse/Pragmatics(ë‹´í™”/í™”ìš©ë¡ )**\n\nì‹¤ì œ ëŒ€í™” ë“±ê³¼ ê°™ì€ ëª©í‘œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì•ì„œ ë³´ì•˜ë˜ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì´ìš©í•œë‹¤.\n\në§Œì•½, í•´ë‹¹ ë°ì´í„°ë¥¼ í†µí•´ì„œ í•˜ê³ ì í•˜ëŠ” ê²ƒì´ ì´ ì´ì•¼ê¸°ë¥¼ í•œ ì‚¬ëŒì´ ì‹ë‹¹ ë‚´ë¶€ì— ìˆëŠ”ì§€ë¥¼ íŒë‹¨í•˜ê³ ì í•œë‹¤ê³  ê°€ì •í•´ë³´ì.\n\n| input                                                                                                 | output |\n| :---------------------------------------------------------------------------------------------------- | :----- |\n| ((astronomer/NNS)NP/agent ((see/VBD)V ((star/NNS)NP/affected ((with/IN)P (telescope/NNS)NP)PP)NP)VP)S | False  |\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- text to phonetic converter, \u003chttps://tophonetics.com\u003e\n","slug":"nlp-linguistics","date":"2022-10-19 09:03","title":"[NLP] 1. Linguistics","category":"AI","tags":["NLP","Languagistics"],"desc":"Natural Language(ìì—°ì–´, ì‚¬ëŒì´ ì‚¬ìš©í•˜ëŠ” í†µìƒ ì–¸ì–´)ë¥¼ inputìœ¼ë¡œ í™œìš©í•˜ê³ ì í•˜ëŠ” ë…¸ë ¥ì€ ì»´í“¨í„°ì˜ ë“±ì¥ë¶€í„° ì‹œì‘í•˜ì—¬ ì—¬ëŸ¬ ë²ˆ ì‹œë„ë˜ì–´ ì™”ë‹¤. ì§€ê¸ˆê¹Œì§€ë„ ì™„ë²½í•˜ê²Œ ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ í˜ë“¤ë‹¤. ì™œ Natural Languageë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì€ ì–´ë µê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ NLPì—ì„œëŠ” ì–´ë–¤ ë°©ì‹ì„ í™œìš©í• ì§€ì— ëŒ€í•œ ê°œëµì ì¸ overviewë¥¼ ì œì‹œí•œë‹¤. ë˜í•œ, Natural Languageì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ë‹¨ê³„ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œ Linguistics(ì–¸ì–´í•™)ì„ ê°„ëµí•˜ê²Œ ì •ë¦¬í•œë‹¤.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"}],"params":{"subject":"NLP"}},"__N_SSG":true},"page":"/tags/[subject]","query":{"subject":"NLP"},"buildId":"Zq6IxrKlCwpKTIcL6SE7Z","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>