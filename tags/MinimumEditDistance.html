<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta property="og:type" content="blog"/><meta property="og:site_name" content="JustLog"/><meta property="og:image" content="https://euidong.github.io/logo192.png"/><title>#MinimumEditDistance | JustLog</title><meta name="description" content="#MinimumEditDistance 관련 Posting"/><meta property="og:description" content="#MinimumEditDistance 관련 Posting"/><meta property="og:title" content="#MinimumEditDistance | JustLog"/><link rel="canonical" href="https://euidong.github.io/tags/MinimumEditDistance"/><meta property="og:url" content="https://euidong.github.io/tags/MinimumEditDistance"/><meta name="next-head-count" content="11"/><link rel="icon" href="https://euidong.github.io/favicon.png"/><link rel="apple-touch-icon" href="https://euidong.github.io/logo192.png"/><link rel="preload" href="/_next/static/css/d4ec5c8b3df09443.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d4ec5c8b3df09443.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6dc16d084a5153e5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6dc16d084a5153e5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" id="Adsense-id" data-ad-client="ca-pub-7452732177557701" async="" defer="" data-nscript="beforeInteractive"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-81da43a8dcd978d9.js" defer=""></script><script src="/_next/static/chunks/main-7b6c38cbad60dfcf.js" defer=""></script><script src="/_next/static/chunks/pages/_app-f72011f76003c049.js" defer=""></script><script src="/_next/static/chunks/675-ae8e8a351ce30ae2.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Bsubject%5D-0fc8f67b45fbbd0f.js" defer=""></script><script src="/_next/static/6gkm4yPnMomFWBCHeuSM4/_buildManifest.js" defer=""></script><script src="/_next/static/6gkm4yPnMomFWBCHeuSM4/_ssgManifest.js" defer=""></script><script src="/_next/static/6gkm4yPnMomFWBCHeuSM4/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_wrapper__dKJSz root"><header class="Layout_header__XosLl" style="position:static"><div><button tabindex="1" class="SideBarToggler_search_bar_toggler__CEuUg"><svg stroke="currentColor" fill="none" stroke-width="0" viewBox="0 0 24 24" height="35px" width="35px" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></button><nav class="SideBar_side_bar__wrapper--close__8Nwnr"><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/">Home</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/tags">Tags</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Algorithm">Algorithm<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Computer%20Architecture">Computer Architecture<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->7<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Tech">Tech<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->17<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Web">Web<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Paper">Paper<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->2<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Network">Network<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/AI">AI<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->15<!-- -->)<!-- --></span></a></nav></div><a class="Logo_logo___yD0t" tabindex="1" href="/"></a><div><button class="SearchBarToggler_search_bar_toggler__3dHbA" tabindex="2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg></button></div></header><section><div class="RowCard_row_card__list__background___xFj5"><h1 class="RowCard_row_card__list__title__t4a2h"> Minimum Edit Distance</h1><label class="RowCard_row_card__list__select__wrapper__TZ4_9"><select class="RowCard_row_card__list__select__dxkxA"><option class="RowCard_row_card__list__select__option__GRKZU">최신순<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">AtoZ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">ZtoA<!-- --></option></select></label><ul class="RowCard_row_card__list__wrapper__5Gtgi"><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/nlp-text-processing"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[NLP] 2. Text Processing" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[NLP] 2. Text Processing" srcSet="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/nlp-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/nlp-text-processing">[NLP] 2. Text Processing</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 19일 21시 59분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NLP"># <!-- -->NLP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Regex"># <!-- -->Regex<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Tokenization"># <!-- -->Tokenization<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Collocation"># <!-- -->Collocation<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/MinimumEditDistance"># <!-- -->MinimumEditDistance<!-- --></a></ul></div></div></ul></div></section><footer class="Layout_footer__EL5v8"><div class="Layout_footer__copyright__r5baC"><span>Copyright © euidong</span><br/><span>모든 컨텐츠에 대한 저작권은 작성자에게 존재합니다. <!-- --><br/>불법 복제를 통한 상업적 사용을 절대적으로 금지합니다. <!-- --><br/>단, 비상업적 이용의 경우 출처 및 링크를 적용한다면 자유롭게 사용가능 합니다.<!-- --></span><span>Also I use photos by<!-- --> <!-- --><a href="https://unsplash.com/@lorenzoherrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Lorenzo Herrera</a> <!-- -->on<!-- --> <!-- --><a href="https://unsplash.com/s/photos/tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Unsplash</a></span></div><div class="Layout_footer__contents__YZWSm"><a class="Layout_footer__contents__link__K_TKH" href="https://github.com/euidong" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>github</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://euidong.github.io/portfolio" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M6 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm-5 6s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H1zM11 3.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 0 1h-4a.5.5 0 0 1-.5-.5zm.5 2.5a.5.5 0 0 0 0 1h4a.5.5 0 0 0 0-1h-4zm2 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2zm0 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2z"></path></svg><span>portfolio</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://chrome.google.com/webstore/detail/bonfire/nkooidijgbppkojdgkoafcoppnohdfka?hl=ko" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 16c-1.066-2.219-0.498-3.49 0.321-4.688 0.897-1.312 1.129-2.61 1.129-2.61s0.706 0.917 0.423 2.352c1.246-1.387 1.482-3.598 1.293-4.445 2.817 1.969 4.021 6.232 2.399 9.392 8.631-4.883 2.147-12.19 1.018-13.013 0.376 0.823 0.448 2.216-0.313 2.893-1.287-4.879-4.468-5.879-4.468-5.879 0.376 2.516-1.364 5.268-3.042 7.324-0.059-1.003-0.122-1.696-0.649-2.656-0.118 1.823-1.511 3.309-1.889 5.135-0.511 2.473 0.383 4.284 3.777 6.197z"></path></svg><span>chat</span></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"content":"\n## Intro\n\nNLP를 수행할 때, 우리는 sequence of character를 처리하는 방식을 제대로 알아야 제대로 된 전처리와 후처리 등이 가능하다. 따라서 해당 chapter에서는 어떻게 원본 문자/단어/문장을 처리하는 방식을 다룰 것이다.\n\n## Regular Express\n\n아주 기본적인 문자열 처리 방법이다. 이를 알고 있어야 실질적인 처리가 가능하다. 해당 내용은 별도의 Posting으로 분리하여 다루었다. ([🔗 Regex](/posts/regex))를 살펴보도록 하자.\n\n## Text Normalization\n\n우리가 사용할 NL은 정제되어 있지 않아서 여러 전처리를 수행해야 한다. 그 중에서 대중적으로 좋다고 알려진 방법들을 살펴볼 것이다. 기본적으로는 아래 단계를 처리하는 것이 일반적이다.\n\n1. Word Tokenization  \n   말 그대로 NL 데이터가 입력되었을 때, 이를 단어 단위로 쪼개는 것이다.\n2. Word Reformating  \n   단어를 나누었다면, 각 단어의 형태를 처리하기 쉬운 형태로 Normalizing하는 것이다.  \n3. Sentence Segmentation  \n   문장 단위로 구분해는 과정이다.\n\n이제 각 단계를 세부적으로 다뤄보겠다.\n\n### 1. Word Tokenization\n\n우선 쉽게 생각할 수 있는 것은 단순히 띄어쓰기를 기준으로 구분하는 것이다. 그렇게 하면, 우리는 입력으로 주어진 Corpus에서 token을 추출할 수 있다.\n\n하지만, \"San Francisco\"와 같은 단어가 두 개의 token으로 나누는 것이 아니라 하나의 token으로 처리되기를 원할 수 있다. 뿐만 아니라 일부 언어들(특히 중국어와 일본어)의 경우 띄어쓰기 없이 작성하는 언어들의 경우 문제는 더 커질 수 있다. 이 경우에는 **Word Segmenting**이라는 알고리즘을 활용할 수도 있는데, 원리는 매우 간단하다. 언어의 모든 단어를 포함하는 사전을 기반으로 문장에서 사전에 일치하는 가장 긴 문자열을 찾을 수 있을 때까지 token을 연장해서 만드는 방식이다.\n\n그러나 이 방식도 결국은 특정 언어(중국어, 등)에서는 잘 작동하지만, 일부 언어(영어 등)에서는 잘 작동하지 않는 경우가 많다. 따라서, 최근에는 확률에 기반하여 같이 등장하는 횟수가 많을 경우 하나의 token으로 묶는 형태의 tokenization을 더 선호한다.\n\n이 과정에서 우리가 추가적으로 수행하는 것이 바로 word의 갯수를 추출하는 것이다. 대게 우리가 관심 있어하는 수는 총 3가지이다.\n\n1. **number of tokens**  \n   즉, 띄어쓰기로 나뉘어지는 token들의 총 갯수를 의미한다.\n2. **number of types(Vocabulary)**  \n   띄어쓰기로 나뉘어진 token들의 중복을 제거한 종류들의 갯수를 의미한다. 대게 이러한 type들의 모음을 Vocabulary라고 한다.\n3. **number of each type's tokens**  \n   각 종류의 token이 얼마나 많이 등장했는지를 의미한다.\n\n여기서 이러한 token이나 type이 서로 같나는 것을 어떻게 구분할 수 있을까? 이를 위해서 Word Reformating을 수행하여 좀 더 일반적인 형태로 변형하여 위의 수들을 파악하기도 한다.\n\n### 2. Word Normalization and Stemming(Word Reformating)\n\n대게의 언어는 word의 형태가 여러 개로 존재한다. 이 과정에서 우리가 고려해야 할 것이 정말 많다. 그 중에서 가장 기본적으로 수행되어야 할 내용은 다음과 같다. 해당 내용은 영어에 중심을 둔 설명이다.\n\n1. **Uppercase**  \n   영어에서 첫 글자는 대문자로 시작한다는 규칙이 있다. 또는 강조하고 싶은 단어를 대문자로 표현하기도 한다. 그 결과 token의 종류를 추출하는 과정에서 문제를 일으키기도 한다. 따라서, 이를 모두 lowercase로 바꿔버리는 것이다. 하지만, 모든 경우에 이를 적용할 수 잇는 것은 아니다. 가장 대표적인 예시로 US와 us의 의미가 다르다는 것이다. 또, 고유 명사인 General Motors와 같은 경우도 다르게 처리하는 것이 좋다. 따라서, 이를 고려해서 먼저 처리한 이후에 전체 데이터를 lowercase로 변환하는 방식을 수행한다.  \n2. **Lemmatization**  \n   Lemma(기본형, 사전형)로 단어를 변환하는 것이다. 가장 기본적인 것은 am, are, is와 같은 be동사를 모두 be로 변환하거나 car, cars, car's를 모두 기본형태인 car로 바꾸는 것이다. 대게의 경우에는 이 과정에서 의미를 일부 잃어버리기 때문에 lemma + tag로 기존 token을 복구할 수 있도록 하는 tag를 포함하는 것이 좋다.\n3. **Stemming**  \n   morpheme(형태소)은 중심 의미를 가지는 stem과 핵심 의미는 아니지만 stem에 추가 의미를 더해주는 affixes로 나누어 word를 나눌 수 있다. 따라서, 각 token을 가장 core의 의미를 가지는 stem으로 나타내는 방식이다. 대표적인 예시가 automate, automatic, automation을 automat으로 변환하는 것이다. 이는 lemmatization보다 넓은 범위의 word를 하나로 묶기 때문에 세부의미가 더 손실될 수 있다. 따라서, 기존 의미로 복구할 수 있는 tag를 포함하는 것이 좋다.\n\n### 3. Sentence Segmentation\n\n문장을 구분할 수 있는 도구로 우리는 \"?\", \"!\", \".\"을 활용한다. \"?\"와 \"!\" 같은 경우는 문장의 끝을 의미하는것이 대게 자명하다. 하지만, \".\"은 꽤나 애매할 수 있다. 소수점, Abbreviation(Mr., Dr., Inc.)와 같은 경우에 빈번하게 사용되기 때문이다. 따라서, 이를 판단하기 위해서 Decision Tree를 만들어서 이를 수행한다. 아래와 같이 사람이 직접 규칙을 정할 수도 있지만 현재는 대게 통계 기반으로 수행한다.\n\n![nlp-sentence-segmentation](/images/nlp-sentence-segmentation.jpg)\n\n## Collocation(연어) processing\n\nText Normalization을 통해서 우리는 sentence를 구분하고, word를 추출할 수 있었다. 하지만, 단순히 하나의 word를 기반으로 처리하는 것이 아니라 주변 단어를 활용하여 처리해야만 얻을 수 있는 정보들이 있다. 우리는 이를 Collocation(연어)를 활용하여 수행한다. 이는 특정 단어쌍이 높은 빈도로 같이 붙어 사용되는 현상을 말한다. **\"모든 단어는 이를 동반하는 주변 단어에 의해 특성 지어진다.\"** 따라서, 우리는 이 collocation을 co-ocurrence로 생각할 수 있다. 이를 통해서 우리는 다음과 같은 것들을 할 수 있다.\n\n1. **lexicography(사전 편찬)** : 같가니 유사한 뜻을 가지는 단어는 빈번하게 붙어서 사용되는데 이를 이용해서 하나의 단어의 뜻을 안다면, 이를 통해서 다른 단어의 뜻을 추론하며 확장해나갈 수 있다.\n2. **language modeling** : NL를 통해서 원하는 결과를 얻기 위해서 특정 parameter를 추정해내는 것을 language modeling이라고 하는데 이 과정에서 collocation을 활용하는 것이 단일 단어를 활용하는 것보다 context를 활용할 수 있다는 점에서 장점을 발휘할 수 있다.\n3. **NL generation** : 우리는 문맥상 매끄러운 문장을 원한다. 즉, \"감을 잡다\"를 \"감을 붙잡다\"라고 했을 때, 뜻을 이해할 수는 있지만 어색하다고 느낀다. 따라서, 이 관계를 활용해서 NL을 생성해야 하기 때문에 collocation을 고려해야 한다.\n\n그렇다면, 이러한 Collocation을 어떻게 찾을 수 있을까?\n\n1. **Frequency**  \n   가장 간단하게 단순히 동시 발생 빈도를 확인하는 것이다. 정확한 파악을 위해서는 빈번하게 등장하는 의미 없는 단어를 먼저 filtering할 필요가 있다. 대표적인 예시로 a, the, and 등이 있다.\n2. **Hypothesis Testing**  \n   가설 검증으로 우리가 가정한 collocation을 지정하고, 이 사건이 일어날 가능성을 굉장히 낮게 하는 가설을 반대로 가정한 후에 이것이 불가능하다는 것을 증명하는 Null Hypothesis를 이용한 증명으로 타당성을 확보하는 것이다. 따라서, 우리가 보이고자 하는 것은 word1, word2가 있을 때, 두 단어가 서로 의존적이라는 것을 증명하고 싶은 것이다. 따라서, Null Hypothesis로 두 단어는 독립이다라고 지정하면, 우리는 다음 식을 얻을 수 있다.  \n   $p(w_{1}, w_{2}) = p(w_{1})p(w_{2})$  \n   이를 바탕으로 t-검증을 다음과 같이 수행할 수 있다.  \n   $t = {{p(w_{1}, w_{2}) - p(w_{1})p(w_{2})} \\over \\sqrt{p(w_{1}, w_{2})\\over{N}}} $  \n   t값이 이제 커질 수록 우리는 해당 가설이 틀렸음을 증명하여 collocation임을 주장할 수 있다.\n\n## Minimum Edit Distance\n\n단어 또는 문장 간 유사도를 측정할 때, 사전을 기반으로 수행할 수도 있지만 참고할 corpus가 마땅하지 않거나 더 추가적인 수치가 필요하다면, Minimum Edit Distance로 유사도를 측정하기도 한다. 즉, 두 문자열이 같아지기 위해서 어느정도의 수정이 필요한지를 수치화한 것이다. 여기서 연산은 새로운 문자 추가, 삭제, 대체만 가능하다.\n\n```plaintext\nS - N O W Y  | - S N O W - Y\nS U N N - Y  | S U N - - N Y\ndistance : 3 | distance : 5\n```\n\n다음과 같이 표현이 가능하다.\n\n1. 문자열 x, y가 있을 때, $E(i, j)$는 x의 0\\~i까지를 포함하는 문자열과 y의 0~j까지를 포함하는 문자열의 distance라고 하자.\n2. 이렇게 되면, $E(i,j)$에서 우리는 끝문자의 규칙을 볼 수 있다.\n\n    오른쪽 끝 문자가 가질 수 있는 조합은 3가지 밖에 없다.\n\n    ```plaintext\n    x[i]        | -           | x[i]\n    -           | y[j]        | y[j]\n    distance: 1 | distance: 1 | distance: 0 or 1\n    ```\n\n3. 그렇다면 우리는 하나의 사실을 알게 된다.\n\n    $E(i,j)$는 다음 경우의 수 중 하나여야만 한다.\n\n    - $E(i-1, j) + 1$\n    - $E(i, j-1) + 1$\n    - $E(i-1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )$\n4. 따라서, 다음과 같은 식을 유도할 수 있다.\n\n    $E(i, j) =\\min( \\\\\n      \\quad E(i-1, j) + 1, \\\\\n      \\quad E(i, j-1) + 1,  \\\\\n      \\quad E(i - 1, j-1) + ((x[i] == y[j])\\text{ ? 0 : 1} )\\\\\n    )$\n\n만약, 각 연산의 비용이 다를 경우라면, 1 대신에 그 값을 넣어주면 충분히 풀 수 있으며, 추가적으로 최적의 이동형태를 알고 싶다면, back pointer 하나를 추가하는 것으로 충분하다.\n\n## Reference\n\n- Tumbnail : Photo by [David Ballew](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@daveballew?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"nlp-text-processing","date":"2022-10-19 21:59","title":"[NLP] 2. Text Processing","category":"AI","tags":["NLP","Regex","Tokenization","Collocation","MinimumEditDistance"],"desc":"NLP를 수행할 때, 우리는 sequence of character를 처리하는 방식을 제대로 알아야 제대로 된 전처리와 후처리 등이 가능하다. 따라서 해당 chapter에서는 어떻게 원본 문자/단어/문장을 처리하는 방식을 다룰 것이다.","thumbnailSrc":"https://euidong.github.io/images/nlp-thumbnail.jpg"}],"params":{"subject":"MinimumEditDistance"}},"__N_SSG":true},"page":"/tags/[subject]","query":{"subject":"MinimumEditDistance"},"buildId":"6gkm4yPnMomFWBCHeuSM4","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>