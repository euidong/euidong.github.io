<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta property="og:type" content="blog"/><meta property="og:site_name" content="JustLog"/><meta property="og:image" content="https://euidong.github.io/logo192.png"/><title>#Clustering | JustLog</title><meta name="description" content="#Clustering ê´€ë ¨ Posting"/><meta property="og:description" content="#Clustering ê´€ë ¨ Posting"/><meta property="og:title" content="#Clustering | JustLog"/><link rel="canonical" href="https://euidong.github.io/tags/Clustering"/><meta property="og:url" content="https://euidong.github.io/tags/Clustering"/><meta name="next-head-count" content="11"/><link rel="icon" href="https://euidong.github.io/favicon.png"/><link rel="apple-touch-icon" href="https://euidong.github.io/logo192.png"/><link rel="preload" href="/_next/static/css/d4ec5c8b3df09443.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d4ec5c8b3df09443.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6dc16d084a5153e5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6dc16d084a5153e5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" id="Adsense-id" data-ad-client="ca-pub-7452732177557701" async="" defer="" data-nscript="beforeInteractive"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-81da43a8dcd978d9.js" defer=""></script><script src="/_next/static/chunks/main-7b6c38cbad60dfcf.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d260d0d813c54456.js" defer=""></script><script src="/_next/static/chunks/675-ae8e8a351ce30ae2.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Bsubject%5D-0fc8f67b45fbbd0f.js" defer=""></script><script src="/_next/static/pI-qeh2DPTobWYk-Pbn2o/_buildManifest.js" defer=""></script><script src="/_next/static/pI-qeh2DPTobWYk-Pbn2o/_ssgManifest.js" defer=""></script><script src="/_next/static/pI-qeh2DPTobWYk-Pbn2o/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_wrapper__dKJSz root"><header class="Layout_header__XosLl" style="position:static"><div><button tabindex="1" class="SideBarToggler_search_bar_toggler__CEuUg"><svg stroke="currentColor" fill="none" stroke-width="0" viewBox="0 0 24 24" height="35px" width="35px" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></button><nav class="SideBar_side_bar__wrapper--close__8Nwnr"><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/">Home</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/tags">Tags</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Algorithm">Algorithm<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Computer%20Architecture">Computer Architecture<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->7<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Tech">Tech<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->17<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Web">Web<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Paper">Paper<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->2<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Network">Network<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/AI">AI<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->20<!-- -->)<!-- --></span></a></nav></div><a class="Logo_logo___yD0t" tabindex="1" href="/"></a><div><button class="SearchBarToggler_search_bar_toggler__3dHbA" tabindex="2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg></button></div></header><section><div class="RowCard_row_card__list__background___xFj5"><h1 class="RowCard_row_card__list__title__t4a2h"> Clustering</h1><label class="RowCard_row_card__list__select__wrapper__TZ4_9"><select class="RowCard_row_card__list__select__dxkxA"><option class="RowCard_row_card__list__select__option__GRKZU">ìµœì‹ ìˆœ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">AtoZ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">ZtoA<!-- --></option></select></label><ul class="RowCard_row_card__list__wrapper__5Gtgi"><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-clustering"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 9. Clustering" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 9. Clustering" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-clustering">[ML] 9. Clustering</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 11ì›” 23ì¼ 09ì‹œ 19ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/UnsupervisedLearning"># <!-- -->UnsupervisedLearning<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Clustering"># <!-- -->Clustering<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/K-means"># <!-- -->K-means<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/GMM"># <!-- -->GMM<!-- --></a></ul></div></div></ul></div></section><footer class="Layout_footer__EL5v8"><div class="Layout_footer__copyright__r5baC"><span>Copyright Â© euidong</span><br/><span>ëª¨ë“  ì»¨í…ì¸ ì— ëŒ€í•œ ì €ì‘ê¶Œì€ ì‘ì„±ìì—ê²Œ ì¡´ì¬í•©ë‹ˆë‹¤. <!-- --><br/>ë¶ˆë²• ë³µì œë¥¼ í†µí•œ ìƒì—…ì  ì‚¬ìš©ì„ ì ˆëŒ€ì ìœ¼ë¡œ ê¸ˆì§€í•©ë‹ˆë‹¤. <!-- --><br/>ë‹¨, ë¹„ìƒì—…ì  ì´ìš©ì˜ ê²½ìš° ì¶œì²˜ ë° ë§í¬ë¥¼ ì ìš©í•œë‹¤ë©´ ììœ ë¡­ê²Œ ì‚¬ìš©ê°€ëŠ¥ í•©ë‹ˆë‹¤.<!-- --></span><span>Also I use photos by<!-- --> <!-- --><a href="https://unsplash.com/@lorenzoherrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Lorenzo Herrera</a> <!-- -->on<!-- --> <!-- --><a href="https://unsplash.com/s/photos/tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Unsplash</a></span></div><div class="Layout_footer__contents__YZWSm"><a class="Layout_footer__contents__link__K_TKH" href="https://github.com/euidong" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>github</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://euidong.github.io/portfolio" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M6 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm-5 6s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H1zM11 3.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 0 1h-4a.5.5 0 0 1-.5-.5zm.5 2.5a.5.5 0 0 0 0 1h4a.5.5 0 0 0 0-1h-4zm2 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2zm0 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2z"></path></svg><span>portfolio</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://chrome.google.com/webstore/detail/bonfire/nkooidijgbppkojdgkoafcoppnohdfka?hl=ko" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 16c-1.066-2.219-0.498-3.49 0.321-4.688 0.897-1.312 1.129-2.61 1.129-2.61s0.706 0.917 0.423 2.352c1.246-1.387 1.482-3.598 1.293-4.445 2.817 1.969 4.021 6.232 2.399 9.392 8.631-4.883 2.147-12.19 1.018-13.013 0.376 0.823 0.448 2.216-0.313 2.893-1.287-4.879-4.468-5.879-4.468-5.879 0.376 2.516-1.364 5.268-3.042 7.324-0.059-1.003-0.122-1.696-0.649-2.656-0.118 1.823-1.511 3.309-1.889 5.135-0.511 2.473 0.383 4.284 3.777 6.197z"></path></svg><span>chat</span></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"content":"\n## Intro\n\nì´ì „ê¹Œì§€ì˜ Postingì—ì„œëŠ” Supervised Learning ì¦‰, ì´ë¯¸ Labelingì´ ì™„ë£Œëœ ë°ì´í„°ì— ì˜í•œ Learningì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ì—ˆë‹¤. ì§€ê¸ˆë¶€í„°ëŠ” Unsupervised Learningì— ëŒ€í•´ì„œ ì¡°ê¸ˆ ë” ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤. ëŒ€í‘œì ì¸ Unsupervised Learningì€ Clustering, Feature Selection(or Dimensionality Reduction), Generative Model ë“±ì´ ì¡´ì¬í•œë‹¤. ì´ë“¤ì— ëŒ€í•´ì„œ ì°¨ê·¼ì°¨ê·¼ ì‚´í´ë³´ë„ë¡ í•˜ê³ , í•´ë‹¹ Postingì—ì„œëŠ” ê°€ì¥ ëŒ€í‘œì ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” Clusteringì„ ë¨¼ì € ì‚´í´ë³´ë©´ì„œ Unsupervised Learningì— ëŒ€í•œ ê³„ëµì ì¸ ì´í•´ë¥¼ í•´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## Clustering\n\nClusteringì€ unlabeled dataë¥¼ dataê°„ ìœ ì‚¬ì„± ë˜ëŠ” ê±°ë¦¬ ì§€í‘œ ë“±ì„ í™œìš©í•˜ì—¬ ë¯¸ë¦¬ ì§€ì •í•œ ìˆ˜ ë§Œí¼ì˜ partitioningí•˜ëŠ” ì‘ì—…ì„ ì˜ë¯¸í•œë‹¤. ì¦‰, ìš°ë¦¬ê°€ í•™ìŠµì„ ì§„í–‰í•¨ì— ìˆì–´ dataëŠ” labelì´ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” dataê°„ì˜ ê´€ê³„ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì„œ ì´ë¥¼ ë¶„ë¥˜í•´ë‚´ëŠ” ê²ƒì´ ëª©í‘œì¸ ê²ƒì´ë‹¤.\n\nì´ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë°©ë²•ì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.\n\n1. **Non-Parametric Approach**  \n   ì´ë¦„ ê·¸ëŒ€ë¡œ í™•ë¥ ì  ë¶„í¬ë¥¼ ê°€ì •í•œ í›„, Parameterë¥¼ ì°¾ì•„ê°€ëŠ” ë°©ì‹ì´ ì•„ë‹Œ ì§ê´€ì ì¸ ë°©ë²•(Huristic Approach)ì„ í™œìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê·¸ë ‡ê¸°ì— í™•ë¥ ì ì¸ í•´ì„ì´ ë’·ë°›ì¹¨ë˜ê¸° ë³´ë‹¤ëŠ” Algorithmì„ í†µí•´ì„œ ì´ë¥¼ ì„¤ëª…í•œë‹¤. ëŒ€í‘œì ì¸ ë°©ë²•ì´ K-Means Clusteringì´ë‹¤.\n2. **Parametric Approach**  \n   í™•ë¥ ì  ë¶„í¬ë¥¼ ê°€ì •í•œ í›„, Parameterë¥¼ ì°¾ì•„ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ, ëŒ€í‘œì ì¸ ë°©ë²•ì´ Gaussian ë¶„í¬ë¥¼ ê°€ì •í•˜ê³  ì°¾ì•„ë‚˜ê°€ëŠ” Gaussian Mixture Model(GMM, or MoG, Mixture of Gaussian)ì´ ìˆë‹¤.\n\në”°ë¼ì„œ, Clusteringì„ ëŒ€í‘œí•˜ëŠ” K-means Clusteringê³¼ GMMì„ ê° ê° ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n### K-Means Clustering\n\nK-Means Clusteringì€ Kê°œì˜ í‰ê· ê°’ì„ í†µí•œ Clusteringìœ¼ë¡œ í•´ì„í•˜ë©´ ì˜ë¯¸ íŒŒì•…ì´ ì‰½ë‹¤. ì¦‰, Kê°œì˜ Partitionì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ Kê°œì˜ í‰ê· ê°’ì„ ì°¾ì•„ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë” ê°€ê¹Œìš´ í‰ê· ê°’ì— ì†í•˜ëŠ” Partitionì— dataë¥¼ ë¶„ë°°í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ êµ¬í•´ì•¼í•  ê°’ì€ ê° dataê°€ ì–´ëŠ Partitionì— ì†í•˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´($\\bold{r}\\leftarrow\\text{one hot vector}$)ì™€ ê° Partitionì˜ í‰ê· ê°’($\\mu$)ì´ë‹¤. ì¦‰, K-means Clusteringì—ì„œëŠ” ê¸°ì¡´ dataë“¤ì„ í†µí•´ì„œ Kê°œì˜ í‰ê· ê°’(K-means)ì„ ì°¾ì•„ì„œ(**Learning**) ì´í›„ì— ì¶”ê°€ë¡œ ë“¤ì–´ì˜¬ dataì— ëŒ€í•´ì„œë„ ë˜‘ê°’ì€ K-meansë¥¼ í†µí•´ì„œ Partitionì„ ì°¾ì„ ìˆ˜ ìˆë‹¤(**Inference**).ë˜ëŠ” ëª¨ë“  dataë¥¼ ì €ì¥í•´ë‘ì—ˆë‹¤ê°€ K-meansë¥¼ ë‹¤ì‹œ ê³„ì‚°í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤(Online K-means).\n\nê·¸ë ‡ë‹¤ë©´, $\\boldsymbol{\\mu}(=\\{\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}\\})$ì™€ $\\bold{R}$(ëª¨ë“  dataì˜ $\\bold{r}$ë¡œ ì´ë£¨ì–´ì§„ Matrix)ì„ ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? ì´ì— ëŒ€í•œ í•´ë‹µì€ ë‹¤ìŒê³¼ ê°™ì€ Cost Functionì„ ì œì‹œí•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}||x_{i} - \\mu_{k}||^{2}\n$$\n\ní˜„ì¬ dataì˜ pointë¡œ ë¶€í„° ê°€ì¥ ê°€ê¹Œìš´ í‰ê· ì„ ì„ íƒí•˜ëŠ” ê²½ìš°ë¥¼ ìµœëŒ€í™”í•´ì•¼ í•´ë‹¹ ê°’ì´ ê°€ì¥ ì‘ì•„ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. \u003cmark\u003eì¦‰, ì—¬ê¸°ì„œëŠ” í‰ê· ê³¼ì˜ ê±°ë¦¬ë¥¼ ìœ ì‚¬ì„±ì˜ ì§€í‘œë¡œ ì‚¬ìš©í•œ ê²ƒì´ë‹¤.\u003c/mark\u003e ì—¬ê¸°ì„œëŠ” Euclidean distance(L2-norm)ë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, Manhatan distance(L1-norm)ì„ í™œìš©í•  ìˆ˜ë„ ìˆê³  ì•„ì˜ˆ ë‹¤ë¥¸ ì§€í‘œë¥¼ í™œìš©í•  ìˆ˜ë„ ìˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€ Cost Functionì´ ì•„ë˜ì™€ ê°™ì€ formì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\times(\\text{Similarity measure})\n$$\n\nê·¸ë ‡ë‹¤ë©´, ì‹¤ì œë¡œ ìœ„ì—ì„œ ì œì‹œí•œ Cost Functionì„ í™œìš©í•˜ì—¬ ì–´ë–»ê²Œ $\\boldsymbol{\\mu}$ì™€ $\\bold{R}$ì„ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? minimizeí•˜ê³ ìí•˜ëŠ” ìš”ì†Œê°€ ë‘ ê°œì´ê¸° ë•Œë¬¸ì— ë¯¸ë¶„ì„ í•˜ê¸°ë„ ë‹¤ì†Œ ë‚œí•´í•˜ë‹¤. ë”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” EM Algorithmì´ë¼ëŠ” ë°©ì‹ì„ ì œì‹œí•œë‹¤. ì´ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒ Postingì— ëŒ€í•´ì„œ ìì„¸íˆ ë‹¤ë£¨ê² ì§€ë§Œ, ê°„ë‹¨íˆ ì„¤ëª…í•˜ìë©´ í•˜ë‚˜ì˜ Variableì„ Randomí•˜ê²Œ ì§€ì •í•˜ê³ , ë‹¤ë¥¸ Variableì˜ ìµœì ê°’ì„ êµ¬í•œ í›„ ì´ë¥¼ ë‹¤ì‹œ ëŒ€ì…í•˜ê³  ë°˜ëŒ€ Variableì„ ìµœì ê°’ìœ¼ë¡œ êµ¬í•˜ê¸°ë¥¼ ë°˜ë³µí•˜ë©´ì„œ ë” ì´ìƒ Variableì´ ìœ ì˜ë¯¸í•˜ê²Œ ë³€ê²½ë˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ë°˜ë³µí•´ì„œ êµ¬í•œ ê°’ì´ ìµœì ê°’ê³¼ ê·¼ì‚¬í•œë‹¤ëŠ” ì ì„ í™œìš©í•œ Algorithmì´ë‹¤. ì§€ê¸ˆì€ ë‹¤ì†Œ ì—‰ëš±í•  ìˆ˜ ìˆì§€ë§Œ, ì§€ê¸ˆì€ í•´ë‹¹ ë°©ë²•ì„ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ë‹¤. ì¦ëª…ì´ ê¶ê¸ˆí•˜ë‹¤ë©´, í•´ë‹¹ Posting([ğŸ”— [ML] 10. EM Algorithm](/posts/ml-em-algorithm))ì„ ì°¸ê³ í•˜ì.\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ ìˆ˜í–‰í•  ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. $\\boldsymbol{\\mu}$ë¥¼ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•œë‹¤.\n2. Assignment step: $\\boldsymbol{\\mu}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $\\bold{R}$ì„ êµ¬í•œë‹¤.  \n   $$\n   R_{ik} = \\begin{cases}\n    1 \u0026 \\text{if}\\quad k = \\argmin_{k}||x_{i} - \\mu_{k}||^{2} \\\\\n    0 \u0026 \\text{otherwise}\n   \\end{cases}\n   $$\n3. Update step: $\\bold{R}$ì´ ì£¼ì–´ì¡Œì„ ë•Œ, $\\boldsymbol{\\mu}$ë¥¼ êµ¬í•œë‹¤.  \n   ìš°ë¦¬ê°€ ë¶„ë¥˜í•œ $R$ì„ í™œìš©í•˜ì—¬ ê° kì— ì†í•˜ëŠ” dataì˜ í‰ê· ì„ í†µí•´ì„œ $\\boldsymbol{\\mu}$ë¥¼ êµ¬í•œë‹¤.\n   $$\n   \\mu_{k} = \\frac{\\sum_{i=1}^{N}R_{ik}x_{i}}{\\sum_{i=1}^{N}R_{ik}}\n   $$\n4. íŠ¹ì •ê°’ìœ¼ë¡œ $\\boldsymbol{\\mu}$ê°€ ìˆ˜ë ´í•  ë•Œê¹Œì§€ 2ë²ˆ, 3ë²ˆ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.\n\nì•„ë˜ëŠ” ì´ ê³¼ì •ì„ ê·¸ë¦¼ì„ í†µí•´ì„œ í‘œí˜„í•œ ê²ƒì´ë‹¤.\n\n![ml-clustering-1](/images/ml-clustering-1.jpg)\n\nK-means ë°©ì‹ì€ ìœ„ì™€ ê°™ì€ Iteration ì ˆì°¨ë¥¼ ë§ì´ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ë„ ëª‡ë²ˆì˜ ìˆ˜í–‰ë§Œìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤ëŠ” ê²ƒì„ ê´€ì¸¡í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, Assignment ì‹œì—ëŠ” $O(KND)$ì˜ ì‹œê°„ì´ ì†Œëª¨ë˜ê³ , Update ì‹œì—ëŠ” $O(N)$ ë§Œí¼ì˜ ì‹œê°„ì´ ì†Œëª¨ë˜ê¸° ë•Œë¬¸ì— ë¬´ê²ì§€ ì•Šê³ , êµ‰ì¥íˆ ê°„ë‹¨í•˜ë‹¤ëŠ” ì¥ì ì„ ê°–ê³  ìˆë‹¤. í•˜ì§€ë§Œ, ì´ ë°©ë²•ì€ Global Optimalì„ ì°¾ì„ ê²ƒì´ë¼ëŠ” í™•ì‹ ì„ ì¤„ ìˆ˜ ì—†ë‹¤. ê·¸ë ‡ê¸°ì— ì´ˆê¸°ê°’ì„ ì–´ë–»ê²Œ ì¡ëŠëƒì— ë”°ë¼ì„œ ê²°ê³¼ê°€ í¬ê²Œ ë³€í•  ìˆ˜ë„ ìˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼, outlier dataì— ëŒ€í•´ì„œë„ êµ‰ì¥íˆ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ ì‚¬ì§„ì—ì„œ ì™¼ìª½ë³´ë‹¤ ì˜¤ë¥¸ìª½ì´ ë” ì„±ê³µì ì¸ Clusteringì´ë¼ê³  ë§í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n![ml-clustering-2](/images/ml-clustering-2.jpg)\n\nì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ë“¤ì´ ì œì‹œë˜ì—ˆë‹¤.\n\n1. **K-means++**: ì´ˆê¸°ê°’ì„ ì˜ ì„¤ì •í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ, ì´ˆê¸°ê°’ì„ ì˜ ì„¤ì •í•˜ë©´ ìˆ˜ë ´í•˜ëŠ” ì†ë„ê°€ ë¹¨ë¼ì§€ê³ , Global Optimalì— ìˆ˜ë ´í•  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§„ë‹¤.\n2. **K-mendoids**: K-meansì—ì„œëŠ” ì¤‘ì‹¬ì ì„ dataì˜ í‰ê· ìœ¼ë¡œ ì„¤ì •í–ˆì§€ë§Œ, K-mendoidsì—ì„œëŠ” ì¤‘ì‹¬ì ì„ dataì˜ ì¤‘ê°„ê°’ìœ¼ë¡œ ì„¤ì •í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´ outlierì— ë¯¼ê°í•˜ì§€ ì•Šê²Œ ëœë‹¤.\n\n\u003e \u003cmark\u003e**Soft K-means**\u003c/mark\u003e\n\në§ˆì§€ë§‰ìœ¼ë¡œ K-means Clusteringì—ì„œ í™•ë¥ ì ì¸ ì ‘ê·¼ì„ ì‹œë„í•œ ë°©ë²• ë˜í•œ ì†Œê°œí•˜ê² ë‹¤. ì• ì„œ ë³¸ (Hard)K-meansì—ì„œëŠ” $\\bold{R}_{ik}$ë¥¼ 0 ë˜ëŠ” 1ë¡œ ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ, ì´ë¥¼ í™•ë¥ ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì— ëŒ€í•´ì„œ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰, ë‹¤ìŒê³¼ ê°™ì´ soft-max functionì„ í™œìš©í•œë‹¤ë©´ í‘œí˜„ì´ ê°€ëŠ¥í•  ê²ƒì´ë‹¤.\n\n$$\n\\bold{R}_{ik} = \\frac{\\exp(-\\beta||x_{i}-\\mu_{k}||^{2})}{\\sum_{l \\in {1, 2, \\cdots, K}} \\exp(-\\beta||x_{i}-\\mu_{l}||^{2})}\n$$\n\nì´ë ‡ê²Œ í™•ë¥ ì ìœ¼ë¡œ í‘œí˜„í•˜ê²Œ ë˜ë©´, ìš°ë¦¬ëŠ” ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ íŠ¹ì • Clusterë¡œ í•´ë‹¹ í™•ë¥ ì´ í¸í–¥ë˜ì–´ ìˆì„ ìˆ˜ë¡ ë” ì¢‹ì€ ë¶„ë¥˜ì¼ ê²ƒì´ë¼ëŠ” ì‚¬ì „ ì§€ì‹(Prior)ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Cost Functionì„ ë³€ê²½í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = \\min_{\\boldsymbol{\\mu}}\\min_{\\bold{R}} \\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}||x_{i} - \\mu_{k}||^{2} - \\frac{1}{\\beta}\\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\log\\bold{R}_{ik}\n$$\n\në’· ë¶€ë¶„ì— ìƒˆë¡œ ì¶”ê°€ëœ $-\\frac{1}{\\beta}\\sum_{i \\in \\{1,2, \\cdots, N\\}}\\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\bold{R}_{ik}\\log\\bold{R}_{ik}$ëŠ” $R_{ik}$ê°€ í™•ë¥ ì´ ë˜ì—ˆê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ìƒ Entropyë¥¼ ì˜ë¯¸í•œë‹¤. EntropyëŠ” ê· í˜•ì¡íŒ ë¶„í¬ì¼ ìˆ˜ë¡ ì»¤ì§€ê³ , skewëœ ê²½ìš°ì—ëŠ” ì‘ì•„ì§€ê¸° ë•Œë¬¸ì— ì ì ˆí•œ ì§€í‘œë¼ê³  í•  ìˆ˜ ìˆë‹¤. $\\beta$ëŠ” ì´ëŸ¬í•œ priorë¥¼ ì–¼ë§ˆë‚˜ ì‚¬ìš©í• ì§€ì— ëŒ€í•œ hyperparameterì´ë‹¤. $\\beta$ê°€ í´ ìˆ˜ë¡ ì‚¬ì‹¤ìƒ Hard K-meansì™€ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ê²Œ ë˜ê³ , $\\beta$ê°€ ì‘ì„ ìˆ˜ë¡ Entropyë¥¼ ë” ì¤‘ìš”ì‹œí•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê²Œ ëœë‹¤.\n\n### Gaussian Mixture Model\n\nGaussian Mixture Model, ì¼ëª… GMMì€ Finite Mixture Modelì˜ ì¼ì¢…ì´ë‹¤. Finite Mixture Modelì€ ìš°ë¦¬ê°€ ì¶”ì •í•˜ê³ ì í•˜ëŠ” í™•ë¥  ë¶„í¬ê°€ ë‹¤ì–‘í•œ í™•ë¥  ë¶„í¬ ëª‡ ê°œì˜ ì¡°í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë¶„í¬ë¼ê³  ê°€ì •í•˜ê³ , í•´ë‹¹ í™•ë¥  ë¶„í¬ì˜ Parameterë¥¼ í•™ìŠµ(Learning) ë‹¨ê³„ì—ì„œ ì°¾ì•„ë‚´ê³ , ì´ë¥¼ ì´ìš©í•´ì„œ ìƒˆë¡œìš´ dataì— ëŒ€í•´ì„œ ì¶”ì •(Inference)í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n\n$$\np(x) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x, z=k) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x|z=k)p(z=k) = \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p_{k}(x)p(z=k)\n$$\n\nì—¬ê¸°ì„œ $z$ëŠ” ê´€ì¸¡í•  ìˆ˜ ì—†ëŠ” latent(hidden) variableë¡œ dataê°€ ëª‡ ë²ˆì§¸ í™•ë¥  ë¶„í¬ì— ì†í•  ê²ƒì¸ì§€ë¥¼ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ, $p(z=k)$ kë²ˆì§¸ ë¶„í¬ì— ì†í•  í™•ë¥ ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ëŒ€ê²Œ ì´ê²ƒì´ ì–´ëŠì •ë„ë¡œ í™•ë¥  ë¶„í¬ë¥¼ ì„ëŠ”ì§€ë¥¼ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì— mixing parameterë¼ê³ ë„ ë¶€ë¥¸ë‹¤.\n\nì´ì— ë”°ë¼ GMMì€ ê° $p_{k}(x)$ê°€ Gaussian Distributionì´ë¼ê³  ê°€ì •í•˜ëŠ” Finite Mixture Modelì¸ ê²ƒì´ë‹¤.\n\n![ml-gmm-graphical-form](/images/ml-gmm-graphical-form.jpg)\n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Graphical Model í˜•íƒœë¡œ Finite Mixture Modelì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ $\\pi,\\, \\mu,\\, \\Sigma$ëŠ” Parameterë¥¼ ì˜ë¯¸í•œë‹¤.\n\n- $\\pi_{k} = p(z = k)$\n- $\\mu_{k} = E[x|z=k]$ ì¦‰, Gaussianì˜ ê¸°ëŒ“ê°’ì„ ì˜ë¯¸í•œë‹¤.\n- $\\Sigma_{k} = Cov[x|z=k]$ ì¦‰, Gaussianì˜ ë¶„ì‚°ì„ ì˜ë¯¸í•œë‹¤.\n\nì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” ìœ„ì—ì„œ ì œì‹œí•œ í™•ë¥ ì„ ë‹¤ìŒê³¼ ê°™ì´ ì¬ì •ì˜í•  ìˆ˜ ìˆë‹¤. (Joint Probabilityë¥¼ Bayesian Networkë¡œ í‘¼ ì‹ì´ë‹¤. ëª¨ë¥´ê² ë‹¤ë©´, [ğŸ”— [ML] 8. Graphical Model](/posts/ml-graphical-model#Graphical-Model)ì—ì„œ Bayesian Networkë¥¼ ë‹¤ì‹œ ì‚´í´ë³´ê³  ì˜¤ì.)\n\n$$\n\\begin{align*}\np(x) \u0026= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(x, z=k) \\\\\n\u0026= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}p(z=k| \\pi_{k})p(x|z=k, \\mu_{k}, \\Sigma_{k}) \\\\\n\u0026= \\sum_{k \\in \\{1, 2, \\cdots, K\\}}\\pi_{k}\\mathcal{N}(x|\\mu_{k}, \\Sigma_{k})\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì‹¤ì œë¡œ ì¶”ì¸¡(Inference)ì„ í•  ë•Œì—ëŠ” $p(z|x)$ê°€ í•„ìš”í•˜ë‹¤. ì´ëŠ” ìš°ë¦¬ê°€ posteriorë¥¼ í™œìš©í•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\hat{k} \u0026=\\argmax_{k}p(z=k|x) \\\\\n\u0026= \\argmax_{k}\\frac{p(x|z=k)p(z=k)}{p(x)} \\\\\n\u0026= \\argmax_{k}p(x|z=k)p(z=k)\\\\\n\u0026= \\argmax_{k}\\pi_{k}\\mathcal{N}(x|\\mu_{k}, \\Sigma_{k})\n\\end{align*}\n$$\n\ní•™ìŠµ(Learning)ì„ í•  ë•Œì—ëŠ” ê²°êµ­ $\\pi,\\, \\mu,\\, \\Sigma$ ì´ ì„¸ ê°œì˜ parameter ê°’ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì´ê²ƒì€ ìš°ë¦¬ê°€ Parametric Estimationì—ì„œ ì¤„ê¸°ì°¨ê²Œ í–ˆë˜ MLEë¥¼ ì´ìš©í•˜ë©´ ëœë‹¤. ì´ë¥¼ ìœ„í•œ LikelihoodëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\pi,\\, \\mu,\\, \\Sigma) \u0026= \\log{p(\\mathcal{D} | \\pi,\\, \\mu,\\, \\Sigma)} \\\\\n\u0026= \\log{\\prod_{i=1}^{N}{p(x_{i} | \\pi,\\, \\mu,\\, \\Sigma)}} \\\\\n\u0026= \\sum_{i=1}^{N}{\\log{p(x_{i}| \\pi,\\, \\mu,\\, \\Sigma)}} \\\\\n\u0026= \\sum_{i=1}^{N}{\\log{\\sum_{k=1}^{K}{\\pi_{k}\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma_{k})}}}\n\\end{align*}\n$$\n\ní•˜ì§€ë§Œ, ì´ê²ƒì„ ë‹¨ìˆœí•œ Optimization Techniqueìœ¼ë¡œëŠ” í’€ ìˆ˜ ì—†ë‹¤. ì™œëƒí•˜ë©´, ë‹¨ìˆœí•œ ë¯¸ë¶„ìœ¼ë¡œ ê° parameterë¥¼ êµ¬í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ, EM Algorithmì„ ì´ìš©í•´ì„œ í’€ì–´ì•¼ í•œë‹¤. (ì´ê²ƒì€ [ğŸ”— [ML] 10. EM Algorithm](/posts/ml-em-algorithm)ì—ì„œ ë‹¤ë£¬ë‹¤.)\n\në”°ë¼ì„œ, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì„ì˜ì˜ $\\pi,\\, \\mu,\\, \\Sigma$ë¥¼ ê°€ì •í•œ ìƒíƒœì—ì„œ dataì— ì•Œë§ëŠ” ìµœì ì˜ Cluster setì„ êµ¬í•˜ê³ , dataì— clusterê°€ labelëœ ìƒíƒœì—ì„œ ìµœì ì˜ $\\pi,\\, \\mu,\\, \\Sigma$ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ëŠ” ê²ƒì´ë‹¤.\n\n![ml-gmm-1](/images/ml-gmm-1.jpg)\n\nê·¸ë ‡ë‹¤ë©´, ì´ë¥¼ ì‹¤ì œë¡œ ì–´ë–»ê²Œ í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤. í•˜ì§€ë§Œ, ê·¸ëƒ¥ ëª¨ë“  Gaussian í˜•íƒœë¥¼ ìœ„í•œ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ë‹¤ì†Œ ì‹ì´ ë³µì¡í•´ì§€ê¸° ë•Œë¬¸ì— isotropic Gaussian(ëª¨ë“  ë°©í–¥ì—ì„œ ë¶„ì‚°ì´ ë™ì¼í•œ Gaussian)ì„ ê°€ì •ìœ¼ë¡œ í•˜ê² ë‹¤.\n\në˜í•œ, ë‹¤ìŒê³¼ ê°™ì€ ìš”ì†Œë¥¼ ì¶”ê°€ë¡œ ì •ì˜í•˜ì.\n\n1. $z_{i} \\in \\{1, 2, \\cdots, K\\}$ : $i$ë²ˆì§¸ dataê°€ ì†í•˜ëŠ” clusterì˜ index  \n   $z_{ik} = \\begin{cases} 1 \u0026 \\text{if } z_{i} = k \\\\ 0 \u0026 \\text{otherwise} \\end{cases}$\n2. $\\theta_{k} = (\\pi_{k},\\, \\mu_{k},\\, \\Sigma_{k})$ : $k$ë²ˆì§¸ clusterë¥¼ ìœ„í•œ parameterì˜ ì§‘í•©  \n   $\\theta = (\\pi,\\, \\mu,\\, \\Sigma)$ : parameterì˜ ì§‘í•©  \n\nì• ì„œ ë§í•œ ë°”ì™€ ê°™ì´ ì´ì œ ìš°ë¦¬ëŠ” $\\theta$ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ $z_{i}$ì— í•´ë‹¹í•˜ëŠ” ì •ë³´ë„ ì•Œê³  ìˆë‹¤. ë”°ë¼ì„œ, Likelihood ì‹ë„ ë³€í˜•ë˜ì–´ì•¼ í•œë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta) \u0026= \\log{p(\\mathcal{D} | \\theta)} \\\\\n\u0026\\geq \\log{p(X, Z | \\theta)} \\\\\n\u0026= \\sum_{i=1}^{N}{\\log{p(x_{i}, z_{i}| \\theta)}} \\\\\n\u0026= \\sum_{i=1}^{N}{\\log{p(z_{i} | \\theta) \\times p(x_{i}| z_{i}, \\theta)}} \\\\\n\u0026= \\sum_{i=1}^{N}{\\log{(\\prod_{k=1}^{K}{\\pi_{k}^{z_{ik}}} \\times \\prod_{k=1}^{K}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)^{z_{ik}}})}} \\\\\n\u0026= \\sum_{i=1}^{N}{\\sum_{k=1}^{K}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})^{z_{ik}}}} \\\\\n\u0026= \\sum_{i=1}^{N}{\\sum_{k=1}^{K}z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}} \\\\\n\\end{align*}\n$$\n\nì´ì— ë”°ë¼ì„œ ìš°ë¦¬ëŠ” EM Algorithmì˜ $\\mathcal{Q}$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{Q}(\\theta; \\theta^{\\prime}) \u0026= \\sum_{i=1}^{N}E_{z_{i}|x_{i}, \\theta^{\\prime}}[\\log p(x_{i}, z_{i} | \\theta)] \\\\\n\u0026= \\sum_{i=1}^{N}E_{z_{i}|x_{i}, \\theta^{\\prime}}[\\sum_{k=1}^{K}z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}] (\\because \\text{ìœ„ì˜ ì‹ì—ì„œ 3ë²ˆì§¸ ì¤„ì„ ì°¸ê³ })\\\\\n\u0026= \\sum_{i=1}^{N}\\sum_{k=1}^{K}E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}] \\\\\n\u0026= \\sum_{i=1}^{N}\\sum_{k=1}^{K}E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})}] \\\\\n\u0026= \\sum_{i=1}^{N}\\sum_{k=1}^{K}E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}]\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})} \\\\\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê° stepì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n- **E-step**  \n  $\\mathcal{Q}$ì—ì„œ parameter($\\pi,\\, \\mu,\\, \\Sigma$)ë¥¼ ì œì™¸í•˜ê³ , ì•„ì§ ë¯¸ì§€ìˆ˜ë¡œ ë‚¨ì•„ìˆëŠ” ê°’ì€ $E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}]$ì´ë‹¤. ì¦‰, ì´ ê°’ë§Œ êµ¬í•˜ë©´ $\\mathcal{Q}$ë¥¼ êµ¬í–ˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.  \n  $$\n  \\begin{align*}\n  E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}] \u0026= \\sum_{k=1}^{K}{z_{ik}p(z_{i} = k | x_{i}, \\theta^{\\prime})} \\\\\n  \u0026= p(z_{i} = k^{*} | x_{i}, \\theta^{\\prime}) = r_{ik^{*}}\n  \\end{align*}\n  $$  \n  ê²°êµ­ ìš°ë¦¬ê°€ í•´ë‹¹ ë‹¨ê³„ì—ì„œ êµ¬í•  ê²ƒì€ ê´€ì¸¡ ê°€ëŠ¥í•œ dataì™€ ì´ì „ parameterê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì†í•˜ê²Œ ë˜ëŠ” clusterì—ì„œì˜ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì„ ëª¨ë“  dataì— ëŒ€í•´ì„œ êµ¬í•˜ë©´, $\\mathcal{Q}$ì—ì„œ parameterë¥¼ ì œì™¸í•œ ëª¨ë“  ë¶€ë¶„ì„ êµ¬í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì‹ì„ ì¢€ ë” ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ë¡ ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n  $$\n  \\begin{align*}\n  E_{z_{i}|x_{i}, \\theta^{\\prime}}[z_{ik}] = r_{ik^{*}} \u0026= \\frac{p(x_{i}, z_{i}=k^{*} | \\theta^{\\prime})}{p(x_{i}|\\theta^{\\prime})} \\\\\n  \u0026= \\frac{\\pi_{k^{*}}^{\\prime}{\\mathcal{N}(x_{i}|\\mu_{k^{*}}^{\\prime}, \\Sigma_{k^{*}}^{\\prime} I)}}{\\sum_{l=1}^{K}{\\pi_{l}{\\mathcal{N}(x_{i}|z_{i} = l, \\mu_{l}, \\Sigma_{l} I)}}}\n  \\end{align*}\n  $$  \n  \n- **M-step**  \n  ê²°ë¡ ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ $\\mathcal{Q}$ì™€ constraintë¥¼ ì–»ì—ˆë‹¤.  \n  $$\n  \\begin{align*}\n  \\text{maximize}\u0026\\quad \\mathcal{Q}(\\theta; \\theta^{\\prime}) = \\sum_{i=1}^{N}\\sum_{k=1}^{K}r_{ik}\\log{({\\pi_{k}}{\\mathcal{N}(x_{i}|\\mu_{k}, \\Sigma I)})} \\\\\n  \\text{subject to}\u0026\\quad \\sum_{k=1}^{K}{\\pi_{k}} = 1\n  \\end{align*}\n  $$  \n  ì´ì œ ìš°ë¦¬ëŠ” ì´ë¥¼ Optimization ë°©ì‹ì„ í™œìš©í•˜ì—¬ í’€ê¸°ë§Œ í•˜ë©´ ëì´ë‹¤. ([ğŸ”— ì°¸ê³ (Base Knowledge)](/posts/ml-base-knowledge))  \n  $$\n  \\begin{align*}\n  \\mu_{k} \u0026= \\frac{\\sum_{i=1}^{N}{r_{ik}x_{i}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\Sigma_{k} \u0026= \\frac{\\sum_{i=1}^{N}{r_{ik}||x_{i} - \\mu_{k}||^{2}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\pi_{k} \u0026= \\frac{1}{N}\\sum_{i=1}^{N}{r_{ik}}\n  \\end{align*}\n  $$\n\n---\n\në§ˆì§€ë§‰ìœ¼ë¡œ ì§šê³  ë„˜ì–´ê°ˆ ê²ƒì€, ë°”ë¡œ K-means Clusteringì€ ì‚¬ì‹¤ GMMì˜ í•˜ë‚˜ì˜ special caseë¼ëŠ” ê²ƒì´ë‹¤. ë§Œì•½, ìš°ë¦¬ê°€ $\\pi_{k},\\, \\Sigma_{k}$ë¥¼ ëª¨ë‘ ê°™ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ë©´, $\\pi_{k} = \\frac{1}{K}$ì´ê³  $\\Sigma_{k} = \\Sigma$ê°€ ëœë‹¤ê³  í•˜ì. ì´ë•Œ EM algorithmì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n- **E-step**  \n  $$\n  r_{ik} = \\begin{cases} 1 \u0026 k = \\argmax_{l\\in \\{1, 2, \\cdots, K\\}} p(x_{i}|z_{i} = l, \\mu_{l}, \\Sigma) \\\\ 0 \u0026 \\text{otherwise} \\end{cases}\n  $$  \n  ì´ëŠ” ì‚¬ì‹¤ìƒ K-means Clusteringì—ì„œ ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬ë¥¼ í†µí•´ì„œ êµ¬í–ˆë˜ ê²ƒê³¼ ë§¤ìš° ìœ ì‚¬í•œ ì‹ì´ë‹¤.\n- **M-step**  \n  $$\n  \\begin{align*}\n  \\mu_{k} \u0026= \\frac{\\sum_{i=1}^{N}{r_{ik}x_{i}}}{\\sum_{i=1}^{N}{r_{ik}}} \\\\\n  \\pi_{k} \u0026= \\frac{1}{N}\\sum_{i=1}^{N}{r_{ik}}\n  \\end{align*}\n  $$  \n  $\\pi_{k}$ê°€ ì¶”ê°€ë˜ê¸°ëŠ” í–ˆì§€ë§Œ, $\\mu_{k}$ë¥¼ êµ¬í•˜ëŠ” ì‹ì€ ì™„ì „ ë™ì¼í•˜ë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"ml-clustering","date":"2022-11-23 09:19","title":"[ML] 9. Clustering","category":"AI","tags":["ML","UnsupervisedLearning","Clustering","K-means","GMM"],"desc":"ì´ì „ê¹Œì§€ì˜ Postingì—ì„œëŠ” Supervised Learning ì¦‰, ì´ë¯¸ Labelingì´ ì™„ë£Œëœ ë°ì´í„°ì— ì˜í•œ Learningì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ì—ˆë‹¤. ì§€ê¸ˆë¶€í„°ëŠ” Unsupervised Learningì— ëŒ€í•´ì„œ ì¡°ê¸ˆ ë” ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤. ëŒ€í‘œì ì¸ Unsupervised Learningì€ Clustering, Feature Selection(or Dimensionality Reduction), Generative Model ë“±ì´ ì¡´ì¬í•œë‹¤. ì´ë“¤ì— ëŒ€í•´ì„œ ì°¨ê·¼ì°¨ê·¼ ì‚´í´ë³´ë„ë¡ í•˜ê³ , í•´ë‹¹ Postingì—ì„œëŠ” ê°€ì¥ ëŒ€í‘œì ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” Clusteringì„ ë¨¼ì € ì‚´í´ë³´ë©´ì„œ Unsupervised Learningì— ëŒ€í•œ ê³„ëµì ì¸ ì´í•´ë¥¼ í•´ë³´ë„ë¡ í•˜ê² ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"Clustering"}},"__N_SSG":true},"page":"/tags/[subject]","query":{"subject":"Clustering"},"buildId":"pI-qeh2DPTobWYk-Pbn2o","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>