<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Network ë¶„ì•¼ì— ê´€ì‹¬ì´ ë§ì€ ê°œë°œìë¡œ Computer Engineering ê´€ë ¨ Postingì„ ì£¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤."/><meta property="og:description" content="Network ë¶„ì•¼ì— ê´€ì‹¬ì´ ë§ì€ ê°œë°œìë¡œ Computer Engineering ê´€ë ¨ Postingì„ ì£¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤."/><meta property="og:type" content="blog"/><meta property="og:site_name" content="JustLog"/><meta property="og:image" content="https://euidong.github.io/logo192.png"/><title>#NewtonMethod | JustLog</title><meta property="og:title" content="#NewtonMethod | JustLog"/><link rel="canonical" href="https://euidong.github.io/tags/NewtonMethod"/><meta property="og:url" content="https://euidong.github.io/tags/NewtonMethod"/><meta name="next-head-count" content="11"/><link rel="icon" href="https://euidong.github.io/favicon.png"/><link rel="apple-touch-icon" href="https://euidong.github.io/logo192.png"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7452732177557701" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/css/d4ec5c8b3df09443.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d4ec5c8b3df09443.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6dc16d084a5153e5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6dc16d084a5153e5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-81da43a8dcd978d9.js" defer=""></script><script src="/_next/static/chunks/main-7b6c38cbad60dfcf.js" defer=""></script><script src="/_next/static/chunks/pages/_app-16ba73d9a19266be.js" defer=""></script><script src="/_next/static/chunks/675-ae8e8a351ce30ae2.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Bsubject%5D-2ecacdd7ae5454e8.js" defer=""></script><script src="/_next/static/eGkZYbzPXM35ApK3V1pre/_buildManifest.js" defer=""></script><script src="/_next/static/eGkZYbzPXM35ApK3V1pre/_ssgManifest.js" defer=""></script><script src="/_next/static/eGkZYbzPXM35ApK3V1pre/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_wrapper__dKJSz root"><header class="Layout_header__XosLl" style="position:sticky"><div><button tabindex="1" class="SideBarToggler_search_bar_toggler__CEuUg"><svg stroke="currentColor" fill="none" stroke-width="0" viewBox="0 0 24 24" height="35px" width="35px" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></button><nav class="SideBar_side_bar__wrapper--close__8Nwnr"><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/">Home</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/tags">Tags</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Algorithm">Algorithm<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Computer%20Architecture">Computer Architecture<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->7<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Tech">Tech<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->17<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Web">Web<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Network">Network<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/AI">AI<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->8<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Paper">Paper<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->1<!-- -->)<!-- --></span></a></nav></div><a class="Logo_logo___yD0t" tabindex="1" href="/"></a><div><button class="SearchBarToggler_search_bar_toggler__3dHbA" tabindex="2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg></button></div></header><section><div class="RowCard_row_card__list__background___xFj5"><h1 class="RowCard_row_card__list__title__t4a2h"> Newton Method</h1><label class="RowCard_row_card__list__select__wrapper__TZ4_9"><select class="RowCard_row_card__list__select__dxkxA"><option class="RowCard_row_card__list__select__option__GRKZU">ìµœì‹ ìˆœ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">AtoZ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">ZtoA<!-- --></option></select></label><ul class="RowCard_row_card__list__wrapper__5Gtgi"><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-logistic-regression"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 3. Logistic Regression" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 3. Logistic Regression" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-logistic-regression">[ML] 3. Logistic Regression</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 10ì›” 18ì¼ 09ì‹œ 58ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/LogisticRegression"># <!-- -->LogisticRegression<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Classification"># <!-- -->Classification<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/SigmoidFunction"># <!-- -->SigmoidFunction<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/SoftmaxFunction"># <!-- -->SoftmaxFunction<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NewtonMethod"># <!-- -->NewtonMethod<!-- --></a></ul></div></div></ul></div></section><footer class="Layout_footer__EL5v8"><div class="Layout_footer__copyright__r5baC"><span>Copyright Â© euidong</span><br/><span>ëª¨ë“  ì»¨í…ì¸ ì— ëŒ€í•œ ì €ì‘ê¶Œì€ ì‘ì„±ìì—ê²Œ ì¡´ì¬í•©ë‹ˆë‹¤. <!-- --><br/>ë¶ˆë²• ë³µì œë¥¼ í†µí•œ ìƒì—…ì  ì‚¬ìš©ì„ ì ˆëŒ€ì ìœ¼ë¡œ ê¸ˆì§€í•©ë‹ˆë‹¤. <!-- --><br/>ë‹¨, ë¹„ìƒì—…ì  ì´ìš©ì˜ ê²½ìš° ì¶œì²˜ ë° ë§í¬ë¥¼ ì ìš©í•œë‹¤ë©´ ììœ ë¡­ê²Œ ì‚¬ìš©ê°€ëŠ¥ í•©ë‹ˆë‹¤.<!-- --></span><span>Also I use photos by<!-- --> <!-- --><a href="https://unsplash.com/@lorenzoherrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Lorenzo Herrera</a> <!-- -->on<!-- --> <!-- --><a href="https://unsplash.com/s/photos/tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Unsplash</a></span></div><div class="Layout_footer__contents__YZWSm"><a class="Layout_footer__contents__link__K_TKH" href="https://github.com/euidong" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>github</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://euidong.github.io/portfolio" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M6 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm-5 6s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H1zM11 3.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 0 1h-4a.5.5 0 0 1-.5-.5zm.5 2.5a.5.5 0 0 0 0 1h4a.5.5 0 0 0 0-1h-4zm2 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2zm0 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2z"></path></svg><span>portfolio</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://chrome.google.com/webstore/detail/bonfire/nkooidijgbppkojdgkoafcoppnohdfka?hl=ko" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 16c-1.066-2.219-0.498-3.49 0.321-4.688 0.897-1.312 1.129-2.61 1.129-2.61s0.706 0.917 0.423 2.352c1.246-1.387 1.482-3.598 1.293-4.445 2.817 1.969 4.021 6.232 2.399 9.392 8.631-4.883 2.147-12.19 1.018-13.013 0.376 0.823 0.448 2.216-0.313 2.893-1.287-4.879-4.468-5.879-4.468-5.879 0.376 2.516-1.364 5.268-3.042 7.324-0.059-1.003-0.122-1.696-0.649-2.656-0.118 1.823-1.511 3.309-1.889 5.135-0.511 2.473 0.383 4.284 3.777 6.197z"></path></svg><span>chat</span></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"content":"\n## Intro\n\nì´ì „ê¹Œì§€ ìš°ë¦¬ëŠ” input dataê°€ ë“¤ì–´ì™”ì„ ë•Œ, continuosí•œ outputì„ ì–»ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ” ëŒ€ê²Œ ì •í™•í•œ ìˆ˜ì¹˜ë³´ë‹¤ëŠ” íŠ¹ì • ë¶„ë¥˜ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ íš¨ê³¼ì ì¸ ê²½ìš°ê°€ ë§ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, spam í•„í„°ë§, object detection, ë“± ë“±. ë”°ë¼ì„œ, í•´ë‹¹ í¬ìŠ¤íŒ…ì—ì„œëŠ” classificationì„ ìœ„í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” logistic regressionì— ëŒ€í•´ì„œ ì‚´í´ë³¼ ê²ƒì´ë‹¤.\n\n## Classification\n\n**Classification**ì´ë€ ê²°êµ­ íŠ¹ì • inputì´ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ í•˜ë‚˜ì˜ Classë¼ëŠ” outputì„ ë‚´ë³´ë‚´ëŠ” ê²ƒì´ë‹¤. ì¦‰, outputì€ ì—°ì†ì ì´ì§€ ì•Šê³ , descretí•˜ë‹¤. ëŒ€ê²Œ Classificationì—ì„œëŠ” Classì˜ ê°¯ìˆ˜ë¥¼ Kë¼ê³  í‘œê¸°í•˜ê³ , $C_k$ëŠ” k ë²ˆì§¸ Classë¼ëŠ” ì˜ë¯¸ë¡œ ì‚¬ìš©ë˜ì–´ì§„ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´, ì–´ë–»ê²Œ Classë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ê²ƒì¼ê¹Œ? ë§¤ìš° ë‹¨ìˆœí•˜ê²Œë„ ì´ëŠ” **Decision Boundary**ë¼ëŠ” ì„ ì„ ê·¸ì–´ì„œ í•´ê²° í•  ìˆ˜ ìˆë‹¤.\n\n![decision-boundary](/images/decision-boundary.jpg)\n\nìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼ ìš°ë¦¬ëŠ” ì„ ì„ í•˜ë‚˜ ê·¸ì–´ì„œ $\\red{\\text{x}}$ì™€ $\\blue{\\text{o}}$ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” Class 1ì— í•´ë‹¹í•  ê²ƒì´ë¼ê³  ì˜ˆì¸¡í•˜ëŠ” êµ¬ê°„ $R_1$ì´ ë§Œë“¤ì–´ì§€ê³ , Class 2ë¼ê³  ì˜ˆì¸¡í•˜ëŠ” êµ¬ê°„ $R_2$ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.\n\nì¦‰, classificationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ í•´ì•¼í•  ì¼ì€ ê¸°ì¡´ì˜ Regression ê³¼ì •ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\nê²°êµ­ ì°¾ê³ ì í•˜ëŠ” ê²ƒì´ ì„ ì´ë¼ë©´, ì´ê²ƒì„ Linear Regressionìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ Linear Regressionì„ ë°”ê¿”ì„œ ìƒê°í•  ìˆ˜ ìˆë‹¤.\n\n- ì˜ˆì¸¡ê°’($\\hat{y}$, $h(\\bold{x})$)  \n  $h(\\bold{x}) = \\text{sign}(\\bold{w}^{\\top}\\bold{x}) = \\begin{cases} +1 \u0026 \\bold{w}^{\\top}\\bold{x} \\geq 0 \\\\ -1 \u0026 \\text{otherwise}\\end{cases}$\n- Least Squared Error(LS, MLE)  \n  ì‹¤ì œë¡œ parameterë¥¼ êµ¬í•  ë•Œì—ëŠ” signì„ ì·¨í•˜ì§€ ì•ŠëŠ”ë°, signì„ ì·¨í•˜ê²Œ ë˜ë©´ ëª¨ë‘ LSëŠ” ê²°êµ­ ì˜¤ë‹µì˜ ê°¯ìˆ˜ ì •ë„ë¡œ ì·¨ê¸‰ëœë‹¤. ì¦‰, ì–¼ë§ˆë‚˜ ì˜ˆì¸¡ì´ ì˜ëª»ë˜ì—ˆëŠ”ì§€ë¥¼ ë°˜ì˜í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì´ëŠ” ê¸°ì¡´ Linear Regressionì˜ LSë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ ìˆ˜í–‰í•œë‹¤.  \n  $\\argmin_{w} {1\\over2}\\sum_{n=1}^{N}{(y_n - (\\bold{w}^{\\top}\\bold{x}))^2}$\n\nì´ë ‡ê²Œ Linear Regressionì„ ì ìš©í•˜ë©´ ë¬¸ì œê°€ ì—†ì„ ê±° ê°™ë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë¬¸ì œê°€ ìˆë‹¤. ë°”ë¡œ, ë°ì´í„°ê°€ ë¶ˆê· í˜•í•  ë•Œì´ë‹¤. ë§Œì•½ ë°ì´í„°ê°€ decision boundaryë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ì¹­(symmetric)ì¸ í˜•íƒœë¡œ ì¡´ì¬í•œë‹¤ë©´, ë¬¸ì œê°€ ì—†ë‹¤. í•˜ì§€ë§Œ, ë¹„ëŒ€ì¹­(asymmetric)ì¸ ê²½ìš° ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤. ì™œëƒí•˜ë©´, linear regressionì€ ìµœì ì—ì„œ ë°ì´í„°ì˜ í‰ê· ì„ ë°˜ì˜í•˜ëŠ”ë° ë¶ˆê· í˜•í•œ ê²½ìš° ë°ì´í„°ì˜ í‰ê· ì´ Decision Boundaryê°€ ë˜ëŠ” ê²ƒì€ ë¬¸ì œê°€ ìˆë‹¤.\n\n![linear-in-classification](/images/linear-in-classification.jpg)\n\n## Logistic Regression\n\nìœ„ì—ì„œ ì œì‹œí•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ Classificationì—ì„œëŠ” Linear Regressionì´ ì•„ë‹Œ Logistic Regressionì„ í™œìš©í•œë‹¤. ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œ ê¸°ë°˜ì´ ë  ìš”ì†Œë“¤ì„ ë¨¼ì € ì‚´í´ë³´ì.\n\n\u003e **Discriminant Function**\n\níŒë³„í•¨ìˆ˜(Discriminant Function, Score Function) ë“±ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” í•´ë‹¹ í•¨ìˆ˜ëŠ” íŠ¹ì • dataê°€ íŠ¹ì • classì— ì†í•  ê°€ëŠ¥ì„±(likelihood, probability, score)ì„ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ì´ë‹¤. ì¦‰, inputìœ¼ë¡œ dataë¥¼ ë°›ê³ , outputìœ¼ë¡œ classì— ì†í•  í™•ë¥ ì„ ë‚´ë³´ë‚¸ë‹¤.\n\nì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ í•  ìˆ˜ ìˆë‹¤.\n\në§Œì•½, $f_k(\\bold{x}) \\gt f_j(\\bold{x})$ì´ë¼ë©´, $\\bold{x}$ì˜ classëŠ” $C_k$ì´ë‹¤.\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ Classê°€ ìˆëŠ” ê³µê°„ì—ì„œ dataë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.\n\n$$\nh(\\bold{x}) = \\argmax_{k}f_{k}(\\bold{x})\n$$\n\nê·¸ë ‡ë‹¤ë©´, Discriminant Functionìœ¼ë¡œ ì–´ë–¤ ê°’ì„ ì“°ë©´ ì¢‹ì„ê¹Œ? ì´ì— ëŒ€í•œ í•´ê²°ì±…ì„ Bayes Decision Ruleì—ì„œ ì œì‹œí•œë‹¤.\n\n\u003e **Bayes Decision Rule**\n\në§Œì•½ ìš°ë¦¬ê°€ íŠ¹ì • dataê°€ íŠ¹ì • Classì— ì†í•  í™•ë¥ ì„ êµ¬í•œë‹¤ê³  í•˜ì. ìš°ë¦¬ëŠ” ë¨¼ì € Likelihoodë¥¼ ìƒê°í•  ìˆ˜ ìˆë‹¤. $P(x|C = k), P(x|C = j)$ë¥¼ êµ¬í•˜ì—¬ ê° Classì— ì†í•  í™•ë¥ ì„ ë¹„êµí•  ìˆ˜ ìˆì„ê¹Œ?  \në¬¼ë¡  ë¹„êµëŠ” ê°€ëŠ¥í•˜ë‹¤ í•˜ì§€ë§Œ, ë°˜ìª½ì§œë¦¬ ë¹„êµë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë§Œì•½, class kì— ì†í•˜ëŠ” ë°ì´í„°ë³´ë‹¤ class jì— ì†í•˜ëŠ” ë°ì´í„°ê°€ í›¨ì”¬ ë§ë‹¤ê³  í•˜ì. ê·¸ëŸ¬ë©´, ì¼ë°˜ì ìœ¼ë¡œ class jê°€ ë°œìƒí•  í™•ë¥  ìì²´ê°€ ë†’ë‹¤. í•˜ì§€ë§Œ, likelihoodëŠ” ì´ëŸ¬í•œ ê²½í–¥ì„ ë°˜ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤. ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ì.\n\n```plaintext\n ğŸ¤” ì–´ë–¤ ë™ë¬¼ì˜ í„¸ì— ì¡´ì¬í•˜ëŠ” ìƒ‰ì˜ ê°¯ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê³ ì–‘ì´ì¼ í™•ë¥ ê³¼ í˜¸ë‘ì´ì¼ í™•ë¥ ì´ë¼ê³  í•˜ì.\n\n  ê·¸ë¦¬ê³ , input dataëŠ” í„¸ì— ì¡´ì¬í•˜ëŠ” ìƒ‰ì˜ ìˆ˜ë¼ê³  í•˜ì. (í˜¸ë‘ì´ëŠ” ëŒ€ê²Œ 3ê°€ì§€ ìƒ‰, ë°±í˜¸ = 2ê°€ì§€ ìƒ‰, ê³ ì–‘ì´ëŠ” ë§¤ìš° ë‹¤ì–‘)\n  ê·¸ë ‡ë‹¤ë©´, P(í„¸ì˜ ìƒ‰ = 3|C = í˜¸ë‘ì´), P(í„¸ì˜ ìƒ‰ = 3|C = ê³ ì–‘ì´)ë¥¼ ë¹„êµí–ˆì„ ë•Œ, ìš°ë¦¬ëŠ” ë‹¹ì—°íˆ ì „ìê°€ í¬ë‹¤ê³  ìƒê°í•  ê²ƒì´ë‹¤.\n  í•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ê³ ë ¤í•˜ì§€ ì•Šì€ ê²ƒì´ ìˆë‹¤. ë°”ë¡œ ì „ì²´ ê³ ì–‘ì´ì™€ í˜¸ë‘ì´ì˜ ë¹„ìœ¨ì´ë‹¤. \n  ìƒëŒ€ì ìœ¼ë¡œ ê³ ì–‘ì´ê°€ í˜¸ë‘ì´ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ë‹¤ëŠ” ê²ƒì„ ê³ ë ¤í–ˆì„ ë•Œ, ê³ ì–‘ì´ì˜ í™•ë¥ ì´ ë” ë†’ì„ ìˆ˜ë„ ìˆë‹¤. \n\n  ì¦‰, ì–´ë–¤ ë™ë¬¼ì˜ í„¸ì— ì¡´ì¬í•˜ëŠ” ìƒ‰ì˜ ê°¯ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê³ ì–‘ì´ì¼ í™•ë¥ ì€ \n  P(C=ê³ ì–‘ì´|í„¸ì˜ ìƒ‰=3) =  P(í„¸ì˜ ìƒ‰ = 3|C = ê³ ì–‘ì´)P(C=ê³ ì–‘ì´)ì´ë‹¤. (ë¶„ëª¨ëŠ” ìƒëµí•¨.)\n```\n\nì¦‰, Bayes Ruleì— ê¸°ë°˜í•˜ì—¬ ìš°ë¦¬ê°€ ì›í•˜ëŠ” outputì€ Posteriorë¼ëŠ” ê²ƒì„ ëª…í™•íˆ ì•Œ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\np(C_{k}|\\bold{x}) \u0026= {{p(\\bold{x}| C_{k}) p(C_{k})}\\over{\\sum_{j=1}^{K}{p(\\bold{x}|C_{j})p(C_{j})}}} \\\\\n\u0026\\propto p(\\bold{x}| C_{k}) p(C_{k})\n\\end{align*}\n$$\n\nìœ„ì˜ ê²½ìš° Classê°„ì˜ ìƒëŒ€ ë¹„êµì— ì‚¬ìš©í•˜ëŠ” ì§€í‘œë¡œ ì´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ë¶„ëª¨(Normalization Factor, í™•ë¥ ì˜ ì´í•©ì´ 1ì´ ë˜ë„ë¡ í•˜ëŠ” ì—­í• )ë¥¼ ì œì™¸í•˜ì—¬ë„ ìƒê´€ì—†ê¸°ì— ëŒ€ê²Œ ë³µì¡í•œ ë¶„ëª¨ ê³„ì‚°ì„ ì œì™¸í•˜ê³  í‘œí˜„í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\n\në˜í•œ, ì•ì„  ì˜ˆì‹œì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” insightëŠ” í¸í–¥ëœ ë°ì´í„°ì¼ìˆ˜ë¡ MLEë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ìœ„ì—ì„œ Linear Regressionì´ Classificationì— ë¶€ì í•¨í•œ ê²½ìš°ë„ ë°ì´í„°ì˜ í¸í–¥ì´ ìˆì„ ê²½ìš°ì´ë‹¤. ì´ ì—­ì‹œ Linear Regressionì´ ê²°êµ­ì€ MLEì— ê¸°ë°˜í•˜ê¸° ë•Œë¬¸ì¸ ê²ƒì´ë‹¤.\n\nìš°ë¦¬ëŠ” ê° Class ìì²´ì˜ í™•ë¥ (Prior)ê³¼ Likelihoodë¥¼ ì´ìš©í•  ìˆ˜ ìˆëŠ” Discriminant Functionì„ êµ¬í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n\u003e **Logistic Regression**\n\nì ì´ì œ ë“œë””ì–´ Logistric Regressionì„ ì‹œì‘í•´ë³´ì. ìš°ë¦¬ëŠ” Discriminant Functionì„ ë¨¼ì € ì§€ì •í•´ì•¼ í•œë‹¤. ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ì´ ìˆì§€ë§Œ, ê°€ì¥ ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€ **Softmax**ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤. **Softmax**ë¥¼ í™œìš©í•˜ì—¬ ì‹ì„ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\n$$\np(y_n = k | \\bold{x}_n, \\bold{w}) = {{\\exp(\\bold{w}_{k}^{\\top}\\bold{x}_n)}\\over{\\sum_{j=1}^{K}{\\exp(\\bold{w}_{j}^{\\top}\\bold{x}_n)}}} \n$$\n\në§Œì•½, classê°€ 2ê°œì¸ Binary Classificationì¸ ê²½ìš°ì— **Softmax**ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤. íŠ¹íˆ ì´ë¥¼ **Sigmoid**(**Logit**)ë¼ê³  ì •ì˜í•œë‹¤.\n\n$$\np(y_n = k | \\bold{x}_n, \\bold{w}) = {1\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}\n$$\n\nì´ë¥¼ ìœ ë„í•˜ëŠ” ê³¼ì •ì€ ìƒëµí•˜ì§€ë§Œ, ì—¬íƒ€ ë‹¤ë¥¸ ë¸”ë¡œê·¸ë¥¼ ë” ì°¸ê³ í•˜ë©´ ì¢‹ë‹¤.\n\nì´ë¥¼ Linear Regressionê³¼ ë¹„êµí•´ì„œ ì‚´í´ë³´ì.\n\n![logistic-vs-linear](/images/logistic-vs-linear.jpg)\n\nLinear Regressionì€ íŠ¹ì •ê°’ì„ í–¥í•´ ë‚˜ì•„ê°€ê³  ìˆë‹¤. í•´ë‹¹ ë°©ì‹ì„ ë³´ë©´ xê°€ ëŒ€ìƒì˜ íŠ¹ì„±ì„ ê°•í•˜ê²Œ ê°€ì§€ê³  ìˆë‹¤ë©´, ëª…í™•í•˜ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” **sigmoid**($\\sigma$) í•¨ìˆ˜ê°€ [0, 1] ë²”ìœ„ ë‚´ì—ì„œ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— Regression ê³¼ì •ì—ì„œ ê·¹ë‹¨ ë°ì´í„°(outlier)ê°€ ê°€ì§€ëŠ” ì˜í–¥ë ¥ì´ Linear Regressionë³´ë‹¤ ê·¹ë‹¨ì ìœ¼ë¡œ ì ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\nì ì´ê²ƒì´ ê°€ì§€ëŠ” ì˜ë¯¸ë¥¼ ì´ì „ì— ì‚´í´ë³¸ **Bayes Decision Rule**ì— ê¸°ë°˜í•´ì„œ ìƒê°í•´ë³´ì. **sigmoid**($\\sigma$)ëŠ” ê²°êµ­ ê·¹ë‹¨ì ì¸ ë°ì´í„°ì´ë“ , ì• ë§¤í•œ ë°ì´í„°ì´ë“  ê±°ì˜ ë¹„ìŠ·í•œ ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤. ê·¸ë ‡ë‹¤ëŠ” ê²ƒì€ ê¸°ì¡´ì—ëŠ” í‰ê· ì„ êµ¬í•˜ëŠ”ë°ì— input(x)ì˜ ê°’ì´ í° ì˜í–¥ì„ ë¯¸ì³¤ë‹¤ë©´, **sigmoid**($\\sigma$)ì—ì„œëŠ” íŠ¹ì • classì— ì†í•˜ëŠ” xì˜ ê°¯ìˆ˜ê°€ ë§ì€ ì˜í–¥ì„ ì£¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ì„œ **sigmoid**($\\sigma$)ê°€ ì™„ë²½í•˜ì§€ëŠ” ì•Šì§€ë§Œ, **Bayes Decision Rule**ì„ ë°˜ì˜í–ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\në§ˆì§€ë§‰ìœ¼ë¡œ, MLEë¥¼ í†µí•´ì„œ Logistic Regressionì˜ parameterë¥¼ ì¶”ì •í•´ë³´ì. (MAPëŠ” ê¸°ì¡´ì— ì‚´í´ë³¸ Linear Regressionê³¼ ë™ì¼í•˜ê²Œ regularizerë¥¼ ë”í•´ì£¼ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì— ìƒëµí•œë‹¤.)\n\n$$\n\\begin{align*}\n\\argmax_{w}\\log{p(\\mathcal{D}|\\bold{w})} \u0026= \\argmax_{w}\\sum_{n=1}^{N}{\\log p(y_{n}|\\bold{x}_{n}, \\bold{w})} \\\\\n\u0026= \\argmax_{w}\\sum_{n=1}^{N}{\\log ({1\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}) } \\\\\n\u0026= \\argmax_{w}\\sum_{n=1}^{N}{-\\log (1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})) } \\\\\n\u0026= \\argmin_{w}\\sum_{n=1}^{N}{\\log (1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})) } \\\\\n\\end{align*}\n$$\n\n## Gradient Descent/Ascent\n\nìœ„ì˜ ë³µì¡í•œ ì‹ì„ ë´¤ìœ¼ë©´ ì•Œê² ì§€ë§Œ, ì•ˆíƒ€ê¹ê²Œë„ ì¼ë°˜ì‹ìœ¼ë¡œ $\\bold{w}_{MLE}, \\bold{w}_{MAP}$ ë“±ì„ êµ¬í•  ìˆ˜ëŠ” ì—†ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ê°€ ë¯¿ì„ ê²ƒì€ Gradientë¥¼ ì´ìš©í•œ ë°©ì‹ì´ë‹¤.\n\n\u003e **Gradient Descent**\n\në¨¼ì €, ìœ„ì—ì„œ ë´¤ê² ì§€ë§Œ, LossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\mathcal{L} = \\sum_{n=1}^{N}{\\log(1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n}))}\n$$\n\nì´ì œ ì´ë¥¼ ë¯¸ë¶„í•´ì„œ Gradientë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}) = \\sum_{n=1}^{N}{{{-y_{n}\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}\\over{1+\\exp(-y_{n}\\bold{w}^{\\top}\\bold{x}_{n})}}\\bold{x}_{n}}\n$$\n\në”°ë¼ì„œ, Gradient Descent ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ëœë‹¤.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\alpha\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}_{t})\n$$\n\n\u003e **Gradient Ascent**\n\nìœ„ì˜ ë°©ì‹ì´ ê°€ì¥ ì¼ë°˜ì ì´ì§€ë§Œ, ìš°ë¦¬ê°€ sigmoidì˜ classê°’ìœ¼ë¡œ $y \\in \\{-1, 1\\}$ ëŒ€ì‹  $y \\in \\{0, 1\\}$ì„ ì‚¬ìš©í–ˆì„ ê²½ìš° ë‹¤ë¥¸ ì‹ìœ¼ë¡œë„ ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë‹¤.\n\nì´ ê²½ìš°ì—ëŠ” Lossë¼ê¸° ë³´ê¸° ì–´ë µì§€ë§Œ, ë‹¤ë¥¸ í˜•íƒœì˜ optimization í˜•íƒœê°€ ë§Œë“¤ì–´ì§„ë‹¤. (ì—¬ê¸°ì„œ $\\sigma$ëŠ” sigmoid í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.)\n\n$$\n\\argmax_{\\bold{w}} \\sum_{n=1}^{N}y_{n}\\log{\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}) + (1-y_{n})\\log{(1-\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}))} }\n$$\n\nì´ë¥¼ ë˜‘ê°™ì´ ë¯¸ë¶„í•˜ì—¬ ì‚¬ìš©í•˜ì§€ë§Œ, ë°˜ëŒ€ë¡œ ì´ ê²½ìš°ì—ëŠ” maximization ì´ê¸° ë•Œë¬¸ì— Gradient Ascentë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. \n\nìš°ì„  ë¯¸ë¶„ ê²°ê³¼ ì–»ëŠ” GradientëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}) = \\sum_{n=1}^{N}{[y_{n} - \\sigma(\\bold{w}^{\\top}\\bold{x}_{n})]\\bold{x}_{n}}\n$$\n\nêµ‰ì¥íˆ ê°„ë‹¨í•˜ê²Œ ì •ë¦¬ê°€ ë˜ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} - \\alpha\\nabla_{\\bold{w}}\\mathcal{L}(\\bold{w}_{t})\n$$\n\në”°ë¼ì„œ, ì•„ë˜ì™€ ê°™ì´ Gradient Ascentë¥¼ í™œìš©í•˜ì—¬ ê³„ì‚°í•˜ëŠ” ê²ƒë„ ì¶©ë¶„íˆ ê°€ëŠ¥í•˜ë‹¤.\n\n\u003e **Newton Method**\n\nì´ëŸ¬í•œ í˜•íƒœë¡œ ë„˜ì–´ì˜¤ê²Œ ë˜ë©´, êµ‰ì¥íˆ ë§ì€ ì—°ì‚°ì´ ê° updateë§ˆë‹¤ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ ê³¼ì •ì„ ì¶•ì•½í•  ë°©ë²•ì„ ì°¾ê²Œ ëœë‹¤. ê·¸ ì•„ì´ë””ì–´ëŠ” ë°”ë¡œ gradientë¥¼ ì—…ë°ì´íŠ¸ í•  ë•Œ, linear í•˜ê²Œ updateí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ Quadraticí•˜ê²Œ updateí•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•œ ë°©ë²•ë¡ ì´ **Newton Method**ì´ë‹¤. ì´ ë°©ì‹ì„ Logistic Regressionì— ì ìš©í•˜ì˜€ì„ ë•Œ, ì´ë¥¼ IRLS(Iterative Re-weighted Least Squared) Algorithm ì´ë¼ê³  ë¶€ë¥¸ë‹¤.\n\n![newton-method](/images/newton-method.jpg)\n\nìœ„ ê·¸ë˜í”„ì—ì„œ f(x)ê°€ Loss ë¼ê³  í•  ë•Œ, ìš°ë¦¬ëŠ” $x_k$ì—ì„œ ì§ì„ í˜•ì˜ gradientë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ quadratic í˜•íƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´ê°’ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\nì´ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒ 2ê°€ì§€ì— ëŒ€í•œ ì‚¬ì „ ì´í•´ê°€ í•„ìš”í•˜ë‹¤.\n\n- Taylor Series  \n  smoothí•œ í˜•íƒœë¥¼ ê°€ì§„ xì— ëŒ€í•œ í•¨ìˆ˜ë¥¼ xì— ëŒ€í•œ ê¸‰ìˆ˜ì˜ í˜•íƒœë¡œ ë³€í™˜í•œ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n  $T_{\\infin}(x) = \\sum_{k=0}^{\\infin}{f^{(k)}(x_{0})\\over{k\\!}}(x-x_{0})^{k} $  \n  ì¦‰, sine í•¨ìˆ˜ì™€ ê°™ì€ í˜•íƒœì˜ ê·¸ë˜í”„ë„ xì˜ ê¸‰ìˆ˜ í˜•íƒœë¡œ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. Newton Methodì—ì„œëŠ” ë¬´í•œëŒ€ê¹Œì§€ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , ëŒ€ê²Œ K=2ê¹Œì§€ë¥¼ ì“´ë‹¤.\n- Hessian Matrix  \n  íŠ¹ì • í•¨ìˆ˜ $f(\\bold{x})$ë¥¼ ê° featureì— ëŒ€í•´ì„œ ì´ì¤‘ í¸ë¯¸ë¶„í•œ ê²°ê³¼ë¥¼ ì €ì¥í•œ í–‰ë ¬ì´ë‹¤. ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n  $\n  H = \\nabla^{2}f(x) =\n  \\left[\n    \\begin{array}{ccc}\n      \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{1}^{2}} \u0026 \\cdots \u0026 \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{1} \\partial x_{D}} \\\\ \n      \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\n      \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{D} \\partial x_{1}} \u0026 \\cdots \u0026 \\dfrac{\\partial^{2} f(\\mathbf{x})}{\\partial x_{n}^{2}}\n    \\end{array}\n  \\right]\n  $\n\nì´ë¥¼ ì´ìš©í•´ì„œ, Newton Methodì˜ ê²°ê³¼ê°’ì„ ì •ë¦¬í•˜ë©´ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\bold{w}^{(k+1)} = \\bold{w}^{(k)} - [\\nabla^{2}\\mathcal{J}(\\bold{w}^{(k)})]^{-1}\\nabla\\mathcal{J}(\\bold{w}^{(k)})\n$$\n\nì ì´ì œ ì´ê²ƒì„ ì‹¤ì œë¡œ Logistic Regression ì‹ì— ëŒ€ì…í•´ë³´ì.\n\n$$\n\\begin{align*}\n  \\nabla\\mathcal{J}(w) \u0026= - \\sum_{n=1}^{N}(y_{n}-\\hat{y}_{n})x_{n} \\\\\n  \\nabla^{2}\\mathcal{J}(w) \u0026= \\sum_{n=1}^{N}\\hat{y}_{n}(1-\\hat{y}_{n})\\bold{x}_{n}\\bold{x}_{n}^{\\top}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ, ì•„ë˜ì™€ ê°™ì´ ë³€ìˆ˜ë¥¼ ì •ì˜í•˜ë©´,\n\n$$\nS = \n  \\begin{bmatrix}\n    \\hat{y}_{1}(1-\\hat{y}_1)  \u0026 \\cdots  \u0026 0                         \\\\\n    \\vdots                    \u0026 \\ddots  \u0026 \\vdots                     \\\\\n    0                         \u0026 \\cdots  \u0026 \\hat{y}_{N}(1-\\hat{y}_N)  \\\\\n  \\end{bmatrix},\n\n\\bold{b} = \n  \\begin{bmatrix}\n    {{y_{1} - \\hat{y}_{1}}\\over{\\hat{y}_{1}(1-\\hat{y}_{1})}} \\\\\n    \\vdots \\\\\n    {{y_{N} - \\hat{y}_{N}}\\over{\\hat{y}_{N}(1-\\hat{y}_{N})}}\n  \\end{bmatrix}\n$$\n\nê²°ê³¼ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\bold{w}_{k+1} \u0026= \\bold{w}_{k} + (XS_{k}X^{\\top})^{-1}XS_{k}\\bold{b}_{k} \\\\\n\u0026= (XS_{k}X^{\\top})^{-1}[(XS_{k}X^{\\top})\\bold{w}_{k} + XS_{k}\\bold{b}_{k}] \\\\\n\u0026= (XS_{k}X^{\\top})^{-1}XS_{k}[X^{\\top}\\bold{w}_{k} + \\bold{b}_{k}]\n\\end{align*}\n$$\n\nì´ëŠ” ê²°ì½” ê³„ì‚° ê³¼ì •ì´ ë‹¨ìˆœí•˜ë‹¤ê³ ëŠ” í•  ìˆ˜ ì—†ì§€ë§Œ, ë¹ ë¥´ê²Œ ìˆ˜ë ´í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê°€ì¹˜ìˆëŠ” ë°©ë²•ì´ë‹¤.\n\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)","slug":"ml-logistic-regression","date":"2022-10-18 09:58","title":"[ML] 3. Logistic Regression","category":"AI","tags":["ML","LogisticRegression","Classification","SigmoidFunction","SoftmaxFunction","NewtonMethod"],"desc":"ì´ì „ê¹Œì§€ ìš°ë¦¬ëŠ” input dataê°€ ë“¤ì–´ì™”ì„ ë•Œ, continuosí•œ outputì„ ì–»ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ” ëŒ€ê²Œ ì •í™•í•œ ìˆ˜ì¹˜ë³´ë‹¤ëŠ” íŠ¹ì • ë¶„ë¥˜ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ íš¨ê³¼ì ì¸ ê²½ìš°ê°€ ë§ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, spam í•„í„°ë§, object detection, ë“± ë“±. ë”°ë¼ì„œ, í•´ë‹¹ í¬ìŠ¤íŒ…ì—ì„œëŠ” classificationì„ ìœ„í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” logistic regressionì— ëŒ€í•´ì„œ ì‚´í´ë³¼ ê²ƒì´ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"NewtonMethod"}},"__N_SSG":true},"page":"/tags/[subject]","query":{"subject":"NewtonMethod"},"buildId":"eGkZYbzPXM35ApK3V1pre","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>