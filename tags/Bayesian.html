<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Network ë¶„ì•¼ì— ê´€ì‹¬ì´ ë§ì€ ê°œë°œìë¡œ Computer Engineering ê´€ë ¨ Postingì„ ì£¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤."/><meta property="og:description" content="Network ë¶„ì•¼ì— ê´€ì‹¬ì´ ë§ì€ ê°œë°œìë¡œ Computer Engineering ê´€ë ¨ Postingì„ ì£¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤."/><meta property="og:type" content="blog"/><meta property="og:site_name" content="JustLog"/><meta property="og:image" content="https://euidong.github.io/logo192.png"/><title>#Bayesian | JustLog</title><meta property="og:title" content="#Bayesian | JustLog"/><link rel="canonical" href="https://euidong.github.io/tags/Bayesian"/><meta property="og:url" content="https://euidong.github.io/tags/Bayesian"/><meta name="next-head-count" content="11"/><link rel="icon" href="https://euidong.github.io/favicon.png"/><link rel="apple-touch-icon" href="https://euidong.github.io/logo192.png"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7452732177557701" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/css/51df76ed4222d2c7.css" as="style"/><link rel="stylesheet" href="/_next/static/css/51df76ed4222d2c7.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6dc16d084a5153e5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6dc16d084a5153e5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-81da43a8dcd978d9.js" defer=""></script><script src="/_next/static/chunks/main-7b6c38cbad60dfcf.js" defer=""></script><script src="/_next/static/chunks/pages/_app-4b56c138af06f610.js" defer=""></script><script src="/_next/static/chunks/675-ae8e8a351ce30ae2.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Bsubject%5D-2ecacdd7ae5454e8.js" defer=""></script><script src="/_next/static/MaZpmJYXKLLgn_2WNS5lL/_buildManifest.js" defer=""></script><script src="/_next/static/MaZpmJYXKLLgn_2WNS5lL/_ssgManifest.js" defer=""></script><script src="/_next/static/MaZpmJYXKLLgn_2WNS5lL/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_wrapper__dKJSz root"><header class="Layout_header__XosLl" style="position:sticky"><div><button tabindex="1" class="SideBarToggler_search_bar_toggler__CEuUg"><svg stroke="currentColor" fill="none" stroke-width="0" viewBox="0 0 24 24" height="35px" width="35px" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></button><nav class="SideBar_side_bar__wrapper--close__8Nwnr"><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/">Home</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/tags">Tags</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Algorithm">Algorithm<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Computer%20Architecture">Computer Architecture<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->7<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Tech">Tech<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->16<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Web">Web<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Network">Network<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/AI">AI<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->2<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Paper">Paper<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->1<!-- -->)<!-- --></span></a></nav></div><a class="Logo_logo___yD0t" tabindex="1" href="/"></a><div><button class="SearchBarToggler_search_bar_toggler__3dHbA" tabindex="2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg></button></div></header><section><div class="RowCard_row_card__list__background___xFj5"><h1 class="RowCard_row_card__list__title__t4a2h"> Bayesian</h1><label class="RowCard_row_card__list__select__wrapper__TZ4_9"><select class="RowCard_row_card__list__select__dxkxA"><option class="RowCard_row_card__list__select__option__GRKZU">ìµœì‹ ìˆœ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">AtoZ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">ZtoA<!-- --></option></select></label><ul class="RowCard_row_card__list__wrapper__5Gtgi"><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-parametric-estimation"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 1. Parametric Estimation" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 1. Parametric Estimation" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-parametric-estimation">[ML] 1. Parametric Estimation</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 10ì›” 15ì¼ 11ì‹œ 25ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/MLE"># <!-- -->MLE<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/MAP"># <!-- -->MAP<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Bayesian"># <!-- -->Bayesian<!-- --></a></ul></div></div></ul></div></section><footer class="Layout_footer__EL5v8"><div class="Layout_footer__copyright__r5baC"><span>Copyright Â© euidong</span><br/><span>ëª¨ë“  ì»¨í…ì¸ ì— ëŒ€í•œ ì €ì‘ê¶Œì€ ì‘ì„±ìì—ê²Œ ì¡´ì¬í•©ë‹ˆë‹¤. <!-- --><br/>ë¶ˆë²• ë³µì œë¥¼ í†µí•œ ìƒì—…ì  ì‚¬ìš©ì„ ì ˆëŒ€ì ìœ¼ë¡œ ê¸ˆì§€í•©ë‹ˆë‹¤. <!-- --><br/>ë‹¨, ë¹„ìƒì—…ì  ì´ìš©ì˜ ê²½ìš° ì¶œì²˜ ë° ë§í¬ë¥¼ ì ìš©í•œë‹¤ë©´ ììœ ë¡­ê²Œ ì‚¬ìš©ê°€ëŠ¥ í•©ë‹ˆë‹¤.<!-- --></span><span>Also I use photos by<!-- --> <!-- --><a href="https://unsplash.com/@lorenzoherrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Lorenzo Herrera</a> <!-- -->on<!-- --> <!-- --><a href="https://unsplash.com/s/photos/tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Unsplash</a></span></div><div class="Layout_footer__contents__YZWSm"><a class="Layout_footer__contents__link__K_TKH" href="https://github.com/euidong" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>github</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://euidong.github.io/portfolio" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M6 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm-5 6s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H1zM11 3.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 0 1h-4a.5.5 0 0 1-.5-.5zm.5 2.5a.5.5 0 0 0 0 1h4a.5.5 0 0 0 0-1h-4zm2 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2zm0 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2z"></path></svg><span>portfolio</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://chrome.google.com/webstore/detail/bonfire/nkooidijgbppkojdgkoafcoppnohdfka?hl=ko" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 16c-1.066-2.219-0.498-3.49 0.321-4.688 0.897-1.312 1.129-2.61 1.129-2.61s0.706 0.917 0.423 2.352c1.246-1.387 1.482-3.598 1.293-4.445 2.817 1.969 4.021 6.232 2.399 9.392 8.631-4.883 2.147-12.19 1.018-13.013 0.376 0.823 0.448 2.216-0.313 2.893-1.287-4.879-4.468-5.879-4.468-5.879 0.376 2.516-1.364 5.268-3.042 7.324-0.059-1.003-0.122-1.696-0.649-2.656-0.118 1.823-1.511 3.309-1.889 5.135-0.511 2.473 0.383 4.284 3.777 6.197z"></path></svg><span>chat</span></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"content":"\n## Intro\n\nMachine Learningì€ íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„°ë¡œ ë¶€í„° pattern ë˜ëŠ” ê°€ì • ë“±ì„ ìœ ë„í•´ë‚´ëŠ” ë°©ë²•ì´ë‹¤.\nì´ë¥¼ ìœ„í•œ ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì€ ì—¬ëŸ¬ ê°œì˜ í™•ë¥ ë¶„í¬ì™€ ì´ê²ƒì˜ parameterì˜ ì¡°í•©(probabilistic model)ë“¤ ì¤‘ì—ì„œ ì¸¡ì •ëœ ë°ì´í„°ë“¤ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” í•˜ë‚˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤.\nê·¸ ì¤‘ì—ì„œ, í™•ë¥  ë¶„í¬ë¥¼ ê²°ì •í•œ ìƒíƒœì—ì„œ parameterë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” í˜•íƒœì˜ ì ‘ê·¼ë²•ì„ ìš°ë¦¬ëŠ” Parametric Estimationì´ë¼ê³  í•œë‹¤. ê·¸ ì™¸ì—ë„ Nonparametric, Semi-parametric ë°©ì‹ë„ ì¡´ì¬í•˜ì§€ë§Œ ì´ëŠ” ì—¬ê¸°ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤.\n\n## Small Example\n\nê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ í†µí•´ì„œ Parametric Estimationì˜ íë¦„ì„ ìµí˜€ë³´ì.\n\ní•œ í•™ê¸‰ì—ì„œ í•™ìƒë“¤ì˜ í˜•ì œìë§¤ ìˆ˜ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•˜ê³  ì‹¶ë‹¤ê³  í•˜ì.  \nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ë¨¼ì € ì¡°ì‚¬(ê´€ì¸¡)ë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. ì´ë¥¼ í†µí•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ë¥¼ ì–»ê²Œ ë˜ì—ˆë‹¤ê³  í•˜ì.\n\n| x        | 1    | 2    | 3    | 4    | 5    | 6    | x$\\geq$7 |\n| :------- | :--- | :--- | :--- | :--- | :--- | :--- | :------- |\n| $p(X=x)$ | 17   | 59   | 15   | 6    | 2    | 0    | 1        |\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ì—¬ëŸ¬ ì‚¬ì „ ì§€ì‹ì„ í™œìš©í•˜ì—¬ í•´ë‹¹ ë°ì´í„°ë¥¼ ë³´ì•˜ì„ ë•Œ, í•´ë‹¹ ë¶„í¬ê°€ Poisson ë¶„í¬ì˜ í˜•íƒœë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.  \në”°ë¼ì„œ, ìš°ë¦¬ëŠ” í•´ë‹¹ ë¶„í¬ë¥¼ Poissonì´ë¼ê³  ê°€ì •í•œ ë‹¤ìŒì—ëŠ” ë‹¨ìˆœíˆ í•´ë‹¹ ë¶„í¬ì— ëŒ€ì…í•˜ë©°, ê°€ì¥ ì ì ˆí•œ parameterë§Œ ì°¾ìœ¼ë©´ ëœë‹¤.  \n\nì´ ê³¼ì •ê³¼ ë‹¨ìˆœíˆ ê° xì—ì„œì˜ í™•ë¥ ê°’ì„ êµ¬í•˜ëŠ” ë°©ì‹ì´ë‘ ë¬´ì—‡ì´ ë‹¤ë¥¸ì§€ë¥¼ ì•Œì•„ì•¼ì§€ í•´ë‹¹ ê³¼ì •ì˜ ì˜ì˜ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.\në¨¼ì €, ìš°ë¦¬ê°€ í•˜ê³ ì í•˜ëŠ” ì¼ì´ í˜•ì œìë§¤ì˜ í‰ê·  ìˆ˜ë¥¼ êµ¬í•œë‹¤ê³  í•˜ì. ì´ë•Œì˜ í‰ê·  ê°’ê³¼ Poisson ë¶„í¬ì—ì„œì˜ í™•ë¥ ê°’ì€ ë‹¤ë¥¼ ìˆ˜ ë°–ì— ì—†ë‹¤.\n\nì´ë ‡ê²Œ í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•˜ëŠ” ê²ƒì˜ ì˜ë¯¸ëŠ” ì´ê²ƒë§ê³ ë„ ë³´ì§€ ì•Šì€ ë°ì´í„°(unseen data)ë¥¼ ì²˜ë¦¬í•¨ì— ìˆë‹¤. ìš°ë¦¬ê°€ ë§Œì•½ ëª¨ë“  ê°€ëŠ¥í•œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ëª¨ë‘ ì•Œê³  ìˆê³ , ì´ë¥¼ ì €ì¥í•  ê³µê°„ì´ ì¶©ë¶„í•˜ë‹¤ë©´,\nì´ëŸ¬í•œ í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•  í•„ìš”ê°€ ì—†ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì¶”ì¸¡ì€ unseen dataì— ëŒ€í•´ì„œë„ ê·¸ëŸ´ì‚¬í•´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ê²°êµ­ í™•ë¥  ë¶„í¬ê°€ í•„ìš”í•˜ë‹¤.\n\nìœ„ì˜ ì˜ˆì‹œì—ì„œ ë§Œì•½, í˜•ì œìë§¤ê°€ 3ëª…ì¸ ê²½ìš°ì˜ ë°ì´í„°ê°€ ì—†ë‹¤ê³  í•˜ì. ì´ ê²½ìš°ì—ë„ í™•ë¥ ë¶„í¬ë¥¼ í†µí•œ ì¶”ì¸¡ì„ í•œë‹¤ë©´, ìš°ë¦¬ëŠ” ìœ ì˜ë¯¸í•œ ê°’ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.\n\n## Parametric Estimation\n\n\u003e **ì •ì˜**\n\nsample space $\\Omega$ì—ì„œ í†µê³„ ì‹¤í—˜ì˜ ê´€ì¸¡ ê²°ê³¼ë¥¼ í†µí•´ì„œ ì–»ì€ sample $X_1$, $X_2$, ... , $X_n$ì´ ìˆë‹¤ê³  í•˜ì. ê° sampleì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ìš°ë¦¬ëŠ” $p_\\theta$ë¼ê³  í•œë‹¤.\nì—¬ê¸°ì„œ $\\theta$ëŠ” íŠ¹ì • í™•ë¥  ë¶„í¬ì—ì„œì˜ parameterë¥¼ ì˜ë¯¸í•œë‹¤. ë§Œì•½, bernoulli ë¼ë©´, ë‹¨ì¼ ì‹œí–‰ì— ëŒ€í•œ í™•ë¥ ì´ ë  ê²ƒì´ê³ , binomialì´ë¼ë©´, ë‹¨ì¼ ì‹œí–‰ì˜ í™•ë¥ ê³¼ íšŸìˆ˜ê°€ í•´ë‹¹ ê°’ì´ ë  ê²ƒì´ë‹¤.\n\n\u003e **Risk**\n\nì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì°¾ê¸°ë¥¼ ì›í•˜ëŠ” ê²ƒì€ ì „ì²´ sample space $\\Omega$ë¥¼ ëª¨ë‘ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ” $\\theta_{*}$(ì‹¤ì œ true $\\theta$)ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.(ì´ë¯¸ í™•ë¥  ë¶„í¬ì˜ í˜•íƒœ(í•¨ìˆ˜, ex. Bernoulli, Binomial)ëŠ” ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤.)  \nê·¸ë ‡ë‹¤ë©´, ì‹¤ì œ $\\theta_*$ì™€ ì¶”ì¸¡ì„ í†µí•´ ë§Œë“  $\\hat{\\theta}$ ì‚¬ì´ì˜ ë¹„êµë¥¼ ìœ„í•œ ì§€í‘œë„ í•„ìš”í•  ê²ƒì´ë‹¤. ì´ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” **Risk**ë¼ëŠ” ê²ƒì„ ì‚¬ìš©í•œë‹¤.  \nê°„ë‹¨í•˜ê²Œë„ ì‹¤ì œ $\\theta_*$ì™€ $\\hat{\\theta}$ì˜ Mean Square Errorë¥¼ ê³„ì‚°í•œë‹¤.\n\n$$ \n\\begin{align*}\nRisk \u0026= E[(\\hat{\\theta} - \\theta_*)^2] = E[\\hat{\\theta}^2 - 2\\hat{\\theta}\\theta_* + \\theta_*^2] \\\\\n\u0026= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 \\\\\n\u0026= E[\\hat{\\theta}^2] - 2\\theta_*E[\\hat{\\theta}] + \\theta_*^2 + (E^2[\\hat{\\theta}] - E^2[\\hat{\\theta}]) \\\\\n\u0026= (E[\\hat{\\theta}] - \\theta_*)^2 + E[\\hat{\\theta}^2] - E^2[\\hat{\\theta}] \\\\\n\u0026= {Bias}^2 + Var[\\hat{\\theta}]\n\\end{align*}\n$$\n\ní•´ë‹¹ ì‹ì„ ë¶„ì„í•´ë³´ë©´, ì´ì™€ ê°™ì€ ì˜ë¯¸ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. ìš°ë¦¬ê°€ íŠ¹ì • í™•ë¥  ë¶„í¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë‹¨ í•˜ë‚˜ë¡œ ë‹¨ì •í•˜ê³  Riskë¥¼ ê³„ì‚°í•˜ëŠ” ê²½ìš°ëŠ” Variance ê°’ì€ 0ì´ë‹¤. ì¦‰, í•´ë‹¹ í™•ë¥  ë¶„í¬ê°€ ê°€ì§€ëŠ” RiskëŠ” ë‹¨ìˆœíˆ í•´ë‹¹ parameterì™€ ì‹¤ì œ parameterê°€ ì–¼ë§ˆë‚˜ ì°¾ì´ê°€ ë‚˜ëŠ”ê°€ë¥¼ ì˜ë¯¸í•œë‹¤.\n\ní•˜ì§€ë§Œ, parameterë¥¼ íŠ¹ì •í•˜ì§€ ì•Šê³ , ë²”ìœ„ë¡œ ì§€ì •í•œë‹¤ë©´, (ì˜ˆë¥¼ ë“¤ì–´, ì£¼ì‚¬ìœ„ë¥¼ ë˜ì ¸ 3ì´ ë‚˜ì˜¬ í™•ë¥ ì€ 1/6 ~ 1/3ì´ë‹¤.) í•´ë‹¹ í™•ë¥ ì˜ í‰ê· ê³¼ Varianceê°€ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë‹¤.  \në‹¤ì†Œ ì²˜ìŒì—ëŠ” í—·ê°ˆë¦´ ìˆ˜ ìˆì§€ë§Œ, í•´ë‹¹ ì‹ì—ì„œ í‰ê· ì´ ì˜ë¯¸ëŠ” ì˜ í™•ì¸í•˜ì. íŠ¹ì • í™•ë¥  ë¶„í¬ë¥¼ ê°€ì§€ë„ë¡ í•˜ëŠ” $\\theta$ê°€ $\\theta_*$ ì— ì–¼ë§ˆë‚˜ ê·¼ì ‘í•œì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì‹ì´ë¼ëŠ” ê²ƒì„ ë‹¤ì‹œ í•œ ë²ˆ ê¸°ì–µí•˜ì.\n\n\u003e **Estimation**\n\nì´ì œë¶€í„°ëŠ” ì•ì—ì„œ ì‚´í´ë³´ì•˜ë˜, parameteric estimationì—ì„œ ì–´ë–»ê²Œ $\\hat{\\theta}$ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë‹¤ë£° ê²ƒì´ë‹¤. í™•ë¥ /í†µê³„ ì´ë¡ ì—ì„œëŠ” í¬ê²Œ 3ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê° ê°ì„ ì‚´í´ë³´ë„ë¡ í•˜ì.\n\n\u003cmark\u003e**1. MLE**\u003c/mark\u003e\n\nMaximum Likelihood Estimationì˜ ì•½ìì´ë‹¤. ì—¬ê¸°ì„œ, LikelihoodëŠ” ê°€ëŠ¥ì„±ì´ë¼ëŠ” ëœ»ì„ ê°€ì§€ë©°, í™•ë¥ /í†µê³„ ì´ë¡ ì—ì„œ ì´ëŠ” í™•ë¥ ì„ í•´ë‹¹ ì‚¬ê±´ì´ ë°œìƒí•  ê°€ëŠ¥ì„±ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ì´ìš©í•´ì„œ ìš°ë¦¬ê°€ í’€ê³ ì í•˜ëŠ” ë¬¸ì œ, ìš°ë¦¬ê°€ ì¶”ì¸¡í•œ $\\theta$ê°€ ìš°ë¦¬ê°€ ê°€ì§„ Datasetë¥¼ ë§Œì¡±ì‹œí‚¬ ê°€ëŠ¥ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤. ì•„ë˜ ìˆ˜ì‹ì„ ë³´ì.\n\n$$\n\\begin{align*}\n\\mathcal{L}(\\theta;\\mathcal{D}) \u0026= p(\\mathcal{D}|\\theta) = p(x_1, x_2, ..., x_n|\\theta) \\\\\n\u0026= \\prod_{i=1}^{n}{p(x_i|\\theta)}\n\\end{align*}\n$$\n\n(ìœ„ ì‹ì„ ì´í•´í•˜ë ¤ë©´, ë¨¼ì € Datasetì˜ ê° dataë“¤ì€ ì„œë¡œ independentí•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¸°ì–µí•˜ì.)  \nê²°êµ­ $\\theta$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Datasetì¼ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ë‹¤ì‹œ ìƒê°í•˜ë©´, $\\theta$ê°€ ì–¼ë§ˆë‚˜ ë°ì´í„°ì…‹ì˜ í™•ë¥ ì„ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ê°€ì™€ ê°™ë‹¤.\n\nì´ê²ƒì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ë ¤ë©´ í•˜ë‚˜ì˜ ì˜ˆì‹œë¥¼ ë³´ë©´ ì¢‹ë‹¤.\n\n![MLE example](/images/MLE-example.png)\n\nì²« ë²ˆì§¸ ê·¸ë˜í”„ëŠ” ê°™ì€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ í•¨ìˆ˜ë¥¼ ì“°ë©´ì„œ, parameterë§Œ ë‹¤ë¥´ê²Œ í•œ ê²½ìš°ì´ê³ , ì•„ë˜ëŠ” ì‹¤ì œ ë°ì´í„°ì˜ ë¶„í¬ë¼ê³  í•˜ì.(ë¹¨ê°„ìƒ‰ ì„  í•˜ë‚˜ í•˜ë‚˜ê°€ ë°ì´í„°ë¥¼ ì˜ë¯¸)  \nì´ë•Œ, Likelihoodë¥¼ ê° ê° êµ¬í•˜ë©´ ê° xì—ì„œì˜ í™•ë¥ ë¶„í¬ì˜ í™•ë¥ ê°’ì„ ëª¨ë‘ ê³±í•˜ë©´ ëœë‹¤. ê·¸ ê²½ìš° ì–´ë–¤ ê²ƒì´ ì œì¼ í´ì§€ëŠ” ë¶„ëª…í•˜ë‹¤. ë°”ë¡œ íŒŒë€ìƒ‰ ë¶„í¬ì¼ ê²ƒì´ë‹¤.  \n\nê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€? ë°”ë¡œ ê°€ì¥ ë†’ì€ ê°€ëŠ¥ì„±ì„ ê°€ì§€ê²Œ í•˜ëŠ” $\\theta$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ì‹ìœ¼ë¡œ í‘œì‹œí•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\n$$\n\\hat{\\theta}_{MLE} = \\argmax_{\\theta}\\mathcal{L}(\\theta;\\mathcal{D})\n$$\n\nì—¬ê¸°ì„œ í•˜ë‚˜ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆë‹¤. ë°”ë¡œ, ì»´í“¨í„°ë¡œ ì—°ì‚°í•˜ê²Œ ë˜ë©´ underflowê°€ ë°œìƒí•˜ëŠ” ê²ƒì´ë‹¤. íŠ¹ì • ì–¸ì–´ê°€ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ì†Œìˆ˜ì  ë²”ìœ„ë¥¼ ë²—ì–´ë‚œë‹¤ë©´, ì œëŒ€ë¡œ ëœ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ì—†ë‹¤. ì´ì™€ ê°™ì€ ë¬¸ì œë¥¼ **vanishing likelihood**ë¼ê³  í•œë‹¤.  \në”°ë¼ì„œ, ìš°ë¦¬ëŠ” logë¥¼ ì·¨í–ˆì„ ë•Œì™€ logë¥¼ ì·¨í•˜ì§€ ì•Šì•˜ì„ ë•Œì˜ ê²½í–¥ì„±ì´ ê°™ìŒì„ ë°”íƒ•ìœ¼ë¡œ likelihoodì— logë¥¼ ì·¨í•œ ê°’ì„ ì´ìš©í•˜ì—¬ MLEë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ì´ ë°©ì‹ì„ maxmum log likelihood estimation ì´ë¼ê³  ë¶€ë¥¸ë‹¤.\n\n$$\n\\mathcal{l}(\\theta;\\mathcal{D}) = \\sum_{i=1}^{n}{\\log{(p(x_i|\\theta))}}\n$$\n\nì´ ë°©ì‹ì„ ì´ìš©í•˜ê²Œ ë˜ë©´, ê³±ì…ˆì´ ëª¨ë‘ ë§ì…ˆìœ¼ë¡œ ë°”ë€Œê¸° ë•Œë¬¸ì— ê³„ì‚°ì—ì„œë„ ìš©ì´í•˜ë‹¤.\n\nì—¬ê¸°ê¹Œì§€ ì‚´í´ë³´ë©´, í•˜ë‚˜ì˜ ì˜ë¬¸ì´ ë“¤ ìˆ˜ë„ ìˆë‹¤. ë°”ë¡œ, $p(\\theta|\\mathcal{D})$ë„ ì¸¡ì • ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ ì•ŠëƒëŠ” ê²ƒì´ë‹¤. ì´ ì—­ì‹œë„ Datasetì´ ì£¼ì–´ì§ˆ ë•Œ, $\\theta$ì¼ í™•ë¥ ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.  \nì–´ì°Œë³´ë©´, ì‚¬ëŒì˜ ìƒê°ìœ¼ë¡œëŠ” ì´ê²Œ ë” ë‹¹ì—°í•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ë„ ìˆë‹¤. ì´ëŠ” ë°”ë¡œ ë‹¤ìŒ MAPì—ì„œ ë‹¤ë£° ê²ƒì´ë‹¤. ìš°ì„  MLEë¥¼ ë¨¼ì €í•œ ì´ìœ ëŠ” ì´ê²ƒì´ ë” êµ¬í•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì„ì„ ê¸°ì–µí•´ë‘ì. \n\n```plaintext\n ğŸ¤” ì¦ëª…\n\n (*í•´ë‹¹ ë‚´ìš©ì€ ì •ë³´ ì´ë¡ ì— ê¸°ë°˜í•œ MLEì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì´í•´ë¥¼ ìœ„í•œ ë‚´ìš©ì…ë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ìì„¸íˆ ì•Œ í•„ìš”ê¹Œì§€ëŠ” ì—†ìŠµë‹ˆë‹¤.)\n\n ë‘ í™•ë¥  ë¶„í¬ ê°„ information entrophyì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” KL divergenceì˜ ìµœì†Ÿê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ ëª©í‘œë¼ê³  ì •ì˜í•  ìˆ˜ ìˆë‹¤.  \n ë”°ë¼ì„œ, ìš°ë¦¬ê°€ ê²°êµ­ ì–»ê³ ì í•˜ëŠ” ê²ƒì€ í™•ë¥  ë¶„í¬ í•¨ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ,  \n nì´ ë¬´í•œëŒ€ë¡œ ê°ˆ ë•Œ, ê²½í—˜ì  í™•ë¥ (empirical probability)ì— ê°€ì¥ ê·¼ì‚¬í•˜ëŠ” parameterë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.  \n ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” KL divergenceì˜ ìµœì†Ÿê°’ì„ êµ¬í•˜ë©´ ëœë‹¤.\n```\n\n$$\n\\begin{align*}\n\\argmin_\\theta KL(\\tilde{p}||p_\\theta) \u0026= \\argmin_\\theta \\int\\tilde{p}(x)\\log{\\tilde{p}(x)\\over{p_\\theta(x)}}dx \\\\ \n\u0026=\\argmin_\\theta[-\\int\\tilde{p}(x)\\log{\\tilde{p}(x)dx} - \\int\\tilde{p}(x)\\log{p_\\theta(x)dx}] \\\\\n\u0026= \\argmax_\\theta\\int{\\tilde{p}(x)\\log{p_\\theta(x)}dx} \\\\\n\u0026= \\argmax_\\theta\\sum_{i=1}^{n}{\\log{p_\\theta(x_i)}} \\\\\n\u0026= \\theta_{MLE}\n\\end{align*} \n$$\n\n\u003cmark\u003e**2. MAP**\u003c/mark\u003e\n\nMaximum A Posterioriì˜ ì•½ìì´ë‹¤. PosterioriëŠ” ì‚¬í›„ í™•ë¥ ì´ë¼ê³ ë„ ë¶€ë¥´ë©°, datasetì´ ì£¼ì–´ì¡Œì„ ë•Œ, $\\theta$ì¼ í™•ë¥ ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.  \nì´ë¥¼ ë°”ë¡œ êµ¬í•˜ëŠ” ê²ƒì€ ë‹¤ì†Œ ì–´ë µë‹¤. ì™œëƒí•˜ë©´, Datasetì´ ì¡°ê±´ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” í˜•íƒœì´ê¸° ë•Œë¬¸ì´ë‹¤. ($p(\\theta|\\mathcal{D})$)  \në”°ë¼ì„œ, ìš°ë¦¬ëŠ” Bayes' Theoremì— ë”°ë¼ì„œ ì´ì „ì— ë°°ìš´ Likelihoodì™€ parameterì˜ í™•ë¥ , ê·¸ë¦¬ê³  Datasetì˜ í™•ë¥ ì„ í™œìš©íˆì—¬ í’€ì–´ë‚¼ ê²ƒì´ë‹¤.\n\n$$\np(\\theta|\\mathcal{D}) = {p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}}\n$$\n\nì—¬ê¸°ì„œ ì£¼ì˜í•´ì„œ ë³¼ ê²ƒì€ ë°”ë¡œ $p(\\theta|\\mathcal{D})$ì™€ $p(\\theta)$ì˜ ê´€ê³„ì´ë‹¤. datasetì´ ì£¼ì–´ì§ˆ ë•Œì˜ parameterì˜ í™•ë¥ ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ ì›ë˜ parameterì˜ í™•ë¥ ì´ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.  \nì–´ì°Œë³´ë©´ êµ‰ì¥íˆ ëª¨ìˆœë˜ì–´ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ìš°ë¦¬ê°€ ì´ê²ƒì„ ì‚¬ì „ í™•ë¥ (priori)ë¡œ ë³¸ë‹¤ë©´ ë‹¤ë¥´ê²Œ ë³¼ ì—¬ì§€ê°€ ìˆë‹¤.  \nì˜ˆë¥¼ ë“¤ë©´, ìš°ë¦¬ê°€ ìˆ˜ìƒí•œ ì£¼ì‚¬ìœ„ë¡œ í•˜ëŠ” ê²Œì„ì— ì°¸ê°€í•œë‹¤ê³  í•˜ì. ì´ë•Œ, ìš°ë¦¬ëŠ” ìˆ˜ìƒí•œ ì£¼ì‚¬ìœ„ì˜ ì‹¤ì œ í™•ë¥ ì€ ì•Œ ìˆ˜ ì—†ì§€ë§Œ, ì£¼ì‚¬ìœ„ ìì²´ì˜ í™•ë¥ ì€ ëª¨ë‘ 1/6ì´ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤. ë”°ë¼ì„œ, $p(\\theta={1\\over6}) = \\alpha, p(\\theta\\neq{1\\over6}) = \\beta$ ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë§Œì•½ ì •ë§ ìˆ˜ìƒí•´ë³´ì¸ë‹¤ë©´, ìš°ë¦¬ëŠ” $\\alpha$ê°€ ì ì  ì‘ì•„ì§„ë‹¤ëŠ” ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê³ , í•˜ë‚˜ë„ ìˆ˜ìƒí•´ë³´ì´ì§€ ì•ŠëŠ” ì¼ë°˜ ì£¼ì‚¬ìœ„ë¼ë©´, $\\alpha=1, \\beta=0$ìœ¼ë¡œ í•  ìˆ˜ë„ ìˆë‹¤. ì´ ê²½ìš°ì—ëŠ” likelihood ê°’ì— ìƒê´€ì—†ì´ ë‹¤ë¥¸ ëª¨ë“  ê°’ì´ 0ì´ê¸° ë•Œë¬¸ì— ê²°êµ­ì€ $p(\\theta|\\mathcal{D}) = p(\\theta)$ ê°€ ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\nìµœì¢…ì ìœ¼ë¡œ, MAPë„ ê²°êµ­ì€ Datasetì„ ì–¼ë§ˆë‚˜ parameterê°€ ì˜ í‘œí˜„í•˜ëŠ”ê°€ì— ëŒ€í•œ ì§€í‘œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. \në”°ë¼ì„œ, ì´ë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” parameterëŠ” $\\theta_*$ì™€ êµ‰ì¥íˆ ê·¼ì ‘í•  ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{MAP} \u0026= \\argmax_{\\theta}p(\\theta|\\mathcal{D}) \\\\\n\u0026= \\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)\\over{p(\\mathcal{D})}} \\\\\n\u0026=\\argmax_\\theta{p(\\mathcal{D}|\\theta)p(\\theta)} \\\\\n\u0026=\\argmax_\\theta{[\\red{\\log{p(\\mathcal{D}|\\theta)}} + \\blue{\\log{p(\\theta)}}]}\n\\end{align*}\n$$\n\nMLEì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì´ ë˜í•œ ì—°ì‚° ë° **vanishing**ì„ ë§‰ê¸° ìœ„í•´ì„œ logë¥¼ ì·¨í•œë‹¤. ì‚¬ì‹¤ìƒ likelihoodì™€ ì‚¬ì „ í™•ë¥ ì˜ í•©ì„ ìµœëŒ€ë¡œ í•˜ëŠ” $\\theta$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\n\u003cmark\u003e**3. Bayesian Inference**\u003c/mark\u003e\n\nì´ì œ ë§ˆì§€ë§‰ ë°©ë²•ìœ¼ë¡œ ì œì‹œë˜ëŠ” Bayesian Inferenceì´ë‹¤. ì´ëŠ” ëŒ€ê²Œ Bayesian Estimationì´ë¼ê³  ë§ì´ ë¶ˆë¦¬ëŠ” ê²ƒ ê°™ë‹¤. ì´ì „ê¹Œì§€ MLE, MAPëŠ” ê²°êµ­ ì£¼ì–´ì§„ ì‹ì„ ìµœëŒ€ë¡œ í•˜ëŠ” í™•ì •ì  $\\theta$ í•˜ë‚˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆë‹¤.\n\nBayesian InferenceëŠ” Datasetì´ ì£¼ì–´ì¡Œì„ ë•Œ, $\\theta$ì˜ í‰ê· ê°’ì„ í™œìš©í•œë‹¤. ë” ìì„¸íˆ ë§í•˜ë©´, Posteriori(ì‚¬í›„ í™•ë¥ )ì˜ í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.  \nì´ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ë©´ ì´í•´í•˜ëŠ”ë° ë„ì›€ì´ ë  ê²ƒì´ë‹¤. í•œ ë²ˆ ì‚´í´ë³´ì.\n\n$$\n\\begin{align*}\n\\hat{\\theta}_{BE}\u0026= E[\\theta|\\mathcal{D}] \\\\\n\u0026= {\\int_{0}^{1}{{\\theta}p(\\theta|\\mathcal{D})}d\\theta} \\\\\n\u0026= {\\int_{0}^{1}{\\theta}{{p(\\mathcal{D}|\\theta)p(\\theta)}\\over{p(\\mathcal{D})}}d\\theta} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)p(\\mathcal{D}|\\theta)}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{p(\\mathcal{D})}} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\mathcal{D}|\\theta)p(\\theta)}d\\theta}} \\\\\n\u0026= {{\\int_{0}^{1}{\\theta}{p(\\theta)\\prod_{i=1}^{n}{p(x_i|\\theta)}}d\\theta}\\over{\\int_0^1{p(\\theta)\\prod_{i=1}^{n}p(x_i|\\theta)}d\\theta}} \\\\\n\\end{align*}\n$$\n\nì´ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì€ ì´ì „ê³¼ëŠ” ë‹¤ë¥´ê²Œ ìƒëŒ€ê°’ì´ ì•„ë‹Œ í‰ê· ì„ êµ¬í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— posteriori(ì‚¬í›„ í™•ë¥ ,$p(\\theta|\\mathcal{D})$)ë¥¼ êµ¬í•´ì•¼ í•œë‹¤.\n\ní•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ì¡ê¸°ìˆ ì´ í•˜ë‚˜ ì¡´ì¬í•œë‹¤. ë°”ë¡œ **Conjugate Prior**ì´ë‹¤.\n\në°”ë¡œ ë‘ í™•ë¥  ë¶„í¬ í•¨ìˆ˜(likelihood, prior)ì— ì˜í•œ posteriorì˜ í˜•íƒœê°€ ì •í•´ì§„ ê²½ìš°ê°€ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\n\n| Prior $p(\\theta \\mid \\alpha)$  | Likelihood $p(\\mathcal{D} \\mid \\theta)$                 | Posterior $p(\\theta \\mid \\mathcal{D}, \\alpha)$                                                                                                                                                                                                                      | Expectation of Posterior                                                                                                                                                         |\n| :----------------------------- | :------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Beta ($\\alpha, \\beta$)         | Benoulli ($\\sum _{i=1}^{n}x_{i}$)                       | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +n-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                                               | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + n}}$                                                                                                                     |\n| Beta ($\\alpha, \\beta$)         | Binomial ($\\sum _{i=1}^{n}N_{i}, \\sum _{i=1}^{n}x_{i}$) | Beta ($\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}$)                                                                                                                                                                            | ${{\\alpha + \\sum _{i=1}^{n}x_{i}}\\over{\\alpha + \\beta + \\sum _{i=1}^{n}N_{i}}}$                                                                                                  |\n| Gaussian ($\\mu_0, \\sigma_0^2$) | Gaussian ($\\mu, \\sigma^2$)                              | Gaussian (${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu _{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum _{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right),\\left({\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}\\right)^{-1}}$) | ${\\displaystyle {\\frac {1}{{\\frac {1}{\\sigma _{0}^{2}}}+{\\frac {n}{\\sigma ^{2}}}}}\\left({\\frac {\\mu _{0}}{\\sigma _{0}^{2}}}+{\\frac {\\sum _{i=1}^{n}x_{i}}{\\sigma ^{2}}}\\right)}$ |\n\nì´ë¥¼ ì´ìš©í•˜ë©´, ìš°ë¦¬ëŠ” ê°„ë‹¨í•˜ê²Œ Posterioriì˜ í‰ê· ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n  ","slug":"ml-parametric-estimation","date":"2022-10-15 11:25","title":"[ML] 1. Parametric Estimation","category":"AI","tags":["ML","MLE","MAP","Bayesian"],"desc":"Machine Learningì€ íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„°ë¡œ ë¶€í„° pattern ë˜ëŠ” ê°€ì • ë“±ì„ ìœ ë„í•´ë‚´ëŠ” ë°©ë²•ì´ë‹¤.ì´ë¥¼ ìœ„í•œ ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì€ ì—¬ëŸ¬ ê°œì˜ í™•ë¥ ë¶„í¬ì™€ ì´ê²ƒì˜ parameterì˜ ì¡°í•©(probabilistic model)ë“¤ ì¤‘ì—ì„œ ì¸¡ì •ëœ ë°ì´í„°ë“¤ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” í•˜ë‚˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤.ê·¸ ì¤‘ì—ì„œ, í™•ë¥  ë¶„í¬ë¥¼ ê²°ì •í•œ ìƒíƒœì—ì„œ parameterë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” í˜•íƒœì˜ ì ‘ê·¼ë²•ì„ ìš°ë¦¬ëŠ” Parametric Estimationì´ë¼ê³  í•œë‹¤. ê·¸ ì™¸ì—ë„ Nonparametric, Semi-parametric ë°©ì‹ë„ ì¡´ì¬í•˜ì§€ë§Œ ì´ëŠ” ì—¬ê¸°ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"Bayesian"}},"__N_SSG":true},"page":"/tags/[subject]","query":{"subject":"Bayesian"},"buildId":"MaZpmJYXKLLgn_2WNS5lL","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>