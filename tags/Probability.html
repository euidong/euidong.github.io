<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Network 분야에 관심이 많은 개발자로 Computer Engineering 관련 Posting을 주로 다룹니다."/><meta property="og:description" content="Network 분야에 관심이 많은 개발자로 Computer Engineering 관련 Posting을 주로 다룹니다."/><meta property="og:type" content="blog"/><meta property="og:site_name" content="JustLog"/><meta property="og:image" content="https://euidong.github.io/logo192.png"/><title>#Probability | JustLog</title><meta property="og:title" content="#Probability | JustLog"/><link rel="canonical" href="https://euidong.github.io/tags/Probability"/><meta property="og:url" content="https://euidong.github.io/tags/Probability"/><meta name="next-head-count" content="11"/><link rel="icon" href="https://euidong.github.io/favicon.png"/><link rel="apple-touch-icon" href="https://euidong.github.io/logo192.png"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7452732177557701" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/css/d4ec5c8b3df09443.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d4ec5c8b3df09443.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6dc16d084a5153e5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6dc16d084a5153e5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-81da43a8dcd978d9.js" defer=""></script><script src="/_next/static/chunks/main-7b6c38cbad60dfcf.js" defer=""></script><script src="/_next/static/chunks/pages/_app-39c1851cc8b7677c.js" defer=""></script><script src="/_next/static/chunks/675-ae8e8a351ce30ae2.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Bsubject%5D-2ecacdd7ae5454e8.js" defer=""></script><script src="/_next/static/1ehB7ZNShCf4mFVo2JXt6/_buildManifest.js" defer=""></script><script src="/_next/static/1ehB7ZNShCf4mFVo2JXt6/_ssgManifest.js" defer=""></script><script src="/_next/static/1ehB7ZNShCf4mFVo2JXt6/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_wrapper__dKJSz root"><header class="Layout_header__XosLl" style="position:sticky"><div><button tabindex="1" class="SideBarToggler_search_bar_toggler__CEuUg"><svg stroke="currentColor" fill="none" stroke-width="0" viewBox="0 0 24 24" height="35px" width="35px" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></button><nav class="SideBar_side_bar__wrapper--close__8Nwnr"><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/">Home</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/tags">Tags</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Algorithm">Algorithm<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Computer%20Architecture">Computer Architecture<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->7<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Tech">Tech<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->16<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Web">Web<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Network">Network<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/AI">AI<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Paper">Paper<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->1<!-- -->)<!-- --></span></a></nav></div><a class="Logo_logo___yD0t" tabindex="1" href="/"></a><div><button class="SearchBarToggler_search_bar_toggler__3dHbA" tabindex="2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg></button></div></header><section><div class="RowCard_row_card__list__background___xFj5"><h1 class="RowCard_row_card__list__title__t4a2h"> Probability</h1><label class="RowCard_row_card__list__select__wrapper__TZ4_9"><select class="RowCard_row_card__list__select__dxkxA"><option class="RowCard_row_card__list__select__option__GRKZU">최신순<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">AtoZ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">ZtoA<!-- --></option></select></label><ul class="RowCard_row_card__list__wrapper__5Gtgi"><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-base-knowledge"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 0. Base Knowledge" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 0. Base Knowledge" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-base-knowledge">[ML] 0. Base Knowledge</a><div class="RowCard_row_card__tray__date__3cY_j">2022년 10월 14일 19시 28분</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Probability"># <!-- -->Probability<!-- --></a></ul></div></div></ul></div></section><footer class="Layout_footer__EL5v8"><div class="Layout_footer__copyright__r5baC"><span>Copyright © euidong</span><br/><span>모든 컨텐츠에 대한 저작권은 작성자에게 존재합니다. <!-- --><br/>불법 복제를 통한 상업적 사용을 절대적으로 금지합니다. <!-- --><br/>단, 비상업적 이용의 경우 출처 및 링크를 적용한다면 자유롭게 사용가능 합니다.<!-- --></span><span>Also I use photos by<!-- --> <!-- --><a href="https://unsplash.com/@lorenzoherrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Lorenzo Herrera</a> <!-- -->on<!-- --> <!-- --><a href="https://unsplash.com/s/photos/tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Unsplash</a></span></div><div class="Layout_footer__contents__YZWSm"><a class="Layout_footer__contents__link__K_TKH" href="https://github.com/euidong" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>github</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://euidong.github.io/portfolio" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M6 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm-5 6s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H1zM11 3.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 0 1h-4a.5.5 0 0 1-.5-.5zm.5 2.5a.5.5 0 0 0 0 1h4a.5.5 0 0 0 0-1h-4zm2 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2zm0 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2z"></path></svg><span>portfolio</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://chrome.google.com/webstore/detail/bonfire/nkooidijgbppkojdgkoafcoppnohdfka?hl=ko" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 16c-1.066-2.219-0.498-3.49 0.321-4.688 0.897-1.312 1.129-2.61 1.129-2.61s0.706 0.917 0.423 2.352c1.246-1.387 1.482-3.598 1.293-4.445 2.817 1.969 4.021 6.232 2.399 9.392 8.631-4.883 2.147-12.19 1.018-13.013 0.376 0.823 0.448 2.216-0.313 2.893-1.287-4.879-4.468-5.879-4.468-5.879 0.376 2.516-1.364 5.268-3.042 7.324-0.059-1.003-0.122-1.696-0.649-2.656-0.118 1.823-1.511 3.309-1.889 5.135-0.511 2.473 0.383 4.284 3.777 6.197z"></path></svg><span>chat</span></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"content":"\n## Intro\n\nMachine Learning은 data들로 부터 특정 pattern을 나타내는 function을 만드는 것이라고 할 수 있다. 즉, pattern은 data에 대한 간단한 요약본이라고 볼 수 있다.\n확률/통계 이론 및 선형대수/미적분 관련 기본을 해당 포스팅에 정리한다. 여기서 다루는 내용은 대게 많이 추상적인 내용이며, 키워드 중심의 내용이다. 만약, 추가적인 설명이 필요하다면 키워드를 기반으로 더 검색을 하는 것이 좋을 것이다.\n\n## Probability/Statisics\n\n### Probability Space\n\n확률 공간을 정의하는 것은 확률을 이해하는 토대가 된다. 확률을 적용하기 위한 공간을 먼저 살펴보자.\n\n- Sample Space($\\Omega$)  \n  가능한 모든 결과값의 집합이다.  \n  ex. 동전을 두 번을 던져서 나올 수 있는 모든 결과값은 $\\Omega = $ $\\{ hh, ht, th, tt \\}$\n- Event($E$)  \n  Sample Space의 Subset이다. Sample Space에서 발생할 수 있는 event라는 의미로 볼 수 있다.  \n  ex. 동전을 두 번을 던져서 모두 같은 면이 나오는 Event는 $E = $ $\\{ hh, tt \\}$\n- Field($\\mathcal{F}$)  \n  Sample Space에서 발생 가능한 모든 Event들의 집합이다.  \n  ex 동전을 두 번 던져서 나오는 결과값의 Field는 $\\mathcal{F} = $ $\\{$ $\\emptyset$, $\\{hh\\}$, $\\{ht\\}$, $\\{th\\}$, $\\{tt\\}$, $\\{hh, ht\\}$, $\\{hh, th\\}$, $\\{hh, tt\\}$, $\\{ht, th\\}$, $\\{ht, tt\\}$, $\\{th, tt\\}$, $\\{hh, ht, th\\}$, $\\{hh, ht, tt\\}$, $\\{hh, th, tt\\}$, $\\{ht, th, tt\\}$, $\\{hh, ht, th, tt\\}$ $\\}$\n- $\\sigma$-field  \n  자신 내부의 원소를 포함하는 합집합을 모두 포함하는 셀 수 있는 field를 sigma field라고 한다.  \n  이 $\\sigma$-field는 일반적인 확률과 특정 domain에서의 확률을 정의하는데 필요하다.  \n  우리가 sample space($\\Omega$)와 $\\sigma$-field $\\mathcal{F} \\subset 2^{\\Omega}$가 주어질 때, 확률P가 다음과 같이 mapping한다고 하자. $P: \\mathcal{F} \\mapsto [0, 1]$ 이때 P는 다음과 같은 특징을 가진다.\n  - $A \\in \\mathcal{F}$인 모든 A에 대해서 $P(A) \\leq 0$ 이다.  \n    $P(\\emptyset) = 0, P(\\Omega) = 1$\n  - $\\{A_i\\}_{i \\in I}$이고, 서로 다른 모든 i, j에 대해 $ A_{i}\\cup A_{j} = \\emptyset$이라면, 아래 식을 만족한다.  \n    $$P(\\cup_{i \\in I}A_i) = \\sum_{i \\in I}P(A_i)$$\n\n### Important properties of Probability\n\n- **Joint Probability**  \n  두 Event의 Joint Probability는 두 Event의 합집합의 확률을 의미한다.\n  $P(A, B) = P(A \\cap B)$\n- **Marginal Probability**  \n  대게 두 개 이상의 Event가 있을 때, 각 각의 Event의 확률을 특정할 때 사용한다.\n  $P(A), P(B)$\n- **Independence**  \n  두 Event가 독립이라는 의미는 서로의 Event가 서로 영향을 받지 않는다는 의미이다. 주의할 것은 이것이 의미하는 것이 두 Event의 교집합이 없다는 의미가 아니다.  \n  만약, 우리가 위에서 예시로 사용한 두 개의 동전을 던진 결과를 보자. 두 개의 동전이 모두 앞면이 나오는 경우와 모두 뒷면이 나오는 경우는 서로 독립일까? 이는 독립이 아니다. 왜냐하면, 동전이 모두 앞면이 나오는 사건은 필연적으로 모두 뒷면이 나오는 사건은 반드시 일어나지 않을 것이라는 증거가 되기 때문이다. 반대로, 모두 앞면이 나오는 사건과 한 번만 앞면이 나오는 사건을 생각해보자. 하나의 사건이 일어났다고, 반드시 그 사건이 일어났거나 안일어났다는 관계를 밝혀낼 수 없다. 따라서, 이러한 경우 두 사건이 독립적이라고 한다.  \n  이를 확률적으로 표현하면, 다음과 같이 표현할 수 있다.  \n  $P(A, B)=P(A)P(B)$\n- **Conditional Probability**  \n  두 Event가 있을 때, 하나의 Event가 발생했을 때 다른 하나의 Event가 발생할 확률을 의미한다. 따라서, 이는 다음과 같이 수식으로 표현할 수 있다.  \n  $P(A|B) = {{P(A, B)}\\over{P(B)}}, (P(B) \\neq 0)$  \n  여기서 independence 특성을 더 명확하게 확인할 수 있는데, 만약 A와 B가 독립이라면,  \n  $P(A|B) = P(A)$  \n  즉, B가 발생했는지 여부는 A의 결과에 영향을 안준다는 것이다.\n- **Partition**  \n  Sample Space($\\Omega$)를 겹치지 않고, 모두 포함하는 Event의 집합을 의미한다. 따라서, 이를 식으로 다음과 같이 표현할 수 있다.  \n  $\\cup_{i=1}^{n}{P_i} = \\Omega$ 이고, $\\cap_{i=1}^{n}{P_i} = \\emptyset$\n- **Marginalization**  \n  전체 Sample space($\\Omega$)에 대하여 **B**가 이에 대한 partition일 때, 아래 공식이 성립한다.  \n  $P(A) = \\sum_{i=1}^{n}{P(A,B_i)} = \\sum_{i=1}^{n}{P(A|B_i)P(B_i)}$\n- **Bayes' Theorem**  \n  만약 $P(B) \\neq 0$라면, 아래 공식이 성립한다. 간단히 conditional probability를 풀어주면 아래 식을 얻을 수 있다.  \n  $P(A|B) = {P(B|A)P(A)\\over{P(B)}}$  \n  해당 식은 단순히 Joint Probability로 변환하고, 다시 반대 확률로 변경했을 뿐이다. 이 공식이 중요하다기 보다는 이 공식이 가지는 의미를 이해하는 것이 중요하다. 경험에 기반하는 Frequentist Approach에서는 관측을 통해서 특정 데이터가 발생할 확률을 얻는다. 만약 우리가 원하는 확률이 관측을 통해서는 얻을 수 없는 데이터라고 하자. 이 경우에 우리는 확률의 역연산이 필요하다. 위의 공식을 보면 특이한 것이 보이는데, 바로 $P(A|B)$와 $P(A)$이다. 이는 전체 확률을 통해서 Conditional Probability를 찾는 것이다. 그렇기에 우리는 이를 역연산이라고 부르며, 우리가 가지고 있는 기존 사전 확률(Priority, 이전까지 맞을 거라고 생각한 확률)을 통해서 데이터가 주어졌을 때의 사건의 확률을 다시 계산해보는 것이다. 이 과정을 Bayesian Update라고 하는데 이 과정을 통해서 얻은 P(A|B)를 다시 다음 데이터에 대해서는 P(A)로써 활용하는 것이다. 이렇게 해서 우리는 점진적으로 P(A)를 찾아나갈 수 있다.\n\n### Random Variable\n\nRandom Variable이라는 것은 특정 사건을 수학적으로 표현하기 위해서 변형하는 과정을 의미한다. 우리는 이전 예시에서 두 개의 동전을 동시에 던져서 나온 결과를 Sample Space로 두었고, 이를 $\\Omega = $ $\\{ hh, ht, th, tt \\}$라고 표현했다. 하지만, 이와 같은 표기 방식은 수학적인 연산을 적용하기 어렵다. 따라서, 우리는 앞면이 나온 경우를 $X=1$, 뒷면이 나온 경우를 $X=-1$ 라고 하는 형태로 치환하는 것이다. 여기서 만들어진 X를 우리는 Random Variable이라고 부른다. 이런 치환을 통해서 우리는 확률을 Random Variable에 대한 함수로 표현할 수 있다.\n\n또, Random Variable을 정의하여 다음과 같은 값을 연속적으로 정의할 수 있다.\n\n- **Mean**  \n  Random Variable의 평균 또는 기댓값이라고 부른다.  \n  $\\mu_{X} = E[X] = \\sum_{x}{xP(X=x)}$\n- **Variance**  \n  평균에서 데이터가 떨어진 정도를 표현하는 값으로 분산이라고 부른다.  \n  $\\sigma_{X}^{2} = E[(X-\\mu_X)^2] = E[X^2] -\\mu_{X}^{2}$\n- **Covariance**  \n  Random Variable X와 Y의 상관관계(Correlation)을 확인하는 척도로 사용한다.  \n  $\\sigma_{X}^{2} = E[(X-\\mu_X)^2] = E[X^2] -\\mu_{X}^{2}$  \n  만약, 두 X와 Y가 서로 전혀 상관이 없다(Independent)면, $cov(X, Y) = 0$이다.\n- **Correlation Coefficient**  \n  Covariance보다 더 엄격한 상관관계를 확인하는 척도로 사용되는데, 단순히 Covariance를 각 표준편차($\\sigma$)로 나눈 것이다. 이로 인해 결과 값은 [-1, 1] 사이 값이 된다.  \n  $corr(X, Y) = {cov(X,Y)\\over{\\sigma_{X}\\sigma_{Y}}}$  \n  주의할 점은 Correlation Coefficient가 1이라고 X가 Y의 원인이 되는 것은 아니라는 것을 유의해야 한다. 단순히 X가 일어났을 때, Y가 일어날 확률이 높다는 것이다.  \n\n### Law of Large Numbers\n\n경험적 확률과 수학적 확률 사이의 관계를 나타내는 법칙으로, 전체 경우의 수와 이에 따른 확률(모집단)에서 표본(관측한 경우의 수와 이에 따른 확률)의 크기가 커질 수록 표본 평균이 모평균에 가까워짐을 의미한다.\n\n### 자주 사용되는 Probability Distribution Function\n\n- **Bernoulli distribution**  \n  하나의 사건이 일어날 확률을 의미한다. 발생하는 경우를 X=1, 그렇지 않은 경우를 X=0으로 random variable로 치환하여 나타낸 확률 분포(probability distribution)이다.  \n  따라서, 사건이 일어날 확률을 p라고 할 때, 다음과 같이 random variable에 대한 확률을 정의할 수 있다.  \n  $P(X=x) = p^{x}(1-p)^{1-x}$ \n  복잡해보이지만, 실상은 X가 0 또는 1이므로, $P(X=0)=1-p$이고, $P(X=1)=p$이다.\n  - 평균      \n    $E[X] = p$\n  - 분산  \n    $Var[X] = E[X^2] - \\mu_{X}^2 = p - p^2 = p(1-p)$\n- **Binomial Distribution**  \n  확률이 p인 사건을 n번 수행했을 때, x번 발생할 확률을 의미한다. 따라서, random variable X의 범위는 {0, 1, …, n}이 된다.  \n  이에 따라 random variable에 대한 확률을 정의하면 다음과 같다.  \n  $P(X=x) = {n \\choose x}p^x(1-p)^{n-x}$  \n  이 또한 복잡해 보이지만, 사실은 독립적인 Bernoulli의 연속 수행으로 볼 수 있다.  \n  - 평균  \n    $E[X] = np$\n  - 분산  \n    $Var[X] = Var[\\sum_{i}X_i]=\\sum_iVar[X_i]=np(1-p)$\n- **Beta Distribution**  \n  $\\alpha, \\beta \u003e 0$를 만족하는 두 parameter를 이용한 probability distribution이다.  \n  이는 [0, 1]에서 continuous한 random variable를 이용할 수 있다. 이에 따른 확률은 다음과 같다.  \n  $P(X=x) \\propto x^{\\alpha-1}(1-x)^{\\beta-1}$  \n  이에 대한 의미를 이해하자면, 확률에 대한 확률 분포이다. 각 $\\alpha - 1$와 $\\beta - 1$를 성공 횟수, 실패 횟수라고 하자.  이는 이미 알고 있는 모집단(전체 집합)의 계산 결과이다. 그리고 random variable을 특정 event의 확률이라고 하자. 예를들면, 동전 던지기를 할 때, 앞면이 나올 확률이 $1\\over2$이라는 것을 이미 알고 있다. 따라서, 우리는 $\\alpha - 1$ = $\\beta - 1$ 라는 것을 알고 있는 것이다. 하지만, 실제로 동전 던지기를 5번 수행했을 때, 4번 앞면이 나왔다고 하자. 그렇다면, 우리가 추측한 해당 event의 확률은 $4\\over5$이 된다. 그렇다면, 실제로 해당 확률이 $4\\over5$일 확률을 얼마나 될까?  \n  이를 측정하기 위한 것이 Beta distribution인 것이다. 이에 따라, Beta distribution을 PDF로 표현하면 ${\\alpha\\over\\alpha+\\beta}$에서 높은 확률값을 가지는 것을 볼 수 있다.\n  - 평균  \n    $E[X] = {\\alpha\\over{\\alpha+\\beta}}$\n  - 분산  \n    $Var[X] = {\\alpha\\beta\\over{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}}$\n  \n  위의 평균과 분산을 보면 알 수 있듯이, 만약 이전 모집단에서의 평균값에 대한 믿음이 크다면, 각각  $\\alpha, \\beta$의 비율은 유지하면서 상수배를 수행하여 평균은 동일하지만 분산 값을 더 적게 만들어 뾰족한 형태의 분포를 완성할 수도 있다. 이 경우에는 평균과 맞지 않는 표본집합에서의 평균을 굉장히 확률이 낮은 확률로 식별하는 것이다.\n- **Gaussian Distribution**  \n  $\\mu, \\sigma^2$를 parameter로 갖는 probability distribution이다.\n    \n  이는 $[-\\infin, \\infin]$를 구간으로 continuous한 random varible을 이용한다. 이에 따른 확률은 다음과 같다. (단일 random variable인 경우)\n  \n  $P(X) = {1\\over{\\sqrt{2\\pi\\sigma^2}}}\\exp(-{1\\over{2\\sigma^2}}(X-\\mu)^2)$\n\n  이것이 가지는 의미는 다소 복잡하다. (Lindeberg-Levy) Central Limit Theoriem에 따르면, 표본에서 얻은 표본 평균을 구하면 구할 수록 점점 Gaussian Distribution을 따라간다. 즉, $n \\rarr \\infin$이면, 표본 평균이 이루는 분포가 Gaussian이라는 것이다.\n  \n  추가적으로 알아볼 것은 바로 여러 개의 Random Variable로 Gaussian Distribution을 더 높은 차원으로 구성할 수 있다는 것이다. 이를 수행하고, 해당 Random Variable 간의 Covarince, Correlation coefficient를 구하면, 이 변수 간의 상관성을 얻을 수 있다.\n  \n  - 유의 사항 : correlation은 어디까지나 상관성이다. correlation이 높다고 해당 Random Variable이 상관성이 높은 Random Variable 발생의 원인이 될 수는 없다.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n","slug":"ml-base-knowledge","date":"2022-10-14 19:28","title":"[ML] 0. Base Knowledge","category":"AI","tags":["ML","Probability"],"desc":"Machine Learning은 data들로 부터 특정 pattern을 나타내는 function을 만드는 것이라고 할 수 있다. 즉, pattern은 data에 대한 간단한 요약본이라고 볼 수 있다.확률/통계 이론 및 선형대수/미적분 관련 기본을 해당 포스팅에 정리한다. 여기서 다루는 내용은 대게 많이 추상적인 내용이며, 키워드 중심의 내용이다. 만약, 추가적인 설명이 필요하다면 키워드를 기반으로 더 검색을 하는 것이 좋을 것이다.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"Probability"}},"__N_SSG":true},"page":"/tags/[subject]","query":{"subject":"Probability"},"buildId":"1ehB7ZNShCf4mFVo2JXt6","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>