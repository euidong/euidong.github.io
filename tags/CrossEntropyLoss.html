<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta property="og:type" content="blog"/><meta property="og:site_name" content="JustLog"/><meta property="og:image" content="https://euidong.github.io/logo192.png"/><title>#CrossEntropyLoss | JustLog</title><meta name="description" content="#CrossEntropyLoss ê´€ë ¨ Posting"/><meta property="og:description" content="#CrossEntropyLoss ê´€ë ¨ Posting"/><meta property="og:title" content="#CrossEntropyLoss | JustLog"/><link rel="canonical" href="https://euidong.github.io/tags/CrossEntropyLoss"/><meta property="og:url" content="https://euidong.github.io/tags/CrossEntropyLoss"/><meta name="next-head-count" content="11"/><link rel="icon" href="https://euidong.github.io/favicon.png"/><link rel="apple-touch-icon" href="https://euidong.github.io/logo192.png"/><link rel="preload" href="/_next/static/css/d4ec5c8b3df09443.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d4ec5c8b3df09443.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6dc16d084a5153e5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6dc16d084a5153e5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" id="Adsense-id" data-ad-client="ca-pub-7452732177557701" async="" defer="" data-nscript="beforeInteractive"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-81da43a8dcd978d9.js" defer=""></script><script src="/_next/static/chunks/main-7b6c38cbad60dfcf.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8c8de51645108dcb.js" defer=""></script><script src="/_next/static/chunks/675-ae8e8a351ce30ae2.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Bsubject%5D-0fc8f67b45fbbd0f.js" defer=""></script><script src="/_next/static/AJoIMuIX0_fPF45qowUMP/_buildManifest.js" defer=""></script><script src="/_next/static/AJoIMuIX0_fPF45qowUMP/_ssgManifest.js" defer=""></script><script src="/_next/static/AJoIMuIX0_fPF45qowUMP/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_wrapper__dKJSz root"><header class="Layout_header__XosLl" style="position:static"><div><button tabindex="1" class="SideBarToggler_search_bar_toggler__CEuUg"><svg stroke="currentColor" fill="none" stroke-width="0" viewBox="0 0 24 24" height="35px" width="35px" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></button><nav class="SideBar_side_bar__wrapper--close__8Nwnr"><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/">Home</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/tags">Tags</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Algorithm">Algorithm<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Computer%20Architecture">Computer Architecture<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->7<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Tech">Tech<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->17<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Web">Web<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Paper">Paper<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->2<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Network">Network<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/AI">AI<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->19<!-- -->)<!-- --></span></a></nav></div><a class="Logo_logo___yD0t" tabindex="1" href="/"></a><div><button class="SearchBarToggler_search_bar_toggler__3dHbA" tabindex="2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg></button></div></header><section><div class="RowCard_row_card__list__background___xFj5"><h1 class="RowCard_row_card__list__title__t4a2h"> Cross Entropy Loss</h1><label class="RowCard_row_card__list__select__wrapper__TZ4_9"><select class="RowCard_row_card__list__select__dxkxA"><option class="RowCard_row_card__list__select__option__GRKZU">ìµœì‹ ìˆœ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">AtoZ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">ZtoA<!-- --></option></select></label><ul class="RowCard_row_card__list__wrapper__5Gtgi"><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-nn"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 6. Neural Network" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 6. Neural Network" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-nn">[ML] 6. Neural Network</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 10ì›” 20ì¼ 09ì‹œ 00ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/NeuralNetwork"># <!-- -->NeuralNetwork<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Perceptron"># <!-- -->Perceptron<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/Backpropagation"># <!-- -->Backpropagation<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/CrossEntropyLoss"># <!-- -->CrossEntropyLoss<!-- --></a></ul></div></div></ul></div></section><footer class="Layout_footer__EL5v8"><div class="Layout_footer__copyright__r5baC"><span>Copyright Â© euidong</span><br/><span>ëª¨ë“  ì»¨í…ì¸ ì— ëŒ€í•œ ì €ì‘ê¶Œì€ ì‘ì„±ìì—ê²Œ ì¡´ì¬í•©ë‹ˆë‹¤. <!-- --><br/>ë¶ˆë²• ë³µì œë¥¼ í†µí•œ ìƒì—…ì  ì‚¬ìš©ì„ ì ˆëŒ€ì ìœ¼ë¡œ ê¸ˆì§€í•©ë‹ˆë‹¤. <!-- --><br/>ë‹¨, ë¹„ìƒì—…ì  ì´ìš©ì˜ ê²½ìš° ì¶œì²˜ ë° ë§í¬ë¥¼ ì ìš©í•œë‹¤ë©´ ììœ ë¡­ê²Œ ì‚¬ìš©ê°€ëŠ¥ í•©ë‹ˆë‹¤.<!-- --></span><span>Also I use photos by<!-- --> <!-- --><a href="https://unsplash.com/@lorenzoherrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Lorenzo Herrera</a> <!-- -->on<!-- --> <!-- --><a href="https://unsplash.com/s/photos/tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Unsplash</a></span></div><div class="Layout_footer__contents__YZWSm"><a class="Layout_footer__contents__link__K_TKH" href="https://github.com/euidong" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>github</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://euidong.github.io/portfolio" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M6 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm-5 6s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H1zM11 3.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 0 1h-4a.5.5 0 0 1-.5-.5zm.5 2.5a.5.5 0 0 0 0 1h4a.5.5 0 0 0 0-1h-4zm2 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2zm0 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2z"></path></svg><span>portfolio</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://chrome.google.com/webstore/detail/bonfire/nkooidijgbppkojdgkoafcoppnohdfka?hl=ko" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 16c-1.066-2.219-0.498-3.49 0.321-4.688 0.897-1.312 1.129-2.61 1.129-2.61s0.706 0.917 0.423 2.352c1.246-1.387 1.482-3.598 1.293-4.445 2.817 1.969 4.021 6.232 2.399 9.392 8.631-4.883 2.147-12.19 1.018-13.013 0.376 0.823 0.448 2.216-0.313 2.893-1.287-4.879-4.468-5.879-4.468-5.879 0.376 2.516-1.364 5.268-3.042 7.324-0.059-1.003-0.122-1.696-0.649-2.656-0.118 1.823-1.511 3.309-1.889 5.135-0.511 2.473 0.383 4.284 3.777 6.197z"></path></svg><span>chat</span></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"content":"\n## Intro\n\nìš°ë¦¬ëŠ” Linear Regression, Logistic Regression, SVMì„ ê±°ì¹˜ë©° dataë¡œ ë¶€í„° ìœ ì˜ë¯¸í•œ patternì„ ë°œê²¬í•˜ëŠ” ê³¼ì •ì„ ì•Œì•„ë³´ì•˜ë‹¤. ì´ ê³¼ì •ì€ ìš°ë¦¬ì—ê²Œ ëª…í™•í•œ ì‹ í•˜ë‚˜ë¥¼ ì œì‹œí•˜ì˜€ê³ , ëª¨ë“  ê³¼ì •ì„ ìš°ë¦¬ê°€ ì œì–´í•  ìˆ˜ ìˆê²Œ í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ë¥¼ ìš°ë¦¬ê°€ ëª¨ë‘ ëª…í™•í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ê²ƒì¸ì§€ëŠ” ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ì´í•´í•˜ì§€ëŠ” ëª»í•˜ì§€ë§Œ, ì•Œì•„ì„œ ìµœì ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê²Œ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ê¹Œ? ì´ëŸ° ë§ˆë²•ê°™ì€ ì¼ì— ëŒ€í•œ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ” ê²ƒì´ Neural Networkì´ë‹¤.\n  ê²Œ ì•Œì§€ ëª»í•˜ì§€ë§Œ inputì´ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ ì²˜ë¦¬í•´ì„œ outputì„ ì „ë‹¬í•˜ëŠ” ì‹œìŠ¤í…œì„ ìš°ë¦¬ì˜ ì‹ ì²´ì—ì„œ ì°¾ê²Œ ëœë‹¤. ë°”ë¡œ ìš°ë¦¬ ëª¸ì„ ì´ë£¨ëŠ” ì‹ ê²½ë§ì´ë‹¤. ì˜ˆì‹œë¡œ ìš°ë¦¬ëŠ” ëˆˆì„ í†µí•´ ë¹›ì´ë¼ëŠ” inputì„ ë°›ìœ¼ë©´, ìš°ë¦¬ ëˆˆê³¼ ë‡Œì—ì„œ ë¬´ìŠ¨ ì¼ì´ ë°œìƒí•˜ëŠ”ì§€ëŠ” ëª¨ë¥´ì§€ë§Œ ê²°ê³¼ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë¬¼ì²´ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. ì´ ê³¼ì •ì„ ì¶”ì¸¡ì˜ ê³¼ì •ì— ë„ì…í•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ?\n\n## Perceptron\n\nPerception(ì¸ì§€ ëŠ¥ë ¥) + Neuron(ì‹ ê²½ ì„¸í¬)ì˜ í•©ì„±ì–´ì´ë‹¤. ì¤‘ê³ ë“±í•™êµ ìƒëª… ìˆ˜ì—…ì„ ë“¤ì—ˆë‹¤ë©´, ìš°ë¦¬ì˜ ëª¨ë“  ì‹ ê²½ì€ ë‰´ëŸ°ì´ë¼ëŠ” ë‹¨ìœ„ ì„¸í¬ë¡œ ì´ë£¨ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ë°°ì› ì„ ê²ƒì´ë‹¤. ì¦‰, ìš°ë¦¬ì˜ ì‹ ê²½ ì„¸í¬ë¥¼ ì»´í“¨í„° ê³µí•™ì—ì„œ í™œìš©í•˜ê¸° ìœ„í•´ì„œ, ìˆ˜í•™ì ìœ¼ë¡œ ë³€í™˜í•œ ê²ƒì´ë‹¤. í˜•íƒœë¥¼ ë¨¼ì € ì‚´í´ë³´ì.\n\n$$\ny = sign(\\bold{w}^{\\top}\\bold{x} + b)\n$$\n\n![nn-perceptron-1](/images/nn-perceptron-1.jpg)\n\nëŒ€ë‹¨í•œ ê²ƒì„ ê¸°ëŒ€í–ˆë‹¤ë©´ ì‹¤ë§í•˜ê² ì§€ë§Œ, simpleí•œ ê²ƒì´ ìµœê³ ë¼ëŠ” ì—°êµ¬ì˜ ì§„ë¦¬ì— ë”°ë¼ì„œ ìœ„ì˜ ì‹ì€ ê½¤ë‚˜ í•©ë¦¬ì ì´ë‹¤. ìš°ë¦¬ê°€ Linear Regressionê³¼ Logistic Regressionì„ ë°°ì› ìœ¼ë‹ˆ ì•Œ ê²ƒì´ë‹¤. ì´ëŠ” ì‚¬ì‹¤ Linear Regressionì„ ì´ìš©í•´ì„œ ìš°ë¦¬ê°€ Classificationì„ ìˆ˜í–‰í•  ë•Œ ì‚¬ìš©í–ˆë˜ ì‹ì´ë‹¤. ì¦‰, perceptron í•˜ë‚˜ëŠ” inputì„ ì„ í˜•ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” decision boundaryë¥¼ ì°¾ëŠ” ê²ƒê³¼ ê°™ë‹¤.\n\n\u003e **Optimization**\n\nê·¸ë ‡ë‹¤ë©´, í•´ë‹¹ perceptronì„ í†µí•´ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒì„ ë§Œì¡±í•˜ëŠ” $\\bold{w}$ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤.\n\n$$\ny_{n} =\n\\begin{cases}\n1  \u0026\\text{ if  } \\bold{x}_{n} \\in \\mathcal{C}_{1} \\\\\n-1 \u0026\\text{ if  } \\bold{x}_{n} \\in \\mathcal{C}_{2} \\\\\n\\end{cases}\n$$\n$$\ny_{n}\\bold{w}^{\\top}\\bold{x}_{n} \\gt 0, \\forall n\n$$\n\nê²°êµ­ Loss í•¨ìˆ˜ëŠ” perceptronì˜ ì˜ëª»ëœ classification ê²°ê³¼ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\mathcal{J}(\\bold{w}) = - \\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{w}^{\\top}\\bold{x}_{n}} \\quad( \\mathcal{M}(\\bold{w}) = \\{ n : y_{n}\\bold{w}^{\\top}\\bold{x}_{n} \\} )\n$$\n$$\n\\nabla_{\\bold{w}}\\mathcal{J}(\\bold{w}) = - \\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{x}_{n}}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Gradient Descentì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\bold{w}_{t+1} = \\bold{w}_{t} + \\alpha\\sum_{n \\in \\mathcal{M}(\\bold{w})}{y_{n}\\bold{x}_{n}}\n$$\n\nê°„ë‹¨í•œ ì˜ˆì‹œë¡œ AND, OR Gateë¥¼ percentronì„ í†µí•´ í‘œí˜„í•´ë³´ì.\n\n![nn-and-gate](/images/nn-and-gate.jpg)\n![nn-or-gate](/images/nn-or-gate.jpg)\n\ní•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ë°ì´í„°ëŠ” í•­ìƒ ì™„ë²½í•˜ê²Œ ì„ ìœ¼ë¡œ ë‚˜ë‰˜ì–´ì§€ì§€ëŠ” ì•ŠëŠ”ë‹¤. í•˜ë‚˜ì˜ perceptronìœ¼ë¡œëŠ” ì•„ë˜ì˜ XORì¡°ì°¨ë„ êµ¬ë¶„í•´ë‚¼ ìˆ˜ ì—†ë‹¤.\n\n![nn-multi-line-example](/images/nn-multi-line-example.jpg)\n\n## Multilayer Perceptron\n\nìœ„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ë‚˜ì˜¨ ê²ƒì´ perceptronì„ ë‹¤ì¸µìœ¼ë¡œ ìŒ“ì•„ì„œ í•´ê²°í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ì œëŠ” í•˜ë‚˜ì˜ ì‹ ê²½ì„¸í¬ì˜€ë˜ perceptronì„ ì§„ì§œ ì‹ ê²½ë§ì²˜ëŸ¼ ì—°ê²°í•´ë³´ìëŠ” ê²ƒì´ë‹¤.\n\në¨¼ì € ì¶”ìƒì ì¸ ì˜ˆì‹œë¥¼ ìƒê°í•´ë³´ì. ìš°ë¦¬ê°€ XOR Gateë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ì–´ë–¤ Gateë¥¼ ê²°í•©í•´ì•¼í• ê¹Œ?\n\n$$\na \\oplus b = ab + \\bar{a}\\bar{b}\n$$\n\nìš°ë¦¬ëŠ” AND Gate 2ê°œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³ , í•´ë‹¹ ê²°ê³¼ê°’ì„ ì´ìš©í•´ì„œ OR Gate ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë©´ XOR Gateë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ê° GateëŠ” ìš°ë¦¬ê°€ perceptronìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì—ˆëŠ”ë° ê·¸ëƒ¥ ì´ê²ƒì„ gateë¡œ í‘œí˜„í•˜ë“¯ì´ ë˜‘ê°™ì´ ë‚˜íƒ€ë‚´ë©´ í’€ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?\n\nê·¸ë˜ì„œ ì§ì ‘ ìˆ˜í–‰í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n![nn-xor-gate](/images/nn-xor-gate.jpg)\n\n```plaintext\n ğŸ¤” Insight\n\n ìœ„ì˜ ê³¼ì •ì„ ë³´ë‹¤ë³´ë©´ ë†€ë¼ìš´ ê²ƒì„ í•˜ë‚˜ ë°œê²¬í•  ìˆ˜ ìˆë‹¤. ë°”ë¡œ ì™¼ ìª½ ê·¸ë¦¼ì˜ ë³€í™”ì´ë‹¤. \n ì²«ë²ˆì§¸, ë‘ ê°œì˜ perceptronì„ í†µí•´ì„œ ë§Œë“¤ì–´ì§„ outputì´ ì´ë£¨ëŠ” ê²°ê³¼ê°’ì˜ í˜•íƒœë¡œ featureë¥¼ ë³€í™˜í•˜ë©´, \n í•˜ë‚˜ì˜ perceptronìœ¼ë¡œ decision boundaryë¥¼ ê·¸ë¦´ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. \n ì´ëŠ” ë§ˆì¹˜ ì´ì „ linear regressionì—ì„œ ë°°ì› ë˜ basis function(Ï•)ì´ í–ˆë˜ ì—­í• ì´ë‹¤.\n\n ê·¸ë ‡ë‹¤ë©´, ì´ë¥¼ ë”ìš± í™•ì¥í•´ë³´ì. \n ë§Œì•½ í•´ë‹¹ Layerê°€ ë” ê¹Šì–´ì§„ë‹¤ê³  í•´ë„, ì¶œë ¥ ì§ì „ì˜ layerëŠ” ë‹¨ìˆœíˆ ì´ì „ ëª¨ë“  layerëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ê°€ê³µí•´ì„œ\nfeatureë¥¼ ë³€í™˜í•˜ëŠ” í•˜ë‚˜ì˜ basis function(Ï•)ë¥¼ ì·¨í•œ ê²ƒìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n```\n\nê²°ë¡ ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë” ë³µì¡í•˜ê³ , ì–´ë ¤ìš´ ë¬¸ì œì˜ ê²½ìš°ì—ë„ ë” ê¹Šê²Œ ì‹ ê²½ë§ì„ êµ¬ì„±í•˜ë©´ ê²°êµ­ì€ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n\u003e **Universal Approximation Theorem**\n\nìœ„ì™€ ê°™ì€ ê¹Šì€ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì´ìš©í•˜ìëŠ” ì£¼ì¥ë„ ìˆì§€ë§Œ, ì´ì™€ ìœ ì‚¬í•˜ê²Œ ë„“ì€ ì‹ ê²½ë§ì„ ì“°ìëŠ” ì£¼ì¥ë„ ì¡´ì¬í–ˆë‹¤.  \n\n![nn-universal-approx-theorem-1](/images/nn-universal-approx-theorem-1.jpg)\n\në§Œì•½, ìš°ë¦¬ê°€ í•˜ë‚˜ì˜ Layerì™€ outputì—ì„œ ìµœì¢… output perceptronë§Œ ê°–ê³  ì²˜ë¦¬ë¥¼ í•œë‹¤ë©´, ê²°êµ­ ì—¬ëŸ¬ perceptronì˜ weighted í•©ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ ê²½ìš° ìš°ë¦¬ëŠ” ê³„ë‹¨ í•¨ìˆ˜ì˜ weighted í•©ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆëŠ”ë° perceptronì´ ë§ì•„ì§ˆ ìˆ˜ë¡ ì´˜ì´˜í•´ì§€ë©° ì •ë‹µê³¼ ìœ ì‚¬í•œ ì¶”ë¡ ì´ ê°€ëŠ¥í•´ì§„ë‹¤.(ë§ˆì¹˜ ì ë¶„ì˜ ê°œë…ê³¼ ìœ ì‚¬í•˜ë‹¤. ë¬¼ë¡  ì´ëŠ” ì¶”ìƒì ì¸ ì„¤ëª…ì´ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œëŠ” ê³„ë‹¨í•¨ìˆ˜ì˜ í•©ì´ê¸° ë•Œë¬¸ì— ì¢€ ë‹¤ë¥´ë‹¤.)\n\n![nn-universal-approx-theorem-2](/images/nn-universal-approx-theorem-2.jpg)\n\nìœ„ì˜ ê·¸ë¦¼ì„ ë³´ë©´ ì´í•´í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ì´ ë°©ì‹ì€ ê²°êµ­ ëª¨ë“  í•¨ìˆ˜ í˜•íƒœë¥¼ ê¸°ì–µí•˜ëŠ” ê²ƒì´ë‹¤.(**memorizer**) ì´ê²ƒì€ input dataê°€ ë§ì•„ì§ˆ ìˆ˜ë¡ ë³µì¡ë„ê°€ ê¸‰ê²©í•˜ê²Œ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµê³¼ ì˜ˆì¸¡ê³¼ì •ì— êµ‰ì¥íˆ ë§ì€ ì‹œê°„ì„ ì†Œëª¨í•œë‹¤.\n\n\u003e **Multilayer Optimization(Backpropagation)**\n\nê·¸ë ‡ë‹¤ë©´, ë„“ì€ ì‹ ê²½ë§ì´ í•œê³„ê°€ ìˆìœ¼ë‹ˆ ì„ íƒì§€ëŠ” inputê³¼ output ì‚¬ì´ì˜ layer(**hidden layer**)ì˜ ê°¯ìˆ˜ë¥¼ ëŠ˜ë ¤ì„œ ê¹Šì€ ì‹ ê²½ë§ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ê³  ìˆëŠ” perceptronì€ signí•¨ìˆ˜ë¡œ ê°ì‹¸ì ¸ìˆê¸° ë•Œë¬¸ì— ë¯¸ë¶„ ì‹œì— ê¸°ìš¸ê¸°ê°€ 0ì´ë¼ëŠ” ë¬¸ì œë¥¼ ê°–ëŠ”ë‹¤. ë˜í•œ, ê·¸ë ‡ë‹¤ê³  ì •ë‹µì˜ ê°¯ìˆ˜ë¥¼ ì´ìš©í•˜ê¸°ì—ëŠ” ê° perceptronì˜ ì˜í–¥ì„ ì „ë‹¬í•˜ê¸°ì— ë¶€ì¡±í•˜ë‹¤ëŠ” ê²ƒì´ ëª…í™•í•˜ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” perceptronì— ìˆëŠ” ì •ë‹µì„ íŒë³„í•˜ëŠ” í•¨ìˆ˜ signì„ ë‹¤ë¥¸ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•˜ê¸°ë¡œ í•œë‹¤.\n\n![nn-perceptron-2](/images/nn-perceptron-2.jpg)\n\nì—¬ê¸°ì„œ ì´ í•¨ìˆ˜ë¥¼ ìš°ë¦¬ëŠ” **activation function**ì´ë¼ê³  ë¶€ë¥´ê³  ëŒ€í‘œì ìœ¼ë¡œëŠ” ê°™ì€ ì¢…ë¥˜ê°€ ìˆë‹¤.\n\n- **sigmoid**  \n  ìš°ë¦¬ê°€ ê°€ì¥ ì‰½ê²Œ ìƒê°í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ì´ë‹¤. logistic regressionì—ì„œ ì‚¬ìš©í•´ë³¸ë§Œí¼ ê¸°ìš¸ê¸°ê°’ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.\n- **tanh**  \n  sigmodì™€ êµ‰ì¥íˆ ìœ ì‚¬í•œ í•¨ìˆ˜ì´ë‹¤. ë”°ë¼ì„œ, ë¹„ìŠ·í•œ ìš©ë„ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.\n- **ReLU**  \n  ì¶œë ¥ ì‹œì ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, ê° ê°ì˜ hidden layerì—ì„œ ì´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì™œëƒí•˜ë©´, sigmoid í•¨ìˆ˜ëŠ” ì¶œë ¥ê°’ì˜ í˜•íƒœê°€ [0, 1], tanhëŠ” [-1, 1]ì´ê¸° ë•Œë¬¸ì— ë°˜ë³µí•´ì„œ ì ìš©í•˜ë©´, gradientê°€ ì‚¬ë¼ì§€ëŠ” í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ê¸°ìš¸ê¸°ë¥¼ ìˆëŠ” ê·¸ëŒ€ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì´ëŸ¬í•œ í˜•íƒœë¥¼ ì¶œë ¥ ì´ì „ì—ëŠ” ë§ì´ ì‚¬ìš©í•œë‹¤.\n- **Leaky ReLU**  \n  ReLUê°€ ìŒìˆ˜ê°’ì„ ì™„ì „íˆ ë¬´ì‹œí•˜ëŠ”ë° Leaky ReLUëŠ” ì´ëŸ¬í•œ ë°ì´í„°ê°€ ì¡°ê¸ˆì´ë¼ë„ ì˜ë¯¸ ìˆëŠ” ê²½ìš°ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n- **ELU**  \n  Leaky ReLUì™€ ë¹„ìŠ·í•œ ì´ìœ ì´ë‹¤.\n\n  ![activation-functions](/images/activation-functions.png)\n  \nìë£Œê°€ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤ë©´ [ğŸ”— wikipedia](https://en.wikipedia.org/wiki/Activation_function)ë¥¼ ì°¸ê³ í•˜ì.\n\n---\n\nì ì´ì œ ì‹¤ì œë¡œ ì–´ë–»ê²Œ optimizationì„ ìˆ˜í–‰í• ì§€ë¥¼ ì•Œì•„ë³´ë„ë¡ í•˜ì.\n\në¨¼ì €, LossëŠ” ê°€ì¥ ë§ˆì§€ë§‰ layer(output layer)ì˜ outputê³¼ ì‹¤ì œ ê°’ê³¼ì˜ ì°¨ì´ê°€ ë  ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n(ì•„ë˜ì„œ $\\bold{h}_{L}$ì€ Lë²ˆì§¸ layerì˜ outputì„ ì˜ë¯¸í•œë‹¤.)\n\n$$\n\\begin{align*}\n\\mathcal{L} \u0026= \\sum_{n=1}^{N}{\\ell(y_n, \\bold{h}_L)} \\\\\n\u0026= \\sum_{n=1}^{N}{(y_{n} - \\bold{h}_{L})^2} \\\\\n\u0026= \\sum_{n=1}^{N}{(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))^2} \\\\\n\u0026= \\sum_{n=1}^{N}{(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\sigma(\\bold{w}_{L-1}^{\\top}\\bold{h}_{L-2} + b_{L-2}) + b_{L-1}))^2} \\\\\n\u0026= ...\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê²ƒì€ ìš°ë¦¬ëŠ” ì „ì²´ $\\bold{W}$ë¥¼ í•™ìŠµì‹œì¼œì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ì¶œë ¥ì¸µë§Œ í•™ìŠµí•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ì „ì²´ ëª¨ë“  layerì˜ $\\bold{w}_{i}$ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\n\nê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ìš°ë¦¬ëŠ” ${{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{i}}}$ë¥¼ ëª¨ë‘ êµ¬í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì•„ë§ˆ ê°€ì¥ ìŠµê´€ì ìœ¼ë¡œ í•˜ëŠ” í–‰ìœ„ëŠ” ìˆ«ìê°€ ì‘ì€ ê°’ë¶€í„° í¸ë¯¸ë¶„í•˜ë©´ì„œ ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ê·¸ë ‡ê²Œ í•˜ì§€ë§ê³  ë°˜ëŒ€ ìˆœì„œë¡œ ë¯¸ë¶„ì„ í•˜ë¼ëŠ” ê²ƒì´ **backpropagation**ì˜ main ideaì´ë‹¤.\n\n$$\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L}}} = \\sum_{n=1}^{N}\\{({\\partial \\bold{h}_L \\over \\partial \\bold{w}_{L}} )\\times\\red{-2(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))}\\}\n$$\n\n$$\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-1}}} = \\sum_{n=1}^{N}\\{({\\partial \\bold{h}_{L} \\over \\partial \\bold{w}_{L-1} })\\times\\red{-2(y_{n} - \\sigma(\\bold{w}_{L}^{\\top}\\bold{h}_{L-1} + b_{L-1}))}\\}\n$$\n\nì¦‰, ë‹¤ìŒê³¼ ê°™ì€ chain ruleì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L}}} \u0026= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L}}} \\\\\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-1}}} \u0026= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n\u0026= \\red{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{h}_{L-1}}} {{\\partial\\bold{h}_{L-1}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n\u0026= \\blue{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L-1}}} } {{\\partial\\bold{h}_{L-1}}\\over{\\partial \\bold{w}_{L-1}}} \\\\\n{{\\partial\\mathcal{L}}\\over{\\partial \\bold{w}_{L-2}}} \u0026= \\blue{ {{\\partial\\mathcal{L}}\\over{\\partial \\bold{h}_{L-1}}} } {{\\partial\\bold{h}_{L}}\\over{\\partial \\bold{w}_{L-2}}} \\\\\n\\end{align*}\n$$\n\nìš°ë¦¬ëŠ” ë¹¨ê°„ìƒ‰ê³¼ íŒŒë€ìƒ‰ ë¶€ë¶„ì˜ ì—°ì‚°ì„ ì¬í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ë˜í•œ, ${{\\partial\\bold{h}_{l}}\\over{\\partial \\bold{w}_{l}}}$ì€ êµ‰ì¥íˆ ì‰¬ìš´ ì—°ì‚°ì´ê¸°ì— ìš°ë¦¬ê°€ ì‹ ê²½ ì¨ì„œ ê³„ì‚°í•´ì•¼ í•  ê°’ì€ ë§¤ë‹¨ê³„ë¥¼ ì—°ê²°í•´ì¤„ $ {{\\partial\\bold{h}_{l}}\\over{\\partial \\bold{h}_{l-1}}}$ì´ë‹¤.\n\n![ml-backpropagation](/images/ml-backpropagation.jpg)\n\n## Loss Function\n\nìš°ì„  KL-Divergence, Entropy, Cross Entropyì— ëŒ€í•œ ì•½ê°„ì˜ ì´í•´ê°€ í•„ìš”í•˜ë‹ˆ ì´ì „ Posting([ğŸ”— Base Knowledge](posts/ml-base-knowledge))ì„ ì‚´í´ë³´ê³  ì˜¤ì.\n\nìœ„ì—ì„œëŠ” ìì—°ìŠ¤ëŸ½ê²Œ Lossë¥¼ ê³„ì‚°í•  ë•Œ, Squared Errorë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ ê²½ìš°ì— ë”°ë¼ì„œëŠ” ë‹¤ì–‘í•œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. multiclass classificationì—ì„œëŠ” **Cross Entropy Loss**ë¥¼ ì‚¬ìš©í•œë‹¤.\n\nìš°ì„  Cross Entropy LossëŠ” ëŒ€ê²Œ L2 Loss(Squared Error)ì™€ ê°™ì´ ë¹„êµë˜ì–´ì§„ë‹¤. ìš°ì„  ìš°ë¦¬ê°€ ì´ì „ [ğŸ”— Parametric Estimation](posts/ml-parametric-estimation)ì—ì„œ MLEë¥¼ ë‹¤ë£° ë•Œ, KL-Divergenceë¥¼ í†µí•´ì„œ MLEê°€ ìµœì  parameterë¥¼ ì°¾ì„ ê²ƒì´ë¼ëŠ” ê±¸ ì¦ëª…í•œ ì ì´ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ [ğŸ”— Logistic Regression](/posts/ml-logistic-regression)ì—ì„œ Squared Errorë¥¼ í†µí•´ì„œ Lossë¥¼ êµ¬í–ˆë˜ ê³µì‹ì„ í™•ì¸í•´ë³´ì.(Gradient Asecent Part)\n\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê³µì‹ì„ ë´¤ì—ˆë‹¤.\n\n$$\n\\argmax_{\\bold{w}} \\sum_{n=1}^{N}y_{n}\\log{\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}) + (1-y_{n})\\log{(1-\\sigma(\\bold{w}^{\\top}\\bold{x}_{n}))} }\n$$\n\nì´ ê³µì‹ì„ Cross Entropyë¥¼ í†µí•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\nH_{q}(p) \u0026= - \\sum_{x \\in \\Omega}q(x)\\log_{2}p(x) \\\\\n\u0026= \\sum_{n=1}^{N}{[-y_{n}\\log\\hat{y}_{n} - (1- y_{n})\\log(1-\\hat{y}_{n})]}\n\\end{align*}\n$$\n\nì¦‰, ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì–»ì„ ìˆ˜ ìˆëŠ” insightëŠ” Cross EntropyëŠ” sigmoidë¥¼ ì·¨í•œ binary classificationì—ì„œ Squared Errorì™€ ê°™ê³ , ì´ëŸ¬í•œ Cross Entropyë¥¼ Squared Errorê°€ í•  ìˆ˜ ì—†ëŠ” Multiclassì—ëŠ” ì ìš©í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ëŠ” ì ì´ë‹¤. ì™œëƒí•˜ë©´, multiclass classificationì— ì‚¬ìš©ë˜ëŠ” Softmax Functionì„ ì´ìš©í•´ì„œ Sigmoid functionì„ ìœ ë„í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì ì‹œ ê¹Œë¨¹ì—ˆì„ê¹Œë´ Softmax í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ ì ëŠ”ë‹¤.\n\n$$\n\\hat{y}_{k} = {{\\exp(\\bold{w}_{k}^{\\top}\\bold{x})}\\over{\\sum_{i=1}^{K}{\\exp(\\bold{w}_{i}^{\\top}\\bold{x})}}}\n$$\n\në”°ë¼ì„œ, Cross Entropy Lossë¥¼ ëŒ€ì…í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ Lossë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = \\sum_{n=1}^{N}\\sum_{k=1}^{K}[-y_{k,n}\\log\\hat{y}_{k,n}],\\quad y_{k,n} = p(x_{n} \\in C_{k}| x_{n})\n$$\n\nì—¬ê¸°ì„œ $y_{k,n}$ì€ one-hot encodingëœ ë°ì´í„°ë¡œ, ì •ë‹µì¸ classë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ë˜ì–´ ìˆë‹¤. ë”°ë¼ì„œ, multiclass classificationì—ì„œëŠ” ìœ„ì™€ ê°™ì€ Lossë¥¼ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤.\n\nì´ ë‘ê°€ì§€ ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ê°€ì§€ Loss Functionì´ ì´ë¯¸ ì¡´ì¬í•œë‹¤. ì˜ˆì „ì— ì ê¹ ì„¤ëª…í–ˆë˜ L1 Lossë¶€í„° ì‹œì‘í•´ì„œ NLLLoss, KLDivLoss ë“±ë“± ì¡´ì¬í•˜ë©°, dataì˜ íŠ¹ì„±ê³¼ outputì˜ í˜•íƒœì— ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ìŠ¤ìŠ¤ë¡œ Loss Functionì„ ìƒˆë¡œ ì •ì˜í•  ìˆ˜ë„ ìˆë‹¤.\n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- activation function, wikipedia, \u003chttps://en.wikipedia.org/wiki/Activation_function\u003e\n","slug":"ml-nn","date":"2022-10-20 09:00","title":"[ML] 6. Neural Network","category":"AI","tags":["ML","NeuralNetwork","Perceptron","Backpropagation","CrossEntropyLoss"],"desc":"ìš°ë¦¬ëŠ” Linear Regression, Logistic Regression, SVMì„ ê±°ì¹˜ë©° dataë¡œ ë¶€í„° ìœ ì˜ë¯¸í•œ patternì„ ë°œê²¬í•˜ëŠ” ê³¼ì •ì„ ì•Œì•„ë³´ì•˜ë‹¤. ì´ ê³¼ì •ì€ ìš°ë¦¬ì—ê²Œ ëª…í™•í•œ ì‹ í•˜ë‚˜ë¥¼ ì œì‹œí•˜ì˜€ê³ , ëª¨ë“  ê³¼ì •ì„ ìš°ë¦¬ê°€ ì œì–´í•  ìˆ˜ ìˆê²Œ í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ë¥¼ ìš°ë¦¬ê°€ ëª¨ë‘ ëª…í™•í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ê²ƒì¸ì§€ëŠ” ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ê°€ ì´í•´í•˜ì§€ëŠ” ëª»í•˜ì§€ë§Œ, ì•Œì•„ì„œ ìµœì ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê²Œ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ê¹Œ? ì´ëŸ° ë§ˆë²•ê°™ì€ ì¼ì— ëŒ€í•œ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ” ê²ƒì´ Neural Networkì´ë‹¤.  ê²Œ ì•Œì§€ ëª»í•˜ì§€ë§Œ inputì´ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ ì²˜ë¦¬í•´ì„œ outputì„ ì „ë‹¬í•˜ëŠ” ì‹œìŠ¤í…œì„ ìš°ë¦¬ì˜ ì‹ ì²´ì—ì„œ ì°¾ê²Œ ëœë‹¤. ë°”ë¡œ ìš°ë¦¬ ëª¸ì„ ì´ë£¨ëŠ” ì‹ ê²½ë§ì´ë‹¤. ì˜ˆì‹œë¡œ ìš°ë¦¬ëŠ” ëˆˆì„ í†µí•´ ë¹›ì´ë¼ëŠ” inputì„ ë°›ìœ¼ë©´, ìš°ë¦¬ ëˆˆê³¼ ë‡Œì—ì„œ ë¬´ìŠ¨ ì¼ì´ ë°œìƒí•˜ëŠ”ì§€ëŠ” ëª¨ë¥´ì§€ë§Œ ê²°ê³¼ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë¬¼ì²´ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. ì´ ê³¼ì •ì„ ì¶”ì¸¡ì˜ ê³¼ì •ì— ë„ì…í•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ?","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"CrossEntropyLoss"}},"__N_SSG":true},"page":"/tags/[subject]","query":{"subject":"CrossEntropyLoss"},"buildId":"AJoIMuIX0_fPF45qowUMP","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>