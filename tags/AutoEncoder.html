<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta property="og:type" content="blog"/><meta property="og:site_name" content="JustLog"/><meta property="og:image" content="https://euidong.github.io/logo192.png"/><title>#AutoEncoder | JustLog</title><meta name="description" content="#AutoEncoder ê´€ë ¨ Posting"/><meta property="og:description" content="#AutoEncoder ê´€ë ¨ Posting"/><meta property="og:title" content="#AutoEncoder | JustLog"/><link rel="canonical" href="https://euidong.github.io/tags/AutoEncoder"/><meta property="og:url" content="https://euidong.github.io/tags/AutoEncoder"/><meta name="next-head-count" content="11"/><link rel="icon" href="https://euidong.github.io/favicon.png"/><link rel="apple-touch-icon" href="https://euidong.github.io/logo192.png"/><link rel="preload" href="/_next/static/css/d4ec5c8b3df09443.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d4ec5c8b3df09443.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6dc16d084a5153e5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6dc16d084a5153e5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" id="Adsense-id" data-ad-client="ca-pub-7452732177557701" async="" defer="" data-nscript="beforeInteractive"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-81da43a8dcd978d9.js" defer=""></script><script src="/_next/static/chunks/main-7b6c38cbad60dfcf.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8d9b43c3d8042477.js" defer=""></script><script src="/_next/static/chunks/675-ae8e8a351ce30ae2.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Bsubject%5D-0fc8f67b45fbbd0f.js" defer=""></script><script src="/_next/static/NW1eS0cWXd1yc-tCvTKHm/_buildManifest.js" defer=""></script><script src="/_next/static/NW1eS0cWXd1yc-tCvTKHm/_ssgManifest.js" defer=""></script><script src="/_next/static/NW1eS0cWXd1yc-tCvTKHm/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_wrapper__dKJSz root"><header class="Layout_header__XosLl" style="position:static"><div><button tabindex="1" class="SideBarToggler_search_bar_toggler__CEuUg"><svg stroke="currentColor" fill="none" stroke-width="0" viewBox="0 0 24 24" height="35px" width="35px" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></button><nav class="SideBar_side_bar__wrapper--close__8Nwnr"><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/">Home</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/tags">Tags</a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Algorithm">Algorithm<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Computer%20Architecture">Computer Architecture<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->7<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Tech">Tech<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->17<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Web">Web<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->4<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Paper">Paper<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->2<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Network">Network<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->11<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/AI">AI<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->20<!-- -->)<!-- --></span></a><a class="SideBar_side_bar__li__crDBH" tabindex="-1" href="/categories/Memoir">Memoir<!-- --><span class="SideBar_side_bar__li__cnt__9QV_z">(<!-- -->1<!-- -->)<!-- --></span></a></nav></div><a class="Logo_logo___yD0t" tabindex="1" href="/"></a><div><button class="SearchBarToggler_search_bar_toggler__3dHbA" tabindex="2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="25px" width="25px" xmlns="http://www.w3.org/2000/svg"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg></button></div></header><section><div class="RowCard_row_card__list__background___xFj5"><h1 class="RowCard_row_card__list__title__t4a2h"> Auto Encoder</h1><label class="RowCard_row_card__list__select__wrapper__TZ4_9"><select class="RowCard_row_card__list__select__dxkxA"><option class="RowCard_row_card__list__select__option__GRKZU">ìµœì‹ ìˆœ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">AtoZ<!-- --></option><option class="RowCard_row_card__list__select__option__GRKZU">ZtoA<!-- --></option></select></label><ul class="RowCard_row_card__list__wrapper__5Gtgi"><div class="RowCard_row_card__wrapper__kohuv"><a class="RowCard_row_card__thumbnail__wrapper__bedY4" href="/posts/ml-dimensionality-reduction"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:200px;height:200px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="[ML] 11. Dimensionality Reduction" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="RowCard_row_card__thumbnail__Dh_84" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="[ML] 11. Dimensionality Reduction" srcSet="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=256 1x, https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640 2x" src="https://euidong.github.io/images/ml-thumbnail.jpg?imwidth=640" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" class="RowCard_row_card__thumbnail__Dh_84" loading="lazy"/></noscript></span></a><div class="RowCard_row_card__tray__trcA5"><a class="RowCard_row_card__tray__title__lVniM" tabindex="-1" href="/posts/ml-dimensionality-reduction">[ML] 11. Dimensionality Reduction</a><div class="RowCard_row_card__tray__date__3cY_j">2022ë…„ 12ì›” 4ì¼ 14ì‹œ 19ë¶„</div><ul class="RowCard_row_card__tray__tag__qXmOl"><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/ML"># <!-- -->ML<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/PCA"># <!-- -->PCA<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/KernelPCA"># <!-- -->KernelPCA<!-- --></a><a class="RowCard_row_card__tray__tag__li__7_3Zt" tabindex="-1" href="/tags/AutoEncoder"># <!-- -->AutoEncoder<!-- --></a></ul></div></div></ul></div></section><footer class="Layout_footer__EL5v8"><div class="Layout_footer__copyright__r5baC"><span>Copyright Â© euidong</span><br/><span>ëª¨ë“  ì»¨í…ì¸ ì— ëŒ€í•œ ì €ì‘ê¶Œì€ ì‘ì„±ìì—ê²Œ ì¡´ì¬í•©ë‹ˆë‹¤. <!-- --><br/>ë¶ˆë²• ë³µì œë¥¼ í†µí•œ ìƒì—…ì  ì‚¬ìš©ì„ ì ˆëŒ€ì ìœ¼ë¡œ ê¸ˆì§€í•©ë‹ˆë‹¤. <!-- --><br/>ë‹¨, ë¹„ìƒì—…ì  ì´ìš©ì˜ ê²½ìš° ì¶œì²˜ ë° ë§í¬ë¥¼ ì ìš©í•œë‹¤ë©´ ììœ ë¡­ê²Œ ì‚¬ìš©ê°€ëŠ¥ í•©ë‹ˆë‹¤.<!-- --></span><span>Also I use photos by<!-- --> <!-- --><a href="https://unsplash.com/@lorenzoherrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Lorenzo Herrera</a> <!-- -->on<!-- --> <!-- --><a href="https://unsplash.com/s/photos/tech?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" tabindex="-1">Unsplash</a></span></div><div class="Layout_footer__contents__YZWSm"><a class="Layout_footer__contents__link__K_TKH" href="https://github.com/euidong" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>github</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://euidong.github.io/portfolio" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M6 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm-5 6s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H1zM11 3.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 0 1h-4a.5.5 0 0 1-.5-.5zm.5 2.5a.5.5 0 0 0 0 1h4a.5.5 0 0 0 0-1h-4zm2 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2zm0 3a.5.5 0 0 0 0 1h2a.5.5 0 0 0 0-1h-2z"></path></svg><span>portfolio</span></a><a class="Layout_footer__contents__link__K_TKH" href="https://chrome.google.com/webstore/detail/bonfire/nkooidijgbppkojdgkoafcoppnohdfka?hl=ko" target="_blank" rel="noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" version="1.1" viewBox="0 0 16 16" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 16c-1.066-2.219-0.498-3.49 0.321-4.688 0.897-1.312 1.129-2.61 1.129-2.61s0.706 0.917 0.423 2.352c1.246-1.387 1.482-3.598 1.293-4.445 2.817 1.969 4.021 6.232 2.399 9.392 8.631-4.883 2.147-12.19 1.018-13.013 0.376 0.823 0.448 2.216-0.313 2.893-1.287-4.879-4.468-5.879-4.468-5.879 0.376 2.516-1.364 5.268-3.042 7.324-0.059-1.003-0.122-1.696-0.649-2.656-0.118 1.823-1.511 3.309-1.889 5.135-0.511 2.473 0.383 4.284 3.777 6.197z"></path></svg><span>chat</span></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"content":"\n## Intro\n\nClusteringê³¼ ê°™ì€ Unsupervised Learningìœ¼ë¡œ Feature Selection ë˜ëŠ” Feature Extraction ë“± ì—¬ëŸ¬ê°€ì§€ ì´ë¦„ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” Dimensionality Reduction ê¸°ë²•ì— ëŒ€í•´ì„œ ë‹¤ë£° ê²ƒì´ë‹¤. íŠ¹ì • dataì—ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” data ì „ì²´ë¥¼ ë³¼ í•„ìš”ê°€ ì—†ë‹¤. ë”°ë¼ì„œ, featureë“¤ì„ ìµœì†Œí•œìœ¼ë¡œ ì¤„ì´ë©´ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ë§Œ ìˆë‹¤ë©´ êµ‰ì¥íˆ íš¨ìœ¨ì ì¸ inferencingê³¼ learningì„ í•  ìˆ˜ ìˆë‹¤.\n\n## Dimensionality Reduction\n\nê°€ì¥ AIê°€ í™œë°œí•˜ê²Œ ì—°êµ¬ë˜ê³  ìˆëŠ” ë¶„ì•¼ëŠ” Visonê³¼ Natural Language ë¶„ì•¼ì¼ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ë“¤ ë¶„ì•¼ì˜ ê³µí†µì ì€ dataì˜ í¬ê¸°ê°€ êµ‰ì¥íˆ í¬ë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìˆ«ìë¥¼ í‘œí˜„í•˜ëŠ” í•˜ë‚˜ì˜ ì´ë¯¸ì§€ê°€ $1920\\times1080$ ì •ë„ì˜ í™”ì§ˆì´ê³ , ê²€ì€ ìƒ‰ ë˜ëŠ” í°ìƒ‰ìœ¼ë¡œ êµ¬ë¶„í•˜ëŠ” bitmap í˜•ì‹ì´ë¼ê³  í•˜ì. ì´ ë•Œ ìš°ë¦¬ê°€ ê° imageì— ì¡´ì¬í•˜ëŠ” ìˆ«ìë¥¼ classificationí•˜ê³  ì‹¶ì€ ê²½ìš°, dataì— ì‚¬ìš©ë˜ëŠ” featureê°€ $1920\\times1080=2,073,600$ì´ë‹¤. ìƒë‹¹íˆ í° ìˆ«ìì´ê³ , í™”ì§ˆì´ ì»¤ì§ˆ ìˆ˜ë¡ ê·¸ë¦¬ê³  ìƒ‰ì´ ìƒê¸¸ ìˆ˜ë¡ ì´ ê°’ì€ ì»¤ì§ˆ ê²ƒì´ë‹¤. Natural Language ë¶„ì•¼ì—ì„œë„ ë§ˆì°¬ê°€ì§€ì´ë‹¤. ì˜ì–´ ë‹¨ì–´ì˜ ì¢…ë¥˜ë§Œ ë”°ì ¸ë„ 170,000ê°œê°€ ë„˜ëŠ”ë‹¤. ì¦‰, ì´ë“¤ ê° ê°ì„ ë‹¨ìˆœ one-hot encodingìœ¼ë¡œ í‘œí˜„í•œë‹¤ë©´, dataì— ì‚¬ìš©ë˜ëŠ” featureì˜ ìˆ˜ëŠ” 170,000ê°œê°€ ë„˜ëŠ”ë‹¤. ì´ë ‡ê²Œ featureì˜ ìˆ˜ê°€ ë§ì•„ì§€ë©´, ìš°ë¦¬ëŠ” dataë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê¸°ê°€ ë§¤ìš° ì–´ë ¤ì›Œì§„ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” featureì˜ ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ë°˜ë“œì‹œ í•„ìš”í•˜ë‹¤.\n\nì´ë¥¼ ìœ„í•´ì„œ, ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ insightê°€ í•„ìš”í•˜ë‹¤. dataì—ì„œ í•„ìš”í•œ ë¶€ë¶„ì´ ëˆˆìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒì´ ì•„ë‹ ìˆ˜ë„ ìˆë‹¤ëŠ” ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìš°ë¦¬ê°€ $10\\times10$ í¬ê¸°ì˜ bitmapì— ìˆ«ìê°€ ì“°ì—¬ìˆë‹¤ê³  í•˜ì.\n\n![ml-observed-dim](/images/ml-observed-dim.png)\n\nì´ë•Œ ìš°ë¦¬ê°€ ê°–ëŠ” ê²½ìš°ì˜ ìˆ˜ëŠ” $2^{100}$ê°œì¼ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ìˆ«ì í˜•íƒœë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” dataê°€ ì‚¬ì‹¤ì€ ë” ë§ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ìš°ë¦¬ëŠ” ì´ ê²½ìš°ì˜ ìˆ˜ë„ ê³ ë ¤í•˜ëŠ” dimensionì—ì„œ inferencingê³¼ learningì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì‹¤ì œë¡œ ê´€ì¸¡í•œ observed dimensionalityë³´ë‹¤ëŠ” ë” í›Œë¥­í•œ true dimensionalityë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. ì´ë¥¼ ìœ„í•´ì„œ, ë‹¤ì–‘í•œ Dimensionality Reduction ê´€ë ¨ ë°©ë²•ë“¤ì´ ì¡´ì¬í•œë‹¤. ê·¸ ì¤‘ì—ì„œ PCAê°€ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆëŠ” ë°©ë²•ì´ë‹¤.\n\n## Principal Component Analysis\n\nPrincipal Component Analysis(PCA)ëŠ” í•µì‹¬ì´ ë˜ëŠ” principal component(basis, ê¸°ì €, ì°¨ì› ì¶•)ë¥¼ ì¬ì •ì˜í•˜ì—¬ ì°¨ì›ì„ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì–´ë–¤ basisê°€ ì¢‹ì€ basisì¸ì§€ë¥¼ ì•Œ ìˆ˜ ìˆì„ê¹Œ? ì•„ë§ˆë„ ì¢‹ì€ basisëŠ” ì´ì „ basisì—ì„œ ê°€ì§€ê³  ìˆë˜ ì •ë³´ë“¤ì„ ìµœëŒ€í•œ ë³´ì¡´í•  ìˆ˜ ìˆë‹¤ë©´ ì¢‹ë‹¤ê³  í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì—¬ê¸°ì„œ ì–´ë–»ê²Œ informationì„ ë§ì´ ë³´ê´€í•  ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ í†µì°°ì´ í•„ìš”í•˜ë‹¤.\n\n![ml-pca-1](/images/ml-pca-1.png)\n\nìœ„ì˜ ê·¸ë¦¼ì„ ë´¤ì„ ë•Œ, ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ ì¤‘ ì–´ë–¤ basisê°€ ë” ì¢‹ì€ basisì¼ì§€ë¥¼ ë³´ì. ì°¨ì›ì„ ì˜®ê¸´ë‹¤ëŠ” ê²ƒì€ ê¸°ì¡´ basisì—ì„œì˜ dataë¥¼ ìƒˆë¡œìš´ basisë¡œ projectioní•˜ëŠ” ê²ƒì´ë‹¤. ë‹¤ì‹œ ë§í•´, í•´ë‹¹ ì¶•ì— ì§ê°ìœ¼ë¡œ dataë¥¼ ë‚´ë ¤ë³´ë‚´ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê²Œ í–ˆì„ ë•Œ, ì™¼ìª½ ê·¸ë¦¼ì´ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ë³´ë‹¤ ë” ë„“ê²Œ dataê°€ í¼ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, ì°¨ì› ì´ë™ ì‹œì— ì¶©ëŒë¡œ ì¸í•´ ì‚¬ë¼ì§€ëŠ” dataì˜ ìˆ˜ê°€ ë” ì ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì´ì— ë”°ë¼ ë” ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì€ dataë¥¼ ìµœëŒ€í•œ ë„“ê²Œ í¼íŠ¸ë¦´ ìˆ˜ ìˆëŠ” basisë¥¼ ê°€ì§€ëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. \u003cmark\u003e**ë”°ë¼ì„œ, PCAëŠ” ê¸°ì¡´ basisë³´ë‹¤ ì ì€ ìˆ˜ì˜ ìƒˆë¡œìš´ basisë¡œ ì˜®ê¸°ëŠ” ê³¼ì •ì—ì„œ ê¸°ì¡´ dimensionì—ì„œì˜ ì •ë³´ë¥¼ ìµœëŒ€í•œ í¬í•¨í•˜ê¸° ìœ„í•´ì„œ varianceê°€ ìµœëŒ€ê°€ ë˜ëŠ” basisë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.**\u003c/mark\u003e ê·¸ëŸ¼ ì´ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•´ë³¼ ê²ƒì´ë‹¤. ìš°ì„ ì€ ì´í•´ë¥¼ ìœ„í•´ì„œ ìƒˆë¡œìš´ basisì˜ ìˆ˜ë¥¼ 1ì´ë¼ê³  ê°€ì •í•˜ê³  ìš°ë¦¬ëŠ” sample meanê³¼ varianceë¥¼ ì´ìš©í•´ì„œ ì´ë¥¼ ì¶”ë¡ í•  ê²ƒì´ë‹¤. (í•´ë‹¹ basisê°€ $u_{1}$ì´ë‹¤.)\n\n$$\n\\begin{align*}\nVar[u_{1}^{\\top}x] \u0026= E[(u_{1}^{\\top}x - E[u_{1}^{\\top}x])^{2}] \\\\\n\u0026= E[(u_{1}^{\\top}x - u_{1}^{\\top}\\bar{x})^{2}] \\quad (\\bar{x} = E[x] = \\frac{1}{N}\\sum_{n\\in[N]}x_{n}) \\\\\n\u0026=\\frac{1}{N}\\sum_{n\\in[N]}(u_{1}^{\\top}x_{n} - u_{1}^{\\top}\\bar{x})^{2} \\\\\n\u0026=\\frac{1}{N}\\sum_{n\\in[N]}(u_{1}^{\\top}x_{n} - u_{1}^{\\top}\\bar{x})^{\\top}(u_{1}^{\\top}x_{n} - u_{1}^{\\top}\\bar{x}) \\\\\n\u0026=\\frac{1}{N}\\sum_{n\\in[N]}(u_{1}^{\\top}(x_{n}-\\bar{x})(x_{n} -\\bar{x})^{\\top}u_{1}) \\\\\n\u0026=u_{1}^{\\top} \\times \\{\\frac{1}{N}\\sum_{n\\in[N]}(x_{n}-\\bar{x})(x_{n} -\\bar{x})^{\\top}\\} \\times u_{1} \\\\\n\u0026=u_{1}^{\\top} S u_{1} \\quad (S = \\frac{1}{N}\\sum_{n\\in[N]}(x_{n}-\\bar{x})(x_{n} -\\bar{x})^{\\top})\n\\end{align*}\n$$\n\nê²°êµ­ ìš°ë¦¬ê°€ ì–»ê³ ì í•˜ëŠ” ë‹¤ë¥¸ basisì—ì„œì˜ varianceë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” ê¸°ì¡´ ì°¨ì›ì—ì„œì˜ ëª¨ë“  dataë“¤ì˜ covariance($S_{ij} = Cov[x_{i}, x_{j}]$)ë¥¼ êµ¬í•´ì•¼í•œë‹¤.\n\nì ì´ì œ ì´ë¥¼ maximizationí•˜ëŠ” ë‹µì„ ì°¾ì•„ë³¼ ê²ƒì´ë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œ, ìš°ì„  basisì˜ í¬ê¸° ì—­ì‹œ varianceì˜ ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ unit vectorë¡œ ì œí•œí•´ì•¼ í•œë‹¤. ì´ë¥¼ ì¢…í•©í•˜ë©´ ê²°ë¡ ì ìœ¼ë¡œ dimensionì„ 1ë¡œ ë°”ê¾¸ëŠ” PCAëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\text{maximize }\u0026\\quad u_{1}^{\\top} S u_{1} \\\\\n\\text{subject to }\u0026\\quad u_{1}^{\\top}u_{1} = 1\n\\end{align*}\n$$\n\nìœ„ì˜ maximization problemì„ í•´ê²°í•´ë³´ì. ê·¸ëŸ¬ë©´ ë‹¤ìŒê³¼ ê°™ì€ lagrange functionì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\mathcal{L} = u_{1}^{\\top} S u_{1} + \\lambda(1-u_{1}^{\\top}u_{1})\n$$\n\nì´ë¥¼ ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n$$\n\\begin{align*}\n\\frac{\\partial\\mathcal{L}}{\\partial u_{1}} \u0026= 2S u_{1} - 2\\lambda u_{1} = 0 \\\\\nS u_{1} \u0026= \\lambda u_{1}\n\\end{align*}\n$$\n\nì¦‰, ìš°ë¦¬ê°€ ì°¾ê³ ì í•˜ëŠ” basisëŠ” S matrixì˜ eigenvector ì¤‘ í•˜ë‚˜ì˜ eigenvectorì¸ ê²ƒì´ë‹¤. ê°„ë‹¨í•˜ê²Œ eigenvectorì™€ eigenvalueê°€ ë­”ì§€ë¥¼ ì„¤ëª…í•˜ìë©´, ìœ„ì²˜ëŸ¼ ë™ì¼í•œ vectorì— ëŒ€í•´ì„œ, matrixì™€ vectorì˜ ê³±ì´ scalarì™€ vectorê³¼ ë™ì¼í•˜ê²Œ í•˜ëŠ” scalar(eigenvalue)ì™€ vector(eigenvector)ë¥¼ ì˜ë¯¸í•œë‹¤. (í›„ì— ì‹œê°„ì´ ë˜ë©´ í•´ë‹¹ ê°œë…ì„ ë‹¤ë£¨ê² ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” eigenvalueì™€ eigenvectorì— ëŒ€í•œ ê°œë…ì€ ìƒëµí•œë‹¤.)\n\nê·¸ë¦¬ê³  ìš°ë¦¬ê°€ êµ¬í•œ ì‹ì„ maximization ì‹ì— í•œ ë²ˆ ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\text{maximize }\u0026 \\quad u_{1}^{\\top} S u_{1} \\\\\n\\text{maximize }\u0026 \\quad u_{1}^{\\top} \\lambda u_{1} \\\\\n\\text{maximize }\u0026 \\quad \\lambda u_{1}^{\\top} u_{1} \\\\\n\\text{maximize }\u0026 \\quad \\lambda\n\\end{align*}\n$$\n\në”°ë¼ì„œ, eigenvalue ì¤‘ ê°€ì¥ í° ê°’ì„ ê°€ì§ˆ ë•Œì˜ eigenvectorê°€ ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” basisê°€ ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ, ê²°ë¡ ì ìœ¼ë¡œ $u_{1}^{\\top} S u_{1} = \\lambda$ì´ê¸° ë•Œë¬¸ì— $\\lambda$ê°€ varianceê°€ ëœë‹¤ëŠ” ê²ƒë„ í¬ì¸íŠ¸ ì¤‘ í•˜ë‚˜ì´ë‹¤.\n\në˜í•œ ìµœì¢…ì ìœ¼ë¡œ ì´ë¥¼ í™•ì¥í•´ì„œ ì´ì œ Mê°œì˜ basisë¥¼ ì‚¬ìš©í•˜ëŠ” PCAë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\text{maximize }\\quad\u0026 \\sum_{i \\in [M]} u_{i}^{\\top} S u_{i} \\\\\n\\text{subject to }\\quad\u0026 u_{i}^{\\top}u_{j} = \\delta_{ij} \\quad \\forall i,j \\in [M] \\\\\n\u0026(\\delta_{ij} = \\begin{cases} 1 \u0026 i=j \\\\ 0 \u0026 i \\neq j \\end{cases})\n\\end{align*}\n$$\n\në”°ë¼ì„œ, PCAëŠ” dataì˜ covariance matrixì— ëŒ€í•œ eigenvalue decompositionì„ í†µí•´ ì–»ì€ eigenvalue ì¤‘ì— í° ê°’ì„ ê°€ì§€ëŠ” ê²ƒì„ ì´ Mê°œ ë½‘ê³ , ì´ì— ìƒì‘í•˜ëŠ” eigenvectorë“¤ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³ , ì´ì— ë”°ë¼ì„œ ìš°ë¦¬ê°€ ê°€ì§€ëŠ” Mê°œì˜ basisì˜ eigenvalue($\\lambda$)ì˜ í•©ì€ ìš°ë¦¬ê°€ ì˜®ê¸´ ì°¨ì›ì—ì„œ ê°–ê³  ìˆëŠ” ì´ varianceë¥¼ ì˜ë¯¸í•œë‹¤.\n\n### Other Approaches\n\nìœ„ì—ì„œ ìš°ë¦¬ëŠ” PCAë¥¼ varianceë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” basisë¥¼ ì°¾ëŠ” ê²ƒìœ¼ë¡œ ì •ì˜í–ˆë‹¤. í•˜ì§€ë§Œ, ê´€ì ì„ ë°”ê¿”ì„œ ë¬¸ì œë¥¼ projection error, ê¸°ì¡´ dataì™€ projected data ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•  ìˆ˜ ìˆëŠ” basisë¥¼ ì°¾ëŠ” ê²ƒìœ¼ë¡œ ì •ì˜í•  ìˆ˜ë„ ìˆë‹¤. ì´ ë˜í•œ ìƒê°í•˜ê¸°ì— í•©ë¦¬ì ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆëŠ”ë° ì´ë¥¼ ì‹¤ì œë¡œ ìˆ˜í•™ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ ì•Œì•„ë³´ê² ë‹¤.\n\në¨¼ì €, ìš°ë¦¬ê°€ ì´ Dê°œì˜ unit vectorê°€ ìˆê³ , ì´ ì¤‘ì— ì„ì˜ì˜ Mê°œì˜ vectorë¥¼ basisë¡œ í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ reductionì„ í•œë‹¤ê³  í•´ë³´ì. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Nê°œì˜ dataë“¤ ì¤‘ í•˜ë‚˜ì¸ $x_{n}$ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\n  x_{n} \u0026= \\sum_{i=1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\} \\\\\n  \u0026= \\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\}\n\\end{align*}\n$$\n\nê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” ì˜®ê²¨ì§„ data($\\tilde{x}_{n}$)ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n  \\tilde{x}_{n} \u0026= \\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(\\bar{x}^{\\top}u_{i})u_{i}\\}\n\\end{align*}\n$$\n\nì—¬ê¸°ì„œ ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆë‹¤. $\\sum_{i=M+1}^{D}$ ë¶€ë¶„ì´ ì™€ë‹¿ì§€ ì•Šì„ ê²ƒì´ë‹¤. ì™œëƒí•˜ë©´, ìš°ë¦¬ëŠ” projected dataê°€ ì‹¤ì œë¡œëŠ” $\\sum_{i=1}^{M}$ ë¶€ë¶„ë§Œì„ ê°€ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ë’· ë¶€ë¶„ì´ ì¶”ê°€ëœ ì´ìœ ëŠ” í•´ë‹¹ Mì°¨ì› dataë¥¼ Dì°¨ì›ì˜ ê³µê°„ì— í‘œí˜„í•  ë•Œ, ì–´ëŠ ë¶€ë¶„ì„ ì¤‘ì ìœ¼ë¡œ í• ì§€ë¥¼ ì •í•œ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ì• ì„œ ë³´ì•˜ë˜ ì „ì²´ dataì˜ í‰ê· ë§Œí¼ì„ ë‚¨ì€ ëª¨ë“  ë°©í–¥ì— ë”í•´ì¤€ ê²ƒì´ë‹¤. ì´ ê°’ì€ ëª¨ë“  projected dataì— ë™ì¼í•˜ê²Œ ë”í•´ì§€ëŠ” ìƒìˆ˜ê°’ì´ë¼ê³  ë´ë„ ë¬´ë°©í•˜ë‹¤. ì´ë˜ë„ ì´í•´ê°€ ì¡°ê¸ˆ ì–´ë µë‹¤ë©´ ì•„ë˜ ê·¸ë¦¼ì„ í†µí•´ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n\n![ml-pca-3](/images/ml-pca-3.png)\n\nìœ„ì™€ ê°™ì´ 3ê°œì˜ vectorëŠ” 1ì°¨ì›ì—ì„œëŠ” ë™ì¼í•œ vectorì´ë‹¤. í•˜ì§€ë§Œ, projected dataì™€ ì›ë˜ dataê°„ì˜ ì ì ˆí•œ ê±°ë¦¬ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ì¤‘ì•™ì— ìˆëŠ” í˜•íƒœë¡œ basisë¡œ ì˜®ê²¨ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ í•„ìš”í•œ ê²ƒì´ ë’¤ì˜ ìš”ì†Œì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì•„ë˜ì‹ê³¼ ê°™ì´ ë‘ projected dataì™€ ì›ë˜ dataê°„ì˜ ê±°ë¦¬ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n  x_{n} - \\tilde{x}_{n} \u0026= (\\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\}) - (\\sum_{i=1}^{M}\\{(x_{n}^{\\top}u_{i})u_{i}\\} + \\sum_{i = M+1}^{D}\\{(\\bar{x}^{\\top}u_{i})u_{i}\\}) \\\\\n  \u0026= \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i})u_{i}\\} - \\sum_{i = M+1}^{D}\\{(\\bar{x}^{\\top}u_{i})u_{i}\\} \\\\\n  \u0026= \\sum_{i = M+1}^{D}\\{(x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})u_{i}\\}\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ê°€ êµ¬í•˜ê³ ìí•˜ëŠ” ìµœì¢… ëª©ì í•¨ìˆ˜ë¥¼ mean squared errorë¼ê³  í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\mathcal{J} \u0026= \\frac{1}{N}\\sum_{n=1}^{N}||x_{n} - \\tilde{x}_{n}||^2 \\\\\n\u0026= \\frac{1}{N}\\sum_{n=1}^{N}(\\sum_{i = M+1}^{D}x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})^{\\top}(\\sum_{i = M+1}^{D}x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i}) \\\\\n\u0026= \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{i = M+1}^{D}(x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})^{2}u_{i}^{\\top}u_{i} \\\\\n\u0026= \\sum_{i = M+1}^{D}\\frac{1}{N}\\sum_{n=1}^{N}(x_{n}^{\\top}u_{i} - \\bar{x}^{\\top}u_{i})^{2}\\\\\n\u0026= \\sum_{i=M+1}^{D} u_{i}^{\\top} S u_{i}\\quad (\\because \\text{ì• ì„œ ì‚´í´ë³¸ ì²« ë²ˆì§¸ ê´€ì ê³¼ ë™ì¼})\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²°ë¡ ì— ë„ë‹¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\text{minimize }\\quad\u0026 \\sum_{i=M+1}^{D} u_{i}^{\\top} S u_{i} \\\\\n\\text{subject to }\\quad\u0026 u_{i}^{\\top}u_{j} = \\delta_{ij} \\quad \\forall i,j \\in [M] \\\\\n\u0026(\\delta_{ij} = \\begin{cases} 1 \u0026 i=j \\\\ 0 \u0026 i \\neq j \\end{cases})\n\\end{align*}\n$$\n\nê¸°ì¡´ ì‹ê³¼ ë‹¤ë¥¸ ì ì´ë¼ë©´, maximizationì´ minimizationìœ¼ë¡œ ë°”ë€Œì—ˆê³ , ë²”ìœ„ê°€ $[1, M]$ì—ì„œ $[M+1, D]$ë¡œ ë°”ë€Œì—ˆë‹¤ëŠ” ì ì´ë‹¤. ê·¸ë¦¬ê³ , ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í†µì°°ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. eigenvalueë“¤ ì¤‘ì—ì„œ ê°€ì¥ í° ê°’ë¶€í„° Më²ˆì§¸ë¡œ í° ê°’ì„ êµ¬í•˜ëŠ” ê²ƒê³¼ M+1ë¶€í„° ì‹œì‘í•´ì„œ ê°€ì¥ ì‘ì€ eigenvalueë¥¼ ì°¾ëŠ” ê³¼ì •ì€ ë™ì¼í•˜ë¯€ë¡œ ë‘ ì‹ì€ ì‚¬ì‹¤ìƒ ë™ì¼í•˜ë‹¤.\n\n![ml-pca-4](/images/ml-pca-4.png)\n\n### Very High Dimensional Data\n\nì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ê°€ PCAë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œ eigenvectorë¥¼ êµ¬í•˜ëŠ” ë¹„ìš©ì€ O($D^3$)ì´ë‹¤. í•˜ì§€ë§Œ, ì°¨ì›ì´ dataì˜ ìˆ˜ë³´ë‹¤ í° ê²½ìš°ì— ì•½ê°„ì˜ ê¼¼ìˆ˜ë¥¼ ì“¸ ìˆ˜ ìˆë‹¤. ê°„ë‹¨í•˜ê²Œ Së¥¼ (M x M) matrixì—ì„œ (N x N) matrixë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ì´ìš©í•˜ë©´, O($N^3$)ë¡œ ì‹œê°„ ë³µì¡ë„ë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. ì‹ì€ ì•„ë˜ë¥¼ ì°¸ê³  í•˜ì. ì•„ë˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” $X$ëŠ” ê° í–‰ì´ $(x_{n} - \\bar{x})^{\\top}$ì¸ matrixì´ë‹¤.\n\n$$\n\\begin{align*}\n  Su_{i} \u0026= \\lambda_{i}u_{i} \u0026\\\\\n  \\frac{1}{N}X^{\\top}Xu_{i} \u0026= \\lambda_{i}u_{i} \u0026\\leftarrow X^{\\top}X \\in R^{D\\times D} \\\\\n  X \\times \\frac{1}{N}X^{\\top}Xu_{i} \u0026= X \\times \\lambda_{i}u_{i}\u0026 \\\\\n  \\frac{1}{N}XX^{\\top}(Xu_{i}) \u0026= \\lambda_{i}(Xu_{i})\u0026 \\\\\n  \\frac{1}{N}XX^{\\top}(v_{i}) \u0026= \\lambda_{i}(v_{i}) \u0026\\leftarrow XX^{\\top} \\in R^{N\\times N}\\\\\n\\end{align*}\n$$\n\n## Probabilistic PCA\n\nPCAë¥¼ í†µí•´ì„œ dataë¥¼ ë‹¤ë¥¸ ì°¨ì›ìœ¼ë¡œ ì˜®ê¸°ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í–ˆë‹¤. ì‚¬ì‹¤ ì´ê²ƒìœ¼ë¡œ ëë‚˜ê¸°ëŠ” ì¡°ê¸ˆ ì•„ì‰½ë‹¤. ì™œëƒí•˜ë©´, ìš°ë¦¬ëŠ” í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” dataì— ëŒ€ì‘í•˜ëŠ” ì°¨ì›ìœ¼ë¡œì˜ ì´ë™ì„ ìˆ˜í–‰í•œ ê²ƒì´ë‹¤. ì¦‰, ìš°ë¦¬ê°€ ì‹¤ì œë¡œ inferencing ë‹¨ê³„ì—ì„œ unseen dataë¥¼ ë³´ê²Œ ë˜ì—ˆì„ ë•Œ ì œëŒ€ë¡œ ë™ì‘í• ì§€ëŠ” ë¯¸ì§€ìˆ˜ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œ, continueousí•œ í˜•íƒœë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ, PCAë¥¼ í™•ë¥ ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤. í›„ì— ë” ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.\n\n## Kernel PCA\n\nì—¬íƒœê¹Œì§€ ì•ì—ì„œ ì‚´í´ë´¤ë˜ PCAëŠ” ìƒˆë¡œìš´ basisê°€ ê¸°ì¡´ Dimenalalityì—ì„œ linearí•˜ê²Œ ë§Œë“¤ì—ˆë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” dataê°€ ì‚¬ì‹¤ì€ ê·¸ë ‡ì§€ ì•Šì€ í˜•íƒœì¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ìš°ë¦¬ê°€ ê´€ì¸¡í•œ íŠ¹ì§•ë“¤ì— ì˜í•œ ì¢Œí‘œ ê³µê°„ì—ì„œ ì„ í˜• í˜•íƒœë¡œ dataê°€ ì¡´ì¬í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ë” ë³µì¡í•œ ê³¡ì„  í˜•íƒœë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ê°€ì¥ ì¼ë°˜ì ì¸ dataê°€ ì›í˜•ìœ¼ë¡œ ì´ë£¨ì–´ì§„ data ë¶„í¬ì´ë‹¤. ì›í˜•ìœ¼ë¡œ ë°˜ì§€ë¦„ì´ 0.5ì´í•˜ì¸ dataì™€ ë°˜ì§€ë¦„ì´ 1ì¸ dataë¥¼ êµ¬ë¶„í•˜ê³  ì‹¶ë‹¤ê³  í•˜ì. ì¼ë°˜ì ì¸ x, y ê³µê°„ì—ì„œ linear basisë¥¼ ì´ìš©í•´ì„œ ì´ë¥¼ ì ì ˆí•˜ê²Œ ë‚˜ëˆ„ë ¤ë©´, dimensionality reductionì—ì„œëŠ” ì •ë³´ë¥¼ ëª¨ë‘ ê±°ì˜ ê· ì¼í•˜ê²Œ ìƒì„ ìˆ˜ ë°–ì— ì—†ë‹¤.\n\n![ml-kernel-pca-1](/images/ml-kernel-pca-1.png)\n\ní•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¤‘ì‹¬ê³¼ ë–¨ì–´ì§„ ì •ë„($x^{2} + y^{2}$)ì™€ ê°™ì€ ê¸°ì¡´ featureë¥¼ non-linearí•˜ê²Œ ì¡°í•©í•˜ì—¬ í™œìš©í•˜ë©´ ë” íš¨ê³¼ì ì¸ êµ¬ë¶„ì´ ê°€ëŠ¥í•  ê²ƒì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. (ì•„ë˜ ê·¸ë¦¼ì€ ë‹¤ë¥¸ ë°©ì‹ì„ ì‚¬ìš©í•œ ê²ƒì´ì§€ë§Œ, ì›ì˜ ì¤‘ì‹¬ê³¼ ë¹„ìŠ·í•˜ë‹¤.)\n\n![ml-kernel-pca-2](/images/ml-kernel-pca-2.png)\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê¸°ì¡´ ì°¨ì›ì—ì„œ ì„ í˜•ìœ¼ë¡œ basisë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë¹„ì„ í˜•ìœ¼ë¡œ basisë¥¼ ì°¾ê³  ì‹¶ì€ ê²ƒì´ë‹¤. ê¸°ì¡´ ì°¨ì›ê³¼ ë¹„êµí–ˆì„ ë•Œ ë¹„ì„ í˜•ì˜ basisë¥¼ í†µí•´ì„œ ë§Œë“¤ì–´ì§€ëŠ” ì°¨ì›ì„ manifoldë¼ê³  í•˜ë©°, ì•„ë˜ ì™¼ìª½ ìœ„ì™€ ê°™ì€ manifoldë¥¼ ë°œê²¬ë§Œ í•œë‹¤ë©´, Dimensionality reductionì„ ê¸°ì¡´ linear ë°©ì‹ê³¼ ë¹„êµí–ˆì„ ë•Œ ë” íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\n![ml-kernel-pca-3](/images/ml-kernel-pca-3.png)\n\nì ê·¸ë ‡ë‹¤ë©´ ì´ ë˜í•œ ì–´ë–»ê²Œ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆì„ê¹Œ? ì´ì „ì— ì¼ë˜ Maximum Variance ë°©ì‹ì„ ì´ìš©í•  ê²ƒì´ë‹¤. ìš°ì„  ìš°ë¦¬ê°€ data($x$)ë¥¼ non-linear ê³µê°„ìœ¼ë¡œ ì°¨ì› ì´ë™ì‹œí‚¨ ê°’ì„ $\\phi(x)$ë¼ê³  ì •ì˜í•˜ê³ , $\\sum_{i=1}^{N}\\phi(x_{i})=0$ì´ ë˜ëŠ” ìƒí™©ì´ë¼ê³  ê°€ì •ì„ í•´ë³´ì.(ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ êµ‰ì¥íˆ ìˆ˜ì‹ì´ ë³µì¡í•´ì§€ê¸° ë•Œë¬¸ì— ìš°ì„  ì´ë ‡ê²Œ ê°€ì •ì„ í•  ê²ƒì´ë‹¤.)\n\n$$\n\\begin{align*}\nVar[u_{1}^{\\top}\\phi(x)] \u0026= E[(u_{1}^{\\top}\\phi(x) - E[u_{1}^{\\top}\\phi(x)])^{2}] \\\\\n\u0026= E[(u_{1}^{\\top}\\phi(x) - u_{1}^{\\top}\\bar{\\phi}(x))^{2}]\\quad (\\bar{\\phi}(x) = E[\\phi(x)] = \\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})) \\\\\n\u0026= E[(u_{1}^{\\top}\\phi(x))^2] \\\\\n\u0026= \\frac{1}{N}\\sum_{n \\in [N]}(u_{1}^{\\top}\\phi(x_{n}))^{2} \\\\\n\u0026= u_{1}^{\\top}\\frac{1}{N}\\{\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top}\\}u_{1} \\\\\n\u0026= u_{1}^{\\top}\\bar{S}u_{1} \\quad (\\bar{S} = \\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê²°ë¡ ìƒ ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\text{maximize}\\quad\u0026 \\sum_{i \\in [M]}u_{i}^{\\top}\\bar{S}u_{i} \\\\\n\\text{subject to}\\quad\u0026 u_{i}^{\\top}u_{j} = \\delta_{ij}\\quad \\forall i,j \\in [M] \\\\\n\\end{align*}\n$$\n\nì´ë¥¼ í‘¸ëŠ” ê³¼ì •ì€ ì•ì—ì„œ ì‚´í´ë´¤ìœ¼ë‹ˆ ì •ë¦¬ë¥¼ í•˜ìë©´, ê²°êµ­ ë‹¤ìŒê³¼ ê°™ì€ eigenvalue, eigenvectorë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\bar{S}u_{i} = \\lambda_{i}u_{i}\n$$\n\ní•˜ì§€ë§Œ, ì´ë¥¼ í‘¸ëŠ” ê²ƒì€ êµ‰ì¥íˆ ê³¤í˜¹ìŠ¤ëŸ½ë‹¤. ì™œëƒí•˜ë©´, ìš°ë¦¬ëŠ” ëª¨ë“  dataì— ëŒ€í•´ì„œ ì°¨ì› ë³€í™˜ í•¨ìˆ˜ì¸ $\\phi$ë¥¼ ì ìš©í•´ì£¼ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì—°ì‚°ì˜ ë³µì¡ë„ëŠ” ê¸‰ê²©í•˜ê²Œ ì¦ê°€í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ì´ë¥¼ ì°¨ì› ë³€í™˜ í•¨ìˆ˜ $\\phi$ì˜ ì—°ì‚° ê³¼ì •ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤. ì´ê²ƒì´ kernel í•¨ìˆ˜ì´ë‹¤. ì´ëŠ” ì• ì„œ ì‚´í´ë´¤ì—ˆë˜, [ğŸ”— 5. Multiclass Classification](/posts/ml-multiclass-classification-in-svm#Kernel-Method)ì˜ Kernel Methodì™€ ë™ì¼í•˜ë‹¤. ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ìë©´, $\\phi(x_{i})^{\\top}\\phi(x_{j})$ì˜ ê²°ê³¼ì™€ ë™ì¼í•œ $\\kappa(x_{i}, x_{j})$ì˜ ì—°ì‚°ì„ í™œìš©í•´ì„œ ì´ 2ë²ˆì˜ ì°¨ì› ë³€í™˜ê³¼ ê³±ì…ˆ ì—°ì‚°ì„ ë‘ ê°œì˜ ë³€ìˆ˜ë¥¼ ë°›ëŠ” í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•œë‹¤ëŠ” ideaì´ë‹¤. ì´ë¥¼ í†µí•´ì„œ, ì—°ì‚° ë¹„ìš©ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê¸°ì¡´ ì‹ì—ì„œ $\\phi$ë¥¼ ì—†ì• ê³ , kernel ë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ í˜•íƒœë¡œ ë°”ê¿€ ê²ƒì´ë‹¤.\n\nì´ë¥¼ ìœ„í•´ì„œ, ìš°ë¦¬ëŠ” ë¨¼ì € $u_{i}$ë¥¼ ë‹¤ì‹œ í‘œí˜„í•´ë³´ì.\n\n$$\n\\begin{align*}\n\\bar{S}u_{i} \u0026= (\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})u_{i} \\\\\n\u0026= \\frac{1}{N}\\sum_{n \\in [N]}\\{\\phi(x_{n})\\times (\\phi(x_{n})^{\\top}u_{i})\\} \\\\\n\u0026= \\frac{1}{N}\\sum_{n \\in [N]} \u003c\\phi(x_{n}), u_{i}\u003e\\phi(x_{n})\\quad(\u003c\\phi(x_{n}), u_{i}\u003e = \\phi(x_{n})^{\\top}u_{i})  \\\\\n\\\\\n\\lambda_{i}u_{i} \u0026= \\bar{S}u_{i} \\\\\n\\lambda_{i}u_{i} \u0026= \\frac{1}{N}\\sum_{n \\in [N]} \u003c\\phi(x_{n}), u_{i}\u003e\\phi(x_{n}) \\\\\nu_{i} \u0026= \\sum_{n \\in [N]} \\frac{\u003c\\phi(x_{n}), u_{i}\u003e}{N\\lambda_{i}}\\phi(x_{n}) \\\\\n\\therefore u_{i} \u0026= \\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})\n\\end{align*}\n$$\n\ní•´ë‹¹ ê³¼ì •ì„ í†µí•´ì„œ, ìš°ë¦¬ê°€ ì–»ê³ ì í•˜ëŠ” basisëŠ” ì„ì˜ì˜ ìƒìˆ˜ $\\alpha_{in}$ì— ì˜í•´ì„œ ì •ì˜ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì´ì œë¶€í„° ë¬¸ì œëŠ” $\\alpha_{in}$ì„ ëª¨ë“  Nê³¼ Mì— ëŒ€í•´ì„œ ì°¾ê¸°ë§Œ í•˜ë©´ ë˜ëŠ” ê²ƒì´ë‹¤. ì´ì œ ê¸°ì¡´ ì‹ì„ ë‹¤ì‹œ ì •ë¦¬í•´ë³´ë„ë¡ í•˜ì.\n\n$$\n\\begin{align*}\n\\bar{S}u_{i} \u0026= \\lambda_{i}u_{i} \\\\\n(\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})(\\sum_{m \\in [N]} \\alpha_{in}\\phi(x_{m})) \u0026= \\lambda_{i}(\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})) \\\\\n\\phi(x_{\\ell})^{\\top}\\times (\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{n})\\phi(x_{n})^{\\top})(\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{m})) \u0026= \\phi(x_{\\ell})^{\\top}\\times \\lambda_{i}(\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})) \\\\\n\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{\\ell})^{\\top}\\phi(x_{n})\\phi(x_{n})^{\\top}\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{m}) \u0026= \\lambda_{i}\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{\\ell})^{\\top}\\phi(x_{n}) \\\\\n\\frac{1}{N}\\sum_{n \\in [N]}\\phi(x_{\\ell})^{\\top}\\phi(x_{n})\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{n})^{\\top}\\phi(x_{m}) \u0026= \\lambda_{i}\\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{\\ell})^{\\top}\\phi(x_{n}) \\\\\n\\frac{1}{N}\\sum_{n \\in [N]}\\kappa(x_{\\ell}, x_{n})\\sum_{m \\in [N]} \\alpha_{im} \\kappa(x_{n}, x_{m}) \u0026= \\lambda_{i}\\sum_{n \\in [N]} \\alpha_{in}\\kappa(x_{\\ell}, x_{n}) \\\\\n\\sum_{n \\in [N]}\\kappa(x_{\\ell}, x_{n})\\begin{bmatrix} \\kappa(x_{n}, x_{1}) \\\\ \\kappa(x_{n}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{n}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \u0026= N\\lambda_{i} \\begin{bmatrix} \\kappa(x_{\\ell}, x_{1}) \\\\ \\kappa(x_{\\ell}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{\\ell}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \\\\\n\\begin{bmatrix} \\kappa(x_{\\ell}, x_{1}) \\\\ \\kappa(x_{\\ell}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{\\ell}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix} \\kappa(x_{1}, x_{1}) \u0026 \\kappa(x_{1}, x_{2}) \u0026 \\cdots \u0026 \\kappa(x_{1}, x_{N}) \\\\\\kappa(x_{2}, x_{1}) \u0026 \\kappa(x_{2}, x_{2}) \u0026 \\cdots \u0026 \\kappa(x_{2}, x_{N}) \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\kappa(x_{N}, x_{1}) \u0026 \\kappa(x_{N}, x_{2}) \u0026 \\cdots \u0026 \\kappa(x_{N}, x_{N}) \\end{bmatrix}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \u0026= N\\lambda_{i} \\begin{bmatrix} \\kappa(x_{\\ell}, x_{1}) \\\\ \\kappa(x_{\\ell}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{\\ell}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \\\\\n\\begin{bmatrix} \\kappa(x_{1}, x_{1}) \u0026 \\kappa(x_{1}, x_{2}) \u0026 \\cdots \u0026 \\kappa(x_{1}, x_{N}) \\\\\\kappa(x_{2}, x_{1}) \u0026 \\kappa(x_{2}, x_{2}) \u0026 \\cdots \u0026 \\kappa(x_{2}, x_{N}) \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\kappa(x_{N}, x_{1}) \u0026 \\kappa(x_{N}, x_{2}) \u0026 \\cdots \u0026 \\kappa(x_{N}, x_{N}) \\end{bmatrix}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \u0026= N\\lambda_{i} \\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix} \\\\\nK\\alpha_{i} \u0026= N\\lambda_{i} \\alpha_{i} \\\\\n\\end{align*}\n$$\n\nê²°êµ­ ë˜ ë‹¤ë¥¸ eigenvalue problemì„ ë§Œë“¤ ìˆ˜ ìˆê²Œ ëœë‹¤. í•˜ì§€ë§Œ, ì—¬ê¸°ì„œ $\\alpha_{i}$ì˜ í¬ê¸°ëŠ” 1ì´ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ, $\\alpha_{i}$ë¥¼ ì •ê·œí™”í•´ì£¼ì–´ì•¼ í•œë‹¤.\n\n$$\n\\begin{align*}\nu_{i}^{\\top}u_{i} \u0026= \\sum_{n \\in [N]} \\alpha_{in}\\phi(x_{n})\\sum_{m \\in [N]} \\alpha_{im}\\phi(x_{m}) \\\\\n\u0026= \\sum_{n \\in [N]}\\alpha_{in} \\sum_{m \\in [N]}\\alpha_{im}\\phi(x_{n})\\phi(x_{m}) \\\\\n\u0026= \\sum_{n \\in [N]}\\alpha_{in} \\sum_{m \\in [N]}\\alpha_{im}\\kappa(x_{n}, x_{m})\\\\\n\u0026= \\sum_{n \\in [N]}\\alpha_{in} \\begin{bmatrix} \\kappa(x_{n}, x_{1}) \\\\ \\kappa(x_{n}, x_{2}) \\\\ \\vdots \\\\ \\kappa(x_{n}, x_{N}) \\end{bmatrix}^{\\top}\\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix}\\\\\n\u0026= \\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix}^{\\top} \\begin{bmatrix} \\kappa(x_{1}, x_{1}) \u0026 \\kappa(x_{1}, x_{2}) \u0026 \\cdots \u0026 \\kappa(x_{1}, x_{N}) \\\\\\kappa(x_{2}, x_{1}) \u0026 \\kappa(x_{2}, x_{2}) \u0026 \\cdots \u0026 \\kappa(x_{2}, x_{N}) \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\kappa(x_{N}, x_{1}) \u0026 \\kappa(x_{N}, x_{2}) \u0026 \\cdots \u0026 \\kappa(x_{N}, x_{N}) \\end{bmatrix}  \\begin{bmatrix}\\alpha_{i1} \\\\ \\alpha_{i2} \\\\ \\vdots \\\\ \\alpha_{iN} \\\\\\end{bmatrix}\\\\\n\u0026= \\alpha_{i}^{\\top}K\\alpha_{i} \\\\\n\u0026= \\alpha_{i}^{\\top}N\\lambda_{i} \\alpha_{i}\\quad(\\because K\\alpha_{i} = N\\lambda_{i} \\alpha_{i}) \\\\\n\u0026= N\\lambda_{i}\\alpha_{i}^{\\top}\\alpha_{i} = 1 \\\\\n\n\\therefore ||\\alpha_{i}||^{2} \u0026= \\frac{1}{N\\lambda_{i}}\n\\end{align*}\n$$\n\në”°ë¼ì„œ, ê²°ë¡ ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” nonlinear PCAë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” $\\lambda_{i}$ë¥¼ í°ê°’ë¶€í„° ì‹œì‘í•˜ì—¬ Më²ˆì§¸ê¹Œì§€ì—ì„œì˜ $\\alpha_{i}$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.\n\n$$\n\\begin{align*}\nK\\alpha_{i} \u0026= N\\lambda_{i} \\alpha_{i} \\\\\n||\\alpha_{i}||^{2} \u0026= \\frac{1}{N\\lambda_{i}}\n\\end{align*}\n$$\n\nê·¸ë¦¬ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ dataë¥¼ ìš°ë¦¬ê°€ êµ¬í•œ basisë¡œ projectioní•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\u003cu_{i}, \\phi(x)\u003e \u0026= \\sum_{i\\in[N]}\\alpha_{in}\\phi(x)^{\\top}\\phi(x_{n}) \\\\\n\u0026= \\sum_{i\\in[N]}\\alpha_{in}\\kappa(x, x_{n}) \n\\end{align*}\n$$\n\nì¦‰, ì´ ë˜í•œ ì´ì „ì— ê³„ì‚°í–ˆë˜ kernelì„ í™œìš©í•´ì„œ ê·¸ëŒ€ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.\n\nì ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ í•´ì¤˜ì•¼ í•  ê²ƒì€ ìš°ë¦¬ê°€ ë§¨ ì²˜ìŒ ê°€ì •í–ˆë˜ $\\sum_{i\\in[N]}\\phi(x_{i}) = 0$ ë¶€ë¶„ì´ë‹¤. ì‚¬ì‹¤ ëŒ€ê²Œì˜ ì¼ë°˜ì ì¸ ë³€í™˜ì—ì„œëŠ” ì´ë ‡ê²Œ ì´ë™í•˜ëŠ” ê²ƒì´ ë” ë“œë¬¼ ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ê°•ì œë¡œ í‰ê· ì„ 0ìœ¼ë¡œ ë§ì¶°ì£¼ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. í‰ê· ì´ 0ì´ ë˜ëŠ” ì°¨ì› ë³€í™˜ í•¨ìˆ˜ë¥¼ $\\tilde{\\phi}$ë¼ê³  í•˜ê³ , ì´ë•Œì˜ Kernel Matrixë¥¼ $\\tilde{K}$ë¼ê³  í•˜ì. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì´ ì ìš©í•˜ë©´, ê°•ì œë¡œ í‰ê· ì„ 0ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\n\n$$\n\\tilde{\\phi}(x_{n}) = \\phi(x_{n}) - \\frac{1}{N}\\sum_{i\\in[N]}\\phi(x_{i})\n$$\n\në˜í•œ, $\\tilde{\\kappa}, \\tilde{K}$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\n\\begin{align*}\n\\tilde{\\kappa}(x_{n}, x_{m}) \u0026= \\tilde{\\phi}(x_{n})^{\\top}\\tilde{\\phi}(x_{m}) \\\\\n\u0026= (\\phi(x_{n}) - \\frac{1}{N}\\sum_{i\\in[N]}\\phi(x_{i}))^{\\top}(\\phi(x_{m}) - \\frac{1}{N}\\sum_{j\\in[N]}\\phi(x_{j})) \\\\\n\u0026= \\phi(x_{n})^{\\top}\\phi(x_{m}) - \\frac{1}{N}\\sum_{i\\in[N]}\\phi(x_{i})^{\\top}\\phi(x_{m}) - \\frac{1}{N}\\sum_{j\\in[N]}\\phi(x_{n})^{\\top}\\phi(x_{j}) + \\frac{1}{N^{2}}\\sum_{i\\in[N]}\\sum_{j\\in[N]}\\phi(x_{i})^{\\top}\\phi(x_{j}) \\\\\n\u0026= \\kappa(x_{n}, x_{m}) - \\frac{1}{N}\\sum_{i\\in[N]}\\kappa(x_{i}, x_{m}) - \\frac{1}{N}\\sum_{j\\in[N]}\\kappa(x_{j}, x_{n}) + \\frac{1}{N^{2}}\\sum_{i\\in[N]}\\sum_{j\\in[N]}\\kappa(x_{i}, x_{j})\n\\\\\n\u0026\\text{ìœ„ì˜ ì‹ì„ ì¼ë°˜í™”í•˜ì—¬ í–‰ë ¬ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.}\n\\\\\n\\tilde{K} \u0026= K - 1_{N}K - K1_{N} + 1_{N}K1_{N}\\quad(1_{N} = \\frac{1}{N}\\begin{bmatrix}1 \u0026 1 \u0026 \\cdots 1 \\\\ 1 \u0026 1 \u0026 \\cdots \u0026 1 \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ 1 \u0026 1 \u0026 \\cdots \u0026 1\\end{bmatrix})\n\\end{align*}\n$$\n\n## Auto Encoder\n\nìš°ë¦¬ê°€ PCAì—ì„œ ë§ˆì§€ë§‰ì— ì‚´í´ë³¸ Kernel PCAì˜ í•œê³„ëŠ” dataë§ˆë‹¤ kernel í•¨ìˆ˜ë¥¼ ì–´ë–»ê²Œ ì ìš©í•´ì•¼í• ì§€ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ, ì´ë¥¼ ìœ„í•œ ì—¬ëŸ¬ Heuristic ë°©ë²•ë“¤ì´ ì¡´ì¬í•˜ì§€ë§Œ í•­ìƒ ì™„ë²½í•  ìˆ˜ëŠ” ì—†ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ $\\phi$ë¥¼ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³¼ ê²ƒì´ë‹¤.\n\në¨¼ì €, **Encoding**ì´ë€ dataì˜ íŠ¹ì§•(feature)ì„ í™œìš©í•˜ì—¬ ì´ë¥¼ ë³€í˜•í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë˜í•œ, ì´ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ **Encoding**ì„ í†µí•´ ì–»ì€ ê²°ê³¼ë¥¼ ì´ìš©í•´ì„œ ë‹¤ì‹œ ì›ë³¸ dataë¥¼ ë³µêµ¬í•  ìˆ˜  ìˆì–´ì•¼ í•œë‹¤.(ë³µêµ¬í•˜ëŠ” ê³¼ì •ì€ **Decoding**ì´ë¼ê³  í•œë‹¤.) ì´ë¥¼ í†µí•´ì„œ, ìš°ë¦¬ëŠ” ì••ì¶•ì„ í•˜ê¸°ë„ í•˜ê³ , ê´€ì¸¡ dataë¡œë¶€í„° ìˆ¨ê²¨ì§„ featureë¥¼ ì¶”ì¶œí•˜ê¸°ë„ í•œë‹¤. ì—¬ê¸°ì„œëŠ” Dimensionality Reductionì´ Encodingì´ë¼ê³  ë³´ëŠ” ê²ƒì´ë‹¤.\n\nì—¬ê¸°ì„œ **Auto Encoder**ëŠ” **Encoding**ì„ í†µí•´ì„œ Dimensionality Reductionì„ ìˆ˜í–‰í•˜ê³ , ì´ë¥¼ í†µí•´ ì–»ì€ ê²°ê³¼ë¡œ ì›ë˜ dataë¥¼ **Decoding**í•  ìˆ˜ ìˆì„ ë•Œê°€ì§€ ë°˜ë³µí•˜ëŠ” ê²ƒì´ë‹¤. ì´ ëª¨ë“  ê³¼ì •ì˜ ê²°ê³¼ë¡œ ì›ë˜ dataì™€ Decodingëœ data ê°„ì˜ ì°¨ì´ê°€ ì¼ì • ì„ê³„ê°’ ì´í•˜ë¡œ ë–¨ì–´ì¡Œë‹¤ë©´, ìš°ë¦¬ëŠ” ì´ì œ ì´ Encoderë¥¼ $\\phi$ì˜ ëŒ€ìš©ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì´ $\\phi$ë¥¼ ì•ˆë‹¤ë©´, ì´ë¥¼ ê·¼ì‚¬í•˜ëŠ” kernel functionë„ ì°¾ì„ ìˆ˜ ìˆë‹¤.\n\n![ml-auto-encoder-1](/images/ml-auto-encoder-1.png)\n\nì´ëŸ¬í•œ ë°©ì‹ì€ êµ‰ì¥íˆ ë§ì€ ë¶„ì•¼ì—ì„œ ë„“ê²Œ ì‚¬ìš©ë˜ê³  ìˆë‹¤. ì´ë“¤ì„ ê°„ë‹¨í•˜ê²Œ ì‚´í´ë³´ë„ë¡ í•˜ì.\n\n1. \u003cmark\u003e**CNN**\u003c/mark\u003e  \n   ê²°êµ­ CNNì€ ë„ˆë¬´ í° image dataë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œ ì–´ì©” ìˆ˜ ì—†ì´ Dimensionality Reductionì„ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ, ì´ë¥¼ ê°ì†Œì‹œí‚¤ê¸° ìœ„í•´ì„œ Auto Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ true dimensionì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.\n2. \u003cmark\u003e**Semi-Supervised Learning**\u003c/mark\u003e  \n   labelì´ ì¡´ì¬í•˜ëŠ” dataëŠ” êµ‰ì¥íˆ í¬ê·€í•˜ë©°, ì´ labeling ë¹„ìš©ì´ ë§¤ìš° ë§ì´ ë“ ë‹¤. ë”°ë¼ì„œ, labelì´ ì¡´ì¬í•˜ëŠ” dataì™€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” dataë¥¼ ë™ì‹œì— í™œìš©í•˜ëŠ” ë°©ë²•ì´ í•„ìš”í•œ ê²½ìš°ê°€ ë§ë‹¤. ì´ë•Œ auto encodingì„ í™œìš©í•˜ê²Œ ë˜ë©´, ì¼ë‹¨ labeled, unlabeled dataë¥¼ ëª¨ë‘ í™œìš©í•˜ì—¬ ë¨¼ì € true dimensionì„ êµ¬í•œ ë’¤ì— ì—¬ê¸°ì„œ labeled dataë¥¼ í™œìš©í•˜ì—¬ classifierë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ì´ê²ƒì´ unlabed dataë¥¼ í™œìš©í•˜ì—¬ ë” ë†’ì€ ì ì¤‘ë¥ ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‹¤.  \n   ![ml-auto-encoder-2](/images/ml-auto-encoder-2.png)\n\nê·¸ë¦¬ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ Auto Encodingì˜ ì„±ëŠ¥ì„ ì˜¬ë¦¬ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ Maskingê³¼ ê°™ì€ data augmentationì„ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤. ê¸°ì¡´ dataì˜ ì¼ë¶€ë¶„ì„ maskingí•˜ê³ ë„ ì‹¤ì œ ì›ë³¸ dataë¥¼ ë³µêµ¬í•´ë‚¼ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤.\n   \n\n## Reference\n\n- Tumbnail : Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@markuswinkler?utm_source=unsplash\u0026utm_medium=referral\u0026utm_content=creditCopyText)\n- Kernel PCA, \u003chttps://sebastianraschka.com/Articles/2014_kernel_pca.html\u003e\n","slug":"ml-dimensionality-reduction","date":"2022-12-04 14:19","title":"[ML] 11. Dimensionality Reduction","category":"AI","tags":["ML","PCA","KernelPCA","AutoEncoder"],"desc":"Clusteringê³¼ ê°™ì€ Unsupervised Learningìœ¼ë¡œ Feature Selection ë˜ëŠ” Feature Extraction ë“± ì—¬ëŸ¬ê°€ì§€ ì´ë¦„ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” Dimensionality Reduction ê¸°ë²•ì— ëŒ€í•´ì„œ ë‹¤ë£° ê²ƒì´ë‹¤. íŠ¹ì • dataì—ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” data ì „ì²´ë¥¼ ë³¼ í•„ìš”ê°€ ì—†ë‹¤. ë”°ë¼ì„œ, featureë“¤ì„ ìµœì†Œí•œìœ¼ë¡œ ì¤„ì´ë©´ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ë§Œ ìˆë‹¤ë©´ êµ‰ì¥íˆ íš¨ìœ¨ì ì¸ inferencingê³¼ learningì„ í•  ìˆ˜ ìˆë‹¤.","thumbnailSrc":"https://euidong.github.io/images/ml-thumbnail.jpg"}],"params":{"subject":"AutoEncoder"}},"__N_SSG":true},"page":"/tags/[subject]","query":{"subject":"AutoEncoder"},"buildId":"NW1eS0cWXd1yc-tCvTKHm","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>